diff --git a/.gitignore b/.gitignore
index b53411a..b2158d3 100644
--- a/.gitignore
+++ b/.gitignore
@@ -28,7 +28,8 @@ share/python-wheels/
 MANIFEST
 vendor/
 rllab/
-tmp
+tmp/
+log/
 # PyInstaller
 #  Usually these files are written by a python script from a template
 #  before PyInstaller builds the exe, so as to inject date/other infos into it.
diff --git a/configs/algos/slbo_bm_200k.yml b/configs/algos/slbo_bm_200k.yml
index f06ab74..dcf3777 100644
--- a/configs/algos/slbo_bm_200k.yml
+++ b/configs/algos/slbo_bm_200k.yml
@@ -4,7 +4,7 @@ model:
   dev_batch_size: 512
   train_batch_size: 128
 slbo:
-  n_stages: 20
+  n_stages: 100
   n_iters: 20
   n_model_iters: 100
   n_policy_iters: 40
diff --git a/experiments/gym_cheetah_1234/config.yml b/experiments/gym_cheetah_1234/config.yml
new file mode 100644
index 0000000..a6a5ba1
--- /dev/null
+++ b/experiments/gym_cheetah_1234/config.yml
@@ -0,0 +1,75 @@
+OUNoise:
+  sigma: 0.3
+  theta: 0.15
+PPO:
+  clip_range: 0.2
+  ent_coef: 0.005
+  lr: 0.0003
+  n_minibatches: 32
+  n_opt_epochs: 10
+TRPO:
+  cg_damping: 0.1
+  ent_coef: 0.005
+  max_kl: 0.01
+  n_cg_iters: 10
+  n_vf_iters: 5
+  vf_lr: 0.001
+algorithm: OLBO
+ckpt:
+  base: /tmp/mbrl/logs
+  buf_load: null
+  buf_load_index: 0
+  model_load: null
+  n_save_stages: 10
+  policy_load: null
+  warm_up: null
+commit: 89f2d7477d2e77774a2ce85af8e0d0ff7d31654a
+env:
+  id: HalfCheetah
+log_dir: ./experiments/gym_cheetah_1234
+model:
+  G_coef: 0.5
+  dev_batch_size: 512
+  hidden_sizes:
+  - 500
+  - 500
+  loss: L2
+  lr: 0.001
+  multi_step: 2
+  optimizer: Adam
+  train_batch_size: 128
+  validation_freq: 1
+  weight_decay: 1.0e-05
+pc:
+  bonus_scale: 0.05
+  lamb: 0.01
+plan:
+  max_steps: 1000
+  n_envs: 4
+  n_trpo_samples: 4000
+policy:
+  hidden_sizes:
+  - 32
+  - 32
+  init_std: 1.0
+rollout:
+  max_buf_size: 100000
+  n_dev_samples: 2000
+  n_test_samples: 10000
+  n_train_samples: 2000
+  normalizer: policy
+run_id: null
+runner:
+  gamma: 0.99
+  lambda_: 0.95
+  max_steps: 1000
+seed: 1234
+slbo:
+  n_evaluate_iters: 10
+  n_iters: 20
+  n_model_iters: 100
+  n_policy_iters: 40
+  n_stages: 100
+  opt_model: false
+  start: reset
+use_prev: true
diff --git a/experiments/gym_cheetah_1234/diff.patch b/experiments/gym_cheetah_1234/diff.patch
new file mode 100644
index 0000000..3b330dd
--- /dev/null
+++ b/experiments/gym_cheetah_1234/diff.patch
@@ -0,0 +1,101 @@
+diff --git a/.gitignore b/.gitignore
+index b53411a..b2158d3 100644
+--- a/.gitignore
++++ b/.gitignore
+@@ -28,7 +28,8 @@ share/python-wheels/
+ MANIFEST
+ vendor/
+ rllab/
+-tmp
++tmp/
++log/
+ # PyInstaller
+ #  Usually these files are written by a python script from a template
+ #  before PyInstaller builds the exe, so as to inject date/other infos into it.
+diff --git a/configs/algos/slbo_bm_200k.yml b/configs/algos/slbo_bm_200k.yml
+index f06ab74..dcf3777 100644
+--- a/configs/algos/slbo_bm_200k.yml
++++ b/configs/algos/slbo_bm_200k.yml
+@@ -4,7 +4,7 @@ model:
+   dev_batch_size: 512
+   train_batch_size: 128
+ slbo:
+-  n_stages: 20
++  n_stages: 100
+   n_iters: 20
+   n_model_iters: 100
+   n_policy_iters: 40
+diff --git a/main.py b/main.py
+index 6262ea2..3e25d8b 100644
+--- a/main.py
++++ b/main.py
+@@ -66,7 +66,7 @@ def main():
+ 
+     policy = GaussianMLPPolicy(dim_state, dim_action, normalizer=normalizers.state, **FLAGS.policy.as_dict())
+     # batched noises
+-    noise = OUNoise(env.action_space, theta=FLAGS.OUNoise.theta, sigma=FLAGS.OUNoise.sigma, shape=(1, dim_action))
++    #noise = OUNoise(env.action_space, theta=FLAGS.OUNoise.theta, sigma=FLAGS.OUNoise.sigma, shape=(1, dim_action))
+     vfn = MLPVFunction(dim_state, [64, 64], normalizers.state)
+     model = DynamicsModel(dim_state, dim_action, normalizers, FLAGS.model.hidden_sizes)
+     random_net = RandomNet(dim_state, dim_action, normalizers, FLAGS.model.hidden_sizes)
+@@ -121,10 +121,10 @@ def main():
+             dev_set.clear()
+ 
+         # collect data
+-        recent_train_set, ep_infos = runners['collect'].run(noise.make(policy), FLAGS.rollout.n_train_samples)
++        recent_train_set, ep_infos = runners['collect'].run(policy, FLAGS.rollout.n_train_samples)
+         add_multi_step(recent_train_set, train_set)
+         add_multi_step(
+-            runners['dev'].run(noise.make(policy), FLAGS.rollout.n_dev_samples)[0],
++            runners['dev'].run(policy, FLAGS.rollout.n_dev_samples)[0],
+             dev_set,
+         )
+ 
+@@ -149,7 +149,7 @@ def main():
+         virt_env.update_cov(recent_train_set.state,recent_train_set.action)
+ 
+         if T == 50:
+-            max_ent_coef = 0.
++            virt_env.bonus_scale = 0.
+ 
+         for i in range(FLAGS.slbo.n_iters):
+             if i % FLAGS.slbo.n_evaluate_iters == 0 and i != 0:
+diff --git a/run_experiments.sh b/run_experiments.sh
+index d9e0d30..19a7312 100644
+--- a/run_experiments.sh
++++ b/run_experiments.sh
+@@ -4,6 +4,6 @@ for env_name in $1; do
+     echo "=> Running environment ${env_name}"
+     for random_seed in 1234 2314 2345 1235; do
+         python main.py -c configs/algos/slbo_bm_200k.yml configs/env_tingwu/${env_name}.yml \
+-	    -s log_dir=experiments/${env_name}_${random_seed} seed=${random_seed}
++	    -s log_dir=./experiments/${env_name}_${random_seed} seed=${random_seed}
+     done
+ done
+diff --git a/slbo/utils/__pycache__/flags.cpython-36.pyc b/slbo/utils/__pycache__/flags.cpython-36.pyc
+index b452ad4..038cf5b 100644
+Binary files a/slbo/utils/__pycache__/flags.cpython-36.pyc and b/slbo/utils/__pycache__/flags.cpython-36.pyc differ
+diff --git a/slbo/utils/flags.py b/slbo/utils/flags.py
+index 895554b..60e8e11 100644
+--- a/slbo/utils/flags.py
++++ b/slbo/utils/flags.py
+@@ -30,7 +30,7 @@ class FLAGS(BaseFLAGS):
+         start = 'reset'  # possibly 'buffer'
+ 
+     class plan(BaseFLAGS):
+-        max_steps = 500
++        max_steps = 1000
+         n_envs = None
+         n_trpo_samples = 4000
+ 
+@@ -45,8 +45,8 @@ class FLAGS(BaseFLAGS):
+ 
+     class rollout(BaseFLAGS):
+         normalizer = 'policy'
+-        max_buf_size = 200000
+-        n_train_samples = 10000
++        max_buf_size = 100000
++        n_train_samples = 2000
+         n_dev_samples = 0
+         n_test_samples = 10000
+ 
diff --git a/experiments/gym_cheetah_1234/log.json b/experiments/gym_cheetah_1234/log.json
new file mode 100644
index 0000000..14d0f37
--- /dev/null
+++ b/experiments/gym_cheetah_1234/log.json
@@ -0,0 +1 @@
+{"level": "info", "fmt": "log_dir = %s", "args": ["./experiments/gym_cheetah_1234"], "caller": "slbo/utils/flags.py:160", "time": "2020-12-10T22:38:55.958298"}
diff --git a/experiments/gym_cheetah_1234/src/.gitignore b/experiments/gym_cheetah_1234/src/.gitignore
new file mode 100644
index 0000000..b2158d3
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/.gitignore
@@ -0,0 +1,131 @@
+# Byte-compiled / optimized / DLL files
+__pycache__/
+*.py[cod]
+*$py.class
+
+# C extensions
+*.so
+
+# Distribution / packaging
+.Python
+build/
+develop-eggs/
+dist/
+downloads/
+eggs/
+.eggs/
+lib/
+lib64/
+parts/
+sdist/
+var/
+wheels/
+pip-wheel-metadata/
+share/python-wheels/
+*.egg-info/
+.installed.cfg
+*.egg
+MANIFEST
+vendor/
+rllab/
+tmp/
+log/
+# PyInstaller
+#  Usually these files are written by a python script from a template
+#  before PyInstaller builds the exe, so as to inject date/other infos into it.
+*.manifest
+*.spec
+
+# Installer logs
+pip-log.txt
+pip-delete-this-directory.txt
+
+# Unit test / coverage reports
+htmlcov/
+.tox/
+.nox/
+.coverage
+.coverage.*
+.cache
+nosetests.xml
+coverage.xml
+*.cover
+*.py,cover
+.hypothesis/
+.pytest_cache/
+
+# Translations
+*.mo
+*.pot
+
+# Django stuff:
+*.log
+local_settings.py
+db.sqlite3
+db.sqlite3-journal
+
+# Flask stuff:
+instance/
+.webassets-cache
+
+# Scrapy stuff:
+.scrapy
+
+# Sphinx documentation
+docs/_build/
+
+# PyBuilder
+target/
+
+# Jupyter Notebook
+.ipynb_checkpoints
+
+# IPython
+profile_default/
+ipython_config.py
+
+# pyenv
+.python-version
+
+# pipenv
+#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
+#   However, in case of collaboration, if having platform-specific dependencies or dependencies
+#   having no cross-platform support, pipenv may install dependencies that don't work, or not
+#   install all needed dependencies.
+#Pipfile.lock
+
+# PEP 582; used by e.g. github.com/David-OConnor/pyflow
+__pypackages__/
+
+# Celery stuff
+celerybeat-schedule
+celerybeat.pid
+
+# SageMath parsed files
+*.sage.py
+
+# Environments
+.env
+.venv
+venv/
+ENV/
+env.bak/
+venv.bak/
+
+# Spyder project settings
+.spyderproject
+.spyproject
+
+# Rope project settings
+.ropeproject
+
+# mkdocs documentation
+/site
+
+# mypy
+.mypy_cache/
+.dmypy.json
+dmypy.json
+
+# Pyre type checker
+.pyre/
diff --git a/experiments/gym_cheetah_1234/src/CODE_OF_CONDUCT.md b/experiments/gym_cheetah_1234/src/CODE_OF_CONDUCT.md
new file mode 100644
index 0000000..0d31b1f
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/CODE_OF_CONDUCT.md
@@ -0,0 +1,5 @@
+# Code of Conduct
+
+Facebook has adopted a Code of Conduct that we expect project participants to adhere to.
+Please read the [full text](https://code.fb.com/codeofconduct/)
+so that you can understand what actions will and will not be tolerated.
\ No newline at end of file
diff --git a/experiments/gym_cheetah_1234/src/CONTRIBUTING.md b/experiments/gym_cheetah_1234/src/CONTRIBUTING.md
new file mode 100644
index 0000000..07780f7
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/CONTRIBUTING.md
@@ -0,0 +1,31 @@
+# Contributing to SLBO
+We want to make contributing to this project as easy and transparent as
+possible.
+
+## Pull Requests
+We actively welcome your pull requests.
+
+1. Fork the repo and create your branch from `master`.
+2. Ensure the examples still run.
+3. If you haven't already, complete the Contributor License Agreement ("CLA").
+
+## Contributor License Agreement ("CLA")
+In order to accept your pull request, we need you to submit a CLA. You only need
+to do this once to work on any of Facebook's open source projects.
+
+Complete your CLA here: <https://code.facebook.com/cla>
+
+## Issues
+We use GitHub issues to track public bugs. Please ensure your description is
+clear and has sufficient instructions to be able to reproduce the issue.
+
+Facebook has a [bounty program](https://www.facebook.com/whitehat/) for the safe
+disclosure of security bugs. In those cases, please go through the process
+outlined on that page and do not file a public issue.
+
+## Coding Style  
+We try to follow the PEP style guidelines and encourage you to as well.
+
+## License
+By contributing to SparseConvNet, you agree that your contributions will be licensed
+under the LICENSE file in the root directory of this source tree.
\ No newline at end of file
diff --git a/experiments/gym_cheetah_1234/src/LICENSE b/experiments/gym_cheetah_1234/src/LICENSE
new file mode 100644
index 0000000..1fe4148
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/LICENSE
@@ -0,0 +1,407 @@
+Attribution-NonCommercial 4.0 International
+
+=======================================================================
+
+Creative Commons Corporation ("Creative Commons") is not a law firm and
+does not provide legal services or legal advice. Distribution of
+Creative Commons public licenses does not create a lawyer-client or
+other relationship. Creative Commons makes its licenses and related
+information available on an "as-is" basis. Creative Commons gives no
+warranties regarding its licenses, any material licensed under their
+terms and conditions, or any related information. Creative Commons
+disclaims all liability for damages resulting from their use to the
+fullest extent possible.
+
+Using Creative Commons Public Licenses
+
+Creative Commons public licenses provide a standard set of terms and
+conditions that creators and other rights holders may use to share
+original works of authorship and other material subject to copyright
+and certain other rights specified in the public license below. The
+following considerations are for informational purposes only, are not
+exhaustive, and do not form part of our licenses.
+
+     Considerations for licensors: Our public licenses are
+     intended for use by those authorized to give the public
+     permission to use material in ways otherwise restricted by
+     copyright and certain other rights. Our licenses are
+     irrevocable. Licensors should read and understand the terms
+     and conditions of the license they choose before applying it.
+     Licensors should also secure all rights necessary before
+     applying our licenses so that the public can reuse the
+     material as expected. Licensors should clearly mark any
+     material not subject to the license. This includes other CC-
+     licensed material, or material used under an exception or
+     limitation to copyright. More considerations for licensors:
+	wiki.creativecommons.org/Considerations_for_licensors
+
+     Considerations for the public: By using one of our public
+     licenses, a licensor grants the public permission to use the
+     licensed material under specified terms and conditions. If
+     the licensor's permission is not necessary for any reason--for
+     example, because of any applicable exception or limitation to
+     copyright--then that use is not regulated by the license. Our
+     licenses grant only permissions under copyright and certain
+     other rights that a licensor has authority to grant. Use of
+     the licensed material may still be restricted for other
+     reasons, including because others have copyright or other
+     rights in the material. A licensor may make special requests,
+     such as asking that all changes be marked or described.
+     Although not required by our licenses, you are encouraged to
+     respect those requests where reasonable. More_considerations
+     for the public:
+	wiki.creativecommons.org/Considerations_for_licensees
+
+=======================================================================
+
+Creative Commons Attribution-NonCommercial 4.0 International Public
+License
+
+By exercising the Licensed Rights (defined below), You accept and agree
+to be bound by the terms and conditions of this Creative Commons
+Attribution-NonCommercial 4.0 International Public License ("Public
+License"). To the extent this Public License may be interpreted as a
+contract, You are granted the Licensed Rights in consideration of Your
+acceptance of these terms and conditions, and the Licensor grants You
+such rights in consideration of benefits the Licensor receives from
+making the Licensed Material available under these terms and
+conditions.
+
+
+Section 1 -- Definitions.
+
+  a. Adapted Material means material subject to Copyright and Similar
+     Rights that is derived from or based upon the Licensed Material
+     and in which the Licensed Material is translated, altered,
+     arranged, transformed, or otherwise modified in a manner requiring
+     permission under the Copyright and Similar Rights held by the
+     Licensor. For purposes of this Public License, where the Licensed
+     Material is a musical work, performance, or sound recording,
+     Adapted Material is always produced where the Licensed Material is
+     synched in timed relation with a moving image.
+
+  b. Adapter's License means the license You apply to Your Copyright
+     and Similar Rights in Your contributions to Adapted Material in
+     accordance with the terms and conditions of this Public License.
+
+  c. Copyright and Similar Rights means copyright and/or similar rights
+     closely related to copyright including, without limitation,
+     performance, broadcast, sound recording, and Sui Generis Database
+     Rights, without regard to how the rights are labeled or
+     categorized. For purposes of this Public License, the rights
+     specified in Section 2(b)(1)-(2) are not Copyright and Similar
+     Rights.
+  d. Effective Technological Measures means those measures that, in the
+     absence of proper authority, may not be circumvented under laws
+     fulfilling obligations under Article 11 of the WIPO Copyright
+     Treaty adopted on December 20, 1996, and/or similar international
+     agreements.
+
+  e. Exceptions and Limitations means fair use, fair dealing, and/or
+     any other exception or limitation to Copyright and Similar Rights
+     that applies to Your use of the Licensed Material.
+
+  f. Licensed Material means the artistic or literary work, database,
+     or other material to which the Licensor applied this Public
+     License.
+
+  g. Licensed Rights means the rights granted to You subject to the
+     terms and conditions of this Public License, which are limited to
+     all Copyright and Similar Rights that apply to Your use of the
+     Licensed Material and that the Licensor has authority to license.
+
+  h. Licensor means the individual(s) or entity(ies) granting rights
+     under this Public License.
+
+  i. NonCommercial means not primarily intended for or directed towards
+     commercial advantage or monetary compensation. For purposes of
+     this Public License, the exchange of the Licensed Material for
+     other material subject to Copyright and Similar Rights by digital
+     file-sharing or similar means is NonCommercial provided there is
+     no payment of monetary compensation in connection with the
+     exchange.
+
+  j. Share means to provide material to the public by any means or
+     process that requires permission under the Licensed Rights, such
+     as reproduction, public display, public performance, distribution,
+     dissemination, communication, or importation, and to make material
+     available to the public including in ways that members of the
+     public may access the material from a place and at a time
+     individually chosen by them.
+
+  k. Sui Generis Database Rights means rights other than copyright
+     resulting from Directive 96/9/EC of the European Parliament and of
+     the Council of 11 March 1996 on the legal protection of databases,
+     as amended and/or succeeded, as well as other essentially
+     equivalent rights anywhere in the world.
+
+  l. You means the individual or entity exercising the Licensed Rights
+     under this Public License. Your has a corresponding meaning.
+
+
+Section 2 -- Scope.
+
+  a. License grant.
+
+       1. Subject to the terms and conditions of this Public License,
+          the Licensor hereby grants You a worldwide, royalty-free,
+          non-sublicensable, non-exclusive, irrevocable license to
+          exercise the Licensed Rights in the Licensed Material to:
+
+            a. reproduce and Share the Licensed Material, in whole or
+               in part, for NonCommercial purposes only; and
+
+            b. produce, reproduce, and Share Adapted Material for
+               NonCommercial purposes only.
+
+       2. Exceptions and Limitations. For the avoidance of doubt, where
+          Exceptions and Limitations apply to Your use, this Public
+          License does not apply, and You do not need to comply with
+          its terms and conditions.
+
+       3. Term. The term of this Public License is specified in Section
+          6(a).
+
+       4. Media and formats; technical modifications allowed. The
+          Licensor authorizes You to exercise the Licensed Rights in
+          all media and formats whether now known or hereafter created,
+          and to make technical modifications necessary to do so. The
+          Licensor waives and/or agrees not to assert any right or
+          authority to forbid You from making technical modifications
+          necessary to exercise the Licensed Rights, including
+          technical modifications necessary to circumvent Effective
+          Technological Measures. For purposes of this Public License,
+          simply making modifications authorized by this Section 2(a)
+          (4) never produces Adapted Material.
+
+       5. Downstream recipients.
+
+            a. Offer from the Licensor -- Licensed Material. Every
+               recipient of the Licensed Material automatically
+               receives an offer from the Licensor to exercise the
+               Licensed Rights under the terms and conditions of this
+               Public License.
+
+            b. No downstream restrictions. You may not offer or impose
+               any additional or different terms or conditions on, or
+               apply any Effective Technological Measures to, the
+               Licensed Material if doing so restricts exercise of the
+               Licensed Rights by any recipient of the Licensed
+               Material.
+
+       6. No endorsement. Nothing in this Public License constitutes or
+          may be construed as permission to assert or imply that You
+          are, or that Your use of the Licensed Material is, connected
+          with, or sponsored, endorsed, or granted official status by,
+          the Licensor or others designated to receive attribution as
+          provided in Section 3(a)(1)(A)(i).
+
+  b. Other rights.
+
+       1. Moral rights, such as the right of integrity, are not
+          licensed under this Public License, nor are publicity,
+          privacy, and/or other similar personality rights; however, to
+          the extent possible, the Licensor waives and/or agrees not to
+          assert any such rights held by the Licensor to the limited
+          extent necessary to allow You to exercise the Licensed
+          Rights, but not otherwise.
+
+       2. Patent and trademark rights are not licensed under this
+          Public License.
+
+       3. To the extent possible, the Licensor waives any right to
+          collect royalties from You for the exercise of the Licensed
+          Rights, whether directly or through a collecting society
+          under any voluntary or waivable statutory or compulsory
+          licensing scheme. In all other cases the Licensor expressly
+          reserves any right to collect such royalties, including when
+          the Licensed Material is used other than for NonCommercial
+          purposes.
+
+
+Section 3 -- License Conditions.
+
+Your exercise of the Licensed Rights is expressly made subject to the
+following conditions.
+
+  a. Attribution.
+
+       1. If You Share the Licensed Material (including in modified
+          form), You must:
+
+            a. retain the following if it is supplied by the Licensor
+               with the Licensed Material:
+
+                 i. identification of the creator(s) of the Licensed
+                    Material and any others designated to receive
+                    attribution, in any reasonable manner requested by
+                    the Licensor (including by pseudonym if
+                    designated);
+
+                ii. a copyright notice;
+
+               iii. a notice that refers to this Public License;
+
+                iv. a notice that refers to the disclaimer of
+                    warranties;
+
+                 v. a URI or hyperlink to the Licensed Material to the
+                    extent reasonably practicable;
+
+            b. indicate if You modified the Licensed Material and
+               retain an indication of any previous modifications; and
+
+            c. indicate the Licensed Material is licensed under this
+               Public License, and include the text of, or the URI or
+               hyperlink to, this Public License.
+
+       2. You may satisfy the conditions in Section 3(a)(1) in any
+          reasonable manner based on the medium, means, and context in
+          which You Share the Licensed Material. For example, it may be
+          reasonable to satisfy the conditions by providing a URI or
+          hyperlink to a resource that includes the required
+          information.
+
+       3. If requested by the Licensor, You must remove any of the
+          information required by Section 3(a)(1)(A) to the extent
+          reasonably practicable.
+
+       4. If You Share Adapted Material You produce, the Adapter's
+          License You apply must not prevent recipients of the Adapted
+          Material from complying with this Public License.
+
+
+Section 4 -- Sui Generis Database Rights.
+
+Where the Licensed Rights include Sui Generis Database Rights that
+apply to Your use of the Licensed Material:
+
+  a. for the avoidance of doubt, Section 2(a)(1) grants You the right
+     to extract, reuse, reproduce, and Share all or a substantial
+     portion of the contents of the database for NonCommercial purposes
+     only;
+
+  b. if You include all or a substantial portion of the database
+     contents in a database in which You have Sui Generis Database
+     Rights, then the database in which You have Sui Generis Database
+     Rights (but not its individual contents) is Adapted Material; and
+
+  c. You must comply with the conditions in Section 3(a) if You Share
+     all or a substantial portion of the contents of the database.
+
+For the avoidance of doubt, this Section 4 supplements and does not
+replace Your obligations under this Public License where the Licensed
+Rights include other Copyright and Similar Rights.
+
+
+Section 5 -- Disclaimer of Warranties and Limitation of Liability.
+
+  a. UNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE
+     EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS
+     AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF
+     ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS,
+     IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION,
+     WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR
+     PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS,
+     ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT
+     KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT
+     ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU.
+
+  b. TO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE
+     TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION,
+     NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT,
+     INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES,
+     COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR
+     USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN
+     ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR
+     DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR
+     IN PART, THIS LIMITATION MAY NOT APPLY TO YOU.
+
+  c. The disclaimer of warranties and limitation of liability provided
+     above shall be interpreted in a manner that, to the extent
+     possible, most closely approximates an absolute disclaimer and
+     waiver of all liability.
+
+
+Section 6 -- Term and Termination.
+
+  a. This Public License applies for the term of the Copyright and
+     Similar Rights licensed here. However, if You fail to comply with
+     this Public License, then Your rights under this Public License
+     terminate automatically.
+
+  b. Where Your right to use the Licensed Material has terminated under
+     Section 6(a), it reinstates:
+
+       1. automatically as of the date the violation is cured, provided
+          it is cured within 30 days of Your discovery of the
+          violation; or
+
+       2. upon express reinstatement by the Licensor.
+
+     For the avoidance of doubt, this Section 6(b) does not affect any
+     right the Licensor may have to seek remedies for Your violations
+     of this Public License.
+
+  c. For the avoidance of doubt, the Licensor may also offer the
+     Licensed Material under separate terms or conditions or stop
+     distributing the Licensed Material at any time; however, doing so
+     will not terminate this Public License.
+
+  d. Sections 1, 5, 6, 7, and 8 survive termination of this Public
+     License.
+
+
+Section 7 -- Other Terms and Conditions.
+
+  a. The Licensor shall not be bound by any additional or different
+     terms or conditions communicated by You unless expressly agreed.
+
+  b. Any arrangements, understandings, or agreements regarding the
+     Licensed Material not stated herein are separate from and
+     independent of the terms and conditions of this Public License.
+
+
+Section 8 -- Interpretation.
+
+  a. For the avoidance of doubt, this Public License does not, and
+     shall not be interpreted to, reduce, limit, restrict, or impose
+     conditions on any use of the Licensed Material that could lawfully
+     be made without permission under this Public License.
+
+  b. To the extent possible, if any provision of this Public License is
+     deemed unenforceable, it shall be automatically reformed to the
+     minimum extent necessary to make it enforceable. If the provision
+     cannot be reformed, it shall be severed from this Public License
+     without affecting the enforceability of the remaining terms and
+     conditions.
+
+  c. No term or condition of this Public License will be waived and no
+     failure to comply consented to unless expressly agreed to by the
+     Licensor.
+
+  d. Nothing in this Public License constitutes or may be interpreted
+     as a limitation upon, or waiver of, any privileges and immunities
+     that apply to the Licensor or You, including from the legal
+     processes of any jurisdiction or authority.
+
+=======================================================================
+
+Creative Commons is not a party to its public
+licenses. Notwithstanding, Creative Commons may elect to apply one of
+its public licenses to material it publishes and in those instances
+will be considered the “Licensor.” The text of the Creative Commons
+public licenses is dedicated to the public domain under the CC0 Public
+Domain Dedication. Except for the limited purpose of indicating that
+material is shared under a Creative Commons public license or as
+otherwise permitted by the Creative Commons policies published at
+creativecommons.org/policies, Creative Commons does not authorize the
+use of the trademark "Creative Commons" or any other trademark or logo
+of Creative Commons without its prior written consent including,
+without limitation, in connection with any unauthorized modifications
+to any of its public licenses or any other arrangements,
+understandings, or agreements concerning use of licensed material. For
+the avoidance of doubt, this paragraph does not form part of the
+public licenses.
+
+Creative Commons may be contacted at creativecommons.org.
diff --git a/experiments/gym_cheetah_1234/src/README.md b/experiments/gym_cheetah_1234/src/README.md
new file mode 100644
index 0000000..ec09da9
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/README.md
@@ -0,0 +1,29 @@
+# Stochastic Lower Bound Optimization
+
+This is the TensorFlow implementation for the paper [
+Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees](https://arxiv.org/abs/1807.03858).
+A PyTorch version will be released later.  
+
+
+## Requirements
+1. OpenAI Baselines
+2. rllab (commit number `b3a2899`)
+3. MuJoCo (1.5)
+4. TensorFlow (>= 1.9)
+5. NumPy (>= 1.14.5)
+6. Python 3.6
+
+## Run
+
+Before running, please make sure that `rllab` and `baselines` are available 
+
+```bash
+python main.py -c configs/algos/slbo.yml configs/envs/half_cheetah.yml -s log_dir=/tmp
+```
+
+If you want to change hyper-parameters, you can either modify a corresponding `yml` file or 
+change it temporarily by appending `model.hidden_sizes='[1000,1000]'` in the command line.
+
+## License
+
+See [LICENSE](LICENSE) for additional details.
diff --git a/experiments/gym_cheetah_1234/src/configs/algos/mb_trpo.yml b/experiments/gym_cheetah_1234/src/configs/algos/mb_trpo.yml
new file mode 100644
index 0000000..cce36ec
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/algos/mb_trpo.yml
@@ -0,0 +1,12 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 2
+  dev_batch_size: 512
+  train_batch_size: 128
+slbo:
+  n_stages: 100
+  n_iters: 1
+  n_model_iters: 2000
+  n_policy_iters: 200
+TRPO:
+  ent_coef: 0.005
diff --git a/experiments/gym_cheetah_1234/src/configs/algos/mf.yml b/experiments/gym_cheetah_1234/src/configs/algos/mf.yml
new file mode 100644
index 0000000..5d5e317
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/algos/mf.yml
@@ -0,0 +1,11 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+algorithm: MF
+rollout:
+  n_train_samples: 10000
+  n_dev_samples: 128
+  n_test_samples: 10000
+slbo:
+  n_stages: 100
+  n_iters: 1
+  n_policy_iters: 20
+  n_model_iters: 1
diff --git a/experiments/gym_cheetah_1234/src/configs/algos/slbo.yml b/experiments/gym_cheetah_1234/src/configs/algos/slbo.yml
new file mode 100644
index 0000000..dcf3777
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/algos/slbo.yml
@@ -0,0 +1,12 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 2
+  dev_batch_size: 512
+  train_batch_size: 128
+slbo:
+  n_stages: 100
+  n_iters: 20
+  n_model_iters: 100
+  n_policy_iters: 40
+TRPO:
+  ent_coef: 0.005
diff --git a/experiments/gym_cheetah_1234/src/configs/algos/slbo_bm_1m.yml b/experiments/gym_cheetah_1234/src/configs/algos/slbo_bm_1m.yml
new file mode 100644
index 0000000..dcf3777
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/algos/slbo_bm_1m.yml
@@ -0,0 +1,12 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 2
+  dev_batch_size: 512
+  train_batch_size: 128
+slbo:
+  n_stages: 100
+  n_iters: 20
+  n_model_iters: 100
+  n_policy_iters: 40
+TRPO:
+  ent_coef: 0.005
diff --git a/experiments/gym_cheetah_1234/src/configs/algos/slbo_bm_200k.yml b/experiments/gym_cheetah_1234/src/configs/algos/slbo_bm_200k.yml
new file mode 100644
index 0000000..dcf3777
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/algos/slbo_bm_200k.yml
@@ -0,0 +1,12 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 2
+  dev_batch_size: 512
+  train_batch_size: 128
+slbo:
+  n_stages: 100
+  n_iters: 20
+  n_model_iters: 100
+  n_policy_iters: 40
+TRPO:
+  ent_coef: 0.005
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_acrobot.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_acrobot.yml
new file mode 100644
index 0000000..fe01e4a
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_acrobot.yml
@@ -0,0 +1,6 @@
+env:
+  id: Acrobot
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_ant.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_ant.yml
new file mode 100644
index 0000000..07d4c3e
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_ant.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Ant
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_ant_depth_search.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_ant_depth_search.yml
new file mode 100644
index 0000000..1d50248
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_ant_depth_search.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Ant
+runner:
+  max_steps: ENV_LENGTH
+plan:
+  max_steps: PLAN_LENGTH
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cartpole.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cartpole.yml
new file mode 100644
index 0000000..a608f7c
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cartpole.yml
@@ -0,0 +1,6 @@
+env:
+  id: CartPole
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cartpoleO001.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cartpoleO001.yml
new file mode 100644
index 0000000..f83770b
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cartpoleO001.yml
@@ -0,0 +1,6 @@
+env:
+  id: CartPoleO001
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cartpoleO01.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cartpoleO01.yml
new file mode 100644
index 0000000..3f7674f
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cartpoleO01.yml
@@ -0,0 +1,6 @@
+env:
+  id: CartPoleO01
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cheetah.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cheetah.yml
new file mode 100644
index 0000000..ae2c095
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cheetah.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetah
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cheetahA003.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cheetahA003.yml
new file mode 100644
index 0000000..a87b6be
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cheetahA003.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetahA003
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cheetahA01.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cheetahA01.yml
new file mode 100644
index 0000000..5312055
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cheetahA01.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetahA01
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cheetahO001.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cheetahO001.yml
new file mode 100644
index 0000000..960e36e
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cheetahO001.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetahO001
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cheetahO01.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cheetahO01.yml
new file mode 100644
index 0000000..4a5d67d
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cheetahO01.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetahO01
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cheetah_depth_search.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cheetah_depth_search.yml
new file mode 100644
index 0000000..c147ec4
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_cheetah_depth_search.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetah
+runner:
+  max_steps: ENV_LENGTH
+plan:
+  max_steps: PLAN_LENGTH
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_fant.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_fant.yml
new file mode 100644
index 0000000..6773c7b
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_fant.yml
@@ -0,0 +1,6 @@
+env:
+  id: FixedAnt
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_fhopper.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_fhopper.yml
new file mode 100644
index 0000000..2efc44e
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_fhopper.yml
@@ -0,0 +1,6 @@
+env:
+  id: FixedHopper
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_fswimmer.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_fswimmer.yml
new file mode 100644
index 0000000..888b87b
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_fswimmer.yml
@@ -0,0 +1,6 @@
+env:
+  id: FixedSwimmer
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_fwalker2d.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_fwalker2d.yml
new file mode 100644
index 0000000..ee63b73
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_fwalker2d.yml
@@ -0,0 +1,6 @@
+env:
+  id: FixedWalker
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_hopper.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_hopper.yml
new file mode 100644
index 0000000..a061802
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_hopper.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Hopper
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_humanoid.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_humanoid.yml
new file mode 100644
index 0000000..426bdfb
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_humanoid.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: gym_humanoid
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_invertedPendulum.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_invertedPendulum.yml
new file mode 100644
index 0000000..d1062d9
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_invertedPendulum.yml
@@ -0,0 +1,6 @@
+env:
+  id: InvertedPendulum
+runner:
+  max_steps: 100
+plan:
+  max_steps: 100
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_mountain.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_mountain.yml
new file mode 100644
index 0000000..c025aa7
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_mountain.yml
@@ -0,0 +1,6 @@
+env:
+  id: MountainCar
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_nostopslimhumanoid.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_nostopslimhumanoid.yml
new file mode 100644
index 0000000..dabe168
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_nostopslimhumanoid.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: gym_nostopslimhumanoid
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_pendulum.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_pendulum.yml
new file mode 100644
index 0000000..c0caff5
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_pendulum.yml
@@ -0,0 +1,6 @@
+env:
+  id: Pendulum
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_pendulumO001.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_pendulumO001.yml
new file mode 100644
index 0000000..1ebd722
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_pendulumO001.yml
@@ -0,0 +1,6 @@
+env:
+  id: PendulumO001
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_pendulumO01.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_pendulumO01.yml
new file mode 100644
index 0000000..4658d7f
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_pendulumO01.yml
@@ -0,0 +1,6 @@
+env:
+  id: PendulumO01
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_reacher.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_reacher.yml
new file mode 100644
index 0000000..427ad36
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_reacher.yml
@@ -0,0 +1,6 @@
+env:
+  id: Reacher
+runner:
+  max_steps: 50
+plan:
+  max_steps: 50
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_slimhumanoid.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_slimhumanoid.yml
new file mode 100644
index 0000000..ecd0976
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_slimhumanoid.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: gym_slimhumanoid
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_swimmer.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_swimmer.yml
new file mode 100644
index 0000000..b9ba125
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_swimmer.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Swimmer
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_walker2d.yml b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_walker2d.yml
new file mode 100644
index 0000000..8403547
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/env_tingwu/gym_walker2d.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Walker2D
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments/gym_cheetah_1234/src/configs/envs/acrobot.yml b/experiments/gym_cheetah_1234/src/configs/envs/acrobot.yml
new file mode 100644
index 0000000..4ca200d
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/envs/acrobot.yml
@@ -0,0 +1,4 @@
+env:
+  id: Acrobot
+runner:
+  max_steps: 200
\ No newline at end of file
diff --git a/experiments/gym_cheetah_1234/src/configs/envs/ant.yml b/experiments/gym_cheetah_1234/src/configs/envs/ant.yml
new file mode 100644
index 0000000..f24b431
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/envs/ant.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Ant
+runner:
+  max_steps: 1000
\ No newline at end of file
diff --git a/experiments/gym_cheetah_1234/src/configs/envs/cartpole.yml b/experiments/gym_cheetah_1234/src/configs/envs/cartpole.yml
new file mode 100644
index 0000000..9b49dbe
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/envs/cartpole.yml
@@ -0,0 +1,4 @@
+env:
+  id: CartPole
+runner:
+  max_steps: 200
\ No newline at end of file
diff --git a/experiments/gym_cheetah_1234/src/configs/envs/fswimmer.yml b/experiments/gym_cheetah_1234/src/configs/envs/fswimmer.yml
new file mode 100644
index 0000000..2aee16a
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/envs/fswimmer.yml
@@ -0,0 +1,4 @@
+env:
+  id: FixedSwimmer
+runner:
+  max_steps: 1000
\ No newline at end of file
diff --git a/experiments/gym_cheetah_1234/src/configs/envs/half_cheetah.yml b/experiments/gym_cheetah_1234/src/configs/envs/half_cheetah.yml
new file mode 100644
index 0000000..36cfffa
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/envs/half_cheetah.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetah
+runner:
+  max_steps: 1000
diff --git a/experiments/gym_cheetah_1234/src/configs/envs/half_cheetah_short.yml b/experiments/gym_cheetah_1234/src/configs/envs/half_cheetah_short.yml
new file mode 100644
index 0000000..e0e21ed
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/envs/half_cheetah_short.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetah-v2
+plan:
+  max_steps: 200
+runner:
+  max_steps: 200
diff --git a/experiments/gym_cheetah_1234/src/configs/envs/hopper.yml b/experiments/gym_cheetah_1234/src/configs/envs/hopper.yml
new file mode 100644
index 0000000..7b34a11
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/envs/hopper.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Hopper
+runner:
+  max_steps: 1000
\ No newline at end of file
diff --git a/experiments/gym_cheetah_1234/src/configs/envs/humanoid.yml b/experiments/gym_cheetah_1234/src/configs/envs/humanoid.yml
new file mode 100644
index 0000000..1eb1236
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/envs/humanoid.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: POMDPHumanoid-v2
+runner:
+  max_steps: 500
diff --git a/experiments/gym_cheetah_1234/src/configs/envs/inverted_pendulum.yml b/experiments/gym_cheetah_1234/src/configs/envs/inverted_pendulum.yml
new file mode 100644
index 0000000..1d60732
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/envs/inverted_pendulum.yml
@@ -0,0 +1,4 @@
+env:
+  id: InvertedPendulum
+runner:
+  max_steps: 100
\ No newline at end of file
diff --git a/experiments/gym_cheetah_1234/src/configs/envs/mountain.yml b/experiments/gym_cheetah_1234/src/configs/envs/mountain.yml
new file mode 100644
index 0000000..e4cb7f6
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/envs/mountain.yml
@@ -0,0 +1,4 @@
+env:
+  id: MountainCar
+runner:
+  max_steps: 200
\ No newline at end of file
diff --git a/experiments/gym_cheetah_1234/src/configs/envs/pendulum.yml b/experiments/gym_cheetah_1234/src/configs/envs/pendulum.yml
new file mode 100644
index 0000000..8ea91c3
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/envs/pendulum.yml
@@ -0,0 +1,4 @@
+env:
+  id: Pendulum
+runner:
+  max_steps: 200
\ No newline at end of file
diff --git a/experiments/gym_cheetah_1234/src/configs/envs/reacher.yml b/experiments/gym_cheetah_1234/src/configs/envs/reacher.yml
new file mode 100644
index 0000000..bfaf65b
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/envs/reacher.yml
@@ -0,0 +1,4 @@
+env:
+  id: Reacher
+runner:
+  max_steps: 50
\ No newline at end of file
diff --git a/experiments/gym_cheetah_1234/src/configs/envs/swimmer.yml b/experiments/gym_cheetah_1234/src/configs/envs/swimmer.yml
new file mode 100644
index 0000000..abe2842
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/envs/swimmer.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Swimmer
+runner:
+  max_steps: 1000
\ No newline at end of file
diff --git a/experiments/gym_cheetah_1234/src/configs/envs/walker.yml b/experiments/gym_cheetah_1234/src/configs/envs/walker.yml
new file mode 100644
index 0000000..e42fa53
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/envs/walker.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Walker2D
+runner:
+  max_steps: 1000
diff --git a/experiments/gym_cheetah_1234/src/configs/multi_step/1.yml b/experiments/gym_cheetah_1234/src/configs/multi_step/1.yml
new file mode 100644
index 0000000..e102872
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/multi_step/1.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 1
+  dev_batch_size: 1024
+  train_batch_size: 256
diff --git a/experiments/gym_cheetah_1234/src/configs/multi_step/2.yml b/experiments/gym_cheetah_1234/src/configs/multi_step/2.yml
new file mode 100644
index 0000000..c866952
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/multi_step/2.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 2
+  dev_batch_size: 512
+  train_batch_size: 128
diff --git a/experiments/gym_cheetah_1234/src/configs/multi_step/4.yml b/experiments/gym_cheetah_1234/src/configs/multi_step/4.yml
new file mode 100644
index 0000000..16a1e5c
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/multi_step/4.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 4
+  dev_batch_size: 256
+  train_batch_size: 64
diff --git a/experiments/gym_cheetah_1234/src/configs/multi_step/8.yml b/experiments/gym_cheetah_1234/src/configs/multi_step/8.yml
new file mode 100644
index 0000000..7262945
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/configs/multi_step/8.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 8
+  dev_batch_size: 128
+  train_batch_size: 32
diff --git a/experiments/gym_cheetah_1234/src/lunzi/Logger.py b/experiments/gym_cheetah_1234/src/lunzi/Logger.py
new file mode 100644
index 0000000..9f41014
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/lunzi/Logger.py
@@ -0,0 +1,133 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from termcolor import colored
+import datetime
+import sys
+import os
+from collections import Counter, defaultdict
+import json_tricks
+
+
+def a():
+    pass
+
+
+_srcfile = os.path.normcase(a.__code__.co_filename)
+
+
+class BaseSink(object):
+    @staticmethod
+    def _time():
+        return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S,%f')
+
+    def info(self, fmt, *args, **kwargs):
+        raise NotImplementedError
+
+    def warning(self, fmt, *args, **kwargs):
+        self.info(fmt, *args, **kwargs)
+
+    def verbose(self, fmt, *args, **kwargs):
+        pass
+
+
+class StdoutSink(BaseSink):
+    def __init__(self):
+        self.freq_count = Counter()
+
+    def info(self, fmt, *args, freq=1, caller=None):
+        if args:
+            fmt = fmt % args
+        self.freq_count[caller] += 1
+        if self.freq_count[caller] % freq == 0:
+            print("%s - %s - %s" % (colored(self._time(), 'green'),
+                                    colored(caller, 'cyan'), fmt), flush=True)
+
+    def warning(self, fmt, *args, **kwargs):
+        if args:
+            fmt = fmt % args
+        self.info(colored(fmt, 'yellow'), **kwargs)
+
+
+class FileSink(BaseSink):
+    def __init__(self, fn):
+        self.log_file = open(fn, 'w')
+        self.callers = {}
+
+    def info(self, fmt, *args, **kwargs):
+        self._kv(level='info', fmt=fmt, args=args, **kwargs)
+
+    def warning(self, fmt, *args, **kwargs):
+        self._kv(level='warning', fmt=fmt, args=args, **kwargs)
+
+    def _kv(self, **kwargs):
+        kwargs.update(time=datetime.datetime.now())
+        self.log_file.write(json_tricks.dumps(kwargs, primitives=True) + '\n')
+        self.log_file.flush()
+
+    def verbose(self, fmt, *args, **kwargs):
+        self._kv(level='verbose', fmt=fmt, args=args, **kwargs)
+
+
+class LibLogger(object):
+    logfile = ""
+
+    def __init__(self, name='logger', is_root=True):
+        self.name = name
+        self.is_root = is_root
+        self.tab_keys = None
+        self.sinks = []
+        self.key_prior = defaultdict(np.random.randn)
+
+    def add_sink(self, sink):
+        self.sinks.append(sink)
+
+    def info(self, fmt, *args, **kwargs):
+        caller = self.find_caller()
+        for sink in self.sinks:
+            sink.info(fmt, *args, caller=caller, **kwargs)
+
+    def warning(self, fmt, *args, **kwargs):
+        caller = self.find_caller()
+        for sink in self.sinks:
+            sink.warning(fmt, *args, caller=caller, **kwargs)
+
+    def verbose(self, fmt, *args, **kwargs):
+        caller = self.find_caller()
+        for sink in self.sinks:
+            sink.verbose(fmt, *args, caller=caller, **kwargs)
+
+    def find_caller(self):
+        """
+        Copy from `python.logging` module
+
+        Find the stack frame of the caller so that we can note the source
+        file name, line number and function name.
+        """
+        f = sys._getframe(1)
+        if f is not None:
+            f = f.f_back
+        caller = ''
+        while hasattr(f, "f_code"):
+            co = f.f_code
+            filename = os.path.normcase(co.co_filename)
+            if filename == _srcfile:
+                f = f.f_back
+                continue
+            # if stack_info:
+            #     sio = io.StringIO()
+            #     sio.write('Stack (most recent call last):\n')
+            #     traceback.print_stack(f, file=sio)
+            #     sio.close()
+            # rv = (co.co_filename, f.f_lineno, co.co_name, sinfo)
+            rel_path = os.path.relpath(co.co_filename, '')
+            caller = f'{rel_path}:{f.f_lineno}'
+            break
+        return caller
+
+
+def get_logger(name):
+    return LibLogger(name)
+
+
+logger = get_logger('Logger')
+logger.add_sink(StdoutSink())
diff --git a/experiments/gym_cheetah_1234/src/lunzi/__init__.py b/experiments/gym_cheetah_1234/src/lunzi/__init__.py
new file mode 100644
index 0000000..0c5417b
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/lunzi/__init__.py
@@ -0,0 +1,4 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from .stubs import Tensor
+import lunzi.nn
+import lunzi.Logger
diff --git a/experiments/gym_cheetah_1234/src/lunzi/config.py b/experiments/gym_cheetah_1234/src/lunzi/config.py
new file mode 100644
index 0000000..5c39b1e
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/lunzi/config.py
@@ -0,0 +1,97 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import argparse
+import os
+import yaml
+from lunzi.Logger import logger
+
+
+_frozen = False
+_initialized = False
+
+
+def expand(path):
+    return os.path.abspath(os.path.expanduser(path))
+
+
+class MetaFLAGS(type):
+    _initialized = False
+
+    def __setattr__(self, key, value):
+        assert not _frozen, 'Modifying FLAGS after dumping is not allowed!'
+        super().__setattr__(key, value)
+
+    def __getitem__(self, item):
+        return self.__dict__[item]
+
+    def __iter__(self):
+        for key, value in self.__dict__.items():
+            if not key.startswith('_') and not isinstance(value, classmethod):
+                if isinstance(value, MetaFLAGS):
+                    value = dict(value)
+                yield key, value
+
+    def as_dict(self):
+        return dict(self)
+
+    def merge(self, other: dict):
+        for key in other:
+            assert key in self.__dict__, f"Can't find key `{key}`"
+            if isinstance(self[key], MetaFLAGS) and isinstance(other[key], dict):
+                self[key].merge(other[key])
+            else:
+                setattr(self, key, other[key])
+
+    def set_value(self, path, value):
+        key, *rest = path
+        assert key in self.__dict__, f"Can't find key `{key}`"
+        if not rest:
+            setattr(self, key, value)
+        else:
+            self[key]: MetaFLAGS
+            self[key].set_value(rest, value)
+
+    @staticmethod
+    def set_frozen():
+        global _frozen
+        _frozen = True
+
+    def freeze(self):
+        for key, value in self.__dict__.items():
+            if not key.startswith('_'):
+                if isinstance(value, MetaFLAGS):
+                    value.freeze()
+        self.finalize()
+
+    def finalize(self):
+        pass
+
+
+class BaseFLAGS(metaclass=MetaFLAGS):
+    pass
+
+
+def parse(cls):
+    global _initialized
+
+    if _initialized:
+        return
+    parser = argparse.ArgumentParser(description='Stochastic Lower Bound Optimization')
+    parser.add_argument('-c', '--config', type=str, help='configuration file (YAML)', nargs='+', action='append')
+    parser.add_argument('-s', '--set', type=str, help='additional options', nargs='*', action='append')
+
+    args, unknown = parser.parse_known_args()
+    for a in unknown:
+        logger.info('unknown arguments: %s', a)
+    # logger.info('parsed arguments = %s, unknown arguments: %s', args, unknown)
+    if args.config:
+        for config in sum(args.config, []):
+            cls.merge(yaml.load(open(expand(config))))
+    else:
+        logger.info('no config file specified.')
+    if args.set:
+        for instruction in sum(args.set, []):
+            path, *value = instruction.split('=')
+            cls.set_value(path.split('.'), yaml.load('='.join(value)))
+
+    _initialized = True
+
diff --git a/experiments/gym_cheetah_1234/src/lunzi/dataset.py b/experiments/gym_cheetah_1234/src/lunzi/dataset.py
new file mode 100644
index 0000000..7cb5d5f
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/lunzi/dataset.py
@@ -0,0 +1,82 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+
+
+class Dataset(np.recarray):
+    """
+        Overallocation can be supported, by making examinations before
+        each `append` and `extend`.
+    """
+
+    @staticmethod
+    def fromarrays(array_lists, dtype):
+        array = np.rec.fromarrays(array_lists, dtype=dtype)
+        ret = Dataset(dtype, len(array))
+        ret.extend(array)
+        return ret
+
+    def __init__(self, dtype, max_size, verbose=False):
+        super().__init__()
+        self.max_size = max_size
+        self._index = 0
+        self._buf_size = 0
+        self._len = 0
+
+        self.resize(max_size)
+        self._buf_size = max_size
+
+    def __new__(cls, dtype, max_size):
+        return np.recarray.__new__(cls, max_size, dtype=dtype)
+
+    def size(self):
+        return self._len
+
+    def reserve(self, size):
+        cur_size = max(self._buf_size, 1)
+        while cur_size < size:
+            cur_size *= 2
+        if cur_size != self._buf_size:
+            self.resize(cur_size)
+
+    def clear(self):
+        self._index = 0
+        self._len = 0
+        return self
+
+    def append(self, item):
+        self[self._index] = item
+        self._index = (self._index + 1) % self.max_size
+        self._len = min(self._len + 1, self.max_size)
+        return self
+
+    def extend(self, items):
+        n_new = len(items)
+        if n_new > self.max_size:
+            items = items[-self.max_size:]
+            n_new = self.max_size
+
+        n_tail = self.max_size - self._index
+        if n_new <= n_tail:
+            self[self._index:self._index + n_new] = items
+        else:
+            n_head = n_new - n_tail
+            self[self._index:] = items[:n_tail]
+            self[:n_head] = items[n_tail:]
+
+        self._index = (self._index + n_new) % self.max_size
+        self._len = min(self._len + n_new, self.max_size)
+        return self
+
+    def sample(self, size, indices=None):
+        if indices is None:
+            indices = np.random.randint(0, self._len, size=size)
+        return self[indices]
+
+    def iterator(self, batch_size):
+        indices = np.arange(self._len, dtype=np.int32)
+        np.random.shuffle(indices)
+        index = 0
+        while index + batch_size <= self._len:
+            end = index + batch_size
+            yield self[indices[index:end]]
+            index = end
diff --git a/experiments/gym_cheetah_1234/src/lunzi/nn/__init__.py b/experiments/gym_cheetah_1234/src/lunzi/nn/__init__.py
new file mode 100644
index 0000000..2315809
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/lunzi/nn/__init__.py
@@ -0,0 +1,10 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from . import patch
+from .parameter import Parameter
+from .module import Module
+from .container import *
+from . import utils
+from .utils import make_method
+from .layers import *
+from .loss import *
+from .flat_param import FlatParam
diff --git a/experiments/gym_cheetah_1234/src/lunzi/nn/container.py b/experiments/gym_cheetah_1234/src/lunzi/nn/container.py
new file mode 100644
index 0000000..6dbafdc
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/lunzi/nn/container.py
@@ -0,0 +1,35 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import Dict, Any
+from .module import Module
+from .parameter import Parameter
+
+_dict_methods = ['__setitem__', '__getitem__', '__delitem__', '__len__', '__iter__', '__contains__',
+                 'update', 'keys', 'values', 'items', 'clear', 'pop']
+
+
+class ModuleDict(Module, dict):  # use dict for auto-complete
+    """
+        Essentially this exposes some methods of `Module._modules`.
+    """
+    def __init__(self, modules: Dict[Any, Module] = None):
+        super().__init__()
+        for method in _dict_methods:
+            setattr(self, method, getattr(self._modules, method))
+        if modules is not None:
+            self.update(modules)
+
+    def forward(self):
+        raise RuntimeError("ModuleDict is not callable")
+
+
+# Do we need a factory for it?
+class ParameterDict(Module, dict):
+    def __init__(self, parameters: Dict[Any, Parameter] = None):
+        super().__init__()
+        for method in _dict_methods:
+            setattr(self, method, getattr(self._modules, method))
+        if parameters is not None:
+            self.update(parameters)
+
+    def forward(self):
+        raise RuntimeError("ParameterDict is not callable")
diff --git a/experiments/gym_cheetah_1234/src/lunzi/nn/flat_param.py b/experiments/gym_cheetah_1234/src/lunzi/nn/flat_param.py
new file mode 100644
index 0000000..5c4bb52
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/lunzi/nn/flat_param.py
@@ -0,0 +1,34 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+from lunzi.Logger import logger
+from .module import Module
+from .utils import make_method, n_parameters, parameters_to_vector, vector_to_parameters
+
+
+class FlatParam(Module):
+    def __init__(self, parameters):
+        super().__init__()
+        self.params = parameters
+        self.op_feed_flat, self.op_set_flat, self.op_get_flat = \
+            self.enable_flat()
+
+    def enable_flat(self):
+        params = self.params
+        logger.info('Enabling flattening... %s', [p.name for p in params])
+        n_params = n_parameters(params)
+        feed_flat = tf.placeholder(tf.float32, [n_params])
+        get_flat = parameters_to_vector(params)
+        set_flat = tf.group(*[tf.assign(param, value) for param, value in
+                            zip(params, vector_to_parameters(feed_flat, params))])
+        return feed_flat, set_flat, get_flat
+
+    def forward(self):
+        return self.op_get_flat
+
+    @make_method(feed='feed_flat', fetch='set_flat')
+    def set_flat(self, feed_flat):
+        pass
+
+    @make_method(fetch='get_flat')
+    def get_flat(self):
+        pass
diff --git a/experiments/gym_cheetah_1234/src/lunzi/nn/layers.py b/experiments/gym_cheetah_1234/src/lunzi/nn/layers.py
new file mode 100644
index 0000000..a0cd9c2
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/lunzi/nn/layers.py
@@ -0,0 +1,88 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+import numpy as np
+from .module import Module
+from .parameter import Parameter
+
+
+class Linear(Module):
+    def __init__(self, in_features: int, out_features: int, bias=True, weight_initializer=None):
+        super().__init__()
+        self.in_features = in_features
+        self.out_features = out_features
+        if weight_initializer is None:
+            init_range = tf.sqrt(6.0 / (in_features + out_features))
+            weight_initializer = tf.random_uniform_initializer(-init_range, init_range, dtype=tf.float32)
+
+        self.use_bias = bias
+        with self.scope:
+            self.op_input = tf.placeholder(dtype=tf.float32, shape=[None, in_features], name='input')
+            self.weight = Parameter(weight_initializer([in_features, out_features],dtype=tf.float32), name='weight')
+            if bias:
+                self.bias = Parameter(tf.zeros([out_features], dtype=tf.float32), name='bias')
+
+        self.op_output = self(self.op_input)
+
+    def forward(self, x):
+        shape = x.get_shape().as_list()
+        if len(shape) > 2:
+            y = tf.tensordot(x, self.weight, [[len(shape) - 1], [0]])
+        else:
+            y = x.matmul(self.weight)
+        if self.use_bias:
+            y = y + self.bias
+        return y
+
+    def fast(self, x):
+        x = x.dot(self.weight.numpy())
+        if self.use_bias:
+            x = x + self.bias.numpy()
+        return x
+
+    def extra_repr(self):
+        return f'in_features={self.in_features}, out_features={self.out_features}, bias={self.use_bias}'
+
+
+class Sequential(Module):
+    def __init__(self, *modules):
+        super().__init__()
+        for i, module in enumerate(modules):
+            self._modules[i] = module
+
+    def forward(self, x):
+        for module in self._modules.values():
+            x = module(x)
+        return x
+
+    def fast(self, x):
+        for module in self._modules.values():
+            x = module.fast(x)
+        return x
+
+
+class ReLU(Module):
+    def forward(self, x):
+        return tf.nn.relu(x)
+
+    def fast(self, x: np.ndarray):
+        return np.maximum(x, 0)
+
+
+class Tanh(Module):
+    def forward(self, x):
+        return tf.nn.tanh(x)
+
+    def fast(self, x: np.ndarray):
+        return np.tanh(x)
+
+
+class Squeeze(Module):
+    def __init__(self, axis=None):
+        super().__init__()
+        self._axis = axis
+
+    def forward(self, x):
+        return x.squeeze(axis=self._axis)
+
+    def fast(self, x):
+        return x.squeeze(axis=self._axis)
diff --git a/experiments/gym_cheetah_1234/src/lunzi/nn/loss.py b/experiments/gym_cheetah_1234/src/lunzi/nn/loss.py
new file mode 100644
index 0000000..26e662b
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/lunzi/nn/loss.py
@@ -0,0 +1,40 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from lunzi import Tensor
+from .module import Module
+
+
+class PointwiseLoss(Module):
+    def __init__(self, size_average=True, reduce=True):
+        super().__init__()
+        self.size_average = size_average
+        self.reduce = reduce
+
+    def pointwise(self, output: Tensor, target: Tensor):
+        raise NotImplementedError
+
+    def forward(self, output: Tensor, target: Tensor, input: Tensor = None):
+        loss = self.pointwise(output, target)
+        if self.reduce and len(loss.shape) > 1:
+            if self.size_average:
+                loss = loss.reduce_mean(axis=1)
+            else:
+                loss = loss.reduce_sum(axis=1)
+        return loss
+
+
+class L1Loss(PointwiseLoss):
+    def pointwise(self, output: Tensor, target: Tensor):
+        return output.sub(target).abs()
+
+
+class L2Loss(PointwiseLoss):
+    def pointwise(self, output: Tensor, target: Tensor):
+        return output.sub(target).pow(2)
+
+    def forward(self, output: Tensor, target: Tensor, input: Tensor = None):
+        return super().forward(output, target).sqrt()
+
+
+class MSELoss(PointwiseLoss):
+    def pointwise(self, output: Tensor, target: Tensor):
+        return output.sub(target).pow(2)
diff --git a/experiments/gym_cheetah_1234/src/lunzi/nn/module.py b/experiments/gym_cheetah_1234/src/lunzi/nn/module.py
new file mode 100644
index 0000000..8eacf3b
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/lunzi/nn/module.py
@@ -0,0 +1,158 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import Dict, Any, Callable, List
+from collections import Counter
+import tensorflow as tf
+import numpy as np
+
+from lunzi import Tensor
+from lunzi.Logger import logger
+from .parameter import Parameter
+
+
+class Module(object):
+    """
+        A front-end for TensorFlow, heavily inspired by PyTorch's design and implementation.
+
+        Deepcopy is not supported since I didn't find a good way to duplicate `tf.Variables` and `tf.variable_scope`.
+    """
+
+    # To generate unique name scope
+    # The only reason we keep variable scope here is that we want the variables have meaning names,
+    # since the internal operations always look messy, I put no hope maintaining their names,
+    # So let's just do it for variables.
+    prefix_count = Counter()
+
+    @staticmethod
+    def _create_uid(prefix: str) -> str:
+        scope = tf.get_variable_scope().name + '/'
+        uid = Module.prefix_count[scope + prefix]
+        Module.prefix_count[scope + prefix] += 1
+        if uid == 0:
+            return prefix
+        return f'{prefix}_{uid}'
+
+    def __init__(self):
+        scope = Module._create_uid(self.__class__.__name__)
+        with tf.variable_scope(scope, reuse=False) as self._scope:
+            pass
+        # Since we only plan to support Python 3.6+, in which dict is already ordered, we don't use OrderedDict here.
+        self._parameters: Dict[Any, Parameter] = {}
+        self._modules: Dict[Any, Module] = {}
+        self._callables: Dict[Any, Callable] = {}
+
+    def forward(self, *args: List[Any], **kwargs: Dict[str, Any]) -> Tensor:
+        raise NotImplementedError
+
+    def fast(self, *args, **kwargs):
+        pass
+
+    def __setattr__(self, key, value):
+        # dynamically maintain sub modules.
+        modules = self.__dict__.get('_modules')
+        if isinstance(value, Parameter):
+            self._parameters[key] = value
+        if isinstance(value, Module):
+            assert modules is not None, 'Call `super().__init__` before assigning modules'
+            modules[key] = value
+        else:
+            if modules and key in modules:
+                del modules[key]
+        object.__setattr__(self, key, value)
+
+    def __call__(self, *args, **kwargs):
+        return self.forward(*args, **kwargs)
+
+    def register_callable(self, key, callable):
+        self._callables[key] = callable
+
+    def eval(self, fetch: str, **feed: Dict[str, np.ndarray]):
+        cache_key = f'[{" ".join(feed.keys())}] => [{fetch}]'
+        if cache_key not in self._callables:
+            logger.info('[%s] is making TensorFlow callables, key = %s', self.__class__.__name__, cache_key)
+            feed_ops = []
+            for key in feed.keys():
+                feed_ops.append(self.__dict__['op_' + key])
+            if isinstance(fetch, str):
+                fetch_ops = [self.__dict__['op_' + key] for key in fetch.split(' ')]
+                if len(fetch_ops) == 1:
+                    fetch_ops = fetch_ops[0]
+            else:
+                fetch_ops = fetch
+            self.register_callable(cache_key, tf.get_default_session().make_callable(fetch_ops, feed_ops))
+        return self._callables[cache_key](*feed.values())
+
+    def parameters(self, trainable=True, non_trainable=False, recursive=True, out=None) -> List[Parameter]:
+        """
+            We don't introduce `buffers` here. PyTorch has it since it doesn't have non-trainable Parameter.
+            A tensor in `buffers` is essentially a non-trainable Parameter (part of state_dict but isn't
+            optimized over).
+        """
+        if out is None:
+            out = []
+        for param in self._parameters.values():
+            if param.trainable and trainable or not param.trainable and non_trainable:
+                out.append(param)
+        if recursive:
+            for module in self._modules.values():
+                module.parameters(trainable=trainable, non_trainable=non_trainable, recursive=True, out=out)
+        # probably we don't need to sort since we're using `OrderedDict`
+        return out
+
+    @property
+    def scope(self) -> tf.variable_scope:
+        return tf.variable_scope(self._scope, reuse=tf.AUTO_REUSE)
+
+    def extra_repr(self) -> str:
+        return ''
+
+    def named_modules(self) -> dict:
+        return self._modules
+
+    def __repr__(self):
+        def dfs(node, prefix):
+            root_info = node.__class__.__name__
+            modules = node.named_modules()
+            if not modules:
+                return root_info + f'({node.extra_repr()})'
+
+            root_info += '(\n'
+            for key, module in modules.items():
+                module_repr = dfs(module, prefix + '    ')
+                root_info += f'{prefix}    ({key}): {module_repr}\n'
+            root_info += prefix + ')'
+            return root_info
+        return dfs(self, '')
+
+    def state_dict(self, recursive=True):
+        """
+            A better option is to find all parameters and then sess.run(state) but I assume this can't be the
+            bottleneck.
+        """
+        state = {}
+        for key, parameter in self._parameters.items():
+            # although we can use `.numpy()` here, for safety I'd use `.eval()`
+            state[key] = parameter.eval()
+        if recursive:
+            for key, module in self._modules.items():
+                state[key] = module.state_dict()
+        return state
+
+    def load_state_dict(self, state_dict: Dict[Any, Any], recursive=True, strict=True):
+        for key, parameter in self._parameters.items():
+            if key in state_dict:
+                parameter.load(state_dict[key])
+                parameter.invalidate()
+            else:
+                assert not strict, f'Missing Parameter {key} in state_dict'
+        if recursive:
+            for key, module in self._modules.items():
+                if key in state_dict:
+                    module.load_state_dict(state_dict[key], recursive=recursive, strict=strict)
+                else:
+                    assert not strict, f'Missing Module {key} in state_dict.'
+
+    def apply(self, fn):
+        for module in self._modules.values():
+            module.apply(fn)
+        fn(self)
+        return self
diff --git a/experiments/gym_cheetah_1234/src/lunzi/nn/parameter.py b/experiments/gym_cheetah_1234/src/lunzi/nn/parameter.py
new file mode 100644
index 0000000..969d79f
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/lunzi/nn/parameter.py
@@ -0,0 +1,24 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+from lunzi import Tensor
+
+
+def numpy(self):
+    if self.__dict__.get('_numpy_cache', None) is None:
+        self._numpy_cache: Tensor = self.eval()
+    return self._numpy_cache
+
+
+def invalidate(self):
+    self._numpy_cache = None
+
+
+# Q: Why not inherit from `tf.Variable`?
+# A: Since TensorFlow 1.11, `tf.Variable` has a meta class VariableMetaClass, which overrides `__call__`.
+#    And it's `_variable_call` function doesn't explicitly call `tf.Variable` so the return value must
+#    be a `tf.Variable`, which makes inheritance impossible.
+Parameter = tf.Variable
+
+Parameter.numpy = numpy
+Parameter.invalidate = invalidate
+
diff --git a/experiments/gym_cheetah_1234/src/lunzi/nn/patch.py b/experiments/gym_cheetah_1234/src/lunzi/nn/patch.py
new file mode 100644
index 0000000..5db96d8
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/lunzi/nn/patch.py
@@ -0,0 +1,60 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+
+from lunzi.Logger import logger
+
+
+def find_monkey_patch_keys(avoid_set=None):
+    if avoid_set is None:
+        avoid_set = {"shape"}  # tf.shape conflicts with Tensor.shape
+    patched = []
+    for key, value in tf.__dict__.items():
+        if not callable(value) or key in avoid_set:
+            continue
+        doc = value.__doc__
+        if doc is None:
+            continue
+        loc = doc.find('Args:\n')
+        if loc == -1:
+            continue
+
+        # Am I doing NLP?
+        # It seems that PyTorch has better doc. They always write `x (Tensor): ...` which is much easier to parse.
+        first_arg_doc = doc[loc + 6:].split('\n')[0].split(': ')[1]
+        if first_arg_doc.startswith('A `Tensor`') or first_arg_doc.startswith('`Tensor`') or key.startswith('reduce_'):
+            patched.append(key)
+    logger.warning(f'Monkey patched TensorFlow: {patched}')
+    return patched
+
+
+def monkey_patch(avoid_set=None):
+    logger.warning('Monkey patching TensorFlow...')
+
+    patched = ['abs', 'acos', 'acosh', 'add', 'angle', 'argmax', 'argmin', 'asin', 'asinh', 'atan', 'atan2', 'atanh',
+            'betainc', 'cast', 'ceil', 'check_numerics', 'clip_by_average_norm', 'clip_by_norm', 'clip_by_value',
+            'complex', 'conj', 'cos', 'cosh', 'cross', 'cumprod', 'cumsum', 'dequantize', 'diag', 'digamma', 'div',
+            'equal', 'erf', 'erfc', 'exp', 'expand_dims', 'expm1', 'fill', 'floor', 'floor_div', 'floordiv', 'floormod',
+            'gather', 'gather_nd', 'greater', 'greater_equal', 'hessians', 'identity', 'igamma', 'igammac', 'imag',
+            'is_finite', 'is_inf', 'is_nan', 'less', 'less_equal', 'lgamma', 'log', 'log1p', 'logical_and',
+            'logical_not', 'logical_or', 'matmul', 'maximum', 'meshgrid', 'minimum', 'mod', 'multiply', 'negative',
+            'norm', 'not_equal', 'one_hot', 'ones_like', 'pad', 'polygamma', 'pow', 'quantize', 'real', 'realdiv',
+            'reciprocal', 'reduce_all', 'reduce_any', 'reduce_logsumexp', 'reduce_max', 'reduce_mean', 'reduce_min',
+            'reduce_prod', 'reduce_sum', 'reshape', 'reverse', 'rint', 'round', 'rsqrt', 'scatter_nd', 'sign', 'sin',
+            'sinh', 'size', 'slice', 'sqrt', 'square', 'squeeze', 'stop_gradient', 'subtract', 'tan', 'tensordot',
+            'tile', 'to_bfloat16', 'to_complex128', 'to_complex64', 'to_double', 'to_float', 'to_int32', 'to_int64',
+            'transpose', 'truediv', 'truncatediv', 'truncatemod', 'unique', 'where', 'zeros_like', 'zeta']
+    alias = {
+        'mul': 'multiply',
+        'sub': 'subtract',
+    }
+
+    # use the code below for more ops
+    # patched = find_monkey_patch_keys(avoid_set)
+
+    for key, method in list(zip(patched, patched)) + list(alias.items()):
+        value = tf.__dict__[method]
+        setattr(tf.Tensor, key, value)
+        setattr(tf.Variable, key, value)
+
+
+monkey_patch()
diff --git a/experiments/gym_cheetah_1234/src/lunzi/nn/utils.py b/experiments/gym_cheetah_1234/src/lunzi/nn/utils.py
new file mode 100644
index 0000000..9fa5707
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/lunzi/nn/utils.py
@@ -0,0 +1,85 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import Callable, List, Union
+import inspect
+from functools import wraps
+import tensorflow as tf
+import numpy as np
+from lunzi import Tensor
+
+from .parameter import Parameter
+
+
+def make_method(feed: str = None, fetch: str = ''):
+    """
+        The following code:
+
+            @make_method('. w', fetch='d)
+            def func(a, c):
+                pass
+
+        will be converted to
+
+            def func(a, c, fetch='d'):
+                return self.eval(fetch, a=a, w=c)
+
+        Note that `func(1, c=2, b=1)` is also supported. This is
+        useful when writing PyTorch-like object method.
+
+    """
+
+    def decorator(func: Callable):
+        arg_names = inspect.signature(func).parameters.keys()
+        arg_map = {}
+        if feed is None:
+            arg_map = {op_name: op_name for op_name in arg_names if op_name != 'self'}
+        else:
+            feeds = ['-'] + feed.split(' ')  # ignore first `self`
+            for op_name, arg_name in zip(feeds, arg_names):
+                if op_name == '.':
+                    arg_map[op_name] = op_name
+                elif op_name != '-':  # deprecated
+                    arg_map[op_name] = arg_name
+
+        @wraps(func)
+        def wrapper(self, *args, **kwargs):
+            cur_fetch = kwargs.pop('fetch', fetch)
+            call_args = inspect.getcallargs(func, self, *args, **kwargs)
+            feed_dict = {op_name: call_args[arg_name] for op_name, arg_name in arg_map.items()}
+            return self.eval(cur_fetch, **feed_dict)
+
+        return wrapper
+
+    return decorator
+
+
+def n_parameters(params: List[Parameter]) -> int:
+    return sum([np.prod(p.shape) for p in params])
+
+
+def parameters_to_vector(parameters: List[Union[Parameter, Tensor]]) -> Tensor:
+    return tf.concat([param.reshape([-1]) for param in parameters], axis=0)
+
+
+def vector_to_parameters(vec: Tensor, parameters: List[Parameter]) -> List[Tensor]:
+    params: List[Tensor] = []
+    start = 0
+    for p in parameters:
+        end = start + np.prod(p.shape)
+        params.append(vec[start:end].reshape(p.shape))
+        start = end
+    return params
+
+
+def hessian_vec_prod(ys: Tensor, xs: List[Parameter], vs: Tensor) -> Tensor:
+    grad = parameters_to_vector(tf.gradients(ys, xs))
+    aux = (grad * vs).reduce_sum()
+    return parameters_to_vector(tf.gradients(aux, xs))
+
+
+# credit to https://github.com/renmengye/tensorflow-forward-ad/issues/2#issue-234418055
+def jacobian_vec_prod(ys: Tensor, xs: List[Parameter], vs: Tensor) -> Tensor:
+    u = tf.zeros_like(ys)  # dummy variable
+    grad = tf.gradients(ys, xs, grad_ys=u)
+    return tf.gradients(grad, u, grad_ys=vs)
+
+
diff --git a/experiments/gym_cheetah_1234/src/lunzi/stubs.py b/experiments/gym_cheetah_1234/src/lunzi/stubs.py
new file mode 100644
index 0000000..644c477
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/lunzi/stubs.py
@@ -0,0 +1,6 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+
+
+class Tensor(tf.Tensor):
+    pass
\ No newline at end of file
diff --git a/experiments/gym_cheetah_1234/src/lunzi/stubs.pyi b/experiments/gym_cheetah_1234/src/lunzi/stubs.pyi
new file mode 100644
index 0000000..7bab814
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/lunzi/stubs.pyi
@@ -0,0 +1,138 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+
+class Tensor(tf.Tensor):
+    def abs(x, name=None) -> Tensor: ...
+    def acos(x, name=None) -> Tensor: ...
+    def acosh(x, name=None) -> Tensor: ...
+    def add(x, y, name=None) -> Tensor: ...
+    def angle(input, name=None) -> Tensor: ...
+    def argmax(input, axis=None, name=None, dimension=None, output_type=tf.int64) -> Tensor: ...
+    def argmin(input, axis=None, name=None, dimension=None, output_type=tf.int64) -> Tensor: ...
+    def asin(x, name=None) -> Tensor: ...
+    def asinh(x, name=None) -> Tensor: ...
+    def atan(x, name=None) -> Tensor: ...
+    def atan2(y, x, name=None) -> Tensor: ...
+    def atanh(x, name=None) -> Tensor: ...
+    def betainc(a, b, x, name=None) -> Tensor: ...
+    def cast(x, dtype, name=None) -> Tensor: ...
+    def ceil(x, name=None) -> Tensor: ...
+    def check_numerics(tensor, message, name=None) -> Tensor: ...
+    def clip_by_average_norm(t, clip_norm, name=None) -> Tensor: ...
+    def clip_by_norm(t, clip_norm, axes=None, name=None) -> Tensor: ...
+    def clip_by_value(t, clip_value_min, clip_value_max, name=None) -> Tensor: ...
+    def complex(real, imag, name=None) -> Tensor: ...
+    def conj(x, name=None) -> Tensor: ...
+    def cos(x, name=None) -> Tensor: ...
+    def cosh(x, name=None) -> Tensor: ...
+    def cross(a, b, name=None) -> Tensor: ...
+    def cumprod(x, axis=0, exclusive=False, reverse=False, name=None) -> Tensor: ...
+    def cumsum(x, axis=0, exclusive=False, reverse=False, name=None) -> Tensor: ...
+    def dequantize(input, min_range, max_range, mode='MIN_COMBINED', name=None) -> Tensor: ...
+    def diag(diagonal, name=None) -> Tensor: ...
+    def digamma(x, name=None) -> Tensor: ...
+    def div(x, y, name=None) -> Tensor: ...
+    def equal(x, y, name=None) -> Tensor: ...
+    def erf(x, name=None) -> Tensor: ...
+    def erfc(x, name=None) -> Tensor: ...
+    def exp(x, name=None) -> Tensor: ...
+    def expand_dims(input, axis=None, name=None, dim=None) -> Tensor: ...
+    def expm1(x, name=None) -> Tensor: ...
+    def fill(dims, value, name=None) -> Tensor: ...
+    def floor(x, name=None) -> Tensor: ...
+    def floor_div(x, y, name=None) -> Tensor: ...
+    def floordiv(x, y, name=None) -> Tensor: ...
+    def floormod(x, y, name=None) -> Tensor: ...
+    def gather(params, indices, validate_indices=None, name=None, axis=0) -> Tensor: ...
+    def gather_nd(params, indices, name=None) -> Tensor: ...
+    def greater(x, y, name=None) -> Tensor: ...
+    def greater_equal(x, y, name=None) -> Tensor: ...
+    def hessians(ys, xs, name='hessians', colocate_gradients_with_ops=False, gate_gradients=False, aggregation_method=None) -> Tensor: ...
+    def identity(input, name=None) -> Tensor: ...
+    def igamma(a, x, name=None) -> Tensor: ...
+    def igammac(a, x, name=None) -> Tensor: ...
+    def imag(input, name=None) -> Tensor: ...
+    def is_finite(x, name=None) -> Tensor: ...
+    def is_inf(x, name=None) -> Tensor: ...
+    def is_nan(x, name=None) -> Tensor: ...
+    def less(x, y, name=None) -> Tensor: ...
+    def less_equal(x, y, name=None) -> Tensor: ...
+    def lgamma(x, name=None) -> Tensor: ...
+    def log(x, name=None) -> Tensor: ...
+    def log1p(x, name=None) -> Tensor: ...
+    def logical_and(x, y, name=None) -> Tensor: ...
+    def logical_not(x, name=None) -> Tensor: ...
+    def logical_or(x, y, name=None) -> Tensor: ...
+    def matmul(a, b, transpose_a=False, transpose_b=False, adjoint_a=False, adjoint_b=False, a_is_sparse=False, b_is_sparse=False, name=None) -> Tensor: ...
+    def maximum(x, y, name=None) -> Tensor: ...
+    def meshgrid(*args, **kwargs) -> Tensor: ...
+    def minimum(x, y, name=None) -> Tensor: ...
+    def mod(x, y, name=None) -> Tensor: ...
+    def mul(x, y, name=None) -> Tensor: ...
+    def multiply(x, y, name=None) -> Tensor: ...
+    def negative(x, name=None) -> Tensor: ...
+    def norm(tensor, ord='euclidean', axis=None, keepdims=None, name=None) -> Tensor: ...
+    def not_equal(x, y, name=None) -> Tensor: ...
+    def one_hot(indices, depth, on_value=None, off_value=None, axis=None, dtype=None, name=None) -> Tensor: ...
+    def ones_like(tensor, dtype=None, name=None, optimize=True) -> Tensor: ...
+    def pad(tensor, paddings, mode='CONSTANT', name=None, constant_values=0) -> Tensor: ...
+    def polygamma(a, x, name=None) -> Tensor: ...
+    def pow(x, y, name=None) -> Tensor: ...
+    def quantize(input, min_range, max_range, T, mode='MIN_COMBINED', round_mode='HALF_AWAY_FROM_ZERO', name=None) -> Tensor: ...
+    def real(input, name=None) -> Tensor: ...
+    def realdiv(x, y, name=None) -> Tensor: ...
+    def reciprocal(x, name=None) -> Tensor: ...
+    def reduce_all(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_any(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_logsumexp(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_max(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_mean(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_min(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_prod(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_sum(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reshape(tensor, shape, name=None) -> Tensor: ...
+    def reverse(tensor, axis, name=None) -> Tensor: ...
+    def rint(x, name=None) -> Tensor: ...
+    def round(x, name=None) -> Tensor: ...
+    def rsqrt(x, name=None) -> Tensor: ...
+    def scatter_nd(indices, updates, shape, name=None) -> Tensor: ...
+    def sign(x, name=None) -> Tensor: ...
+    def sin(x, name=None) -> Tensor: ...
+    def sinh(x, name=None) -> Tensor: ...
+    def size(input, name=None, out_type=tf.int32) -> Tensor: ...
+    def slice(input_, begin, size, name=None) -> Tensor: ...
+    def sqrt(x, name=None) -> Tensor: ...
+    def square(x, name=None) -> Tensor: ...
+    def squeeze(input, axis=None, name=None, squeeze_dims=None) -> Tensor: ...
+    def stop_gradient(input, name=None) -> Tensor: ...
+    def sub(x, y, name=None) -> Tensor: ...
+    def subtract(x, y, name=None) -> Tensor: ...
+    def tan(x, name=None) -> Tensor: ...
+    def tensordot(a, b, axes, name=None) -> Tensor: ...
+    def tile(input, multiples, name=None) -> Tensor: ...
+    def to_bfloat16(x, name='ToBFloat16') -> Tensor: ...
+    def to_complex128(x, name='ToComplex128') -> Tensor: ...
+    def to_complex64(x, name='ToComplex64') -> Tensor: ...
+    def to_double(x, name='ToDouble') -> Tensor: ...
+    def to_float(x, name='ToFloat') -> Tensor: ...
+    def to_int32(x, name='ToInt32') -> Tensor: ...
+    def to_int64(x, name='ToInt64') -> Tensor: ...
+    def transpose(a, perm=None, name='transpose', conjugate=False) -> Tensor: ...
+    def truediv(x, y, name=None) -> Tensor: ...
+    def truncatediv(x, y, name=None) -> Tensor: ...
+    def truncatemod(x, y, name=None) -> Tensor: ...
+    def unique(x, out_idx=tf.int32, name=None) -> Tensor: ...
+    def where(condition, x=None, y=None, name=None) -> Tensor: ...
+    def zeros_like(tensor, dtype=None, name=None, optimize=True) -> Tensor: ...
+    def zeta(x, q, name=None) -> Tensor: ...
+
+    def __add__(self, other) -> Tensor: ...
+    def __sub__(self, other) -> Tensor: ...
+    def __mul__(self, other) -> Tensor: ...
+    def __rdiv__(self, other) -> Tensor: ...
+    def __itruediv__(self, other) -> Tensor: ...
+    def __rsub__(self, other) -> Tensor: ...
+    def __isub__(self, other) -> Tensor: ...
+    def __imul__(self, other) -> Tensor: ...
+    def __rmul__(self, other) -> Tensor: ...
+    def __radd__(self, other) -> Tensor: ...
\ No newline at end of file
diff --git a/experiments/gym_cheetah_1234/src/main.py b/experiments/gym_cheetah_1234/src/main.py
new file mode 100644
index 0000000..3e25d8b
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/main.py
@@ -0,0 +1,211 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import pickle
+from collections import deque
+import tensorflow as tf
+import numpy as np
+from tqdm import tqdm
+
+import lunzi.nn as nn
+from lunzi.Logger import logger
+from slbo.utils.average_meter import AverageMeter
+from slbo.utils.flags import FLAGS
+from slbo.utils.dataset import Dataset, gen_dtype
+from slbo.utils.OU_noise import OUNoise
+from slbo.utils.normalizer import Normalizers
+from slbo.utils.tf_utils import get_tf_config
+from slbo.utils.runner import Runner
+from slbo.policies.gaussian_mlp_policy import GaussianMLPPolicy
+from slbo.envs.virtual_env import VirtualEnv
+from slbo.dynamics_model import DynamicsModel
+from slbo.v_function.mlp_v_function import MLPVFunction
+from slbo.partial_envs import make_env
+from slbo.loss.multi_step_loss import MultiStepLoss
+from slbo.algos.TRPO import TRPO
+from slbo.random_net import RandomNet
+
+
+def evaluate(settings, tag):
+    for runner, policy, name in settings:
+        runner.reset()
+        _, ep_infos = runner.run(policy, FLAGS.rollout.n_test_samples)
+        returns = np.array([ep_info['return'] for ep_info in ep_infos])
+        logger.info('Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f', tag, name,
+                    len(returns), np.mean(returns), np.std(returns))
+
+
+def add_multi_step(src: Dataset, dst: Dataset):
+    n_envs = 1
+    dst.extend(src[:-n_envs])
+
+    ending = src[-n_envs:].copy()
+    ending.timeout = True
+    dst.extend(ending)
+
+
+def make_real_runner(n_envs):
+    from slbo.envs.batched_env import BatchedEnv
+    batched_env = BatchedEnv([make_env(FLAGS.env.id) for _ in range(n_envs)])
+    return Runner(batched_env, rescale_action=True, **FLAGS.runner.as_dict())
+
+
+def main():
+    FLAGS.set_seed()
+    FLAGS.freeze()
+
+    env = make_env(FLAGS.env.id)
+    dim_state = int(np.prod(env.observation_space.shape))
+    dim_action = int(np.prod(env.action_space.shape))
+
+    env.verify()
+
+    normalizers = Normalizers(dim_action=dim_action, dim_state=dim_state)
+
+    dtype = gen_dtype(env, 'state action next_state reward done timeout')
+    train_set = Dataset(dtype, FLAGS.rollout.max_buf_size)
+    dev_set = Dataset(dtype, FLAGS.rollout.max_buf_size)
+
+    policy = GaussianMLPPolicy(dim_state, dim_action, normalizer=normalizers.state, **FLAGS.policy.as_dict())
+    # batched noises
+    #noise = OUNoise(env.action_space, theta=FLAGS.OUNoise.theta, sigma=FLAGS.OUNoise.sigma, shape=(1, dim_action))
+    vfn = MLPVFunction(dim_state, [64, 64], normalizers.state)
+    model = DynamicsModel(dim_state, dim_action, normalizers, FLAGS.model.hidden_sizes)
+    random_net = RandomNet(dim_state, dim_action, normalizers, FLAGS.model.hidden_sizes)
+
+    virt_env = VirtualEnv(model, make_env(FLAGS.env.id), random_net, FLAGS.plan.n_envs,FLAGS.model.hidden_sizes[-1], 
+                            FLAGS.pc.bonus_scale,FLAGS.pc.lamb, opt_model=FLAGS.slbo.opt_model)
+    virt_runner = Runner(virt_env, **{**FLAGS.runner.as_dict(), 'max_steps': FLAGS.plan.max_steps})
+
+    criterion_map = {
+        'L1': nn.L1Loss(),
+        'L2': nn.L2Loss(),
+        'MSE': nn.MSELoss(),
+    }
+    criterion = criterion_map[FLAGS.model.loss]
+    loss_mod = MultiStepLoss(model, normalizers, dim_state, dim_action, criterion, FLAGS.model.multi_step)
+    loss_mod.build_backward(FLAGS.model.lr, FLAGS.model.weight_decay)
+    algo = TRPO(vfn=vfn, policy=policy, dim_state=dim_state, dim_action=dim_action, **FLAGS.TRPO.as_dict())
+
+    tf.get_default_session().run(tf.global_variables_initializer())
+
+    runners = {
+        'test': make_real_runner(4),
+        'collect': make_real_runner(1),
+        'dev': make_real_runner(1),
+        'train': make_real_runner(FLAGS.plan.n_envs) if FLAGS.algorithm == 'MF' else virt_runner,
+    }
+    settings = [(runners['test'], policy, 'Real Env'), (runners['train'], policy, 'Virt Env')]
+
+    saver = nn.ModuleDict({'policy': policy, 'model': model, 'vfn': vfn})
+    print(saver)
+
+    if FLAGS.ckpt.model_load:
+        saver.load_state_dict(np.load(FLAGS.ckpt.model_load)[()])
+        logger.warning('Load model from %s', FLAGS.ckpt.model_load)
+
+    if FLAGS.ckpt.buf_load:
+        n_samples = 0
+        for i in range(FLAGS.ckpt.buf_load_index):
+            data = pickle.load(open(f'{FLAGS.ckpt.buf_load}/stage-{i}.inc-buf.pkl', 'rb'))
+            add_multi_step(data, train_set)
+            n_samples += len(data)
+        logger.warning('Loading %d samples from %s', n_samples, FLAGS.ckpt.buf_load)
+
+    max_ent_coef = 0
+
+    for T in range(FLAGS.slbo.n_stages):
+        logger.info('------ Starting Stage %d --------', T)
+        evaluate(settings, 'episode')
+
+        if not FLAGS.use_prev:
+            train_set.clear()
+            dev_set.clear()
+
+        # collect data
+        recent_train_set, ep_infos = runners['collect'].run(policy, FLAGS.rollout.n_train_samples)
+        add_multi_step(recent_train_set, train_set)
+        add_multi_step(
+            runners['dev'].run(policy, FLAGS.rollout.n_dev_samples)[0],
+            dev_set,
+        )
+
+        returns = np.array([ep_info['return'] for ep_info in ep_infos])
+
+        if len(returns) > 0:
+            logger.info("episode: %s", np.mean(returns))
+
+        if T == 0:  # check
+            samples = train_set.sample_multi_step(100, 1, FLAGS.model.multi_step)
+            for i in range(FLAGS.model.multi_step - 1):
+                masks = 1 - (samples.done[i] | samples.timeout[i])[..., np.newaxis]
+                assert np.allclose(samples.state[i + 1] * masks, samples.next_state[i] * masks)
+
+        # recent_states = obsvs
+        # ref_actions = policy.eval('actions_mean actions_std', states=recent_states)
+        if FLAGS.rollout.normalizer == 'policy' or FLAGS.rollout.normalizer == 'uniform' and T == 0:
+            normalizers.state.update(recent_train_set.state)
+            normalizers.action.update(recent_train_set.action)
+            normalizers.diff.update(recent_train_set.next_state - recent_train_set.state)
+
+        virt_env.update_cov(recent_train_set.state,recent_train_set.action)
+
+        if T == 50:
+            virt_env.bonus_scale = 0.
+
+        for i in range(FLAGS.slbo.n_iters):
+            if i % FLAGS.slbo.n_evaluate_iters == 0 and i != 0:
+                # cur_actions = policy.eval('actions_mean actions_std', states=recent_states)
+                # kl_old_new = gaussian_kl(*ref_actions, *cur_actions).sum(axis=1).mean()
+                # logger.info('KL(old || cur) = %.6f', kl_old_new)
+                evaluate(settings, 'iteration')
+
+            losses = deque(maxlen=FLAGS.slbo.n_model_iters)
+            grad_norm_meter = AverageMeter()
+            n_model_iters = FLAGS.slbo.n_model_iters
+            for _ in range(n_model_iters):
+                samples = train_set.sample_multi_step(FLAGS.model.train_batch_size, 1, FLAGS.model.multi_step)
+                _, train_loss, grad_norm = loss_mod.get_loss(
+                    samples.state, samples.next_state, samples.action, ~samples.done & ~samples.timeout,
+                    fetch='train loss grad_norm')
+                losses.append(train_loss.mean())
+                grad_norm_meter.update(grad_norm)
+                # ideally, we should define an Optimizer class, which takes parameters as inputs.
+                # The `update` method of `Optimizer` will invalidate all parameters during updates.
+                for param in model.parameters():
+                    param.invalidate()
+
+            if i % FLAGS.model.validation_freq == 0:
+                samples = train_set.sample_multi_step(
+                    FLAGS.model.train_batch_size, 1, FLAGS.model.multi_step)
+                loss = loss_mod.get_loss(
+                    samples.state, samples.next_state, samples.action, ~samples.done & ~samples.timeout)
+                loss = loss.mean()
+                if np.isnan(loss) or np.isnan(np.mean(losses)):
+                    logger.info('nan! %s %s', np.isnan(loss), np.isnan(np.mean(losses)))
+                logger.info('# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f',
+                            i, np.mean(losses), loss, n_model_iters, grad_norm_meter.get())
+
+            for n_updates in tqdm(range(FLAGS.slbo.n_policy_iters)):
+                if FLAGS.algorithm != 'MF' and FLAGS.slbo.start == 'buffer':
+                    runners['train'].set_state(train_set.sample(FLAGS.plan.n_envs).state)
+                else:
+                    runners['train'].reset()
+
+                data, ep_infos = runners['train'].run(policy, FLAGS.plan.n_trpo_samples)
+                advantages, values = runners['train'].compute_advantage(vfn, data)
+                dist_mean, dist_std, vf_loss = algo.train(max_ent_coef, data, advantages, values)
+                returns = [info['return'] for info in ep_infos]
+                #logger.info('[TRPO] # %d: n_episodes = %d, returns: {mean = %.0f, std = %.0f}, '
+                #            'dist std = %.10f, dist mean = %.10f, vf_loss = %.3f',
+                #            n_updates, len(returns), np.mean(returns), np.std(returns) / np.sqrt(len(returns)),
+                #            dist_std, dist_mean, vf_loss)
+
+        if T % FLAGS.ckpt.n_save_stages == 0:
+            np.save(f'{FLAGS.log_dir}/stage-{T}', saver.state_dict())
+            np.save(f'{FLAGS.log_dir}/final', saver.state_dict())
+        if FLAGS.ckpt.n_save_stages == 1:
+            pickle.dump(recent_train_set, open(f'{FLAGS.log_dir}/stage-{T}.inc-buf.pkl', 'wb'))
+
+
+if __name__ == '__main__':
+    with tf.Session(config=get_tf_config()):
+        main()
diff --git a/experiments/gym_cheetah_1234/src/requirements.txt b/experiments/gym_cheetah_1234/src/requirements.txt
new file mode 100644
index 0000000..8991cc0
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/requirements.txt
@@ -0,0 +1,6 @@
+tensorflow
+numpy
+pyyaml
+termcolor
+gym
+json_tricks
diff --git a/experiments/gym_cheetah_1234/src/rllab_requirements.txt b/experiments/gym_cheetah_1234/src/rllab_requirements.txt
new file mode 100644
index 0000000..9fb2d37
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/rllab_requirements.txt
@@ -0,0 +1,7 @@
+theano
+cached_property
+pyopengl
+joblib
+mako
+mujoco_py
+
diff --git a/experiments/gym_cheetah_1234/src/run_experiments.sh b/experiments/gym_cheetah_1234/src/run_experiments.sh
new file mode 100644
index 0000000..19a7312
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/run_experiments.sh
@@ -0,0 +1,9 @@
+#!/usr/bin/env bash
+
+for env_name in $1; do
+    echo "=> Running environment ${env_name}"
+    for random_seed in 1234 2314 2345 1235; do
+        python main.py -c configs/algos/slbo_bm_200k.yml configs/env_tingwu/${env_name}.yml \
+	    -s log_dir=./experiments/${env_name}_${random_seed} seed=${random_seed}
+    done
+done
diff --git a/experiments/gym_cheetah_1234/src/slbo/__init__.py b/experiments/gym_cheetah_1234/src/slbo/__init__.py
new file mode 100644
index 0000000..5c7f19c
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/__init__.py
@@ -0,0 +1 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
diff --git a/experiments/gym_cheetah_1234/src/slbo/algos/TRPO.py b/experiments/gym_cheetah_1234/src/slbo/algos/TRPO.py
new file mode 100644
index 0000000..e8469e3
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/algos/TRPO.py
@@ -0,0 +1,183 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List, Callable
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi import Tensor
+from lunzi.Logger import logger
+from slbo.utils.dataset import Dataset
+from slbo.policies import BaseNNPolicy
+from slbo.v_function import BaseVFunction
+
+
+def average_l2_norm(x):
+    return np.sqrt((x**2).mean())
+
+
+# for damping, modify func_Ax
+def conj_grad(mat_mul_vec: Callable[[np.ndarray], np.ndarray], b, n_iters=10, residual_tol=1e-10, verbose=False):
+    p = b.copy()
+    r = b.copy()
+    x = np.zeros_like(b)
+    r_dot_r = r.dot(r)
+
+    for i in range(n_iters):
+        if verbose:
+            logger.info('[CG] iters = %d, |Res| = %.6f, |x| = %.6f', i, r_dot_r, np.linalg.norm(x))
+        z = mat_mul_vec(p)
+        v = r_dot_r / p.dot(z)
+        x += v * p
+        r -= v * z
+        new_r_dot_r = r.dot(r)
+        if new_r_dot_r < residual_tol:
+            break
+        mu = new_r_dot_r / r_dot_r
+        p = r + mu * p
+        r_dot_r = new_r_dot_r
+    return x
+
+
+class TRPO(nn.Module):
+    def __init__(self, dim_state: int, dim_action: int, policy: BaseNNPolicy, vfn: BaseVFunction, max_kl: float,
+                 n_cg_iters: int, ent_coef=0.0, cg_damping=0.01, vf_lr=3e-4, n_vf_iters=3):
+        super().__init__()
+        self.dim_state = dim_state
+        self.policy = policy
+        self.ent_coef = ent_coef
+        self.vf = vfn
+        self.n_cg_iters = n_cg_iters
+        self.max_kl = max_kl
+        self.cg_damping = cg_damping
+        self.n_vf_iters = n_vf_iters
+        self.vf_lr = vf_lr
+
+        # doing backtrace, so don't need to separate.
+        self.flatten = nn.FlatParam(self.policy.parameters())
+        self.old_policy: nn.Module = policy.clone()
+
+        with self.scope:
+            self.op_returns = tf.placeholder(dtype=tf.float32, shape=[None], name='returns')
+            self.op_advantages = tf.placeholder(dtype=tf.float32, shape=[None], name='advantages')
+            self.op_states = tf.placeholder(dtype=tf.float32, shape=[None, dim_state], name='states')
+            self.op_actions = tf.placeholder(dtype=tf.float32, shape=[None, dim_action], name='actions')
+            self.op_feed_params = tf.placeholder(dtype=tf.float32, shape=[None], name='feed_params')
+
+            self.op_tangents = tf.placeholder(
+                dtype=tf.float32, shape=[nn.utils.n_parameters(self.policy.parameters())])
+            self.op_ent_coef = tf.placeholder(dtype=tf.float32, shape=[], name='ent_coef')
+
+        self.op_mean_kl, self.op_loss, self.op_dist_std, self.op_dist_mean, self.op_policy_loss = \
+            self(self.op_states, self.op_actions, self.op_advantages, self.op_ent_coef)
+
+        self.op_sync_old, self.op_hessian_vec_prod, self.op_flat_grad = \
+            self.compute_natural_grad(self.op_loss, self.op_mean_kl, self.op_tangents)
+
+        self.op_vf_loss, self.op_train_vf = self.compute_vf(self.op_states, self.op_returns)
+
+    def forward(self, states, actions, advantages, ent_coef):
+        old_distribution: tf.distributions.Normal = self.old_policy(states)
+        distribution: tf.distributions.Normal = self.policy(states)
+        mean_kl = old_distribution.kl_divergence(distribution).reduce_sum(axis=1).reduce_mean()
+        entropy = distribution.entropy().reduce_sum(axis=1).reduce_mean()
+        entropy_bonus = ent_coef * entropy
+
+        ratios: Tensor = (distribution.log_prob(actions) - old_distribution.log_prob(actions)) \
+            .reduce_sum(axis=1).exp()
+        # didn't output op_policy_loss since in principle it should be 0.
+        policy_loss = ratios.mul(advantages).reduce_mean()
+
+        # We're doing Gradient Ascent so this is, in fact, gain.
+        loss = policy_loss + entropy_bonus
+
+        return mean_kl, loss, distribution.stddev().log().reduce_mean().exp(), \
+            distribution.mean().norm(axis=1).reduce_mean() / np.sqrt(10), policy_loss
+
+    def compute_natural_grad(self, loss, mean_kl, tangents):
+        params = self.policy.parameters()
+        old_params = self.old_policy.parameters()
+        hessian_vec_prod = nn.utils.hessian_vec_prod(mean_kl, params, tangents)
+        flat_grad = nn.utils.parameters_to_vector(tf.gradients(loss, params))
+        sync_old = tf.group(*[tf.assign(old_v, new_v) for old_v, new_v in zip(old_params, params)])
+
+        return sync_old, hessian_vec_prod, flat_grad
+
+    def compute_vf(self, states, returns):
+        vf_loss = nn.MSELoss()(self.vf(states), returns).reduce_mean()
+        optimizer = tf.train.AdamOptimizer(self.vf_lr)
+        train_vf = optimizer.minimize(vf_loss)
+
+        return vf_loss, train_vf
+
+    @nn.make_method()
+    def get_vf_loss(self, states, returns) -> List[np.ndarray]: pass
+
+    @nn.make_method(fetch='sync_old')
+    def sync_old(self) -> List[np.ndarray]: pass
+
+    @nn.make_method(fetch='hessian_vec_prod')
+    def get_hessian_vec_prod(self, states, tangents, actions) -> List[np.ndarray]: pass
+
+    @nn.make_method(fetch='loss')
+    def get_loss(self, states, actions, advantages, ent_coef) -> List[np.ndarray]: pass
+
+    def train(self, ent_coef, samples, advantages, values):
+        returns = advantages + values
+        advantages = (advantages - advantages.mean()) / np.maximum(advantages.std(), 1e-8)
+        assert np.isfinite(advantages).all()
+        self.sync_old()
+        old_loss, grad, dist_std, mean_kl, dist_mean = self.get_loss(
+            samples.state, samples.action, advantages, ent_coef, fetch='loss flat_grad dist_std mean_kl dist_mean')
+
+        if np.allclose(grad, 0):
+            logger.info('Zero gradient, not updating...')
+            return
+
+        def fisher_vec_prod(x):
+            return self.get_hessian_vec_prod(samples.state, x, samples.action) + self.cg_damping * x
+
+        assert np.isfinite(grad).all()
+        nat_grad = conj_grad(fisher_vec_prod, grad, n_iters=self.n_cg_iters, verbose=False)
+
+        assert np.isfinite(nat_grad).all()
+
+        old_params = self.flatten.get_flat()
+        step_size = np.sqrt(2 * self.max_kl / nat_grad.dot(fisher_vec_prod(nat_grad)))
+
+        for _ in range(10):
+            new_params = old_params + nat_grad * step_size
+            self.flatten.set_flat(new_params)
+            loss, mean_kl = self.get_loss(samples.state, samples.action, advantages, ent_coef, fetch='loss mean_kl')
+            improve = loss - old_loss
+            if not np.isfinite([loss, mean_kl]).all():
+                logger.info('Got non-finite loss.')
+            elif mean_kl > self.max_kl * 1.5:
+                logger.info('Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f',
+                            mean_kl, self.max_kl)
+            elif improve < 0:
+                logger.info("Surrogate didn't improve, shrinking step... %.6f => %.6f", old_loss, loss)
+            else:
+                break
+            step_size *= 0.5
+        else:
+            logger.info("Couldn't find a good step.")
+            self.flatten.set_flat(old_params)
+        for param in self.policy.parameters():
+            param.invalidate()
+
+        # optimize value function
+        vf_dataset = Dataset.fromarrays([samples.state, returns],
+                                        dtype=[('state', ('f8', self.dim_state)), ('return_', 'f8')])
+        vf_loss = self.train_vf(vf_dataset)
+
+        return dist_mean, dist_std, vf_loss
+
+    def train_vf(self, dataset: Dataset):
+        for _ in range(self.n_vf_iters):
+            for subset in dataset.iterator(64):
+                self.get_vf_loss(subset.state, subset.return_, fetch='train_vf vf_loss')
+        for param in self.parameters():
+            param.invalidate()
+        vf_loss = self.get_vf_loss(dataset.state, dataset.return_, fetch='vf_loss')
+        return vf_loss
+
+
diff --git a/experiments/gym_cheetah_1234/src/slbo/algos/__init__.py b/experiments/gym_cheetah_1234/src/slbo/algos/__init__.py
new file mode 100644
index 0000000..5c7f19c
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/algos/__init__.py
@@ -0,0 +1 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
diff --git a/experiments/gym_cheetah_1234/src/slbo/dynamics_model.py b/experiments/gym_cheetah_1234/src/slbo/dynamics_model.py
new file mode 100644
index 0000000..19662ec
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/dynamics_model.py
@@ -0,0 +1,43 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List
+import tensorflow as tf
+from lunzi import Tensor
+import lunzi.nn as nn
+from slbo.utils.normalizer import Normalizers
+from slbo.utils.multi_layer_perceptron import MultiLayerPerceptron
+
+
+class DynamicsModel(MultiLayerPerceptron):
+    op_loss: Tensor
+    op_train: Tensor
+    op_grad_norm: Tensor
+
+    def __init__(self, dim_state: int, dim_action: int, normalizers: Normalizers, hidden_sizes: List[int]):
+        initializer = tf.truncated_normal_initializer(mean=0.0, stddev=1e-5)
+
+        self.dim_state = dim_state
+        self.dim_action = dim_action
+        self.hidden_sizes = hidden_sizes
+        self.op_states = tf.placeholder(tf.float32, shape=[None, self.dim_state], name='states')
+        self.op_actions = tf.placeholder(tf.float32, shape=[None, self.dim_action], name='actions')
+        super().__init__([dim_state + dim_action, *hidden_sizes, dim_state],
+                         activation=nn.ReLU,
+                         weight_initializer=initializer, build=False)
+
+        self.normalizers = normalizers
+        self.build()
+
+    def build(self):
+        self.op_next_states = self.forward(self.op_states, self.op_actions)
+
+    def forward(self, states, actions):
+        assert actions.shape[-1] == self.dim_action
+        inputs = tf.concat([self.normalizers.state(states), actions.clip_by_value(-1., 1.)], axis=1)
+
+        normalized_diffs = super().forward(inputs)
+        next_states = states + self.normalizers.diff(normalized_diffs, inverse=True)
+        next_states = self.normalizers.state(self.normalizers.state(next_states).clip_by_value(-100, 100), inverse=True)
+        return next_states
+
+    def clone(self):
+        return DynamicsModel(self.dim_state, self.dim_action, self.normalizers, self.hidden_sizes)
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/__init__.py b/experiments/gym_cheetah_1234/src/slbo/envs/__init__.py
new file mode 100644
index 0000000..7efdf01
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/__init__.py
@@ -0,0 +1,55 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+import abc
+import gym
+from slbo.utils.dataset import Dataset, gen_dtype
+from lunzi.Logger import logger
+
+
+class BaseBatchedEnv(gym.Env, abc.ABC):
+    # thought about using `@property @abc.abstractmethod` here but we don't need explicit `@property` function here.
+    n_envs: int
+
+    @abc.abstractmethod
+    def step(self, actions):
+        pass
+
+    def reset(self):
+        return self.partial_reset(range(self.n_envs))
+
+    @abc.abstractmethod
+    def partial_reset(self, indices):
+        pass
+
+    def set_state(self, state):
+        logger.warning('`set_state` is not implemented')
+
+
+class BaseModelBasedEnv(gym.Env, abc.ABC):
+    @abc.abstractmethod
+    def mb_step(self, states: np.ndarray, actions: np.ndarray, next_states: np.ndarray):
+        raise NotImplementedError
+
+    def verify(self, n=2000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
+
+    def seed(self, seed: int = None):
+        pass
+
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/batched_env.py b/experiments/gym_cheetah_1234/src/slbo/envs/batched_env.py
new file mode 100644
index 0000000..cbe4bac
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/batched_env.py
@@ -0,0 +1,37 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from gym import Wrapper
+from . import BaseBatchedEnv
+
+
+class BatchedEnv(BaseBatchedEnv, Wrapper):
+    def __init__(self, envs):
+        super().__init__(envs[0])
+        self.envs = envs
+        self.n_envs = len(envs)
+
+    def step(self, actions):
+
+        buf, infos = [], []
+        for env, action in zip(self.envs, actions):
+            next_state, reward, done, info = env.step(action)
+            buf.append((next_state, reward, done))
+            infos.append(info)
+
+        return [*(np.array(x) for x in zip(*buf)), infos]
+
+    def reset(self):
+        return self.partial_reset(range(self.n_envs))
+
+    def partial_reset(self, indices):
+        states = []
+        for index in indices:
+            states.append(self.envs[index].reset())
+        return np.array(states)
+
+    def __repr__(self):
+        return f'Batch<{self.n_envs}x {self.env}>'
+
+    def set_state(self, state):
+        pass
+
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/__init__.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/assets/ant.xml b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/assets/ant.xml
new file mode 100644
index 0000000..18ad38b
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/assets/ant.xml
@@ -0,0 +1,80 @@
+<mujoco model="ant">
+  <compiler angle="degree" coordinate="local" inertiafromgeom="true"/>
+  <option integrator="RK4" timestep="0.01"/>
+  <custom>
+    <numeric data="0.0 0.0 0.55 1.0 0.0 0.0 0.0 0.0 1.0 0.0 -1.0 0.0 -1.0 0.0 1.0" name="init_qpos"/>
+  </custom>
+  <default>
+    <joint armature="1" damping="1" limited="true"/>
+    <geom conaffinity="0" condim="3" density="5.0" friction="1 0.5 0.5" margin="0.01" rgba="0.8 0.6 0.4 1"/>
+  </default>
+  <asset>
+    <texture builtin="gradient" height="100" rgb1="1 1 1" rgb2="0 0 0" type="skybox" width="100"/>
+    <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+    <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+    <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="60 60" texture="texplane"/>
+    <material name="geom" texture="texgeom" texuniform="true"/>
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="40 40 40" type="plane"/>
+    <body name="torso" pos="0 0 0.75">
+      <geom name="torso_geom" pos="0 0 0" size="0.25" type="sphere"/>
+      <joint armature="0" damping="0" limited="false" margin="0.01" name="root" pos="0 0 0" type="free"/>
+      <body name="front_left_leg" pos="0 0 0">
+        <geom fromto="0.0 0.0 0.0 0.2 0.2 0.0" name="aux_1_geom" size="0.08" type="capsule"/>
+        <body name="aux_1" pos="0.2 0.2 0">
+          <joint axis="0 0 1" name="hip_1" pos="0.0 0.0 0.0" range="-30 30" type="hinge"/>
+          <geom fromto="0.0 0.0 0.0 0.2 0.2 0.0" name="left_leg_geom" size="0.08" type="capsule"/>
+          <body pos="0.2 0.2 0">
+            <joint axis="-1 1 0" name="ankle_1" pos="0.0 0.0 0.0" range="30 70" type="hinge"/>
+            <geom fromto="0.0 0.0 0.0 0.4 0.4 0.0" name="left_ankle_geom" size="0.08" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <body name="front_right_leg" pos="0 0 0">
+        <geom fromto="0.0 0.0 0.0 -0.2 0.2 0.0" name="aux_2_geom" size="0.08" type="capsule"/>
+        <body name="aux_2" pos="-0.2 0.2 0">
+          <joint axis="0 0 1" name="hip_2" pos="0.0 0.0 0.0" range="-30 30" type="hinge"/>
+          <geom fromto="0.0 0.0 0.0 -0.2 0.2 0.0" name="right_leg_geom" size="0.08" type="capsule"/>
+          <body pos="-0.2 0.2 0">
+            <joint axis="1 1 0" name="ankle_2" pos="0.0 0.0 0.0" range="-70 -30" type="hinge"/>
+            <geom fromto="0.0 0.0 0.0 -0.4 0.4 0.0" name="right_ankle_geom" size="0.08" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <body name="back_leg" pos="0 0 0">
+        <geom fromto="0.0 0.0 0.0 -0.2 -0.2 0.0" name="aux_3_geom" size="0.08" type="capsule"/>
+        <body name="aux_3" pos="-0.2 -0.2 0">
+          <joint axis="0 0 1" name="hip_3" pos="0.0 0.0 0.0" range="-30 30" type="hinge"/>
+          <geom fromto="0.0 0.0 0.0 -0.2 -0.2 0.0" name="back_leg_geom" size="0.08" type="capsule"/>
+          <body pos="-0.2 -0.2 0">
+            <joint axis="-1 1 0" name="ankle_3" pos="0.0 0.0 0.0" range="-70 -30" type="hinge"/>
+            <geom fromto="0.0 0.0 0.0 -0.4 -0.4 0.0" name="third_ankle_geom" size="0.08" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <body name="right_back_leg" pos="0 0 0">
+        <geom fromto="0.0 0.0 0.0 0.2 -0.2 0.0" name="aux_4_geom" size="0.08" type="capsule"/>
+        <body name="aux_4" pos="0.2 -0.2 0">
+          <joint axis="0 0 1" name="hip_4" pos="0.0 0.0 0.0" range="-30 30" type="hinge"/>
+          <geom fromto="0.0 0.0 0.0 0.2 -0.2 0.0" name="rightback_leg_geom" size="0.08" type="capsule"/>
+          <body pos="0.2 -0.2 0">
+            <joint axis="1 1 0" name="ankle_4" pos="0.0 0.0 0.0" range="30 70" type="hinge"/>
+            <geom fromto="0.0 0.0 0.0 0.4 -0.4 0.0" name="fourth_ankle_geom" size="0.08" type="capsule"/>
+          </body>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="hip_4" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="ankle_4" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="hip_1" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="ankle_1" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="hip_2" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="ankle_2" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="hip_3" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="ankle_3" gear="150"/>
+  </actuator>
+</mujoco>
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/assets/half_cheetah.xml b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/assets/half_cheetah.xml
new file mode 100644
index 0000000..b07aada
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/assets/half_cheetah.xml
@@ -0,0 +1,95 @@
+<!-- Cheetah Model
+
+    The state space is populated with joints in the order that they are
+    defined in this file. The actuators also operate on joints.
+
+    State-Space (name/joint/parameter):
+        - rootx     slider      position (m)
+        - rootz     slider      position (m)
+        - rooty     hinge       angle (rad)
+        - bthigh    hinge       angle (rad)
+        - bshin     hinge       angle (rad)
+        - bfoot     hinge       angle (rad)
+        - fthigh    hinge       angle (rad)
+        - fshin     hinge       angle (rad)
+        - ffoot     hinge       angle (rad)
+        - rootx     slider      velocity (m/s)
+        - rootz     slider      velocity (m/s)
+        - rooty     hinge       angular velocity (rad/s)
+        - bthigh    hinge       angular velocity (rad/s)
+        - bshin     hinge       angular velocity (rad/s)
+        - bfoot     hinge       angular velocity (rad/s)
+        - fthigh    hinge       angular velocity (rad/s)
+        - fshin     hinge       angular velocity (rad/s)
+        - ffoot     hinge       angular velocity (rad/s)
+
+    Actuators (name/actuator/parameter):
+        - bthigh    hinge       torque (N m)
+        - bshin     hinge       torque (N m)
+        - bfoot     hinge       torque (N m)
+        - fthigh    hinge       torque (N m)
+        - fshin     hinge       torque (N m)
+        - ffoot     hinge       torque (N m)
+
+-->
+<mujoco model="cheetah">
+  <compiler angle="radian" coordinate="local" inertiafromgeom="true" settotalmass="14"/>
+  <default>
+    <joint armature=".1" damping=".01" limited="true" solimplimit="0 .8 .03" solreflimit=".02 1" stiffness="8"/>
+    <geom conaffinity="0" condim="3" contype="1" friction=".4 .1 .1" rgba="0.8 0.6 .4 1" solimp="0.0 0.8 0.01" solref="0.02 1"/>
+    <motor ctrllimited="true" ctrlrange="-1 1"/>
+  </default>
+  <size nstack="300000" nuser_geom="1"/>
+  <option gravity="0 0 -9.81" timestep="0.01"/>
+  <asset>
+    <texture builtin="gradient" height="100" rgb1="1 1 1" rgb2="0 0 0" type="skybox" width="100"/>
+    <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+    <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+    <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="60 60" texture="texplane"/>
+    <material name="geom" texture="texgeom" texuniform="true"/>
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="40 40 40" type="plane"/>
+    <body name="torso" pos="0 0 .7">
+      <joint armature="0" axis="1 0 0" damping="0" limited="false" name="rootx" pos="0 0 0" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 0 1" damping="0" limited="false" name="rootz" pos="0 0 0" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 1 0" damping="0" limited="false" name="rooty" pos="0 0 0" stiffness="0" type="hinge"/>
+      <geom fromto="-.5 0 0 .5 0 0" name="torso" size="0.046" type="capsule"/>
+      <geom axisangle="0 1 0 .87" name="head" pos=".6 0 .1" size="0.046 .15" type="capsule"/>
+      <!-- <site name='tip'  pos='.15 0 .11'/>-->
+      <body name="bthigh" pos="-.5 0 0">
+        <joint axis="0 1 0" damping="6" name="bthigh" pos="0 0 0" range="-.52 1.05" stiffness="240" type="hinge"/>
+        <geom axisangle="0 1 0 -3.8" name="bthigh" pos=".1 0 -.13" size="0.046 .145" type="capsule"/>
+        <body name="bshin" pos=".16 0 -.25">
+          <joint axis="0 1 0" damping="4.5" name="bshin" pos="0 0 0" range="-.785 .785" stiffness="180" type="hinge"/>
+          <geom axisangle="0 1 0 -2.03" name="bshin" pos="-.14 0 -.07" rgba="0.9 0.6 0.6 1" size="0.046 .15" type="capsule"/>
+          <body name="bfoot" pos="-.28 0 -.14">
+            <joint axis="0 1 0" damping="3" name="bfoot" pos="0 0 0" range="-.4 .785" stiffness="120" type="hinge"/>
+            <geom axisangle="0 1 0 -.27" name="bfoot" pos=".03 0 -.097" rgba="0.9 0.6 0.6 1" size="0.046 .094" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <body name="fthigh" pos=".5 0 0">
+        <joint axis="0 1 0" damping="4.5" name="fthigh" pos="0 0 0" range="-1 .7" stiffness="180" type="hinge"/>
+        <geom axisangle="0 1 0 .52" name="fthigh" pos="-.07 0 -.12" size="0.046 .133" type="capsule"/>
+        <body name="fshin" pos="-.14 0 -.24">
+          <joint axis="0 1 0" damping="3" name="fshin" pos="0 0 0" range="-1.2 .87" stiffness="120" type="hinge"/>
+          <geom axisangle="0 1 0 -.6" name="fshin" pos=".065 0 -.09" rgba="0.9 0.6 0.6 1" size="0.046 .106" type="capsule"/>
+          <body name="ffoot" pos=".13 0 -.18">
+            <joint axis="0 1 0" damping="1.5" name="ffoot" pos="0 0 0" range="-.5 .5" stiffness="60" type="hinge"/>
+            <geom axisangle="0 1 0 -.6" name="ffoot" pos=".045 0 -.07" rgba="0.9 0.6 0.6 1" size="0.046 .07" type="capsule"/>
+          </body>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor gear="120" joint="bthigh" name="bthigh"/>
+    <motor gear="90" joint="bshin" name="bshin"/>
+    <motor gear="60" joint="bfoot" name="bfoot"/>
+    <motor gear="120" joint="fthigh" name="fthigh"/>
+    <motor gear="60" joint="fshin" name="fshin"/>
+    <motor gear="30" joint="ffoot" name="ffoot"/>
+  </actuator>
+</mujoco>
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/assets/hopper.xml b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/assets/hopper.xml
new file mode 100644
index 0000000..b0ebc0e
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/assets/hopper.xml
@@ -0,0 +1,44 @@
+<mujoco model="hopper">
+  <compiler angle="degree" coordinate="global" inertiafromgeom="true"/>
+  <default>
+    <joint armature="1" damping="1" limited="true"/>
+    <geom conaffinity="1" condim="1" contype="1" margin="0.001" material="geom" rgba="0.8 0.6 .4 1" solimp=".8 .8 .01" solref=".02 1"/>
+    <motor ctrllimited="true" ctrlrange="-.4 .4"/>
+  </default>
+  <option integrator="RK4" timestep="0.002"/>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="20 20 .125" type="plane" material="MatPlane"/>
+    <body name="torso" pos="0 0 1.25">
+      <joint armature="0" axis="1 0 0" damping="0" limited="false" name="rootx" pos="0 0 0" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 0 1" damping="0" limited="false" name="rootz" pos="0 0 0" ref="1.25" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 1 0" damping="0" limited="false" name="rooty" pos="0 0 1.25" stiffness="0" type="hinge"/>
+      <geom friction="0.9" fromto="0 0 1.45 0 0 1.05" name="torso_geom" size="0.05" type="capsule"/>
+      <body name="thigh" pos="0 0 1.05">
+        <joint axis="0 -1 0" name="thigh_joint" pos="0 0 1.05" range="-150 0" type="hinge"/>
+        <geom friction="0.9" fromto="0 0 1.05 0 0 0.6" name="thigh_geom" size="0.05" type="capsule"/>
+        <body name="leg" pos="0 0 0.35">
+          <joint axis="0 -1 0" name="leg_joint" pos="0 0 0.6" range="-150 0" type="hinge"/>
+          <geom friction="0.9" fromto="0 0 0.6 0 0 0.1" name="leg_geom" size="0.04" type="capsule"/>
+          <body name="foot" pos="0.13/2 0 0.1">
+            <joint axis="0 -1 0" name="foot_joint" pos="0 0 0.1" range="-45 45" type="hinge"/>
+            <geom friction="2.0" fromto="-0.13 0 0.1 0.26 0 0.1" name="foot_geom" size="0.06" type="capsule"/>
+          </body>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="thigh_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="leg_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="foot_joint"/>
+  </actuator>
+    <asset>
+        <texture type="skybox" builtin="gradient" rgb1=".4 .5 .6" rgb2="0 0 0"
+            width="100" height="100"/>        
+        <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+        <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+        <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="60 60" texture="texplane"/>
+        <material name="geom" texture="texgeom" texuniform="true"/>
+    </asset>
+</mujoco>
\ No newline at end of file
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/assets/point.xml b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/assets/point.xml
new file mode 100644
index 0000000..e35ef3d
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/assets/point.xml
@@ -0,0 +1,31 @@
+<mujoco>
+  <compiler angle="degree" coordinate="local" inertiafromgeom="true"/>
+  <option integrator="RK4" timestep="0.02"/>
+  <default>
+    <joint armature="0" damping="0" limited="false"/>
+    <geom conaffinity="0" condim="3" density="100" friction="1 0.5 0.5" margin="0" rgba="0.8 0.6 0.4 1"/>
+  </default>
+  <asset>
+    <texture builtin="gradient" height="100" rgb1="1 1 1" rgb2="0 0 0" type="skybox" width="100"/>
+    <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+    <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+    <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="30 30" texture="texplane"/>
+    <material name="geom" texture="texgeom" texuniform="true"/>
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="40 40 40" type="plane"/>
+    <body name="torso" pos="0 0 0">
+      <geom name="pointbody" pos="0 0 0.5" size="0.5" type="sphere"/>
+      <geom name="pointarrow" pos="0.6 0 0.5" size="0.5 0.1 0.1" type="box"/>
+      <joint axis="1 0 0" name="ballx" pos="0 0 0" type="slide"/>
+      <joint axis="0 1 0" name="bally" pos="0 0 0" type="slide"/>
+      <joint axis="0 0 1" limited="false" name="rot" pos="0 0 0" type="hinge"/>
+    </body>
+  </worldbody>
+  <actuator>
+    <!-- Those are just dummy actuators for providing ranges -->
+    <motor ctrllimited="true" ctrlrange="-1 1" joint="ballx"/>
+    <motor ctrllimited="true" ctrlrange="-0.25 0.25" joint="rot"/>
+  </actuator>
+</mujoco>
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/assets/pusher.xml b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/assets/pusher.xml
new file mode 100644
index 0000000..31a5ef7
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/assets/pusher.xml
@@ -0,0 +1,91 @@
+<mujoco model="arm3d">
+  <compiler inertiafromgeom="true" angle="radian" coordinate="local"/>
+  <option timestep="0.01" gravity="0 0 0" iterations="20" integrator="Euler" />
+  
+  <default>
+    <joint armature='0.04' damping="1" limited="true"/>
+    <geom friction=".8 .1 .1" density="300" margin="0.002" condim="1" contype="0" conaffinity="0"/>
+  </default>
+  
+  <worldbody>
+    <light diffuse=".5 .5 .5" pos="0 0 3" dir="0 0 -1"/>
+    <geom name="table" type="plane" pos="0 0.5 -0.325" size="1 1 0.1" contype="1" conaffinity="1"/>
+
+    <body name="r_shoulder_pan_link" pos="0 -0.6 0">
+      <geom name="e1" type="sphere" rgba="0.6 0.6 0.6 1" pos="-0.06 0.05 0.2" size="0.05" />
+      <geom name="e2" type="sphere" rgba="0.6 0.6 0.6 1" pos=" 0.06 0.05 0.2" size="0.05" />
+      <geom name="e1p" type="sphere" rgba="0.1 0.1 0.1 1" pos="-0.06 0.09 0.2" size="0.03" />
+      <geom name="e2p" type="sphere" rgba="0.1 0.1 0.1 1" pos=" 0.06 0.09 0.2" size="0.03" />
+      <geom name="sp" type="capsule" fromto="0 0 -0.4 0 0 0.2" size="0.1" />
+      <joint name="r_shoulder_pan_joint" type="hinge" pos="0 0 0" axis="0 0 1" range="-2.2854 1.714602" damping="1.0" />
+
+      <body name="r_shoulder_lift_link" pos="0.1 0 0">
+        <geom name="sl" type="capsule" fromto="0 -0.1 0 0 0.1 0" size="0.1" />
+        <joint name="r_shoulder_lift_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.5236 1.3963" damping="1.0" />
+
+        <body name="r_upper_arm_roll_link" pos="0 0 0">
+          <geom name="uar" type="capsule" fromto="-0.1 0 0 0.1 0 0" size="0.02" />
+          <joint name="r_upper_arm_roll_joint" type="hinge" pos="0 0 0" axis="1 0 0" range="-1.5 1.7" damping="0.1" />
+
+          <body name="r_upper_arm_link" pos="0 0 0">
+            <geom name="ua" type="capsule" fromto="0 0 0 0.4 0 0" size="0.06" />
+
+            <body name="r_elbow_flex_link" pos="0.4 0 0">
+              <geom name="ef" type="capsule" fromto="0 -0.02 0 0.0 0.02 0" size="0.06" />
+              <joint name="r_elbow_flex_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-2.3213 0" damping="0.1" />
+
+              <body name="r_forearm_roll_link" pos="0 0 0">
+                <geom name="fr" type="capsule" fromto="-0.1 0 0 0.1 0 0" size="0.02" />
+                <joint name="r_forearm_roll_joint" type="hinge" limited="true" pos="0 0 0" axis="1 0 0" damping=".1" range="-1.5 1.5"/>
+
+                <body name="r_forearm_link" pos="0 0 0">
+                  <geom name="fa" type="capsule" fromto="0 0 0 0.291 0 0" size="0.05" />
+
+                  <body name="r_wrist_flex_link" pos="0.321 0 0">
+                    <geom name="wf" type="capsule" fromto="0 -0.02 0 0 0.02 0" size="0.01" />
+                    <joint name="r_wrist_flex_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-1.094 0" damping=".1" />
+
+                    <body name="r_wrist_roll_link" pos="0 0 0">
+                      <joint name="r_wrist_roll_joint" type="hinge" pos="0 0 0" limited="true" axis="1 0 0" damping="0.1" range="-1.5 1.5"/>
+                      <body name="tips_arm" pos="0 0 0">
+                        <geom name="tip_arml" type="sphere" pos="0.1 -0.1 0." size="0.01" />
+                        <geom name="tip_armr" type="sphere" pos="0.1 0.1 0." size="0.01" />
+                      </body>
+                      <geom type="capsule" fromto="0 -0.1 0. 0.0 +0.1 0" size="0.02" contype="1" conaffinity="1" />
+                      <geom type="capsule" fromto="0 -0.1 0. 0.1 -0.1 0" size="0.02" contype="1" conaffinity="1" />
+                      <geom type="capsule" fromto="0 +0.1 0. 0.1 +0.1 0." size="0.02" contype="1" conaffinity="1" />
+                    </body>
+                  </body>
+                </body>
+              </body>
+            </body>
+          </body>
+        </body>
+      </body>
+    </body>
+
+    <!--<body name="object" pos="0.55 -0.3 -0.275" >-->
+    <body name="object" pos="0.45 -0.05 -0.275" >
+      <geom rgba="1 1 1 0" type="sphere" size="0.05 0.05 0.05" density="0.00001" conaffinity="0"/>
+      <geom rgba="1 1 1 1" type="cylinder" size="0.05 0.05 0.05" density="0.00001" contype="1" conaffinity="0"/>
+      <joint name="obj_slidey" type="slide" pos="0 0 0" axis="0 1 0" range="-10.3213 10.3" damping="0.5"/>
+      <joint name="obj_slidex" type="slide" pos="0 0 0" axis="1 0 0" range="-10.3213 10.3" damping="0.5"/>
+    </body>
+
+    <body name="goal" pos="0.45 -0.05 -0.3230">
+      <geom rgba="1 0 0 1" type="cylinder" size="0.08 0.001 0.1" density='0.00001' contype="0" conaffinity="0"/>
+      <joint name="goal_slidey" type="slide" pos="0 0 0" axis="0 1 0" range="-10.3213 10.3" damping="0.5"/>
+      <joint name="goal_slidex" type="slide" pos="0 0 0" axis="1 0 0" range="-10.3213 10.3" damping="0.5"/> 
+    </body>
+  </worldbody>
+
+  <actuator>
+    <motor joint="r_shoulder_pan_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_shoulder_lift_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_upper_arm_roll_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_elbow_flex_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_forearm_roll_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_wrist_flex_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_wrist_roll_joint" ctrlrange="-2.0 2.0" ctrllimited="true"/>
+  </actuator>
+</mujoco>
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/assets/reacher.xml b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/assets/reacher.xml
new file mode 100644
index 0000000..64a67b9
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/assets/reacher.xml
@@ -0,0 +1,39 @@
+<mujoco model="reacher">
+	<compiler angle="radian" inertiafromgeom="true"/>
+	<default>
+		<joint armature="1" damping="1" limited="true"/>
+		<geom contype="0" friction="1 0.1 0.1" rgba="0.7 0.7 0 1"/>
+	</default>
+	<option gravity="0 0 -9.81" integrator="RK4" timestep="0.01"/>
+	<worldbody>
+		<!-- Arena -->
+		<geom conaffinity="0" contype="0" name="ground" pos="0 0 0" rgba="0.9 0.9 0.9 1" size="1 1 10" type="plane"/>
+		<geom conaffinity="0" fromto="-.3 -.3 .01 .3 -.3 .01" name="sideS" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
+		<geom conaffinity="0" fromto=" .3 -.3 .01 .3  .3 .01" name="sideE" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
+		<geom conaffinity="0" fromto="-.3  .3 .01 .3  .3 .01" name="sideN" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
+		<geom conaffinity="0" fromto="-.3 -.3 .01 -.3 .3 .01" name="sideW" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
+		<!-- Arm -->
+		<geom conaffinity="0" contype="0" fromto="0 0 0 0 0 0.02" name="root" rgba="0.9 0.4 0.6 1" size=".011" type="cylinder"/>
+		<body name="body0" pos="0 0 .01">
+			<geom fromto="0 0 0 0.1 0 0" name="link0" rgba="0.0 0.4 0.6 1" size=".01" type="capsule"/>
+			<joint axis="0 0 1" limited="false" name="joint0" pos="0 0 0" type="hinge"/>
+			<body name="body1" pos="0.1 0 0">
+				<joint axis="0 0 1" limited="true" name="joint1" pos="0 0 0" range="-3.0 3.0" type="hinge"/>
+				<geom fromto="0 0 0 0.1 0 0" name="link1" rgba="0.0 0.4 0.6 1" size=".01" type="capsule"/>
+				<body name="fingertip" pos="0.11 0 0">
+					<geom contype="0" name="fingertip" pos="0 0 0" rgba="0.0 0.8 0.6 1" size=".01" type="sphere"/>
+				</body>
+			</body>
+		</body>
+		<!-- Target -->
+		<body name="target" pos=".1 -.1 .01">
+			<joint armature="0" axis="1 0 0" damping="0" limited="true" name="target_x" pos="0 0 0" range="-.27 .27" ref=".1" stiffness="0" type="slide"/>
+			<joint armature="0" axis="0 1 0" damping="0" limited="true" name="target_y" pos="0 0 0" range="-.27 .27" ref="-.1" stiffness="0" type="slide"/>
+			<geom conaffinity="0" contype="0" name="target" pos="0 0 0" rgba="0.9 0.2 0.2 1" size=".009" type="sphere"/>
+		</body>
+	</worldbody>
+	<actuator>
+		<motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="joint0"/>
+		<motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="joint1"/>
+	</actuator>
+</mujoco>
\ No newline at end of file
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/assets/swimmer.xml b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/assets/swimmer.xml
new file mode 100644
index 0000000..cda25da
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/assets/swimmer.xml
@@ -0,0 +1,38 @@
+<mujoco model="swimmer">
+  <compiler angle="degree" coordinate="local" inertiafromgeom="true"/>
+  <option collision="predefined" density="4000" integrator="RK4" timestep="0.01" viscosity="0.1"/>
+  <default>
+    <geom conaffinity="1" condim="1" contype="1" material="geom" rgba="0.8 0.6 .4 1"/>
+    <joint armature='0.1'  />
+  </default>
+  <asset>
+    <texture builtin="gradient" height="100" rgb1="1 1 1" rgb2="0 0 0" type="skybox" width="100"/>
+    <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+    <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+    <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="30 30" texture="texplane"/>
+    <material name="geom" texture="texgeom" texuniform="true"/>
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 -0.1" rgba="0.8 0.9 0.8 1" size="40 40 0.1" type="plane"/>
+    <!--  ================= SWIMMER ================= /-->
+    <body name="torso" pos="0 0 0">
+      <geom density="1000" fromto="1.5 0 0 0.5 0 0" size="0.1" type="capsule"/>
+      <joint axis="1 0 0" name="slider1" pos="0 0 0" type="slide"/>
+      <joint axis="0 1 0" name="slider2" pos="0 0 0" type="slide"/>
+      <joint axis="0 0 1" name="rot" pos="0 0 0" type="hinge"/>
+      <body name="mid" pos="0.5 0 0">
+        <geom density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule"/>
+        <joint axis="0 0 1" limited="true" name="rot2" pos="0 0 0" range="-100 100" type="hinge"/>
+        <body name="back" pos="-1 0 0">
+          <geom density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule"/>
+          <joint axis="0 0 1" limited="true" name="rot3" pos="0 0 0" range="-100 100" type="hinge"/>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot2"/>
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot3"/>
+  </actuator>
+</mujoco>
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/assets/walker2d.xml b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/assets/walker2d.xml
new file mode 100644
index 0000000..cbc074d
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/assets/walker2d.xml
@@ -0,0 +1,61 @@
+<mujoco model="walker2d">
+  <compiler angle="degree" coordinate="global" inertiafromgeom="true"/>
+  <default>
+    <joint armature="0.01" damping=".1" limited="true"/>
+    <geom conaffinity="0" condim="3" contype="1" density="1000" friction=".7 .1 .1" rgba="0.8 0.6 .4 1"/>
+  </default>
+  <option integrator="RK4" timestep="0.002"/>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="40 40 40" type="plane" material="MatPlane"/>
+    <body name="torso" pos="0 0 1.25">
+      <joint armature="0" axis="1 0 0" damping="0" limited="false" name="rootx" pos="0 0 0" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 0 1" damping="0" limited="false" name="rootz" pos="0 0 0" ref="1.25" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 1 0" damping="0" limited="false" name="rooty" pos="0 0 1.25" stiffness="0" type="hinge"/>
+      <geom friction="0.9" fromto="0 0 1.45 0 0 1.05" name="torso_geom" size="0.05" type="capsule"/>
+      <body name="thigh" pos="0 0 1.05">
+        <joint axis="0 -1 0" name="thigh_joint" pos="0 0 1.05" range="-150 0" type="hinge"/>
+        <geom friction="0.9" fromto="0 0 1.05 0 0 0.6" name="thigh_geom" size="0.05" type="capsule"/>
+        <body name="leg" pos="0 0 0.35">
+          <joint axis="0 -1 0" name="leg_joint" pos="0 0 0.6" range="-150 0" type="hinge"/>
+          <geom friction="0.9" fromto="0 0 0.6 0 0 0.1" name="leg_geom" size="0.04" type="capsule"/>
+          <body name="foot" pos="0.2/2 0 0.1">
+            <joint axis="0 -1 0" name="foot_joint" pos="0 0 0.1" range="-45 45" type="hinge"/>
+            <geom friction="0.9" fromto="-0.0 0 0.1 0.2 0 0.1" name="foot_geom" size="0.06" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <!-- copied and then replace thigh->thigh_left, leg->leg_left, foot->foot_right -->
+      <body name="thigh_left" pos="0 0 1.05">
+        <joint axis="0 -1 0" name="thigh_left_joint" pos="0 0 1.05" range="-150 0" type="hinge"/>
+        <geom friction="0.9" fromto="0 0 1.05 0 0 0.6" name="thigh_left_geom" rgba=".7 .3 .6 1" size="0.05" type="capsule"/>
+        <body name="leg_left" pos="0 0 0.35">
+          <joint axis="0 -1 0" name="leg_left_joint" pos="0 0 0.6" range="-150 0" type="hinge"/>
+          <geom friction="0.9" fromto="0 0 0.6 0 0 0.1" name="leg_left_geom" rgba=".7 .3 .6 1" size="0.04" type="capsule"/>
+          <body name="foot_left" pos="0.2/2 0 0.1">
+            <joint axis="0 -1 0" name="foot_left_joint" pos="0 0 0.1" range="-45 45" type="hinge"/>
+            <geom friction="1.9" fromto="-0.0 0 0.1 0.2 0 0.1" name="foot_left_geom" rgba=".7 .3 .6 1" size="0.06" type="capsule"/>
+          </body>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <!-- <motor joint="torso_joint" ctrlrange="-100.0 100.0" isctrllimited="true"/>-->
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="100" joint="thigh_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="100" joint="leg_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="100" joint="foot_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="100" joint="thigh_left_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="100" joint="leg_left_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="100" joint="foot_left_joint"/>
+    <!-- <motor joint="finger2_rot" ctrlrange="-20.0 20.0" isctrllimited="true"/>-->
+  </actuator>
+    <asset>
+        <texture type="skybox" builtin="gradient" rgb1=".4 .5 .6" rgb2="0 0 0"
+            width="100" height="100"/>        
+        <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+        <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+        <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="60 60" texture="texplane"/>
+        <material name="geom" texture="texgeom" texuniform="true"/>
+    </asset>
+</mujoco>
\ No newline at end of file
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/__init__.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/__init__.py
new file mode 100644
index 0000000..eff23c0
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/__init__.py
@@ -0,0 +1,210 @@
+from gym.envs.registration import register
+
+register(
+    id='MBRLHalfCheetah-v0',
+    entry_point='envs.gym.half_cheetah:HalfCheetahEnv',
+    kwargs={'frame_skip': 5},
+    max_episode_steps=1000,
+)
+
+register(
+    id='MBRLWalker2d-v0',
+    entry_point='envs.gym.walker2d:Walker2dEnv',
+    kwargs={'frame_skip': 4},
+    max_episode_steps=1000,
+)
+
+register(
+    id='MBRLSwimmer-v0',
+    entry_point='envs.gym.swimmer:SwimmerEnv',
+    kwargs={'frame_skip': 4},
+    max_episode_steps=1000,
+)
+
+register(
+    id='MBRLAnt-v0',
+    entry_point='envs.gym.ant:AntEnv',
+    kwargs={'frame_skip': 5},
+    max_episode_steps=1000,
+)
+
+register(
+    id='MBRLHopper-v0',
+    entry_point='envs.gym.hopper:HopperEnv',
+    kwargs={'frame_skip': 4},
+    max_episode_steps=1000,
+)
+
+register(
+    id='MBRLReacher-v0',
+    entry_point='envs.gym.reacher:ReacherEnv',
+    max_episode_steps=50,
+)
+
+
+# second batch of environments
+
+register(
+    id='MBRLInvertedPendulum-v0',
+    entry_point='envs.gym.inverted_pendulum:InvertedPendulumEnv',
+    max_episode_steps=100,
+)
+register(
+    id='MBRLAcrobot-v0',
+    entry_point='envs.gym.acrobot:AcrobotEnv',
+    max_episode_steps=200,
+)
+register(
+    id='MBRLCartpole-v0',
+    entry_point='envs.gym.cartpole:CartPoleEnv',
+    max_episode_steps=200,
+
+)
+register(
+    id='MBRLMountain-v0',
+    entry_point='envs.gym.mountain_car:Continuous_MountainCarEnv',
+    max_episode_steps=200,
+
+)
+register(
+    id='MBRLPendulum-v0',
+    entry_point='envs.gym.pendulum:PendulumEnv',
+    max_episode_steps=200,
+)
+
+register(
+    id='gym_petsPusher-v0',
+    entry_point='envs.gym.pets_pusher:PusherEnv',
+    max_episode_steps=150,
+)
+register(
+    id='gym_petsReacher-v0',
+    entry_point='envs.gym.pets_reacher:Reacher3DEnv',
+    max_episode_steps=150,
+)
+register(
+    id='gym_petsCheetah-v0',
+    entry_point='envs.gym.pets_cheetah:HalfCheetahEnv',
+    max_episode_steps=1000,
+)
+
+# noisy env
+register(
+    id='gym_cheetahO01-v0',
+    entry_point='envs.gym.gym_cheetahO01:HalfCheetahEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_cheetahO001-v0',
+    entry_point='envs.gym.gym_cheetahO001:HalfCheetahEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_cheetahA01-v0',
+    entry_point='envs.gym.gym_cheetahA01:HalfCheetahEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_cheetahA003-v0',
+    entry_point='envs.gym.gym_cheetahA003:HalfCheetahEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_pendulumO01-v0',
+    entry_point='envs.gym.gym_pendulumO01:PendulumEnv',
+    max_episode_steps=200,
+)
+register(
+    id='gym_pendulumO001-v0',
+    entry_point='envs.gym.gym_pendulumO001:PendulumEnv',
+    max_episode_steps=200,
+)
+register(
+    id='gym_cartpoleO01-v0',
+    entry_point='envs.gym.gym_cartpoleO01:CartPoleEnv',
+    max_episode_steps=200,
+)
+register(
+    id='gym_cartpoleO001-v0',
+    entry_point='envs.gym.gym_cartpoleO001:CartPoleEnv',
+    max_episode_steps=200,
+)
+
+register(
+    id='gym_fant-v0',
+    entry_point='envs.gym.gym_fant:AntEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_fhopper-v0',
+    entry_point='envs.gym.gym_fhopper:HopperEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_fwalker2d-v0',
+    entry_point='envs.gym.gym_fwalker2d:Walker2dEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_fswimmer-v0',
+    entry_point='envs.gym.gym_fswimmer:fixedSwimmerEnv',
+    max_episode_steps=1000,
+)
+register(
+    id="gym_humanoid-v0",
+    entry_point='envs.gym.gym_humanoid:HumanoidEnv',
+    max_episode_steps=1000,
+)
+register(
+    id="gym_slimhumanoid-v0",
+    entry_point='envs.gym.gym_slimhumanoid:HumanoidEnv',
+    max_episode_steps=1000,
+)
+register(
+    id="gym_nostopslimhumanoid-v0",
+    entry_point='envs.gym.gym_nostopslimhumanoid:HumanoidEnv',
+    max_episode_steps=1000,
+)
+
+env_name_to_gym_registry = {
+    # first batch
+    "half_cheetah": "MBRLHalfCheetah-v0",
+    "swimmer": "MBRLSwimmer-v0",
+    "ant": "MBRLAnt-v0",
+    "hopper": "MBRLHopper-v0",
+    "reacher": "MBRLReacher-v0",
+    "walker2d": "MBRLWalker2d-v0",
+
+    # second batch
+    "invertedPendulum": "MBRLInvertedPendulum-v0",
+    "acrobot": 'MBRLAcrobot-v0',
+    "cartpole": 'MBRLCartpole-v0',
+    "mountain": 'MBRLMountain-v0',
+    "pendulum": 'MBRLPendulum-v0',
+
+    # the pets env
+    "gym_petsPusher": "gym_petsPusher-v0",
+    "gym_petsReacher": "gym_petsReacher-v0",
+    "gym_petsCheetah": "gym_petsCheetah-v0",
+
+    # the noise env
+    "gym_cheetahO01": "gym_cheetahO01-v0",
+    "gym_cheetahO001": "gym_cheetahO001-v0",
+    "gym_cheetahA01": "gym_cheetahA01-v0",
+    "gym_cheetahA003": "gym_cheetahA003-v0",
+
+    "gym_pendulumO01": "gym_pendulumO01-v0",
+    "gym_pendulumO001": "gym_pendulumO001-v0",
+
+    "gym_cartpoleO01": "gym_cartpoleO01-v0",
+    "gym_cartpoleO001": "gym_cartpoleO001-v0",
+
+    "gym_fant": "gym_fant-v0",
+    "gym_fswimmer": "gym_fswimmer-v0",
+    "gym_fhopper": "gym_fhopper-v0",
+    "gym_fwalker2d": "gym_fwalker2d-v0",
+
+    "gym_humanoid": "gym_humanoid-v0",
+    "gym_slimhumanoid": "gym_slimhumanoid-v0",
+    "gym_nostopslimhumanoid": "gym_nostopslimhumanoid-v0",
+}
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/acrobot.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/acrobot.py
new file mode 100644
index 0000000..1af57a6
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/acrobot.py
@@ -0,0 +1,357 @@
+"""classic Acrobot task"""
+from gym import core, spaces
+from gym.utils import seeding
+import numpy as np
+from numpy import sin, cos, pi
+from slbo.utils.dataset import Dataset, gen_dtype
+from lunzi.Logger import logger
+
+
+__copyright__ = "Copyright 2013, RLPy http://acl.mit.edu/RLPy"
+__credits__ = ["Alborz Geramifard", "Robert H. Klein", "Christoph Dann",
+               "William Dabney", "Jonathan P. How"]
+__license__ = "BSD 3-Clause"
+__author__ = "Christoph Dann <cdann@cdann.de>"
+
+# SOURCE:
+# https://github.com/rlpy/rlpy/blob/master/rlpy/Domains/Acrobot.py
+
+
+class AcrobotEnv(core.Env):
+
+    """
+    Acrobot is a 2-link pendulum with only the second joint actuated
+    Intitially, both links point downwards. The goal is to swing the
+    end-effector at a height at least the length of one link above the base.
+    Both links can swing freely and can pass by each other, i.e., they don't
+    collide when they have the same angle.
+    **STATE:**
+    The state consists of the two rotational joint angles and their velocities
+    [theta1 theta2 thetaDot1 thetaDot2]. An angle of 0 corresponds to corresponds
+    to the respective link pointing downwards (angles are in world coordinates).
+    **ACTIONS:**
+    The action is either applying +1, 0 or -1 torque on the joint between
+    the two pendulum links.
+    .. note::
+        The dynamics equations were missing some terms in the NIPS paper which
+        are present in the book. R. Sutton confirmed in personal correspondance
+        that the experimental results shown in the paper and the book were
+        generated with the equations shown in the book.
+        However, there is the option to run the domain with the paper equations
+        by setting book_or_nips = 'nips'
+    **REFERENCE:**
+    .. seealso::
+        R. Sutton: Generalization in Reinforcement Learning:
+        Successful Examples Using Sparse Coarse Coding (NIPS 1996)
+    .. seealso::
+        R. Sutton and A. G. Barto:
+        Reinforcement learning: An introduction.
+        Cambridge: MIT press, 1998.
+    .. warning::
+        This version of the domain uses the Runge-Kutta method for integrating
+        the system dynamics and is more realistic, but also considerably harder
+        than the original version which employs Euler integration,
+        see the AcrobotLegacy class.
+    """
+
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 15
+    }
+
+    dt = .2
+
+    LINK_LENGTH_1 = 1.  # [m]
+    LINK_LENGTH_2 = 1.  # [m]
+    LINK_MASS_1 = 1.  #: [kg] mass of link 1
+    LINK_MASS_2 = 1.  #: [kg] mass of link 2
+    LINK_COM_POS_1 = 0.5  #: [m] position of the center of mass of link 1
+    LINK_COM_POS_2 = 0.5  #: [m] position of the center of mass of link 2
+    LINK_MOI = 1.  #: moments of inertia for both links
+
+    MAX_VEL_1 = 4 * np.pi
+    MAX_VEL_2 = 9 * np.pi
+
+    AVAIL_TORQUE = [-1., 0., +1]
+
+    torque_noise_max = 0.
+
+    #: use dynamics equations from the nips paper or the book
+    book_or_nips = "book"
+    action_arrow = None
+    domain_fig = None
+    actions_num = 3
+
+    def __init__(self):
+        self.viewer = None
+        high = np.array([1.0, 1.0, 1.0, 1.0, self.MAX_VEL_1, self.MAX_VEL_2])
+        low = -high
+        self.observation_space = spaces.Box(low, high)
+        self.action_space = spaces.Box(np.array([-1.0]), np.array([1.0]))
+        self.state = None
+        self._seed()
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _reset(self):
+        self.state = self.np_random.uniform(low=-0.1, high=0.1, size=(4,))
+        return self._get_ob()
+
+    def _step(self, a):
+        # Discretize
+        if a[0] < -.33:
+            action = 0
+        elif a[0] < .33:
+            action = 1
+        else:
+            action = 2
+
+        s = self.state
+        reward = -np.cos(s[0]) - np.cos(s[1] + s[0])
+        torque = self.AVAIL_TORQUE[action]
+
+        # Add noise to the force action
+        if self.torque_noise_max > 0:
+            torque += self.np_random.uniform(-self.torque_noise_max, self.torque_noise_max)
+
+        # Now, augment the state with our force action so it can be passed to
+        # _dsdt
+        s_augmented = np.append(s, torque)
+
+        ns = rk4(self._dsdt, s_augmented, [0, self.dt])
+        # only care about final timestep of integration returned by integrator
+        ns = ns[-1]
+        ns = ns[:4]  # omit action
+        # ODEINT IS TOO SLOW!
+        # ns_continuous = integrate.odeint(self._dsdt, self.s_continuous, [0, self.dt])
+        # self.s_continuous = ns_continuous[-1] # We only care about the state
+        # at the ''final timestep'', self.dt
+
+        ns[0] = wrap(ns[0], -pi, pi)
+        ns[1] = wrap(ns[1], -pi, pi)
+        ns[2] = bound(ns[2], -self.MAX_VEL_1, self.MAX_VEL_1)
+        ns[3] = bound(ns[3], -self.MAX_VEL_2, self.MAX_VEL_2)
+        self.state = ns
+        # terminal = self._terminal()
+        terminal = False
+        # reward = -1. if not terminal else 0.
+        return (self._get_ob(), reward, terminal, {})
+
+    def _get_ob(self):
+        s = self.state
+        return np.array([cos(s[0]), np.sin(s[0]), cos(s[1]), sin(s[1]), s[2], s[3]])
+
+    def _terminal(self):
+        s = self.state
+        return bool(-np.cos(s[0]) - np.cos(s[1] + s[0]) > 1.)
+
+    def _dsdt(self, s_augmented, t):
+        m1 = self.LINK_MASS_1
+        m2 = self.LINK_MASS_2
+        l1 = self.LINK_LENGTH_1
+        lc1 = self.LINK_COM_POS_1
+        lc2 = self.LINK_COM_POS_2
+        I1 = self.LINK_MOI
+        I2 = self.LINK_MOI
+        g = 9.8
+        a = s_augmented[-1]
+        s = s_augmented[:-1]
+        theta1 = s[0]
+        theta2 = s[1]
+        dtheta1 = s[2]
+        dtheta2 = s[3]
+        d1 = m1 * lc1 ** 2 + m2 * \
+            (l1 ** 2 + lc2 ** 2 + 2 * l1 * lc2 * np.cos(theta2)) + I1 + I2
+        d2 = m2 * (lc2 ** 2 + l1 * lc2 * np.cos(theta2)) + I2
+        phi2 = m2 * lc2 * g * np.cos(theta1 + theta2 - np.pi / 2.)
+        phi1 = - m2 * l1 * lc2 * dtheta2 ** 2 * np.sin(theta2) \
+               - 2 * m2 * l1 * lc2 * dtheta2 * dtheta1 * np.sin(theta2)  \
+            + (m1 * lc1 + m2 * l1) * g * np.cos(theta1 - np.pi / 2) + phi2
+        if self.book_or_nips == "nips":
+            # the following line is consistent with the description in the
+            # paper
+            ddtheta2 = (a + d2 / d1 * phi1 - phi2) / \
+                (m2 * lc2 ** 2 + I2 - d2 ** 2 / d1)
+        else:
+            # the following line is consistent with the java implementation and the
+            # book
+            ddtheta2 = (a + d2 / d1 * phi1 - m2 * l1 * lc2 * dtheta1 ** 2 * np.sin(theta2) - phi2) \
+                / (m2 * lc2 ** 2 + I2 - d2 ** 2 / d1)
+        ddtheta1 = -(d2 * ddtheta2 + phi1) / d1
+        return (dtheta1, dtheta2, ddtheta1, ddtheta2, 0.)
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+        from gym.envs.classic_control import rendering
+
+        s = self.state
+
+        if self.viewer is None:
+            self.viewer = rendering.Viewer(500, 500)
+            self.viewer.set_bounds(-2.2, 2.2, -2.2, 2.2)
+
+        if s is None:
+            return None
+
+        p1 = [-self.LINK_LENGTH_1 *
+              np.cos(s[0]), self.LINK_LENGTH_1 * np.sin(s[0])]
+
+        p2 = [p1[0] - self.LINK_LENGTH_2 * np.cos(s[0] + s[1]),
+              p1[1] + self.LINK_LENGTH_2 * np.sin(s[0] + s[1])]
+
+        xys = np.array([[0, 0], p1, p2])[:, ::-1]
+        thetas = [s[0] - np.pi / 2, s[0] + s[1] - np.pi / 2]
+
+        self.viewer.draw_line((-2.2, 1), (2.2, 1))
+        for ((x, y), th) in zip(xys, thetas):
+            l, r, t, b = 0, 1, .1, -.1
+            jtransform = rendering.Transform(rotation=th, translation=(x, y))
+            link = self.viewer.draw_polygon([(l, b), (l, t), (r, t), (r, b)])
+            link.add_attr(jtransform)
+            link.set_color(0, .8, .8)
+            circ = self.viewer.draw_circle(.1)
+            circ.set_color(.8, .8, 0)
+            circ.add_attr(jtransform)
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+            def height(obs):
+                h1 = obs[0]  # Height of first arm
+                h2 = obs[0] * obs[2] - obs[1] * obs[3]  # Height of second arm
+                return -(h1 + h2)  # total height
+
+            start_height = height(data_dict['start_state'])
+        """
+        h1 = obs[:, 0]  # Height of first arm
+        h2 = obs[:, 0] * obs[:, 2] - obs[:, 1] * obs[:, 3]  # Height of second arm
+        return (h1 + h2)
+
+    def verify(self, n=2000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
+
+
+def wrap(x, m, M):
+    """
+    :param x: a scalar
+    :param m: minimum possible value in range
+    :param M: maximum possible value in range
+    Wraps ``x`` so m <= x <= M; but unlike ``bound()`` which
+    truncates, ``wrap()`` wraps x around the coordinate system defined by m,M.\n
+    For example, m = -180, M = 180 (degrees), x = 360 --> returns 0.
+    """
+    diff = M - m
+    while x > M:
+        x = x - diff
+    while x < m:
+        x = x + diff
+    return x
+
+
+def bound(x, m, M=None):
+    """
+    :param x: scalar
+    Either have m as scalar, so bound(x,m,M) which returns m <= x <= M *OR*
+    have m as length 2 vector, bound(x,m, <IGNORED>) returns m[0] <= x <= m[1].
+    """
+    if M is None:
+        M = m[1]
+        m = m[0]
+    # bound x between min (m) and Max (M)
+    return min(max(x, m), M)
+
+
+def rk4(derivs, y0, t, *args, **kwargs):
+    """
+    Integrate 1D or ND system of ODEs using 4-th order Runge-Kutta.
+    This is a toy implementation which may be useful if you find
+    yourself stranded on a system w/o scipy.  Otherwise use
+    :func:`scipy.integrate`.
+    *y0*
+        initial state vector
+    *t*
+        sample times
+    *derivs*
+        returns the derivative of the system and has the
+        signature ``dy = derivs(yi, ti)``
+    *args*
+        additional arguments passed to the derivative function
+    *kwargs*
+        additional keyword arguments passed to the derivative function
+    Example 1 ::
+        ## 2D system
+        def derivs6(x,t):
+            d1 =  x[0] + 2*x[1]
+            d2 =  -3*x[0] + 4*x[1]
+            return (d1, d2)
+        dt = 0.0005
+        t = arange(0.0, 2.0, dt)
+        y0 = (1,2)
+        yout = rk4(derivs6, y0, t)
+    Example 2::
+        ## 1D system
+        alpha = 2
+        def derivs(x,t):
+            return -alpha*x + exp(-t)
+        y0 = 1
+        yout = rk4(derivs, y0, t)
+    If you have access to scipy, you should probably be using the
+    scipy.integrate tools rather than this function.
+    """
+
+    try:
+        Ny = len(y0)
+    except TypeError:
+        yout = np.zeros((len(t),), np.float_)
+    else:
+        yout = np.zeros((len(t), Ny), np.float_)
+
+    yout[0] = y0
+    i = 0
+
+    for i in np.arange(len(t) - 1):
+
+        thist = t[i]
+        dt = t[i + 1] - thist
+        dt2 = dt / 2.0
+        y0 = yout[i]
+
+        k1 = np.asarray(derivs(y0, thist, *args, **kwargs))
+        k2 = np.asarray(derivs(y0 + dt2 * k1, thist + dt2, *args, **kwargs))
+        k3 = np.asarray(derivs(y0 + dt2 * k2, thist + dt2, *args, **kwargs))
+        k4 = np.asarray(derivs(y0 + dt * k3, thist + dt, *args, **kwargs))
+        yout[i + 1] = y0 + dt / 6.0 * (k1 + 2 * k2 + 2 * k3 + k4)
+    return yout
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/ant.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/ant.py
new file mode 100644
index 0000000..a2699aa
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/ant.py
@@ -0,0 +1,81 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class AntEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/ant.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        ob = self._get_obs()
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = old_ob[13]
+        reward_height = -3.0 * np.square(old_ob[0] - 0.57)
+        reward = reward_run + reward_ctrl + reward_height + 1.0
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            # (self.model.data.qpos.flat[:1] - self.prev_qpos[:1]) / self.dt,
+            # self.get_body_comvel("torso")[:1],
+            self.model.data.qpos.flat[2:],
+            self.model.data.qvel.flat,
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        qpos = self.init_qpos + self.np_random.uniform(size=self.model.nq, low=-.1, high=.1)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        # self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 13]
+        reward_height = -3.0 * np.square(obs[:, 0] - 0.57)
+        reward = reward_run + reward_ctrl + reward_height + 1.0
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        # reward_height = -3.0 * tf.square(next_obs[:, 0] - 0.57)
+        reward = reward_run + reward_ctrl # + reward_height
+        return -reward
+        """
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/assets/cartpole.xml b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/assets/cartpole.xml
new file mode 100644
index 0000000..284a58c
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/assets/cartpole.xml
@@ -0,0 +1,35 @@
+<mujoco model="cartpole">
+	<compiler inertiafromgeom="true"/>
+	<default>
+		<joint armature="0" damping="1" limited="true"/>
+		<geom contype="0" friction="1 0.1 0.1" rgba="0.7 0.7 0 1"/>
+		<tendon/>
+		<motor ctrlrange="-3 3" ctrllimited='true'/>
+	</default>
+    <asset>
+		<texture type="skybox" builtin="checker" rgb1="1 1 1" rgb2="1 1 1"
+                    width="256" height="256"/>
+        <texture name="texgeom" type="cube" builtin="flat" mark="cross" width="127" height="1278" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" markrgb="1 1 1" random="0.01" />
+		<texture name="texplane" type="2d" builtin="checker" rgb1=".5 .5 .5" rgb2=".5 .5 .5" width="100" height="100" />
+        <texture name="texplane_show" type="2d" builtin="checker" rgb1="0 0 0" rgb2="0.9 0.9 0.9" width="100" height="100" />
+        <material name='MatPlane' texture="texplane" shininess="1" texrepeat="30 30" specular="1"  reflectance="0.5" />
+        <material name='geom' texture="texgeom" texuniform="true" />
+ 	</asset>
+	<option gravity="0 0 -9.81" integrator="RK4" timestep="0.02"/>
+	<size nstack="3000"/>
+	<worldbody>
+		<geom name="rail" pos="0 0 0" quat="0.707 0 0.707 0" rgba="0.3 0.3 0.7 1" size="0.02 3" type="capsule"/>
+		<body name="cart" pos="0 0 0">
+			<joint axis="1 0 0" limited="true" name="slider" pos="0 0 0" range="-2.5 2.5" type="slide"/>
+			<geom name="cart" pos="0 0 0" quat="0.707 0 0.707 0" size="0.1 0.1" type="capsule"/>
+			<body name="pole" pos="0 0 0">
+				<joint axis="0 1 0" limited="false" name="hinge" pos="0 0 0" range="-180 180" type="hinge"/>
+				<geom fromto="0 0 0 0.001 0 -0.6" name="cpole" rgba="0 0.7 0.7 1" size="0.049 0.3" type="capsule"/>
+			</body>
+		</body>
+	</worldbody>
+	<actuator>
+		<motor gear="100" joint="slider" name="slide"/>
+	</actuator>
+</mujoco>
+
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/assets/fixed_swimmer.xml b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/assets/fixed_swimmer.xml
new file mode 100644
index 0000000..142f344
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/assets/fixed_swimmer.xml
@@ -0,0 +1,43 @@
+<mujoco model="swimmer">
+  <compiler angle="degree" coordinate="local" inertiafromgeom="true"/>
+  <option collision="predefined" density="4000" integrator="RK4" timestep="0.01" viscosity="0.1"/>
+  <default>
+    <geom conaffinity="1" condim="1" contype="1" material="geom" rgba="0.8 0.6 .4 1"/>
+    <joint armature='0.1'  />
+  </default>
+  <asset>
+    <texture builtin="gradient" height="100" rgb1="1 1 1" rgb2="0 0 0" type="skybox" width="100"/>
+    <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+    <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+    <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="30 30" texture="texplane"/>
+    <material name="geom" texture="texgeom" texuniform="true"/>
+  </asset>
+
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 -0.1" rgba="0.8 0.9 0.8 1" size="40 40 0.1" type="plane"/>
+    <body name="podBody_1" pos="0 0 0">
+      <geom name='pod_1' density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule"/>
+      <joint axis="1 0 0" name="slider1" pos="0 0 0" type="slide"/>
+      <joint axis="0 1 0" name="slider2" pos="0 0 0" type="slide"/>
+      <joint axis="0 0 1" name="rot_1" pos="-1.5 0 0" type="hinge"/>
+      <site name="tip" pos="0 0 0" size="0.02 0.02"/>
+      <body name="podBody_2" pos="-1 0 0">
+        <geom name='pod_2' density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule"/>
+        <joint axis="0 0 1" limited="true" name="rot_2" pos="0 0 0" range="-100 100" type="hinge"/>
+        
+        <body name="podBody_3" pos="-1 0 0">
+          <geom name='pod_3' density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule"/>
+          <joint axis="0 0 1" limited="true" name="rot_3" pos="0 0 0" range="-100 100" type="hinge"/>
+        
+        </body>
+
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot_2"/>
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot_3"/>
+  </actuator>
+
+</mujoco>
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/assets/half_cheetah.xml b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/assets/half_cheetah.xml
new file mode 100644
index 0000000..40a1cb6
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/assets/half_cheetah.xml
@@ -0,0 +1,95 @@
+<!-- Cheetah Model
+
+    The state space is populated with joints in the order that they are
+    defined in this file. The actuators also operate on joints.
+
+    State-Space (name/joint/parameter):
+        - rootx     slider      position (m)
+        - rootz     slider      position (m)
+        - rooty     hinge       angle (rad)
+        - bthigh    hinge       angle (rad)
+        - bshin     hinge       angle (rad)
+        - bfoot     hinge       angle (rad)
+        - fthigh    hinge       angle (rad)
+        - fshin     hinge       angle (rad)
+        - ffoot     hinge       angle (rad)
+        - rootx     slider      velocity (m/s)
+        - rootz     slider      velocity (m/s)
+        - rooty     hinge       angular velocity (rad/s)
+        - bthigh    hinge       angular velocity (rad/s)
+        - bshin     hinge       angular velocity (rad/s)
+        - bfoot     hinge       angular velocity (rad/s)
+        - fthigh    hinge       angular velocity (rad/s)
+        - fshin     hinge       angular velocity (rad/s)
+        - ffoot     hinge       angular velocity (rad/s)
+
+    Actuators (name/actuator/parameter):
+        - bthigh    hinge       torque (N m)
+        - bshin     hinge       torque (N m)
+        - bfoot     hinge       torque (N m)
+        - fthigh    hinge       torque (N m)
+        - fshin     hinge       torque (N m)
+        - ffoot     hinge       torque (N m)
+
+-->
+<mujoco model="cheetah">
+  <compiler angle="radian" coordinate="local" inertiafromgeom="true" settotalmass="14"/>
+  <default>
+    <joint armature=".1" damping=".01" limited="true" solimplimit="0 .8 .03" solreflimit=".02 1" stiffness="8"/>
+    <geom conaffinity="0" condim="3" contype="1" friction=".4 .1 .1" rgba="0.8 0.6 .4 1" solimp="0.0 0.8 0.01" solref="0.02 1"/>
+    <motor ctrllimited="true" ctrlrange="-1 1"/>
+  </default>
+  <size nstack="300000" nuser_geom="1"/>
+  <option gravity="0 0 -9.81" timestep="0.01"/>
+  <asset>
+    <texture builtin="gradient" height="100" rgb1="1 1 1" rgb2="0 0 0" type="skybox" width="100"/>
+    <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+    <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+    <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="150 150" texture="texplane"/>
+    <material name="geom" texture="texgeom" texuniform="true"/>
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="200 200 200" type="plane"/>
+    <body name="torso" pos="0 0 .7">
+      <joint armature="0" axis="1 0 0" damping="0" limited="false" name="rootx" pos="0 0 0" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 0 1" damping="0" limited="false" name="rootz" pos="0 0 0" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 1 0" damping="0" limited="false" name="rooty" pos="0 0 0" stiffness="0" type="hinge"/>
+      <geom fromto="-.5 0 0 .5 0 0" name="torso" size="0.046" type="capsule"/>
+      <geom axisangle="0 1 0 .87" name="head" pos=".6 0 .1" size="0.046 .15" type="capsule"/>
+      <!-- <site name='tip'  pos='.15 0 .11'/>-->
+      <body name="bthigh" pos="-.5 0 0">
+        <joint axis="0 1 0" damping="6" name="bthigh" pos="0 0 0" range="-.52 1.05" stiffness="240" type="hinge"/>
+        <geom axisangle="0 1 0 -3.8" name="bthigh" pos=".1 0 -.13" size="0.046 .145" type="capsule"/>
+        <body name="bshin" pos=".16 0 -.25">
+          <joint axis="0 1 0" damping="4.5" name="bshin" pos="0 0 0" range="-.785 .785" stiffness="180" type="hinge"/>
+          <geom axisangle="0 1 0 -2.03" name="bshin" pos="-.14 0 -.07" rgba="0.9 0.6 0.6 1" size="0.046 .15" type="capsule"/>
+          <body name="bfoot" pos="-.28 0 -.14">
+            <joint axis="0 1 0" damping="3" name="bfoot" pos="0 0 0" range="-.4 .785" stiffness="120" type="hinge"/>
+            <geom axisangle="0 1 0 -.27" name="bfoot" pos=".03 0 -.097" rgba="0.9 0.6 0.6 1" size="0.046 .094" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <body name="fthigh" pos=".5 0 0">
+        <joint axis="0 1 0" damping="4.5" name="fthigh" pos="0 0 0" range="-1 .7" stiffness="180" type="hinge"/>
+        <geom axisangle="0 1 0 .52" name="fthigh" pos="-.07 0 -.12" size="0.046 .133" type="capsule"/>
+        <body name="fshin" pos="-.14 0 -.24">
+          <joint axis="0 1 0" damping="3" name="fshin" pos="0 0 0" range="-1.2 .87" stiffness="120" type="hinge"/>
+          <geom axisangle="0 1 0 -.6" name="fshin" pos=".065 0 -.09" rgba="0.9 0.6 0.6 1" size="0.046 .106" type="capsule"/>
+          <body name="ffoot" pos=".13 0 -.18">
+            <joint axis="0 1 0" damping="1.5" name="ffoot" pos="0 0 0" range="-.5 .5" stiffness="60" type="hinge"/>
+            <geom axisangle="0 1 0 -.6" name="ffoot" pos=".045 0 -.07" rgba="0.9 0.6 0.6 1" size="0.046 .07" type="capsule"/>
+          </body>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor gear="120" joint="bthigh" name="bthigh"/>
+    <motor gear="90" joint="bshin" name="bshin"/>
+    <motor gear="60" joint="bfoot" name="bfoot"/>
+    <motor gear="120" joint="fthigh" name="fthigh"/>
+    <motor gear="60" joint="fshin" name="fshin"/>
+    <motor gear="30" joint="ffoot" name="ffoot"/>
+  </actuator>
+</mujoco>
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/assets/pusher.xml b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/assets/pusher.xml
new file mode 100644
index 0000000..9e81b01
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/assets/pusher.xml
@@ -0,0 +1,101 @@
+<mujoco model="arm3d">
+  <compiler inertiafromgeom="true" angle="radian" coordinate="local"/>
+  <option timestep="0.01" gravity="0 0 0" iterations="20" integrator="Euler" />
+  
+  <default>
+    <joint armature='0.04' damping="1" limited="true"/>
+    <geom friction=".8 .1 .1" density="300" margin="0.002" condim="1" contype="0" conaffinity="0"/>
+  </default>
+  <asset>
+    <texture type="skybox" builtin="checker" rgb1="1 1 1" rgb2="1 1 1"
+             width="256" height="256"/>
+    <texture name="texgeom" type="cube" builtin="flat" mark="cross" width="127" height="1278" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" markrgb="1 1 1" random="0.01" />
+    <texture name="texplane" type="2d" builtin="checker" rgb1=".5 .5 .5" rgb2=".5 .5 .5" width="100" height="100" />
+    <texture name="texplane_show" type="2d" builtin="checker" rgb1="0 0 0" rgb2="0.9 0.9 0.9" width="100" height="100" />
+    <material name='MatPlane' texture="texplane" shininess="1" texrepeat="30 30" specular="1"  reflectance="0.5" />
+    <material name='geom' texture="texgeom" texuniform="true" />
+  </asset>
+
+  <worldbody>
+    <light diffuse=".5 .5 .5" pos="0 0 3" dir="0 0 -1"/>
+    <geom name="table" type="plane" pos="0 0.5 -0.325" size="1 1 0.1" contype="1" conaffinity="1"/>
+
+    <body name="r_shoulder_pan_link" pos="0 -0.6 0">
+      <geom name="e1" type="sphere" rgba="0.6 0.6 0.6 1" pos="-0.06 0.05 0.2" size="0.05" />
+      <geom name="e2" type="sphere" rgba="0.6 0.6 0.6 1" pos=" 0.06 0.05 0.2" size="0.05" />
+      <geom name="e1p" type="sphere" rgba="0.1 0.1 0.1 1" pos="-0.06 0.09 0.2" size="0.03" />
+      <geom name="e2p" type="sphere" rgba="0.1 0.1 0.1 1" pos=" 0.06 0.09 0.2" size="0.03" />
+      <geom name="sp" type="capsule" fromto="0 0 -0.4 0 0 0.2" size="0.1" />
+      <!--<joint name="r_shoulder_pan_joint" type="hinge" pos="0 0 0" axis="0 0 1" range="-2.2854 1.714602" damping="1.0" />-->
+      <joint name="r_shoulder_pan_joint" type="hinge" pos="0 0 0" axis="0 0 1" range="-2.2854 2.0" damping="1.0" />
+
+      <body name="r_shoulder_lift_link" pos="0.1 0 0">
+        <geom name="sl" type="capsule" fromto="0 -0.1 0 0 0.1 0" size="0.1" />
+        <joint name="r_shoulder_lift_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.5236 1.3963" damping="1.0" />
+
+        <body name="r_upper_arm_roll_link" pos="0 0 0">
+          <geom name="uar" type="capsule" fromto="-0.1 0 0 0.1 0 0" size="0.02" />
+          <joint name="r_upper_arm_roll_joint" type="hinge" pos="0 0 0" axis="1 0 0" range="-1.5 1.7" damping="0.1" />
+
+          <body name="r_upper_arm_link" pos="0 0 0">
+            <geom name="ua" type="capsule" fromto="0 0 0 0.4 0 0" size="0.06" />
+
+            <body name="r_elbow_flex_link" pos="0.4 0 0">
+              <geom name="ef" type="capsule" fromto="0 -0.02 0 0.0 0.02 0" size="0.06" />
+              <joint name="r_elbow_flex_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-2.3213 0" damping="0.1" />
+
+              <body name="r_forearm_roll_link" pos="0 0 0">
+                <geom name="fr" type="capsule" fromto="-0.1 0 0 0.1 0 0" size="0.02" />
+                <joint name="r_forearm_roll_joint" type="hinge" limited="true" pos="0 0 0" axis="1 0 0" damping=".1" range="-1.5 1.5"/>
+
+                <body name="r_forearm_link" pos="0 0 0">
+                  <geom name="fa" type="capsule" fromto="0 0 0 0.291 0 0" size="0.05" />
+
+                  <body name="r_wrist_flex_link" pos="0.321 0 0">
+                    <geom name="wf" type="capsule" fromto="0 -0.02 0 0 0.02 0" size="0.01" />
+                    <joint name="r_wrist_flex_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-1.094 0" damping=".1" />
+
+                    <body name="r_wrist_roll_link" pos="0 0 0">
+                      <joint name="r_wrist_roll_joint" type="hinge" pos="0 0 0" limited="true" axis="1 0 0" damping="0.1" range="-1.5 1.5"/>
+                      <body name="tips_arm" pos="0 0 0">
+                        <geom name="tip_arml" type="sphere" pos="0.1 -0.1 0." size="0.01" />
+                        <geom name="tip_armr" type="sphere" pos="0.1 0.1 0." size="0.01" />
+                      </body>
+                      <geom type="capsule" fromto="0 -0.1 0. 0.0 +0.1 0" size="0.02" contype="1" conaffinity="1" />
+                      <geom type="capsule" fromto="0 -0.1 0. 0.1 -0.1 0" size="0.02" contype="1" conaffinity="1" />
+                      <geom type="capsule" fromto="0 +0.1 0. 0.1 +0.1 0." size="0.02" contype="1" conaffinity="1" />
+                    </body>
+                  </body>
+                </body>
+              </body>
+            </body>
+          </body>
+        </body>
+      </body>
+    </body>
+
+    <!--<body name="object" pos="0.55 -0.3 -0.275" >-->
+    <body name="object" pos="0.45 -0.05 -0.275" >
+      <geom rgba="1 1 1 0" type="sphere" size="0.05 0.05 0.05" density="0.00001" conaffinity="0"/>
+      <geom rgba="1 1 1 1" type="cylinder" size="0.05 0.05 0.05" density="0.00001" contype="1" conaffinity="0"/>
+      <joint name="obj_slidey" type="slide" pos="0 0 0" axis="0 1 0" range="-10.3213 10.3" damping="0.5"/>
+      <joint name="obj_slidex" type="slide" pos="0 0 0" axis="1 0 0" range="-10.3213 10.3" damping="0.5"/>
+    </body>
+
+    <body name="goal" pos="0.45 -0.05 -0.3230">
+      <geom rgba="1 0 0 1" type="cylinder" size="0.08 0.001 0.1" density='0.00001' contype="0" conaffinity="0"/>
+      <joint name="goal_slidey" type="slide" pos="0 0 0" axis="0 1 0" range="-10.3213 10.3" damping="0.5"/>
+      <joint name="goal_slidex" type="slide" pos="0 0 0" axis="1 0 0" range="-10.3213 10.3" damping="0.5"/> 
+    </body>
+  </worldbody>
+
+  <actuator>
+    <motor joint="r_shoulder_pan_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_shoulder_lift_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_upper_arm_roll_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_elbow_flex_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_forearm_roll_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_wrist_flex_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_wrist_roll_joint" ctrlrange="-2.0 2.0" ctrllimited="true"/>
+  </actuator>
+</mujoco>
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/assets/reacher3d.xml b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/assets/reacher3d.xml
new file mode 100644
index 0000000..a51c71b
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/assets/reacher3d.xml
@@ -0,0 +1,154 @@
+<mujoco model="arm3d">
+
+    <compiler inertiafromgeom="true" angle="radian" coordinate="local" />
+    <option timestep="0.01" gravity="0 0 0" iterations="20" integrator="RK4" />
+    <default>
+        <joint armature="0.04" damping="1" limited="true" />
+        <geom friction=".5 .1 .1" margin="0.002" condim="1" contype="0" conaffinity="0" />
+    </default>
+    <asset>
+        <texture type="skybox" builtin="checker" rgb1="1 1 1" rgb2="1 1 1"
+                 width="256" height="256"/>
+        <texture name="texgeom" type="cube" builtin="flat" mark="cross" width="127" height="1278" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" markrgb="1 1 1" random="0.01" />
+        <texture name="texplane" type="2d" builtin="checker" rgb1=".5 .5 .5" rgb2=".5 .5 .5" width="100" height="100" />
+        <texture name="texplane_show" type="2d" builtin="checker" rgb1="0 0 0" rgb2="0.9 0.9 0.9" width="100" height="100" />
+        <material name='MatPlane' texture="texplane" shininess="1" texrepeat="30 30" specular="1"  reflectance="0.5" />
+        <material name='geom' texture="texgeom" texuniform="true" />
+    </asset>
+    <worldbody>
+        <light pos="0 0 5" />
+        <body name="r_shoulder_pan_link" pos="0 -0.188 0">
+            <geom name="e1" type="sphere" rgba="0.6 0.6 0.6 1" pos="-0.06 0.05 0.2" size="0.05" />
+            <geom name="e2" type="sphere" rgba="0.6 0.6 0.6 1" pos=" 0.06 0.05 0.2" size="0.05" />
+            <geom name="e1p" type="sphere" rgba="0.1 0.1 0.1 1" pos="-0.06 0.09 0.2" size="0.03" />
+            <geom name="e2p" type="sphere" rgba="0.1 0.1 0.1 1" pos=" 0.06 0.09 0.2" size="0.03" />
+            <geom name="sp" type="capsule" fromto="0 0 -0.4 0 0 0.2" size="0.1" />
+            <joint name="r_shoulder_pan_joint" type="hinge" pos="0 0 0" axis="0 0 1" range="-2.2854 1.714602" damping="10.0" />
+
+            <body name="r_shoulder_lift_link" pos="0.1 0 0">
+                <geom name="sl" type="capsule" fromto="0 -0.1 0 0 0.1 0" size="0.1" />
+                <joint name="r_shoulder_lift_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.5236 1.3963" damping="10.0" />
+
+                <body name="r_upper_arm_roll_link" pos="0 0 0">
+                    <geom name="uar" type="capsule" fromto="-0.1 0 0 0.1 0 0" size="0.02" />
+                    <joint name="r_upper_arm_roll_joint" type="hinge" pos="0 0 0" axis="1 0 0" range="-3.9 0.8" damping="0.1" />
+
+                    <body name="r_upper_arm_link" pos="0 0 0">
+                        <geom name="ua" type="capsule" fromto="0 0 0 0.4 0 0" size="0.06" />
+
+                        <body name="r_elbow_flex_link" pos="0.4 0 0">
+                            <geom name="ef" type="capsule" fromto="0 -0.02 0 0.0 0.02 0" size="0.06" />
+                            <joint name="r_elbow_flex_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-2.3213 0" damping="1.0" />
+
+                            <body name="r_forearm_roll_link" pos="0 0 0">
+                                <geom name="fr" type="capsule" fromto="-0.1 0 0 0.1 0 0" size="0.02" />
+                                <joint name="r_forearm_roll_joint" type="hinge" limited="false" pos="0 0 0" axis="1 0 0" damping=".1" />
+
+                                <body name="r_forearm_link" pos="0 0 0">
+                                    <geom name="fa" type="capsule" fromto="0 0 0 0.321 0 0" size="0.05" />
+
+                                    <body name="r_wrist_flex_link" pos="0.321 0 0">
+                                        <geom name="wf" type="capsule" fromto="0 -0.02 0 0 0.02 0" size="0.01" />
+                                        <joint name="r_wrist_flex_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-2.094 0" damping=".1" />
+
+                                        <body name="r_wrist_roll_link" pos="0 0 0">
+                                            <geom name="wr" type="capsule" fromto="-0.02 0 0 0.02 0 0" size="0.01" />
+                                            <joint name="r_wrist_roll_joint" type="hinge" pos="0 0 0" limited="false" axis="1 0 0" damping="0.1" />
+
+                                            <body name="r_gripper_palm_link" pos="0 0 0">
+                                                <geom name="pl" type="capsule" fromto="0.05 0 -0.02 0.05 0 0.02" size="0.05" />
+
+                                                <!--
+                                                <body name="r_gripper_tool_frame" pos="0.18 0 0">
+                                                    <site name="leg_bottom" pos="0 0 -0.15" size="0.01" />
+                                                    <site name="leg_top" pos="0 0 0.15" size="0.01" />
+
+                                                    <body name="ball" pos="0 0 0">
+                                                        <geom name="ball_geom" rgba="0.8 0.6 0.6 1" type="cylinder" fromto="0 0 -0.15 0 0 0.15" size="0.028" density="2000" contype="2" conaffinity="1" />
+                                                    </body>
+                                                </body>
+                                                -->
+
+                                                <body name="r_gripper_l_finger_link" pos="0.07691 0.03 0">
+                                                    <geom name="gf3" type="capsule" fromto="0 0 0 0.09137 0.00495 0" size="0.01" />
+
+                                                    <body name="r_gripper_l_finger_tip_link" pos="0.09137 0.00495 0">
+                                                        <geom name="gf4" type="capsule" fromto="0 0 0 0.09137 0.0 0" size="0.01" />
+                                                    </body>
+                                                </body>
+
+                                                <body name="r_gripper_r_finger_link" pos="0.07691 -0.03 0">
+                                                    <geom name="gf1" type="capsule" fromto="0 0 0 0.09137 -0.00495 0" size="0.01" />
+
+                                                    <body name="r_gripper_r_finger_tip_link" pos="0.09137 -0.00495 0">
+                                                        <geom name="gf2" type="capsule" fromto="0 0 0 0.09137 0.0 0" size="0.01" />
+                                                    </body>
+                                                </body>
+                                            </body>
+                                        </body>
+                                    </body>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                </body>
+            </body>
+        </body>
+
+        <!--
+        <body name="g1" pos="0.034 0.3 -0.47" axisangle="0 1 0 0.05">
+            <geom name="g1" rgba="0.2 0.2 0.2 1" type="box" size="0.003 0.01 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="g2" pos="-0.034 0.3 -0.47" axisangle="0 1 0 -0.05">
+            <geom name="g2" rgba="0.2 0.2 0.2 1" type="box" size="0.003 0.01 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="g3" pos="0.0 0.334 -0.47" axisangle="1 0 0 -0.05">
+            <geom name="g3" rgba="0.2 0.2 0.2 1" type="box" size="0.01 0.003 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="g4" pos="0.0 0.266 -0.47" axisangle="1 0 0 0.05">
+            <geom name="g4" rgba="0.2 0.2 0.2 1" type="box" size="0.01 0.003 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="fl" pos="0.0 0.3 -0.55">
+            <geom name="fl" rgba="0.2 0.2 0.2 1" type="box" size="0.2 0.2 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="w1" pos="0.216 0.3 -0.45">
+            <geom name="w1" rgba="0.2 0.2 0.2 1" type="box" size="0.183 0.3 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="w2" pos="-0.216 0.3 -0.45">
+            <geom name="w2" rgba="0.2 0.2 0.2 1" type="box" size="0.183 0.3 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="w3" pos="0.0 0.516 -0.45">
+            <geom name="w3" rgba="0.2 0.2 0.2 1" type="box" size="0.032 0.183 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="w4" pos="0.0 0.084 -0.45">
+            <geom name="w4" rgba="0.2 0.2 0.2 1" type="box" size="0.032 0.183 0.05" contype="1" conaffinity="1" />
+        </body>
+        -->
+
+        <body name="target" pos="0 0.25 0">
+            <joint armature="0" axis="1 0 0" damping="0" limited="false" name="target_x" pos="0 0 0" ref="0" stiffness="0" type="slide"/>
+            <joint armature="0" axis="0 1 0" damping="0" limited="false" name="target_y" pos="0 0 0" ref="0.25" stiffness="0" type="slide"/>
+            <joint armature="0" axis="0 0 1" damping="0" limited="false" name="target_z" pos="0 0 0" ref="0" stiffness="0" type="slide"/>
+            <geom conaffinity="0" contype="0" name="target" pos="0 0 0" rgba="0.9 0.2 0.2 1" size=".035" type="sphere"/>
+        </body>
+    </worldbody>
+
+    <actuator>
+        <motor joint="r_shoulder_pan_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+        <motor joint="r_shoulder_lift_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+        <motor joint="r_upper_arm_roll_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+        <motor joint="r_elbow_flex_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+        <motor joint="r_forearm_roll_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+        <motor joint="r_wrist_flex_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+        <motor joint="r_wrist_roll_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+    </actuator>
+
+</mujoco>
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/cartpole.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/cartpole.py
new file mode 100644
index 0000000..ddc3d41
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/cartpole.py
@@ -0,0 +1,193 @@
+"""
+Classic cart-pole system implemented by Rich Sutton et al.
+Copied from https://webdocs.cs.ualberta.ca/~sutton/book/code/pole.c
+"""
+
+import logging
+import math
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+from slbo.utils.dataset import Dataset, gen_dtype
+
+
+logger = logging.getLogger(__name__)
+
+
+class CartPoleEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 50
+    }
+
+    def __init__(self):
+        self.gravity = 9.8
+        self.masscart = 1.0
+        self.masspole = 0.1
+        self.total_mass = (self.masspole + self.masscart)
+        self.length = 0.5  # actually half the pole's length
+        self.polemass_length = (self.masspole * self.length)
+        self.force_mag = 10.0
+        self.tau = 0.02  # seconds between state updates
+
+        # Angle at which to fail the episode
+        self.theta_threshold_radians = 12 * 2 * math.pi / 360
+        self.x_threshold = 2.4
+
+        # Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds
+        high = np.array([
+            self.x_threshold * 2,
+            np.finfo(np.float32).max,
+            self.theta_threshold_radians * 2,
+            np.finfo(np.float32).max])
+
+        # self.action_space = spaces.Discrete(2)
+        self.action_space = \
+            spaces.Box(low=np.array([-1.0]), high=np.array([1.0]))
+        self.observation_space = spaces.Box(-high, high)
+
+        self._seed()
+        self.viewer = None
+        self.state = None
+
+        self.steps_beyond_done = None
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, action):
+        action = 1 if action[0] > .0 else 0
+        # assert self.action_space.contains(action), "%r (%s) invalid" % (action, type(action))
+        state = self.state
+        obs = self.state
+        reward = np.cos(obs[2]) - 0.01 * (obs[0] ** 2)
+
+        x, x_dot, theta, theta_dot = state
+        force = self.force_mag if action == 1 else -self.force_mag
+        costheta = math.cos(theta)
+        sintheta = math.sin(theta)
+        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass
+        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta * costheta / self.total_mass))
+        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass
+        x = x + self.tau * x_dot
+        x_dot = x_dot + self.tau * xacc
+        theta = theta + self.tau * theta_dot
+        theta_dot = theta_dot + self.tau * thetaacc
+        self.state = (x, x_dot, theta, theta_dot)
+        '''
+        done = x < -self.x_threshold \
+            or x > self.x_threshold \
+            or theta < -self.theta_threshold_radians \
+            or theta > self.theta_threshold_radians
+        done = bool(done)
+
+        if not done:
+            reward = 1.0
+        elif self.steps_beyond_done is None:
+            # Pole just fell!
+            self.steps_beyond_done = 0
+            reward = 1.0
+        else:
+            if self.steps_beyond_done == 0:
+                logger.warning("You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.")
+            self.steps_beyond_done += 1
+            reward = 0.0
+        '''
+        done = False
+        self.steps_beyond_done = None
+
+        return np.array(self.state), reward, done, {}
+
+    def _reset(self):
+        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))
+        self.steps_beyond_done = None
+        return np.array(self.state)
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        screen_width = 600
+        screen_height = 400
+
+        world_width = self.x_threshold * 2
+        scale = screen_width / world_width
+        carty = 100  # TOP OF CART
+        polewidth = 10.0
+        polelen = scale * 1.0
+        cartwidth = 50.0
+        cartheight = 30.0
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(screen_width, screen_height)
+            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2
+            axleoffset = cartheight / 4.0
+            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            self.carttrans = rendering.Transform()
+            cart.add_attr(self.carttrans)
+            self.viewer.add_geom(cart)
+            l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2
+            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            pole.set_color(.8, .6, .4)
+            self.poletrans = rendering.Transform(translation=(0, axleoffset))
+            pole.add_attr(self.poletrans)
+            pole.add_attr(self.carttrans)
+            self.viewer.add_geom(pole)
+            self.axle = rendering.make_circle(polewidth / 2)
+            self.axle.add_attr(self.poletrans)
+            self.axle.add_attr(self.carttrans)
+            self.axle.set_color(.5, .5, .8)
+            self.viewer.add_geom(self.axle)
+            self.track = rendering.Line((0, carty), (screen_width, carty))
+            self.track.set_color(0, 0, 0)
+            self.viewer.add_geom(self.track)
+
+        if self.state is None:
+            return None
+
+        x = self.state
+        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART
+        self.carttrans.set_translation(cartx, carty)
+        self.poletrans.set_rotation(-x[2])
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        x = obs[:, 0]
+        theta = obs[:, 2]
+        return -(np.cos(theta) - 0.01 * (x ** 2))
+
+    def verify(self, n=2000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
\ No newline at end of file
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_cartpoleO001.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_cartpoleO001.py
new file mode 100644
index 0000000..2407b55
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_cartpoleO001.py
@@ -0,0 +1,178 @@
+"""
+Classic cart-pole system implemented by Rich Sutton et al.
+Copied from https://webdocs.cs.ualberta.ca/~sutton/book/code/pole.c
+"""
+
+import logging
+import math
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+
+
+logger = logging.getLogger(__name__)
+
+
+class CartPoleEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 50
+    }
+
+    def __init__(self):
+        self.gravity = 9.8
+        self.masscart = 1.0
+        self.masspole = 0.1
+        self.total_mass = (self.masspole + self.masscart)
+        self.length = 0.5  # actually half the pole's length
+        self.polemass_length = (self.masspole * self.length)
+        self.force_mag = 10.0
+        self.tau = 0.02  # seconds between state updates
+
+        # Angle at which to fail the episode
+        self.theta_threshold_radians = 12 * 2 * math.pi / 360
+        self.x_threshold = 2.4
+
+        # Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds
+        high = np.array([
+            self.x_threshold * 2,
+            np.finfo(np.float32).max,
+            self.theta_threshold_radians * 2,
+            np.finfo(np.float32).max])
+
+        # self.action_space = spaces.Discrete(2)
+        self.action_space = \
+            spaces.Box(low=np.array([-1.0]), high=np.array([1.0]))
+        self.observation_space = spaces.Box(-high, high)
+
+        self._seed()
+        self.viewer = None
+        self.state = None
+
+        self.steps_beyond_done = None
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, action):
+        action = 1 if action[0] > .0 else 0
+        # assert self.action_space.contains(action), "%r (%s) invalid" % (action, type(action))
+        state = self.state
+        obs = self.state
+        reward = np.cos(obs[2]) - 0.01 * (obs[0] ** 2)
+
+        x, x_dot, theta, theta_dot = state
+        force = self.force_mag if action == 1 else -self.force_mag
+        costheta = math.cos(theta)
+        sintheta = math.sin(theta)
+        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass
+        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta * costheta / self.total_mass))
+        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass
+        x = x + self.tau * x_dot
+        x_dot = x_dot + self.tau * xacc
+        theta = theta + self.tau * theta_dot
+        theta_dot = theta_dot + self.tau * thetaacc
+        self.state = (x, x_dot, theta, theta_dot)
+        '''
+        done = x < -self.x_threshold \
+            or x > self.x_threshold \
+            or theta < -self.theta_threshold_radians \
+            or theta > self.theta_threshold_radians
+        done = bool(done)
+
+        if not done:
+            reward = 1.0
+        elif self.steps_beyond_done is None:
+            # Pole just fell!
+            self.steps_beyond_done = 0
+            reward = 1.0
+        else:
+            if self.steps_beyond_done == 0:
+                logger.warning("You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.")
+            self.steps_beyond_done += 1
+            reward = 0.0
+        '''
+        done = False
+        self.steps_beyond_done = None
+
+        ob = np.array(self.state)
+        ob += np.random.uniform(low=-0.01, high=0.01, size=ob.shape)
+
+        return ob, reward, done, {}
+
+    def _reset(self):
+        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))
+        self.steps_beyond_done = None
+        return np.array(self.state)
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        screen_width = 600
+        screen_height = 400
+
+        world_width = self.x_threshold * 2
+        scale = screen_width / world_width
+        carty = 100  # TOP OF CART
+        polewidth = 10.0
+        polelen = scale * 1.0
+        cartwidth = 50.0
+        cartheight = 30.0
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(screen_width, screen_height)
+            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2
+            axleoffset = cartheight / 4.0
+            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            self.carttrans = rendering.Transform()
+            cart.add_attr(self.carttrans)
+            self.viewer.add_geom(cart)
+            l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2
+            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            pole.set_color(.8, .6, .4)
+            self.poletrans = rendering.Transform(translation=(0, axleoffset))
+            pole.add_attr(self.poletrans)
+            pole.add_attr(self.carttrans)
+            self.viewer.add_geom(pole)
+            self.axle = rendering.make_circle(polewidth / 2)
+            self.axle.add_attr(self.poletrans)
+            self.axle.add_attr(self.carttrans)
+            self.axle.set_color(.5, .5, .8)
+            self.viewer.add_geom(self.axle)
+            self.track = rendering.Line((0, carty), (screen_width, carty))
+            self.track.set_color(0, 0, 0)
+            self.viewer.add_geom(self.track)
+
+        if self.state is None:
+            return None
+
+        x = self.state
+        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART
+        self.carttrans.set_translation(cartx, carty)
+        self.poletrans.set_rotation(-x[2])
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        x = obs[:, 0]
+        theta = obs[:, 2]
+        return -(np.cos(theta) - 0.01 * (x ** 2))
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_cartpoleO01.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_cartpoleO01.py
new file mode 100644
index 0000000..62516c3
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_cartpoleO01.py
@@ -0,0 +1,177 @@
+"""
+Classic cart-pole system implemented by Rich Sutton et al.
+Copied from https://webdocs.cs.ualberta.ca/~sutton/book/code/pole.c
+"""
+
+import logging
+import math
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+
+logger = logging.getLogger(__name__)
+
+
+class CartPoleEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 50
+    }
+
+    def __init__(self):
+        self.gravity = 9.8
+        self.masscart = 1.0
+        self.masspole = 0.1
+        self.total_mass = (self.masspole + self.masscart)
+        self.length = 0.5  # actually half the pole's length
+        self.polemass_length = (self.masspole * self.length)
+        self.force_mag = 10.0
+        self.tau = 0.02  # seconds between state updates
+
+        # Angle at which to fail the episode
+        self.theta_threshold_radians = 12 * 2 * math.pi / 360
+        self.x_threshold = 2.4
+
+        # Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds
+        high = np.array([
+            self.x_threshold * 2,
+            np.finfo(np.float32).max,
+            self.theta_threshold_radians * 2,
+            np.finfo(np.float32).max])
+
+        # self.action_space = spaces.Discrete(2)
+        self.action_space = \
+            spaces.Box(low=np.array([-1.0]), high=np.array([1.0]))
+        self.observation_space = spaces.Box(-high, high)
+
+        self._seed()
+        self.viewer = None
+        self.state = None
+
+        self.steps_beyond_done = None
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, action):
+        action = 1 if action[0] > .0 else 0
+        # assert self.action_space.contains(action), "%r (%s) invalid" % (action, type(action))
+        state = self.state
+        obs = self.state
+        reward = np.cos(obs[2]) - 0.01 * (obs[0] ** 2)
+
+        x, x_dot, theta, theta_dot = state
+        force = self.force_mag if action == 1 else -self.force_mag
+        costheta = math.cos(theta)
+        sintheta = math.sin(theta)
+        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass
+        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta * costheta / self.total_mass))
+        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass
+        x = x + self.tau * x_dot
+        x_dot = x_dot + self.tau * xacc
+        theta = theta + self.tau * theta_dot
+        theta_dot = theta_dot + self.tau * thetaacc
+        self.state = (x, x_dot, theta, theta_dot)
+        '''
+        done = x < -self.x_threshold \
+            or x > self.x_threshold \
+            or theta < -self.theta_threshold_radians \
+            or theta > self.theta_threshold_radians
+        done = bool(done)
+
+        if not done:
+            reward = 1.0
+        elif self.steps_beyond_done is None:
+            # Pole just fell!
+            self.steps_beyond_done = 0
+            reward = 1.0
+        else:
+            if self.steps_beyond_done == 0:
+                logger.warning("You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.")
+            self.steps_beyond_done += 1
+            reward = 0.0
+        '''
+        done = False
+        self.steps_beyond_done = None
+
+        ob = np.array(self.state)
+        ob += np.random.uniform(low=-0.1, high=0.1, size=ob.shape)
+
+        return ob, reward, done, {}
+
+    def _reset(self):
+        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))
+        self.steps_beyond_done = None
+        return np.array(self.state)
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        screen_width = 600
+        screen_height = 400
+
+        world_width = self.x_threshold * 2
+        scale = screen_width / world_width
+        carty = 100  # TOP OF CART
+        polewidth = 10.0
+        polelen = scale * 1.0
+        cartwidth = 50.0
+        cartheight = 30.0
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(screen_width, screen_height)
+            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2
+            axleoffset = cartheight / 4.0
+            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            self.carttrans = rendering.Transform()
+            cart.add_attr(self.carttrans)
+            self.viewer.add_geom(cart)
+            l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2
+            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            pole.set_color(.8, .6, .4)
+            self.poletrans = rendering.Transform(translation=(0, axleoffset))
+            pole.add_attr(self.poletrans)
+            pole.add_attr(self.carttrans)
+            self.viewer.add_geom(pole)
+            self.axle = rendering.make_circle(polewidth / 2)
+            self.axle.add_attr(self.poletrans)
+            self.axle.add_attr(self.carttrans)
+            self.axle.set_color(.5, .5, .8)
+            self.viewer.add_geom(self.axle)
+            self.track = rendering.Line((0, carty), (screen_width, carty))
+            self.track.set_color(0, 0, 0)
+            self.viewer.add_geom(self.track)
+
+        if self.state is None:
+            return None
+
+        x = self.state
+        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART
+        self.carttrans.set_translation(cartx, carty)
+        self.poletrans.set_rotation(-x[2])
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        x = obs[:, 0]
+        theta = obs[:, 2]
+        return -(np.cos(theta) - 0.01 * (x ** 2))
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_cheetahA003.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_cheetahA003.py
new file mode 100644
index 0000000..e759126
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_cheetahA003.py
@@ -0,0 +1,81 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from slbo.envs import BaseModelBasedEnv
+from gym.envs.mujoco import mujoco_env
+
+
+class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/half_cheetah.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        action = np.array(action)
+        action += np.random.uniform(low=-0.03, high=0.03, size=action.shape)
+        start_ob = self._get_obs()
+        reward_run = start_ob[8]
+
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        reward_ctrl = -0.1 * np.square(action).sum()
+
+        reward = reward_run + reward_ctrl
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        qpos = self.init_qpos + \
+            self.np_random.uniform(low=-.1, high=.1, size=self.model.nq)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_cheetahA01.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_cheetahA01.py
new file mode 100644
index 0000000..e496056
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_cheetahA01.py
@@ -0,0 +1,81 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/half_cheetah.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        action = np.array(action)
+        action += np.random.uniform(low=-0.1, high=0.1, size=action.shape)
+        start_ob = self._get_obs()
+        reward_run = start_ob[8]
+
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        reward_ctrl = -0.1 * np.square(action).sum()
+
+        reward = reward_run + reward_ctrl
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        qpos = self.init_qpos + \
+            self.np_random.uniform(low=-.1, high=.1, size=self.model.nq)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_cheetahO001.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_cheetahO001.py
new file mode 100644
index 0000000..252505f
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_cheetahO001.py
@@ -0,0 +1,80 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/half_cheetah.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        start_ob = self._get_obs()
+        reward_run = start_ob[8]
+
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        reward_ctrl = -0.1 * np.square(action).sum()
+
+        reward = reward_run + reward_ctrl
+        done = False
+        ob += np.random.uniform(low=-0.01, high=0.01, size=ob.shape)
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        qpos = self.init_qpos + \
+            self.np_random.uniform(low=-.1, high=.1, size=self.model.nq)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_cheetahO01.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_cheetahO01.py
new file mode 100644
index 0000000..4e6fe93
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_cheetahO01.py
@@ -0,0 +1,80 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/half_cheetah.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        start_ob = self._get_obs()
+        reward_run = start_ob[8]
+
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        reward_ctrl = -0.1 * np.square(action).sum()
+
+        reward = reward_run + reward_ctrl
+        done = False
+        ob += np.random.uniform(low=-0.1, high=0.1, size=ob.shape)
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        qpos = self.init_qpos + \
+            self.np_random.uniform(low=-.1, high=.1, size=self.model.nq)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_fant.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_fant.py
new file mode 100644
index 0000000..29633e2
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_fant.py
@@ -0,0 +1,84 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+
+from slbo.envs import BaseModelBasedEnv
+
+
+class AntEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/ant.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        ob = self._get_obs()
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = old_ob[13]
+        reward_height = -3.0 * np.square(old_ob[0] - 0.57)
+
+        # the alive bonus
+        height = ob[0]
+        done = (height > 1.0) or (height < 0.2)
+        alive_reward = float(not done)
+
+        reward = reward_run + reward_ctrl + reward_height + alive_reward
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[2:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        qpos = self.init_qpos + \
+            self.np_random.uniform(size=self.model.nq, low=-.1, high=.1)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        # self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 13]
+        reward_height = -3.0 * np.square(obs[:, 0] - 0.57)
+
+        height = next_obs[:, 0]
+        done = np.logical_or((height > 1.0), (height < 0.2))
+        alive_reward = 1.0 - np.array(done, dtype=np.float)
+
+        reward = reward_run + reward_ctrl + reward_height + alive_reward
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+
+    def mb_step(self, states, actions, next_states):
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        height = next_states[:, 0]
+        done = np.logical_or((height > 1.0), (height < 0.2))
+        return rewards, done
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_fhopper.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_fhopper.py
new file mode 100644
index 0000000..84edf3d
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_fhopper.py
@@ -0,0 +1,91 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HopperEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=4):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/hopper.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = old_ob[5]
+        reward_height = -3.0 * np.square(old_ob[0] - 1.3)
+        height, ang = ob[0], ob[1]
+        done = (height <= 0.7) or (abs(ang) >= 0.2)
+        alive_reward = float(not done)
+        reward = reward_run + reward_ctrl + reward_height + alive_reward
+
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-.005, high=.005, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-.005, high=.005, size=self.model.nv)
+        )
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 2
+        self.viewer.cam.distance = self.model.stat.extent * 0.75
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 5]
+        reward_height = -3.0 * np.square(obs[:, 0] - 1.3)
+        height, ang = next_obs[:, 0], next_obs[:, 1]
+        done = np.logical_or(height <= 0.7, abs(ang) >= 0.2)
+        alive_reward = 1.0 - np.array(done, dtype=np.float)
+        reward = reward_run + reward_ctrl + reward_height + alive_reward
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        # reward_height = -3.0 * tf.square(next_obs[:, 1] - 1.3)
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+        raise NotImplementedError
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        height, ang = next_states[:, 0], next_states[:, 1]
+        done = np.logical_or(height <= 0.7, abs(ang) >= 0.2)
+        return rewards, done
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_fswimmer.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_fswimmer.py
new file mode 100644
index 0000000..86bdc80
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_fswimmer.py
@@ -0,0 +1,69 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+import os
+
+
+class fixedSwimmerEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+
+    def __init__(self):
+        dir_path = os.path.dirname(os.path.realpath(__file__))
+        mujoco_env.MujocoEnv.__init__(self, '%s/assets/fixed_swimmer.xml' % dir_path, 4)
+        utils.EzPickle.__init__(self)
+
+    def _step(self, a):
+        ctrl_cost_coeff = 0.0001
+
+        """
+        xposbefore = self.model.data.qpos[0, 0]
+        self.do_simulation(a, self.frame_skip)
+        xposafter = self.model.data.qpos[0, 0]
+        """
+
+        self.xposbefore = self.model.data.site_xpos[0][0] / self.dt
+        self.do_simulation(a, self.frame_skip)
+        self.xposafter = self.model.data.site_xpos[0][0] / self.dt
+        self.pos_diff = self.xposafter - self.xposbefore
+
+        reward_fwd = self.xposafter - self.xposbefore
+        reward_ctrl = - ctrl_cost_coeff * np.square(a).sum()
+        reward = reward_fwd + reward_ctrl
+        ob = self._get_obs()
+        return ob, reward, False, dict(reward_fwd=reward_fwd, reward_ctrl=reward_ctrl)
+
+    def _get_obs(self):
+        qpos = self.model.data.qpos
+        qvel = self.model.data.qvel
+        return np.concatenate([qpos.flat[2:], qvel.flat, self.pos_diff.flat])
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-.1, high=.1, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-.1, high=.1, size=self.model.nv)
+        )
+        return self._get_obs()
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.0001 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, -1]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+
+    def verify(self):
+        pass
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_fwalker2d.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_fwalker2d.py
new file mode 100644
index 0000000..25006d3
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_fwalker2d.py
@@ -0,0 +1,99 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class Walker2dEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=4):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/walker2d.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = old_ob[8]
+        reward_height = -3.0 * np.square(old_ob[0] - 1.3)
+
+        height, ang = ob[0], ob[1]
+        done = (height >= 2.0) or (height <= 0.8) or (abs(ang) >= 1.0)
+        alive_reward = float(not done)
+
+        reward = reward_run + reward_ctrl + reward_height + alive_reward
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat
+        ])
+
+    def reset_model(self):
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-.005, high=.005, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-.005, high=.005, size=self.model.nv)
+        )
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 2
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward_height = -3.0 * np.square(next_obs[:, 0] - 1.3)
+        height, ang = next_obs[:, 0], next_obs[:, 1]
+        done = np.logical_or(
+            np.logical_or(height >= 2.0, height <= 0.8),
+            np.abs(ang) >= 1.0
+        )
+        alive_reward = 1.0 - np.array(done, dtype=np.float)
+        reward = reward_run + reward_ctrl + reward_height + alive_reward
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        # reward_height = -3.0 * tf.square(next_obs[:, 1] - 1.3)
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+        raise NotImplementedError
+
+    def verify(self):
+        pass
+
+    def mb_step(self, states, actions, next_states):
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        height, ang = next_states[:, 0], next_states[:, 1]
+        done = np.logical_or(
+            np.logical_or(height >= 2.0, height <= 0.8),
+            np.abs(ang) >= 1.0
+        )
+        return rewards, done
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_humanoid.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_humanoid.py
new file mode 100644
index 0000000..7d220ba
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_humanoid.py
@@ -0,0 +1,89 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+import numpy as np
+from gym.envs.mujoco import mujoco_env
+from gym import utils
+from slbo.envs import BaseModelBasedEnv
+
+
+class HumanoidEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self):
+        mujoco_env.MujocoEnv.__init__(self, 'humanoid.xml', 5)
+        utils.EzPickle.__init__(self)
+
+    def _get_obs(self):
+        data = self.model.data
+        return np.concatenate([data.qpos.flat[2:],
+                               data.qvel.flat,
+                               data.cinert.flat,
+                               data.cvel.flat,
+                               data.qfrc_actuator.flat,
+                               data.cfrc_ext.flat])
+
+    def _step(self, a):
+        data = self.model.data
+        action = a
+        if getattr(self, 'action_space', None):
+            action = np.clip(a, self.action_space.low,
+                             self.action_space.high)
+
+        # reward
+        alive_bonus = 5.0
+        lin_vel_cost = 0.25 / 0.015 * data.qvel.flat[0]
+        quad_ctrl_cost = 0.1 * np.square(action).sum()
+        quad_impact_cost = .5e-6 * np.square(data.cfrc_ext).sum()
+        quad_impact_cost = min(quad_impact_cost, 10)
+
+        self.do_simulation(action, self.frame_skip)
+        reward = lin_vel_cost - quad_ctrl_cost - quad_impact_cost + alive_bonus
+        qpos = self.model.data.qpos
+        done = bool((qpos[2] < 1.0) or (qpos[2] > 2.0))
+        return self._get_obs(), reward, done, dict(reward_linvel=lin_vel_cost, reward_quadctrl=-quad_ctrl_cost, reward_alive=alive_bonus, reward_impact=-quad_impact_cost)
+
+    def reset_model(self):
+        c = 0.01
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-c, high=c, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-c, high=c, size=self.model.nv,)
+        )
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 1
+        self.viewer.cam.distance = self.model.stat.extent * 1.0
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = 0.25 / 0.015 * obs[:, 22]
+
+        quad_impact_cost = .5e-6 * np.square(obs[:, -84:]).sum()
+        quad_impact_cost = min(quad_impact_cost, 10)
+
+        height = next_obs[:, 0]
+        done = np.logical_or((height > 2.0), (height < 1.0))
+        alive_reward = 5 * (1.0 - np.array(done, dtype=np.float))
+
+        reward = reward_run + reward_ctrl + (-quad_impact_cost) + alive_reward
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+
+        height = next_states[:, 0]
+        done = np.logical_or((height > 2.0), (height < 1.0))
+        return rewards, done
+
+    def verify(self):
+        pass
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_nostopslimhumanoid.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_nostopslimhumanoid.py
new file mode 100644
index 0000000..1c87b20
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_nostopslimhumanoid.py
@@ -0,0 +1,81 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+import numpy as np
+from gym.envs.mujoco import mujoco_env
+from gym import utils
+from slbo.envs import BaseModelBasedEnv
+
+
+class HumanoidEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self):
+        mujoco_env.MujocoEnv.__init__(self, 'humanoid.xml', 5)
+        utils.EzPickle.__init__(self)
+
+    def _get_obs(self):
+        data = self.model.data
+        return np.concatenate([data.qpos.flat[2:],
+                               data.qvel.flat])
+
+    def _step(self, a):
+        data = self.model.data
+        action = a
+        if getattr(self, 'action_space', None):
+            action = np.clip(a, self.action_space.low,
+                             self.action_space.high)
+        qpos = self.model.data.qpos
+        done = bool((qpos[2] < 1.0) or (qpos[2] > 2.0))
+
+        # reward
+        alive_bonus = 5 * (1 - float(done))
+        lin_vel_cost = 0.25 / 0.015 * data.qvel.flat[0]
+        quad_ctrl_cost = 0.1 * np.square(action).sum()
+        quad_impact_cost = 0.0
+
+        self.do_simulation(action, self.frame_skip)
+        reward = lin_vel_cost - quad_ctrl_cost - quad_impact_cost + alive_bonus
+        done = False
+        return self._get_obs(), reward, done, dict(reward_linvel=lin_vel_cost, reward_quadctrl=-quad_ctrl_cost, reward_alive=alive_bonus, reward_impact=-quad_impact_cost)
+
+    def reset_model(self):
+        c = 0.01
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-c, high=c, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-c, high=c, size=self.model.nv,)
+        )
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 1
+        self.viewer.cam.distance = self.model.stat.extent * 1.0
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = 0.25 / 0.015 * obs[:, 22]
+
+        quad_impact_cost = 0.0
+
+        height = next_obs[:, 0]
+        done = np.logical_or((height > 2.0), (height < 1.0))
+        alive_reward = 5 * (1.0 - np.array(done, dtype=np.float))
+
+        reward = reward_run + reward_ctrl + (-quad_impact_cost) + alive_reward
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_pendulumO001.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_pendulumO001.py
new file mode 100644
index 0000000..de873df
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_pendulumO001.py
@@ -0,0 +1,138 @@
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+from os import path
+
+
+class PendulumEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 30
+    }
+
+    def __init__(self):
+        self.max_speed = 8
+        self.max_torque = 2.
+        self.dt = .05
+        self.viewer = None
+
+        high = np.array([1., 1., self.max_speed])
+        self.action_space = spaces.Box(low=-self.max_torque, high=self.max_torque, shape=(1,))
+        self.observation_space = spaces.Box(low=-high, high=high)
+
+        self._seed()
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, u):
+        th, thdot = self.state  # th := theta
+        '''
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+        '''
+
+        # for the reward
+        y, x, thetadot = np.cos(th), np.sin(th), thdot
+        u = np.clip(u, -self.max_torque, self.max_torque)[0]
+        costs = y + .1 * np.abs(x) + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        reward = -costs
+
+        g = 10.
+        m = 1.
+        l = 1.
+        dt = self.dt
+
+        self.last_u = u  # for rendering
+        # costs = angle_normalize(th) ** 2 + .1 * thdot ** 2 + .001 * (u ** 2)
+
+        newthdot = thdot + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3. / (m * l ** 2) * u) * dt
+        newth = th + newthdot * dt
+        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)  # pylint: disable=E1111
+
+        self.state = np.array([newth, newthdot])
+        ob = self._get_obs()
+        ob += np.random.uniform(low=-0.01, high=0.01, size=ob.shape)
+        return ob, reward, False, {}
+
+    def _reset(self):
+        high = np.array([np.pi, 1])
+        self.state = self.np_random.uniform(low=-high, high=high)
+        self.last_u = None
+        return self._get_obs()
+
+    def _get_obs(self):
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(500, 500)
+            self.viewer.set_bounds(-2.2, 2.2, -2.2, 2.2)
+            rod = rendering.make_capsule(1, .2)
+            rod.set_color(.8, .3, .3)
+            self.pole_transform = rendering.Transform()
+            rod.add_attr(self.pole_transform)
+            self.viewer.add_geom(rod)
+            axle = rendering.make_circle(.05)
+            axle.set_color(0, 0, 0)
+            self.viewer.add_geom(axle)
+            fname = path.join(path.dirname(__file__), "assets/clockwise.png")
+            self.img = rendering.Image(fname, 1., 1.)
+            self.imgtrans = rendering.Transform()
+            self.img.add_attr(self.imgtrans)
+
+        self.viewer.add_onetime(self.img)
+        self.pole_transform.set_rotation(self.state[0] + np.pi / 2)
+        if self.last_u:
+            self.imgtrans.scale = (-self.last_u / 2, np.abs(self.last_u) / 2)
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+        dist_vec = obs[:, -3:]
+        reward_dist = - np.linalg.norm(dist_vec, axis=1)
+        reward_ctrl = - np.sum(np.square(acts), axis=1)
+        reward = reward_dist + reward_ctrl
+
+        # for the reward
+        y, x, thetadot = np.cos(th), np.sin(th), thdot
+        u = np.clip(u, -self.max_torque, self.max_torque)[0]
+        costs = y + .1 * x + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        reward = -costs
+
+        def _get_obs(self):
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+
+        """
+        y, x, thetadot = obs[:, 0], obs[:, 1], obs[:, 2]
+        u = np.clip(acts[:, 0], -self.max_torque, self.max_torque)
+        costs = y + .1 * np.abs(x) + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        return costs
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
+
+
+def angle_normalize(x):
+    return (((x + np.pi) % (2 * np.pi)) - np.pi)
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_pendulumO01.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_pendulumO01.py
new file mode 100644
index 0000000..dbd460d
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_pendulumO01.py
@@ -0,0 +1,138 @@
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+from os import path
+
+
+class PendulumEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 30
+    }
+
+    def __init__(self):
+        self.max_speed = 8
+        self.max_torque = 2.
+        self.dt = .05
+        self.viewer = None
+
+        high = np.array([1., 1., self.max_speed])
+        self.action_space = spaces.Box(low=-self.max_torque, high=self.max_torque, shape=(1,))
+        self.observation_space = spaces.Box(low=-high, high=high)
+
+        self._seed()
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, u):
+        th, thdot = self.state  # th := theta
+        '''
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+        '''
+
+        # for the reward
+        y, x, thetadot = np.cos(th), np.sin(th), thdot
+        u = np.clip(u, -self.max_torque, self.max_torque)[0]
+        costs = y + .1 * np.abs(x) + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        reward = -costs
+
+        g = 10.
+        m = 1.
+        l = 1.
+        dt = self.dt
+
+        self.last_u = u  # for rendering
+        # costs = angle_normalize(th) ** 2 + .1 * thdot ** 2 + .001 * (u ** 2)
+
+        newthdot = thdot + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3. / (m * l ** 2) * u) * dt
+        newth = th + newthdot * dt
+        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)  # pylint: disable=E1111
+
+        self.state = np.array([newth, newthdot])
+        ob = self._get_obs()
+        ob += np.random.uniform(low=-0.1, high=0.1, size=ob.shape)
+        return ob, reward, False, {}
+
+    def _reset(self):
+        high = np.array([np.pi, 1])
+        self.state = self.np_random.uniform(low=-high, high=high)
+        self.last_u = None
+        return self._get_obs()
+
+    def _get_obs(self):
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(500, 500)
+            self.viewer.set_bounds(-2.2, 2.2, -2.2, 2.2)
+            rod = rendering.make_capsule(1, .2)
+            rod.set_color(.8, .3, .3)
+            self.pole_transform = rendering.Transform()
+            rod.add_attr(self.pole_transform)
+            self.viewer.add_geom(rod)
+            axle = rendering.make_circle(.05)
+            axle.set_color(0, 0, 0)
+            self.viewer.add_geom(axle)
+            fname = path.join(path.dirname(__file__), "assets/clockwise.png")
+            self.img = rendering.Image(fname, 1., 1.)
+            self.imgtrans = rendering.Transform()
+            self.img.add_attr(self.imgtrans)
+
+        self.viewer.add_onetime(self.img)
+        self.pole_transform.set_rotation(self.state[0] + np.pi / 2)
+        if self.last_u:
+            self.imgtrans.scale = (-self.last_u / 2, np.abs(self.last_u) / 2)
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+        dist_vec = obs[:, -3:]
+        reward_dist = - np.linalg.norm(dist_vec, axis=1)
+        reward_ctrl = - np.sum(np.square(acts), axis=1)
+        reward = reward_dist + reward_ctrl
+
+        # for the reward
+        y, x, thetadot = np.cos(th), np.sin(th), thdot
+        u = np.clip(u, -self.max_torque, self.max_torque)[0]
+        costs = y + .1 * x + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        reward = -costs
+
+        def _get_obs(self):
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+
+        """
+        y, x, thetadot = obs[:, 0], obs[:, 1], obs[:, 2]
+        u = np.clip(acts[:, 0], -self.max_torque, self.max_torque)
+        costs = y + .1 * np.abs(x) + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        return costs
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
+
+
+def angle_normalize(x):
+    return (((x + np.pi) % (2 * np.pi)) - np.pi)
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_slimhumanoid.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_slimhumanoid.py
new file mode 100644
index 0000000..cfaf6ad
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/gym_slimhumanoid.py
@@ -0,0 +1,82 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+import numpy as np
+from gym.envs.mujoco import mujoco_env
+from gym import utils
+from slbo.envs import BaseModelBasedEnv
+
+
+class HumanoidEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self):
+        mujoco_env.MujocoEnv.__init__(self, 'humanoid.xml', 5)
+        utils.EzPickle.__init__(self)
+
+    def _get_obs(self):
+        data = self.model.data
+        return np.concatenate([data.qpos.flat[2:],
+                               data.qvel.flat])
+
+    def _step(self, a):
+        data = self.model.data
+        action = a
+        if getattr(self, 'action_space', None):
+            action = np.clip(a, self.action_space.low,
+                             self.action_space.high)
+
+        # reward
+        alive_bonus = 5.0
+        lin_vel_cost = 0.25 / 0.015 * data.qvel.flat[0]
+        quad_ctrl_cost = 0.1 * np.square(action).sum()
+        quad_impact_cost = 0.0
+
+        self.do_simulation(action, self.frame_skip)
+        reward = lin_vel_cost - quad_ctrl_cost - quad_impact_cost + alive_bonus
+        qpos = self.model.data.qpos
+        done = bool((qpos[2] < 1.0) or (qpos[2] > 2.0))
+        return self._get_obs(), reward, done, dict(reward_linvel=lin_vel_cost, reward_quadctrl=-quad_ctrl_cost, reward_alive=alive_bonus, reward_impact=-quad_impact_cost)
+
+    def reset_model(self):
+        c = 0.01
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-c, high=c, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-c, high=c, size=self.model.nv,)
+        )
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 1
+        self.viewer.cam.distance = self.model.stat.extent * 1.0
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = 0.25 / 0.015 * obs[:, 22]
+
+        quad_impact_cost = 0.0
+
+        height = next_obs[:, 0]
+        done = np.logical_or((height > 2.0), (height < 1.0))
+        alive_reward = 5 * (1.0 - np.array(done, dtype=np.float))
+
+        reward = reward_run + reward_ctrl + (-quad_impact_cost) + alive_reward
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        height = next_states[:, 0]
+        done = np.logical_or((height > 2.0), (height < 1.0))
+        return rewards, done
+
+    def verify(self):
+        pass
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/half_cheetah.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/half_cheetah.py
new file mode 100644
index 0000000..97be1c5
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/half_cheetah.py
@@ -0,0 +1,76 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/half_cheetah.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        start_ob = self._get_obs()
+        reward_run = start_ob[8]
+
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        reward_ctrl = -0.1 * np.square(action).sum()
+
+        reward = reward_run + reward_ctrl
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                             self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        qpos = self.init_qpos + \
+            self.np_random.uniform(low=-.1, high=.1, size=self.model.nq)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/hopper.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/hopper.py
new file mode 100644
index 0000000..bb2f509
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/hopper.py
@@ -0,0 +1,84 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HopperEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=4):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/hopper.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = old_ob[5]
+        reward_height = -3.0 * np.square(old_ob[0] - 1.3)
+        reward = reward_run + reward_ctrl + reward_height + 1.0
+
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-.005, high=.005, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-.005, high=.005, size=self.model.nv)
+        )
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 2
+        self.viewer.cam.distance = self.model.stat.extent * 0.75
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 5]
+        reward_height = -3.0 * np.square(obs[:, 0] - 1.3)
+        reward = reward_run + reward_ctrl + reward_height + 1.0
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        # reward_height = -3.0 * tf.square(next_obs[:, 1] - 1.3)
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+        raise NotImplementedError
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/inverted_pendulum.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/inverted_pendulum.py
new file mode 100644
index 0000000..f05af7b
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/inverted_pendulum.py
@@ -0,0 +1,74 @@
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.utils.dataset import Dataset, gen_dtype
+from lunzi.Logger import logger
+
+
+class InvertedPendulumEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+
+    def __init__(self):
+        utils.EzPickle.__init__(self)
+        mujoco_env.MujocoEnv.__init__(self, 'inverted_pendulum.xml', 2)
+
+    def _step(self, a):
+        # reward = 1.0
+        reward = self._get_reward()
+        self.do_simulation(a, self.frame_skip)
+        ob = self._get_obs()
+        # notdone = np.isfinite(ob).all() and (np.abs(ob[1]) <= .2)
+        # done = not notdone
+        done = False
+        return ob, reward, done, {}
+
+    def reset_model(self):
+        qpos = self.init_qpos + self.np_random.uniform(size=self.model.nq, low=-0.01, high=0.01)
+        qvel = self.init_qvel + self.np_random.uniform(size=self.model.nv, low=-0.01, high=0.01)
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def _get_reward(self):
+        old_ob = self._get_obs()
+        reward = -((old_ob[1]) ** 2)
+        return reward
+
+    def _get_obs(self):
+        return np.concatenate([self.model.data.qpos, self.model.data.qvel]).ravel()
+
+    def viewer_setup(self):
+        v = self.viewer
+        v.cam.trackbodyid = 0
+        v.cam.distance = v.model.stat.extent
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        return ((obs[:, 1]) ** 2)
+
+    def verify(self, n=2000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/mountain_car.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/mountain_car.py
new file mode 100644
index 0000000..01c3444
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/mountain_car.py
@@ -0,0 +1,191 @@
+# -*- coding: utf-8 -*-
+"""
+@author: Olivier Sigaud
+A merge between two sources:
+* Adaptation of the MountainCar Environment from the "FAReinforcement" library
+of Jose Antonio Martin H. (version 1.0), adapted by  'Tom Schaul, tom@idsia.ch'
+and then modified by Arnaud de Broissia
+* the OpenAI/gym MountainCar environment
+itself from
+https://webdocs.cs.ualberta.ca/~sutton/MountainCar/MountainCar1.cp
+"""
+
+import math
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+from slbo.utils.dataset import Dataset, gen_dtype
+from lunzi.Logger import logger
+
+
+class Continuous_MountainCarEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 30
+    }
+
+    def __init__(self):
+        self.min_action = -1.0
+        self.max_action = 1.0
+        self.min_position = -1.2
+        self.max_position = 0.6
+        self.max_speed = 0.07
+        self.goal_position = 0.45  # was 0.5 in gym, 0.45 in Arnaud de Broissia's version
+        self.power = 0.0015
+
+        self.low_state = np.array([self.min_position, -self.max_speed])
+        self.high_state = np.array([self.max_position, self.max_speed])
+
+        self.viewer = None
+
+        self.action_space = spaces.Box(self.min_action, self.max_action, shape=(1,))
+        self.observation_space = spaces.Box(self.low_state, self.high_state)
+
+        self._seed()
+        self.reset()
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, action):
+
+        position = self.state[0]
+        velocity = self.state[1]
+        force = min(max(action[0], -1.0), 1.0)
+        reward = position
+
+        velocity += force * self.power - 0.0025 * math.cos(3 * position)
+        if (velocity > self.max_speed):
+            velocity = self.max_speed
+        if (velocity < -self.max_speed):
+            velocity = -self.max_speed
+        position += velocity
+        if (position > self.max_position):
+            position = self.max_position
+        if (position < self.min_position):
+            position = self.min_position
+        if (position == self.min_position and velocity < 0):
+            velocity = 0
+
+        """
+        done = bool(position >= self.goal_position)
+
+        reward = 0
+        if done:
+            reward = 100.0
+        reward -= math.pow(action[0], 2) * 0.1
+
+        """
+        done = False
+        self.state = np.array([position, velocity])
+        return self.state, reward, done, {}
+
+    def _reset(self):
+        self.state = np.array([self.np_random.uniform(low=-0.6, high=-0.4), 0])
+        return np.array(self.state)
+
+#    def get_state(self):
+#        return self.state
+
+    def _height(self, xs):
+        return np.sin(3 * xs) * .45 + .55
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        screen_width = 600
+        screen_height = 400
+
+        world_width = self.max_position - self.min_position
+        scale = screen_width / world_width
+        carwidth = 40
+        carheight = 20
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(screen_width, screen_height)
+            xs = np.linspace(self.min_position, self.max_position, 100)
+            ys = self._height(xs)
+            xys = list(zip((xs - self.min_position) * scale, ys * scale))
+
+            self.track = rendering.make_polyline(xys)
+            self.track.set_linewidth(4)
+            self.viewer.add_geom(self.track)
+
+            clearance = 10
+
+            l, r, t, b = -carwidth / 2, carwidth / 2, carheight, 0
+            car = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            car.add_attr(rendering.Transform(translation=(0, clearance)))
+            self.cartrans = rendering.Transform()
+            car.add_attr(self.cartrans)
+            self.viewer.add_geom(car)
+            frontwheel = rendering.make_circle(carheight / 2.5)
+            frontwheel.set_color(.5, .5, .5)
+            frontwheel.add_attr(rendering.Transform(translation=(carwidth / 4, clearance)))
+            frontwheel.add_attr(self.cartrans)
+            self.viewer.add_geom(frontwheel)
+            backwheel = rendering.make_circle(carheight / 2.5)
+            backwheel.add_attr(rendering.Transform(translation=(-carwidth / 4, clearance)))
+            backwheel.add_attr(self.cartrans)
+            backwheel.set_color(.5, .5, .5)
+            self.viewer.add_geom(backwheel)
+            flagx = (self.goal_position - self.min_position) * scale
+            flagy1 = self._height(self.goal_position) * scale
+            flagy2 = flagy1 + 50
+            flagpole = rendering.Line((flagx, flagy1), (flagx, flagy2))
+            self.viewer.add_geom(flagpole)
+            flag = rendering.FilledPolygon([(flagx, flagy2), (flagx, flagy2 - 10), (flagx + 25, flagy2 - 5)])
+            flag.set_color(.8, .8, 0)
+            self.viewer.add_geom(flag)
+
+        pos = self.state[0]
+        self.cartrans.set_translation((pos - self.min_position) * scale, self._height(pos) * scale)
+        self.cartrans.set_rotation(math.cos(3 * pos))
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+        position = self.state[0]
+        velocity = self.state[1]
+        force = min(max(action[0], -1.0), 1.0)
+        reward = position
+        """
+        position = obs[:, 0]
+        return -position
+
+    def verify(self, n=2000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/pendulum.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/pendulum.py
new file mode 100644
index 0000000..a64b39c
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/pendulum.py
@@ -0,0 +1,155 @@
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+from os import path
+from slbo.utils.dataset import Dataset, gen_dtype
+from lunzi.Logger import logger
+
+
+class PendulumEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 30
+    }
+
+    def __init__(self):
+        self.max_speed = 8
+        self.max_torque = 2.
+        self.dt = .05
+        self.viewer = None
+
+        high = np.array([1., 1., self.max_speed])
+        self.action_space = spaces.Box(low=-self.max_torque, high=self.max_torque, shape=(1,))
+        self.observation_space = spaces.Box(low=-high, high=high)
+
+        self._seed()
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, u):
+        th, thdot = self.state  # th := theta
+        '''
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+        '''
+
+        # for the reward
+        y, x, thetadot = np.cos(th), np.sin(th), thdot
+        u = np.clip(u, -self.max_torque, self.max_torque)[0]
+        costs = y + .1 * np.abs(x) + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        reward = -costs
+
+        g = 10.
+        m = 1.
+        l = 1.
+        dt = self.dt
+
+        self.last_u = u  # for rendering
+        # costs = angle_normalize(th) ** 2 + .1 * thdot ** 2 + .001 * (u ** 2)
+
+        newthdot = thdot + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3. / (m * l ** 2) * u) * dt
+        newth = th + newthdot * dt
+        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)  # pylint: disable=E1111
+
+        self.state = np.array([newth, newthdot])
+        return self._get_obs(), reward, False, {}
+
+    def _reset(self):
+        high = np.array([np.pi, 1])
+        self.state = self.np_random.uniform(low=-high, high=high)
+        self.last_u = None
+        return self._get_obs()
+
+    def _get_obs(self):
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(500, 500)
+            self.viewer.set_bounds(-2.2, 2.2, -2.2, 2.2)
+            rod = rendering.make_capsule(1, .2)
+            rod.set_color(.8, .3, .3)
+            self.pole_transform = rendering.Transform()
+            rod.add_attr(self.pole_transform)
+            self.viewer.add_geom(rod)
+            axle = rendering.make_circle(.05)
+            axle.set_color(0, 0, 0)
+            self.viewer.add_geom(axle)
+            fname = path.join(path.dirname(__file__), "assets/clockwise.png")
+            self.img = rendering.Image(fname, 1., 1.)
+            self.imgtrans = rendering.Transform()
+            self.img.add_attr(self.imgtrans)
+
+        self.viewer.add_onetime(self.img)
+        self.pole_transform.set_rotation(self.state[0] + np.pi / 2)
+        if self.last_u:
+            self.imgtrans.scale = (-self.last_u / 2, np.abs(self.last_u) / 2)
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+        dist_vec = obs[:, -3:]
+        reward_dist = - np.linalg.norm(dist_vec, axis=1)
+        reward_ctrl = - np.sum(np.square(acts), axis=1)
+        reward = reward_dist + reward_ctrl
+
+        # for the reward
+        y, x, thetadot = np.cos(th), np.sin(th), thdot
+        u = np.clip(u, -self.max_torque, self.max_torque)[0]
+        costs = y + .1 * x + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        reward = -costs
+
+        def _get_obs(self):
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+
+        """
+        y, x, thetadot = obs[:, 0], obs[:, 1], obs[:, 2]
+        u = np.clip(acts[:, 0], -self.max_torque, self.max_torque)
+        costs = y + .1 * np.abs(x) + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        return costs
+
+    def verify(self, n=2000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
+
+
+def angle_normalize(x):
+    return (((x + np.pi) % (2 * np.pi)) - np.pi)
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/pets_cartpole.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/pets_cartpole.py
new file mode 100644
index 0000000..a048535
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/pets_cartpole.py
@@ -0,0 +1,53 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+
+
+class CartpoleEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+    PENDULUM_LENGTH = 0.6
+
+    def __init__(self):
+        utils.EzPickle.__init__(self)
+        dir_path = os.path.dirname(os.path.realpath(__file__))
+        mujoco_env.MujocoEnv.__init__(self, '%s/assets/cartpole.xml' % dir_path, 2)
+
+    def _step(self, a):
+        self.do_simulation(a, self.frame_skip)
+        ob = self._get_obs()
+
+        cost_lscale = CartpoleEnv.PENDULUM_LENGTH
+        reward = np.exp(
+            -np.sum(np.square(self._get_ee_pos(ob) - np.array([0.0, CartpoleEnv.PENDULUM_LENGTH]))) / (cost_lscale ** 2)
+        )
+        reward -= 0.01 * np.sum(np.square(a))
+
+        done = False
+        return ob, reward, done, {}
+
+    def reset_model(self):
+        qpos = self.init_qpos + np.random.normal(0, 0.1, np.shape(self.init_qpos))
+        qvel = self.init_qvel + np.random.normal(0, 0.1, np.shape(self.init_qvel))
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def _get_obs(self):
+        return np.concatenate([self.model.data.qpos, self.model.data.qvel]).ravel()
+
+    @staticmethod
+    def _get_ee_pos(x):
+        x0, theta = x[0], x[1]
+        return np.array([
+            x0 - CartpoleEnv.PENDULUM_LENGTH * np.sin(theta),
+            -CartpoleEnv.PENDULUM_LENGTH * np.cos(theta)
+        ])
+
+    def viewer_setup(self):
+        v = self.viewer
+        v.cam.trackbodyid = 0
+        v.cam.distance = v.model.stat.extent
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/pets_cheetah.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/pets_cheetah.py
new file mode 100644
index 0000000..1f73b66
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/pets_cheetah.py
@@ -0,0 +1,54 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+
+
+class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+
+    def __init__(self):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.realpath(__file__))
+        mujoco_env.MujocoEnv.__init__(self, '%s/assets/half_cheetah.xml' % dir_path, 5)
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = ob[0] - 0.0 * np.square(ob[2])
+        reward = reward_run + reward_ctrl
+
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            (self.model.data.qpos.flat[:1] - self.prev_qpos[:1]) / self.dt,
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        qpos = self.init_qpos + np.random.normal(loc=0, scale=0.001, size=self.model.nq)
+        qvel = self.init_qvel + np.random.normal(loc=0, scale=0.001, size=self.model.nv)
+        self.set_state(qpos, qvel)
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.25
+        self.viewer.cam.elevation = -55
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/pets_pusher.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/pets_pusher.py
new file mode 100644
index 0000000..854f477
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/pets_pusher.py
@@ -0,0 +1,85 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+
+
+class PusherEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+
+    def __init__(self):
+        dir_path = os.path.dirname(os.path.realpath(__file__))
+        mujoco_env.MujocoEnv.__init__(self, '%s/assets/pusher.xml' % dir_path, 4)
+        utils.EzPickle.__init__(self)
+        self.reset_model()
+
+    def _step(self, a):
+        obj_pos = self.get_body_com("object"),
+        vec_1 = obj_pos - self.get_body_com("tips_arm")
+        vec_2 = obj_pos - self.get_body_com("goal")
+
+        reward_near = -np.sum(np.abs(vec_1))
+        reward_dist = -np.sum(np.abs(vec_2))
+        reward_ctrl = -np.square(a).sum()
+        reward = 1.25 * reward_dist + 0.1 * reward_ctrl + 0.5 * reward_near
+
+        self.do_simulation(a, self.frame_skip)
+        ob = self._get_obs()
+        done = False
+        return ob, reward, done, {}
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = -1
+        self.viewer.cam.distance = 4.0
+
+    def reset_model(self):
+        qpos = self.init_qpos
+
+        self.goal_pos = np.asarray([0, 0])
+        self.cylinder_pos = np.array([-0.25, 0.15]) + np.random.normal(0, 0.025, [2])
+
+        qpos[-4:-2] = self.cylinder_pos
+        qpos[-2:] = self.goal_pos
+        qvel = self.init_qvel + \
+            self.np_random.uniform(low=-0.005, high=0.005, size=self.model.nv)
+        qvel[-4:] = 0
+        self.set_state(qpos, qvel)
+        self.ac_goal_pos = self.get_body_com("goal")
+
+        return self._get_obs()
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[:7],
+            self.model.data.qvel.flat[:7],
+            self.get_body_com("tips_arm"),
+            self.get_body_com("object"),
+            self.get_body_com("goal"),
+        ])
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+        to_w, og_w = 0.5, 1.25
+        tip_pos, obj_pos, goal_pos = obs[:, 14:17], obs[:, 17:20], obs[:, -3:]
+
+        tip_obj_dist = np.sum(np.abs(tip_pos - obj_pos), axis=1)
+        obj_goal_dist = np.sum(np.abs(goal_pos - obj_pos), axis=1)
+        return to_w * tip_obj_dist + og_w * obj_goal_dist
+
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward = reward_run + reward_ctrl
+        """
+        to_w, og_w = 0.5, 1.25
+        tip_pos, obj_pos, goal_pos = obs[:, 14:17], obs[:, 17:20], obs[:, -3:]
+
+        tip_obj_dist = -np.sum(np.abs(tip_pos - obj_pos), axis=1)
+        obj_goal_dist = -np.sum(np.abs(goal_pos - obj_pos), axis=1)
+        ctrl_reward = -0.1 * np.sum(np.square(acts), axis=1)
+
+        reward = to_w * tip_obj_dist + og_w * obj_goal_dist + ctrl_reward
+        return -reward
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/pets_reacher.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/pets_reacher.py
new file mode 100644
index 0000000..ba419ac
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/pets_reacher.py
@@ -0,0 +1,95 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+
+
+class Reacher3DEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+
+    def __init__(self):
+        self.viewer = None
+        utils.EzPickle.__init__(self)
+        dir_path = os.path.dirname(os.path.realpath(__file__))
+        self.goal = np.zeros(3)
+        mujoco_env.MujocoEnv.__init__(self, os.path.join(dir_path, 'assets/reacher3d.xml'), 2)
+
+    def _step(self, a):
+        self.do_simulation(a, self.frame_skip)
+        ob = self._get_obs()
+        reward = -np.sum(np.square(self.get_EE_pos(ob[None]) - self.goal))
+        reward -= 0.01 * np.square(a).sum()
+        done = False
+        return ob, reward, done, dict(reward_dist=0, reward_ctrl=0)
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 1
+        self.viewer.cam.distance = 2.5
+        self.viewer.cam.elevation = -30
+        self.viewer.cam.azimuth = 270
+
+    def reset_model(self):
+        qpos, qvel = np.copy(self.init_qpos), np.copy(self.init_qvel)
+        qpos[-3:] += np.random.normal(loc=0, scale=0.1, size=[3])
+        qvel[-3:] = 0
+        self.goal = qpos[-3:]
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def _get_obs(self):
+        raw_obs = np.concatenate([
+            self.model.data.qpos.flat, self.model.data.qvel.flat[:-3],
+        ])
+
+        EE_pos = np.reshape(self.get_EE_pos(raw_obs[None]), [-1])
+
+        return np.concatenate([raw_obs, EE_pos])
+
+    def get_EE_pos(self, states):
+        theta1, theta2, theta3, theta4, theta5, theta6, theta7 = \
+            states[:, :1], states[:, 1:2], states[:, 2:3], states[:, 3:4], states[:, 4:5], states[:, 5:6], states[:, 6:]
+
+        rot_axis = np.concatenate([np.cos(theta2) * np.cos(theta1), np.cos(theta2) * np.sin(theta1), -np.sin(theta2)],
+                                  axis=1)
+        rot_perp_axis = np.concatenate([-np.sin(theta1), np.cos(theta1), np.zeros(theta1.shape)], axis=1)
+        cur_end = np.concatenate([
+            0.1 * np.cos(theta1) + 0.4 * np.cos(theta1) * np.cos(theta2),
+            0.1 * np.sin(theta1) + 0.4 * np.sin(theta1) * np.cos(theta2) - 0.188,
+            -0.4 * np.sin(theta2)
+        ], axis=1)
+
+        for length, hinge, roll in [(0.321, theta4, theta3), (0.16828, theta6, theta5)]:
+            perp_all_axis = np.cross(rot_axis, rot_perp_axis)
+            x = np.cos(hinge) * rot_axis
+            y = np.sin(hinge) * np.sin(roll) * rot_perp_axis
+            z = -np.sin(hinge) * np.cos(roll) * perp_all_axis
+            new_rot_axis = x + y + z
+            new_rot_perp_axis = np.cross(new_rot_axis, rot_axis)
+            new_rot_perp_axis[np.linalg.norm(new_rot_perp_axis, axis=1) < 1e-30] = \
+                rot_perp_axis[np.linalg.norm(new_rot_perp_axis, axis=1) < 1e-30]
+            new_rot_perp_axis /= np.linalg.norm(new_rot_perp_axis, axis=1, keepdims=True)
+            rot_axis, rot_perp_axis, cur_end = new_rot_axis, new_rot_perp_axis, cur_end + length * new_rot_axis
+
+        return cur_end
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+        def obs_cost_fn(self, obs):
+            self.ENV.goal = obs[:, 7: 10]
+            ee_pos = obs[:, -3:]
+            return np.sum(np.square(ee_pos - self.ENV.goal), axis=1)
+
+        @staticmethod
+        def ac_cost_fn(acs):
+            return 0.01 * np.sum(np.square(acs), axis=1)
+        """
+        reward_ctrl = -0.01 * np.sum(np.square(acts), axis=1)
+        goal = obs[:, 7: 10]
+        ee_pos = obs[:, -3:]
+
+        reward = -np.sum(np.square(ee_pos - goal), axis=1) + reward_ctrl
+        return -reward
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/reacher.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/reacher.py
new file mode 100644
index 0000000..636cdde
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/reacher.py
@@ -0,0 +1,66 @@
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class ReacherEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self):
+        utils.EzPickle.__init__(self)
+        mujoco_env.MujocoEnv.__init__(self, 'reacher.xml', 2)
+
+    def _step(self, a):
+        vec = self.get_body_com("fingertip") - self.get_body_com("target")
+
+        if getattr(self, 'action_space', None):
+            a = np.clip(a, self.action_space.low,
+                        self.action_space.high)
+        reward_dist = - np.linalg.norm(vec)
+        reward_ctrl = - np.square(a).sum()
+        reward = reward_dist + reward_ctrl
+        self.do_simulation(a, self.frame_skip)
+        ob = self._get_obs()
+        done = False
+        return ob, reward, done, dict(reward_dist=reward_dist, reward_ctrl=reward_ctrl)
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 0
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                             self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        qpos = self.np_random.uniform(low=-0.1, high=0.1, size=self.model.nq) + self.init_qpos
+        while True:
+            self.goal = self.np_random.uniform(low=-.2, high=.2, size=2)
+            if np.linalg.norm(self.goal) < 2:
+                break
+        qpos[-2:] = self.goal
+        qvel = self.init_qvel + self.np_random.uniform(low=-.005, high=.005, size=self.model.nv)
+        qvel[-2:] = 0
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def _get_obs(self):
+        theta = self.model.data.qpos.flat[:2]
+        return np.concatenate([
+            np.cos(theta),
+            np.sin(theta),
+            self.model.data.qpos.flat[2:],
+            self.model.data.qvel.flat[:2],
+            self.get_body_com("fingertip") - self.get_body_com("target")
+        ])
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        dist_vec = obs[:, -3:]
+        reward_dist = - np.linalg.norm(dist_vec, axis=1)
+        reward_ctrl = - np.sum(np.square(acts), axis=1)
+        reward = reward_dist + reward_ctrl
+        return -reward
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/readme.md b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/readme.md
new file mode 100644
index 0000000..28f84a0
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/readme.md
@@ -0,0 +1,5 @@
+# reference
+
+1. mbbl/env/gym_env/walker.py or mbbl/env/gym_env/reacher.py 
+
+2. https://github.com/openai/gym/blob/v0.7.4/gym/envs/mujoco/half_cheetah.py
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/swimmer.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/swimmer.py
new file mode 100644
index 0000000..33e8d79
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/swimmer.py
@@ -0,0 +1,77 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class SwimmerEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=4):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/swimmer.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        ob = self._get_obs()
+
+        reward_ctrl = -0.0001 * np.square(action).sum()
+        reward_run = old_ob[3]
+        reward = reward_run + reward_ctrl
+
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            # (self.model.data.qpos.flat[:1] - self.prev_qpos[:1]) / self.dt,
+            # self.get_body_comvel("torso")[:1],
+            self.model.data.qpos.flat[2:],
+            self.model.data.qvel.flat,
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                             self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-.1, high=.1, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-.1, high=.1, size=self.model.nv)
+        )
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.0001 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 3]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        """
+        reward_ctrl = -0.0001 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+        raise NotImplementedError
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/walker2d.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/walker2d.py
new file mode 100644
index 0000000..1031b32
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym/walker2d.py
@@ -0,0 +1,84 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class Walker2dEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=4):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/walker2d.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = old_ob[8]
+        reward_height = -3.0 * np.square(old_ob[0] - 1.3)
+        reward = reward_run + reward_ctrl + reward_height + 1.0
+
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-.005, high=.005, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-.005, high=.005, size=self.model.nv)
+        )
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 2
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward_height = -3.0 * np.square(obs[:, 0] - 1.3)
+        reward = reward_run + reward_ctrl + reward_height + 1.0
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        # reward_height = -3.0 * tf.square(next_obs[:, 1] - 1.3)
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+        raise NotImplementedError
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym_env.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym_env.py
new file mode 100644
index 0000000..32b9434
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/gym_env.py
@@ -0,0 +1,141 @@
+# import gym
+# import gym.wrappers
+# import gym.envs
+# import gym.spaces
+# import traceback
+# import logging
+#
+# try:
+#     from gym.wrappers.monitoring import logger as monitor_logger
+#
+#     monitor_logger.setLevel(logging.WARNING)
+# except Exception as e:
+#     traceback.print_exc()
+#
+# import os
+# import numpy as np
+# from rllab.misc import logger
+#
+#
+# class CappedCubicVideoSchedule(object):
+#     # Copied from gym, since this method is frequently moved around
+#     def __call__(self, count):
+#         if count < 1000:
+#             return int(round(count ** (1. / 3))) ** 3 == count
+#         else:
+#             return count % 1000 == 0
+#
+#
+# class FixedIntervalVideoSchedule(object):
+#     def __init__(self, interval):
+#         self.interval = interval
+#
+#     def __call__(self, count):
+#         return count % self.interval == 0
+#
+#
+# class NoVideoSchedule(object):
+#     def __call__(self, count):
+#         return False
+#
+#
+# class GymEnv(object):
+#     def __init__(self, env_name, record_video=True, video_schedule=None, log_dir=None, record_log=True,
+#                  force_reset=False):
+#         if log_dir is None:
+#             if logger.get_snapshot_dir() is None:
+#                 logger.log("Warning: skipping Gym environment monitoring since snapshot_dir not configured.")
+#             else:
+#                 log_dir = os.path.join(logger.get_snapshot_dir(), "gym_log")
+#
+#         env = gym.make(env_name)
+#         self.env = env
+#         self.env_id = env.spec.id
+#
+#         assert not (not record_log and record_video)
+#
+#         if log_dir is None or record_log is False:
+#             self.monitoring = False
+#         else:
+#             if not record_video:
+#                 video_schedule = NoVideoSchedule()
+#             else:
+#                 if video_schedule is None:
+#                     video_schedule = CappedCubicVideoSchedule()
+#             self.env = gym.wrappers.Monitor(self.env, log_dir, video_callable=video_schedule, force=True)
+#             self.monitoring = True
+#
+#         self._observation_space = env.observation_space
+#         self._action_space = env.action_space
+#         self._horizon = env.spec.tags['wrapper_config.TimeLimit.max_episode_steps']
+#         self._log_dir = log_dir
+#         self._force_reset = force_reset
+#
+#         self.metadata = {'render.modes': ['human', 'rgb_array']}
+#         self.reward_range = (-np.inf, np.inf)
+#         self.unwrapped = self
+#         self._configured = False
+#         self.spec = None
+#
+#     @property
+#     def inner_env(self):
+#         env = self.env
+#         while hasattr(env, "env"):
+#             env = env.env
+#         return env
+#
+#     @property
+#     def observation_space(self):
+#         return self._observation_space
+#
+#     @property
+#     def action_space(self):
+#         return self._action_space
+#
+#     @property
+#     def horizon(self):
+#         return self._horizon
+#
+#     def reset(self):
+#         if self._force_reset and hasattr(self.env, 'stats_recorder'):
+#             recorder = self.env.stats_recorder
+#             if recorder is not None:
+#                 recorder.done = True
+#
+#         return self.env.reset()
+#
+#     def step(self, action_or_predicted_result):
+#         if isinstance(action_or_predicted_result, dict):
+#             return self._step_with_predicted_dynamics(**action_or_predicted_result)
+#         else:
+#             next_obs, reward, done, info = self.env.step(action_or_predicted_result)
+#             return next_obs, reward, done, info
+#
+#     def _step_with_predicted_dynamics(self, next_obs, reward, done):
+#         qpos = self.inner_env.model.data.qpos.flatten()
+#         qvel = self.inner_env.model.data.qvel.flatten()
+#         self.env.env.set_state(qpos, qvel)
+#         return next_obs, reward, done, {}
+#
+#     def cost_np_vec(self, obs, acts, next_obs):
+#         return self.env.env.cost_np_vec(obs, acts, next_obs)
+#
+#     def cost_tf_vec(self, obs, acts, next_obs):
+#         return self.env.env.cost_tf_vec(obs, acts, next_obs)
+#
+#     def render(self, **kwargs):
+#         return self.env.render(**kwargs)
+#
+#     def terminate(self):
+#         if self.monitoring:
+#             self.env._close()
+#             if self._log_dir is not None:
+#                 print("""
+#     ***************************
+#     Training finished! You can upload results to OpenAI Gym by running the following command:
+#     python scripts/submit_gym.py %s
+#     ***************************
+#                 """ % self._log_dir)
+#
+#     def get_geom_xpos(self):
+#         return self.inner_env.data.geom_xpos
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/neural_env.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/neural_env.py
new file mode 100644
index 0000000..243aa73
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/neural_env.py
@@ -0,0 +1,112 @@
+import numpy as np
+
+
+class NeuralNetEnv:
+
+    def __init__(self, env, inner_env, dynamics):
+        self.vectorized = True
+        self.env = env
+        self.inner_env = inner_env
+        self.is_done = getattr(inner_env, 'is_done', lambda x, y: np.asarray([False] * len(x)))
+        self.dynamics = dynamics
+
+    @property
+    def observation_space(self):
+        return self.env.observation_space
+
+    @property
+    def action_space(self):
+        return self.env.action_space
+
+    @property
+    def spec(self):
+        return self.env.spec
+
+    def reset(self):
+        self.state = self.env.reset()
+        observation = np.copy(self.state)
+        return observation
+
+    def step(self, action, use_states=None):
+        action = np.clip(action, *self.action_space.bounds)
+        if use_states is not None:
+            next_observation = self.dynamics.predict([use_states], [action])[0]
+            obs_dim = self.env.observation_space.shape[0]
+            next_observation[:obs_dim] = np.clip(next_observation[:obs_dim], *self.observation_space.bounds)
+            next_observation[:obs_dim] = np.clip(next_observation[:obs_dim], -1e5, 1e5)
+        else:
+            next_observation = self.dynamics.predict([self.state], [action])[0]
+            next_observation = np.clip(next_observation, *self.observation_space.bounds)
+            next_observation = np.clip(next_observation, -1e5, 1e5)
+
+        if hasattr(self.inner_env, "env"):
+            reward = - self.inner_env.env.cost_np_vec(self.state[None], action[None], np.array([next_observation]))[0]
+        else:
+            reward = - self.inner_env.cost_np_vec(self.state[None], action[None], np.array([next_observation]))[0]
+
+        done = self.is_done(self.state[None], next_observation)[0]
+        self.state = np.reshape(next_observation, -1)
+        return self.inner_env.step({"next_obs": next_observation, "reward": reward, "done": done})
+
+    def render(self):
+        print('current state:', self.state)
+
+    def vec_env_executor(self, n_envs, max_path_length):
+        return VecSimpleEnv(env=self, inner_env=self.inner_env, n_envs=n_envs, max_path_length=max_path_length)
+
+    def terminate(self):
+        self.env.terminate()
+
+
+class VecSimpleEnv(object):
+
+    def __init__(self, env, inner_env, n_envs, max_path_length):
+        self.env = env
+        self.inner_env = inner_env
+        self.n_envs = n_envs
+        self.num_envs = n_envs
+        self.ts = np.zeros((self.n_envs,))
+        self.max_path_length = max_path_length
+        self.obs_dim = env.observation_space.shape[0]
+        self.states = np.zeros((self.n_envs, self.obs_dim))
+
+    def reset(self, dones=None):
+        if dones is None:
+            dones = np.asarray([True] * self.n_envs)
+        else:
+            dones = np.cast['bool'](dones)
+        for i, done in enumerate(dones):
+            if done:
+                self.states[i] = self.env.reset()
+        self.ts[dones] = 0
+        return self.states[dones]
+
+    def step(self, actions, use_states=None):
+        self.ts += 1
+        actions = np.clip(actions, *self.env.action_space.bounds)
+        next_observations = self.get_next_observation(actions, use_states=use_states)
+        if use_states is not None:
+            obs_dim = self.env.observation_space.shape[0]
+            next_observations[:, :obs_dim] = np.clip(next_observations[:, :obs_dim], *self.env.observation_space.bounds)
+            next_observations[:, :obs_dim] = np.clip(next_observations[:, :obs_dim], -1e5, 1e5)
+        else:
+            next_observations = np.clip(next_observations, *self.env.observation_space.bounds)
+            next_observations = np.clip(next_observations, -1e5, 1e5)
+        if hasattr(self.env.inner_env, "cost_np_vec"):
+            rewards = - self.env.inner_env.cost_np_vec(self.states, actions, next_observations)
+        else:
+            rewards = - self.env.inner_env.env.cost_np_vec(self.states, actions, next_observations)
+        self.states = next_observations
+        dones = self.env.is_done(self.states, next_observations)
+        dones[self.ts >= self.max_path_length] = True
+        if np.any(dones):
+            self.reset(dones)
+        return self.states, rewards, dones, dict()
+
+    def get_next_observation(self, actions, use_states=None):
+        if use_states is not None:
+            return self.env.dynamics.predict(use_states, actions)
+        return self.env.dynamics.predict(self.states, actions)
+
+    def terminate(self):
+        self.env.terminate()
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/proxy_env.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/proxy_env.py
new file mode 100644
index 0000000..6381ab4
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/proxy_env.py
@@ -0,0 +1,89 @@
+from gym.core import Env
+from gym.spaces import Box as GymBox
+from gym.wrappers.monitoring import Monitor
+import numpy as np
+import tensorflow as tf
+
+
+class Box:
+
+    def __init__(self, gym_box: GymBox):
+        self.gym_box = gym_box
+
+    @property
+    def flat_dim(self):
+        return np.prod(self.gym_box.shape)
+
+    @property
+    def shape(self):
+        return self.gym_box.shape
+
+    @property
+    def dtype(self):
+        return tf.float32
+
+    @property
+    def bounds(self):
+        return self.gym_box.low, self.gym_box.high
+
+    def flatten_n(self, xs):
+        xs = np.asarray(xs)
+        return xs.reshape((xs.shape[0], -1))
+
+    def sample(self):
+        return self.gym_box.sample()
+
+    def flatten(self, x):
+        return np.asarray(x).flatten()
+
+    def __repr__(self):
+        return "Box Wrapper of shape {}".format(self.shape)
+
+    def __eq__(self, other):
+        return self.gym_box.__eq__(other)
+
+
+class ProxyEnv(Env):
+
+    def __init__(self, wrapped_env: Env):
+        self._wrapped_env = wrapped_env
+        self._wrapped_observation_space = Box(wrapped_env.observation_space)
+        self._wrapped_action_space = Box(wrapped_env.action_space)
+
+    @property
+    def wrapped_env(self):
+        return self._wrapped_env
+
+    def reset(self, **kwargs):
+        return self._wrapped_env.reset(**kwargs)
+
+    @property
+    def action_space(self):
+        return self._wrapped_action_space
+
+    @property
+    def observation_space(self):
+        return self._wrapped_observation_space
+
+    def step(self, action, **kwargs):
+        return self._wrapped_env.step(action, **kwargs)
+
+    def render(self, *args, **kwargs):
+        return self._wrapped_env.render(*args, **kwargs)
+
+    def log_diagnostics(self, paths, *args, **kwargs):
+        self._wrapped_env.log_diagnostics(paths, *args, **kwargs)
+
+    @property
+    def horizon(self):
+        return self._wrapped_env.horizon
+
+    def terminate(self):
+        if isinstance(self._wrapped_env, Monitor):
+            self._wrapped_env._close()
+
+    def get_param_values(self):
+        return self._wrapped_env.get_param_values()
+
+    def set_param_values(self, params):
+        self._wrapped_env.set_param_values(params)
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/vec_env.py b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/vec_env.py
new file mode 100644
index 0000000..d0369e1
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/bm_envs/vec_env.py
@@ -0,0 +1,45 @@
+import numpy as np
+from libs.misc import tensor_utils
+
+
+class VecEnvExecutor(object):
+    def __init__(self, envs, max_path_length, **kwargs):
+        self.envs = envs
+        self._action_space = envs[0].action_space
+        self._observation_space = envs[0].observation_space
+        self.ts = np.zeros(len(self.envs), dtype='int')
+        self.max_path_length = max_path_length
+
+    def step(self, action_n, **kwargs):
+        all_results = [env.step(a) for (a, env) in zip(action_n, self.envs)]
+        obs, rewards, dones, env_infos = list(map(list, list(zip(*all_results))))
+        dones = np.asarray(dones)
+        rewards = np.asarray(rewards)
+        self.ts += 1
+        if self.max_path_length is not None:
+            dones[self.ts >= self.max_path_length] = True
+        for (i, done) in enumerate(dones):
+            if done:
+                obs[i] = self.envs[i].reset()
+                self.ts[i] = 0
+        return obs, rewards, dones, tensor_utils.stack_tensor_dict_list(env_infos)
+
+    def reset(self):
+        results = [env.reset() for env in self.envs]
+        self.ts[:] = 0
+        return results
+
+    @property
+    def num_envs(self):
+        return len(self.envs)
+
+    @property
+    def action_space(self):
+        return self._action_space
+
+    @property
+    def observation_space(self):
+        return self._observation_space
+
+    def terminate(self):
+        pass
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/mujoco/__init__.py b/experiments/gym_cheetah_1234/src/slbo/envs/mujoco/__init__.py
new file mode 100644
index 0000000..5c7f19c
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/mujoco/__init__.py
@@ -0,0 +1 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/mujoco/ant_env.py b/experiments/gym_cheetah_1234/src/slbo/envs/mujoco/ant_env.py
new file mode 100644
index 0000000..2a300af
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/mujoco/ant_env.py
@@ -0,0 +1,50 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from rllab.envs.mujoco import ant_env
+from rllab.envs.base import Step
+from slbo.envs import BaseModelBasedEnv
+
+
+class AntEnv(ant_env.AntEnv, BaseModelBasedEnv):
+    def get_current_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat,  # 15
+            self.model.data.qvel.flat,  # 14
+            # np.clip(self.model.data.cfrc_ext, -1, 1).flat,  # 84
+            self.get_body_xmat("torso").flat,  # 9
+            self.get_body_com("torso"),  # 9
+            self.get_body_comvel("torso"),  # 3
+        ]).reshape(-1)
+
+    def step(self, action):
+        self.forward_dynamics(action)
+        comvel = self.get_body_comvel("torso")
+        forward_reward = comvel[0]
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+        ctrl_cost = 0.5 * 1e-2 * np.sum(np.square(action / scaling))
+        contact_cost = 0.
+        # contact_cost = 0.5 * 1e-3 * np.sum(
+        #     np.square(np.clip(self.model.data.cfrc_ext, -1, 1))),
+        survive_reward = 0.05
+        reward = forward_reward - ctrl_cost - contact_cost + survive_reward
+        state = self._state
+        notdone = np.isfinite(state).all() and state[2] >= 0.2 and state[2] <= 1.0
+        done = not notdone
+        ob = self.get_current_obs()
+        return Step(ob, float(reward), done)
+
+    def mb_step(self, states: np.ndarray, actions: np.ndarray, next_states: np.ndarray):
+        comvel = next_states[..., -3:]
+        forward_reward = comvel[..., 0]
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+        ctrl_cost = 0.5 * 1e-2 * np.sum(np.square(actions / scaling), axis=-1)
+        contact_cost = 0.
+        # contact_cost = 0.5 * 1e-3 * np.sum(
+        #     np.square(np.clip(self.model.data.cfrc_ext, -1, 1))),
+        survive_reward = 0.05
+        reward = forward_reward - ctrl_cost - contact_cost + survive_reward
+        notdone = np.all([next_states[..., 2] >= 0.2, next_states[..., 2] <= 1.0], axis=0)
+        return reward, 1. - notdone
+
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/mujoco/half_cheetah_env.py b/experiments/gym_cheetah_1234/src/slbo/envs/mujoco/half_cheetah_env.py
new file mode 100644
index 0000000..29b1502
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/mujoco/half_cheetah_env.py
@@ -0,0 +1,20 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from rllab.envs.mujoco import half_cheetah_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HalfCheetahEnv(half_cheetah_env.HalfCheetahEnv, BaseModelBasedEnv):
+    def get_current_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat,  # 9
+            self.model.data.qvel.flat,  # 9
+            self.get_body_com("torso").flat,  # 3
+            self.get_body_comvel("torso").flat,  # 3
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        actions = np.clip(actions, *self.action_bounds)
+        reward_ctrl = -0.05 * np.sum(np.square(actions), axis=-1)
+        reward_fwd = next_states[..., 21]
+        return reward_ctrl + reward_fwd, np.zeros_like(reward_fwd, dtype=np.bool)
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/mujoco/hopper_env.py b/experiments/gym_cheetah_1234/src/slbo/envs/mujoco/hopper_env.py
new file mode 100644
index 0000000..23c9c50
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/mujoco/hopper_env.py
@@ -0,0 +1,26 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from rllab.envs.mujoco import hopper_env
+from rllab.envs.base import Step
+from slbo.envs import BaseModelBasedEnv
+
+
+class HopperEnv(hopper_env.HopperEnv, BaseModelBasedEnv):
+    def get_current_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat,  # 6
+            self.model.data.qvel.flat,  # 6
+            self.get_body_com("torso").flat,  # 3
+            self.get_body_comvel("torso"),  # 3
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+        vel = next_states[:, -3]
+        reward = vel + self.alive_coeff - 0.5 * self.ctrl_cost_coeff * np.sum(np.square(actions / scaling), axis=-1)
+
+        done = ~((next_states[:, 3:12] < 100).all(axis=-1) &
+                 (next_states[:, 0] > 0.7) &
+                 (np.abs(next_states[:, 2]) < 0.2))
+        return reward, done
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/mujoco/humanoid_env.py b/experiments/gym_cheetah_1234/src/slbo/envs/mujoco/humanoid_env.py
new file mode 100644
index 0000000..a5e55ae
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/mujoco/humanoid_env.py
@@ -0,0 +1,53 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from rllab.envs.mujoco import simple_humanoid_env
+from rllab.envs.base import Step
+import numpy as np
+from slbo.envs import BaseModelBasedEnv
+
+
+class HumanoidEnv(simple_humanoid_env.SimpleHumanoidEnv, BaseModelBasedEnv):
+    def get_current_obs(self):
+        data = self.model.data
+        return np.concatenate([
+            data.qpos.flat,  # 17
+            data.qvel.flat,  # 16
+            self.get_body_com("torso").flat,  # 3
+            self.get_body_comvel("torso").flat,  # 3
+        ])
+
+    def step(self, action):
+        self.forward_dynamics(action)
+        alive_bonus = self.alive_bonus
+        data = self.model.data
+
+        comvel = self.get_body_comvel("torso")
+        lin_vel_reward = comvel[0]
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+        ctrl_cost = .5 * self.ctrl_cost_coeff * np.sum(
+            np.square(action / scaling))
+        impact_cost = 0.
+        vel_deviation_cost = 0.5 * self.vel_deviation_cost_coeff * np.sum(
+            np.square(comvel[1:]))
+        reward = lin_vel_reward + alive_bonus - ctrl_cost - \
+            impact_cost - vel_deviation_cost
+        pos = data.qpos.flat[2]
+        done = pos < 0.8 or pos > 2.0
+
+        next_obs = self.get_current_obs()
+        return Step(next_obs, reward, done)
+
+    def mb_step(self, states, actions, next_states):
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+
+        alive_bonus = 0.2
+        lin_vel_reward = next_states[:, 36]
+        ctrl_cost = 5.e-4 * np.square(actions / scaling).sum(axis=1)
+        impact_cost = 0.
+        vel_deviation_cost = 5.e-3 * np.square(next_states[:, 37:39]).sum(axis=1)
+        reward = lin_vel_reward + alive_bonus - ctrl_cost - impact_cost - vel_deviation_cost
+
+        dones = (next_states[:, 2] < 0.8) | (next_states[:, 2] > 2.0)
+        return reward, dones
+
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/mujoco/swimmer_env.py b/experiments/gym_cheetah_1234/src/slbo/envs/mujoco/swimmer_env.py
new file mode 100644
index 0000000..f39bee4
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/mujoco/swimmer_env.py
@@ -0,0 +1,22 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from rllab.envs.mujoco import swimmer_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class SwimmerEnv(swimmer_env.SwimmerEnv, BaseModelBasedEnv):
+    def get_current_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat,  # 5
+            self.model.data.qvel.flat,  # 5
+            self.get_body_com("torso").flat,  # 3
+            self.get_body_comvel("torso"),  # 3
+        ]).reshape(-1)
+
+    def mb_step(self, states: np.ndarray, actions: np.ndarray, next_states: np.ndarray):
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+        ctrl_cost = 0.5 * self.ctrl_cost_coeff * np.sum(np.square(actions / scaling), axis=-1)
+        forward_reward = next_states[:, -3]
+        reward = forward_reward - ctrl_cost
+        return reward, np.zeros_like(reward, dtype=np.bool)
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/mujoco/walker2d_env.py b/experiments/gym_cheetah_1234/src/slbo/envs/mujoco/walker2d_env.py
new file mode 100644
index 0000000..8eb16bc
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/mujoco/walker2d_env.py
@@ -0,0 +1,43 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from rllab.envs.mujoco import walker2d_env
+from rllab.envs.base import Step
+from slbo.envs import BaseModelBasedEnv
+
+
+class Walker2DEnv(walker2d_env.Walker2DEnv, BaseModelBasedEnv):
+    def get_current_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat,
+            self.model.data.qvel.flat,
+            self.get_body_com("torso").flat,
+            self.get_body_comvel("torso").flat
+        ])
+
+    def step(self, action):
+        self.forward_dynamics(action)
+        forward_reward = self.get_body_comvel("torso")[0]
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+        ctrl_cost = 1e-3 * np.sum(np.square(action / scaling))
+        alive_bonus = 1.
+        reward = forward_reward - ctrl_cost + alive_bonus
+        qpos = self.model.data.qpos
+        done = not (qpos[0] > 0.8 and qpos[0] < 2.0 and qpos[2] > -1.0 and qpos[2] < 1.0)
+        next_obs = self.get_current_obs()
+        return Step(next_obs, reward, done)
+
+    def mb_step(self, states, actions, next_states):
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+
+        reward_ctrl = -0.001 * np.sum(np.square(actions / scaling), axis=-1)
+        reward_fwd = next_states[:, 21]
+        alive_bonus = 1.
+        rewards = reward_ctrl + reward_fwd + alive_bonus
+
+        dones = ~((next_states[:, 0] > 0.8) &
+                  (next_states[:, 0] < 2.0) &
+                  (next_states[:, 2] > -1.0) &
+                  (next_states[:, 2] < 1.0))
+        return rewards, dones
diff --git a/experiments/gym_cheetah_1234/src/slbo/envs/virtual_env.py b/experiments/gym_cheetah_1234/src/slbo/envs/virtual_env.py
new file mode 100644
index 0000000..2d0cc76
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/envs/virtual_env.py
@@ -0,0 +1,94 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from gym.spaces import Box
+from slbo.dynamics_model import DynamicsModel
+from slbo.random_net import RandomNet
+from slbo.envs import BaseBatchedEnv, BaseModelBasedEnv
+from slbo.utils.pc_utils import compute_cov_pi
+
+
+class VirtualEnv(BaseBatchedEnv):
+    _states: np.ndarray
+
+    def __init__(self, model: DynamicsModel, env: BaseModelBasedEnv, random_net:RandomNet,  n_envs: int, 
+                    feature_size: int, bonus_scale: float, lamb: float, opt_model=False):
+        super().__init__()
+        self.n_envs = n_envs
+        self.observation_space = env.observation_space  # ???
+
+        dim_state = env.observation_space.shape[0]
+        dim_action = env.action_space.shape[0]
+        if opt_model:
+            self.action_space = Box(low=np.r_[env.action_space.low, np.zeros(dim_state) - 1.],
+                                    high=np.r_[env.action_space.high, np.zeros(dim_state) + 1.],
+                                    dtype=np.float32)
+        else:
+            self.action_space = env.action_space
+
+        self._opt_model = opt_model
+        self._model = model
+        self._env = env
+        self._random_net = random_net
+
+        self._states = np.zeros((self.n_envs, dim_state), dtype=np.float32)
+
+        self.feature_size = feature_size
+        self.cov_pis = None
+        self.inv_cov = None
+        self.bonus_scale = bonus_scale
+        self.lamb = lamb
+        self.pre = True
+
+    def _scale_action(self, actions):
+        lo, hi = self.action_space.low, self.action_space.high
+        return lo + (actions + 1.) * 0.5 * (hi - lo)
+
+    def step(self, actions):
+        if self._opt_model:
+            actions = actions[..., :self._env.action_space.shape[0]]
+
+        next_states = self._model.eval('next_states', states=self._states, actions=actions)
+        features = self._random_net.eval('features', states=self._states, actions=actions)
+        #print(features.shape)
+        rewards, dones = self._env.mb_step(self._states, self._scale_action(actions), next_states)
+
+        if not self.pre:
+            bonus = self.compute_bonus(features)
+            rewards = rewards + self.bonus_scale * bonus
+
+        self._states = next_states
+        return self._states.copy(), rewards, dones, [{} for _ in range(self.n_envs)]
+
+    def reset(self):
+        return self.partial_reset(range(self.n_envs))
+
+    def partial_reset(self, indices):
+        initial_states = np.array([self._env.reset() for _ in indices])
+
+        self._states = self._states.copy()
+        self._states[indices] = initial_states
+
+        return initial_states.copy()
+
+    def set_state(self, states):
+        self._states = states.copy()
+
+    def render(self, mode='human'):
+        pass
+
+    def update_cov(self, states, actions):
+        features = self._random_net.eval('features', states=states, actions=actions)
+
+        if self.pre:
+            self.cov_pis = compute_cov_pi(features)
+            self.pre = False
+        else:
+            self.cov_pis = self.cov_pis + compute_cov_pi(features)
+        
+        cur_cov = self.lamb * np.identity(self.feature_size) + self.cov_pis
+        self.inv_cov = np.linalg.inv(cur_cov)
+
+
+    def compute_bonus(self,features):
+        bonus = np.sqrt(np.sum(np.dot(features, self.inv_cov)*features,1))
+        return bonus
diff --git a/experiments/gym_cheetah_1234/src/slbo/loss/__init__.py b/experiments/gym_cheetah_1234/src/slbo/loss/__init__.py
new file mode 100644
index 0000000..5c7f19c
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/loss/__init__.py
@@ -0,0 +1 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
diff --git a/experiments/gym_cheetah_1234/src/slbo/loss/multi_step_loss.py b/experiments/gym_cheetah_1234/src/slbo/loss/multi_step_loss.py
new file mode 100644
index 0000000..a2aadb7
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/loss/multi_step_loss.py
@@ -0,0 +1,65 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi import Tensor
+from slbo.utils.normalizer import Normalizers
+
+
+class MultiStepLoss(nn.Module):
+    op_train: Tensor
+    op_grad_norm: Tensor
+    _step: int
+    _criterion: nn.Module
+    _normalizers: Normalizers
+    _model: nn.Module
+
+    def __init__(self, model: nn.Module, normalizers: Normalizers, dim_state: int, dim_action: int,
+                 criterion: nn.Module, step=4):
+        super().__init__()
+        self._step = step
+        self._criterion = criterion
+        self._model = model
+        self._normalizers = normalizers
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, shape=[step, None, dim_state])
+            self.op_actions = tf.placeholder(tf.float32, shape=[step, None, dim_action])
+            self.op_masks = tf.placeholder(tf.float32, shape=[step, None])
+            self.op_next_states_ = tf.placeholder(tf.float32, shape=[step, None, dim_state])
+
+        self.op_loss = self(self.op_states, self.op_actions, self.op_next_states_, self.op_masks)
+
+    def forward(self, states: Tensor, actions: Tensor, next_states_: Tensor, masks: Tensor):
+        """
+            All inputs have shape [num_steps, batch_size, xxx]
+        """
+
+        cur_states = states[0]
+        loss = []
+        for i in range(self._step):
+            next_states = self._model(cur_states, actions[i])
+            diffs = next_states - cur_states - next_states_[i] + states[i]
+            weighted_diffs = diffs / self._normalizers.diff.op_std.maximum(1e-6)
+            loss.append(self._criterion(weighted_diffs, 0, cur_states))
+
+            if i < self._step - 1:
+                cur_states = states[i + 1] + masks[i].expand_dims(-1) * (next_states - states[i + 1])
+
+        return tf.add_n(loss) / self._step
+
+    @nn.make_method(fetch='loss')
+    def get_loss(self, states, next_states_, actions, masks): pass
+
+    def build_backward(self, lr: float, weight_decay: float, max_grad_norm=2.):
+        loss = self.op_loss.reduce_mean(name='Loss')
+
+        optimizer = tf.train.AdamOptimizer(lr)
+        params = self._model.parameters()
+        regularization = weight_decay * tf.add_n([tf.nn.l2_loss(t) for t in params], name='regularization')
+
+        grads_and_vars = optimizer.compute_gradients(loss + regularization, var_list=params)
+        print([var.name for grad, var in grads_and_vars])
+        clip_grads, op_grad_norm = tf.clip_by_global_norm([grad for grad, _ in grads_and_vars], max_grad_norm)
+        clip_grads_and_vars = [(grad, var) for grad, (_, var) in zip(clip_grads, grads_and_vars)]
+        self.op_train = optimizer.apply_gradients(clip_grads_and_vars)
+        self.op_grad_norm = op_grad_norm
diff --git a/experiments/gym_cheetah_1234/src/slbo/partial_envs.py b/experiments/gym_cheetah_1234/src/slbo/partial_envs.py
new file mode 100644
index 0000000..294524a
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/partial_envs.py
@@ -0,0 +1,26 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from slbo.envs.mujoco.half_cheetah_env import HalfCheetahEnv
+from slbo.envs.mujoco.walker2d_env import Walker2DEnv
+from slbo.envs.mujoco.humanoid_env import HumanoidEnv
+from slbo.envs.mujoco.ant_env import AntEnv
+from slbo.envs.mujoco.hopper_env import HopperEnv
+from slbo.envs.mujoco.swimmer_env import SwimmerEnv
+
+
+def make_env(id: str):
+    envs = {
+        'HalfCheetah-v2': HalfCheetahEnv,
+        'Walker2D-v2': Walker2DEnv,
+        'Humanoid-v2': HumanoidEnv,
+        'Ant-v2': AntEnv,
+        'Hopper-v2': HopperEnv,
+        'Swimmer-v2': SwimmerEnv,
+    }
+    env = envs[id]()
+    if not hasattr(env, 'reward_range'):
+        env.reward_range = (-np.inf, np.inf)
+    if not hasattr(env, 'metadata'):
+        env.metadata = {}
+    env.seed(np.random.randint(2**60))
+    return env
diff --git a/experiments/gym_cheetah_1234/src/slbo/policies/__init__.py b/experiments/gym_cheetah_1234/src/slbo/policies/__init__.py
new file mode 100644
index 0000000..3ffa40d
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/policies/__init__.py
@@ -0,0 +1,13 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import abc
+from typing import Union
+import lunzi.nn as nn
+
+
+class BasePolicy(abc.ABC):
+    @abc.abstractmethod
+    def get_actions(self, states):
+        pass
+
+
+BaseNNPolicy = Union[BasePolicy, nn.Module]  # should be Intersection, see PEP544
diff --git a/experiments/gym_cheetah_1234/src/slbo/policies/gaussian_mlp_policy.py b/experiments/gym_cheetah_1234/src/slbo/policies/gaussian_mlp_policy.py
new file mode 100644
index 0000000..c6bcb27
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/policies/gaussian_mlp_policy.py
@@ -0,0 +1,69 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List
+import tensorflow as tf
+import numpy as np
+from lunzi import Tensor
+from lunzi import nn
+from baselines.common.tf_util import normc_initializer
+from slbo.utils.truncated_normal import LimitedEntNormal
+from . import BasePolicy
+from slbo.utils.normalizer import GaussianNormalizer
+
+
+class GaussianMLPPolicy(nn.Module, BasePolicy):
+    op_states: Tensor
+
+    def __init__(self, dim_state: int, dim_action: int, hidden_sizes: List[int], normalizer: GaussianNormalizer,
+                 init_std=1.):
+        super().__init__()
+        self.dim_state = dim_state
+        self.dim_action = dim_action
+        self.hidden_sizes = hidden_sizes
+        self.init_std = init_std
+        self.normalizer = normalizer
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, shape=[None, dim_state], name='states')
+            self.op_actions_ = tf.placeholder(tf.float32, shape=[None, dim_action], name='actions')
+
+            layers = []
+            # note that the placeholder has size 105.
+            all_sizes = [dim_state, *self.hidden_sizes]
+            for i, (in_features, out_features) in enumerate(zip(all_sizes[:-1], all_sizes[1:])):
+                layers.append(nn.Linear(in_features, out_features, weight_initializer=normc_initializer(1)))
+                layers.append(nn.Tanh())
+            layers.append(nn.Linear(all_sizes[-1], dim_action, weight_initializer=normc_initializer(0.01)))
+            self.net = nn.Sequential(*layers)
+
+            self.op_log_std = nn.Parameter(
+                tf.constant(np.log(self.init_std), shape=[self.dim_action], dtype=tf.float32), name='log_std')
+
+        self.distribution = self(self.op_states)
+        self.op_actions = self.distribution.sample()
+        self.op_actions_mean = self.distribution.mean()
+        self.op_actions_std = self.distribution.stddev()
+        self.op_nlls_ = -self.distribution.log_prob(self.op_actions_).reduce_sum(axis=1)
+
+        self.register_callable('[states] => [actions]', self.fast)
+
+    def forward(self, states):
+        states = self.normalizer(states)
+        actions_mean = self.net(states)
+        distribution = LimitedEntNormal(actions_mean, self.op_log_std.exp())
+
+        return distribution
+
+    @nn.make_method(fetch='actions')
+    def get_actions(self, states): pass
+
+    def fast(self, states, use_log_prob=False):
+        states = self.normalizer.fast(states)
+        actions_mean = self.net.fast(states)
+        noise = np.random.randn(*actions_mean.shape)
+        actions = actions_mean + noise * np.exp(self.op_log_std.numpy())
+        if use_log_prob:
+            log_prob = -noise**2 / 2 - np.log(2 * np.pi) / 2 - self.op_log_std.numpy()
+            return actions, log_prob.sum(axis=1)
+        return actions
+
+    def clone(self):
+        return GaussianMLPPolicy(self.dim_state, self.dim_action, self.hidden_sizes, self.normalizer, self.init_std)
diff --git a/experiments/gym_cheetah_1234/src/slbo/policies/uniform_policy.py b/experiments/gym_cheetah_1234/src/slbo/policies/uniform_policy.py
new file mode 100644
index 0000000..ca9f821
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/policies/uniform_policy.py
@@ -0,0 +1,11 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from . import BasePolicy
+
+
+class UniformPolicy(BasePolicy):
+    def __init__(self, dim_action):
+        self.dim_action = dim_action
+
+    def get_actions(self, states):
+        return np.random.uniform(-1., 1., states.shape[:-1] + (self.dim_action,))
diff --git a/experiments/gym_cheetah_1234/src/slbo/q_function/__init__.py b/experiments/gym_cheetah_1234/src/slbo/q_function/__init__.py
new file mode 100644
index 0000000..e8dfcba
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/q_function/__init__.py
@@ -0,0 +1,13 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import abc
+from typing import Union
+import lunzi.nn as nn
+
+
+class BaseQFunction(abc.ABC):
+    @abc.abstractmethod
+    def get_q(self, states, values):
+        pass
+
+
+BaseNNQFunction = Union[BaseQFunction, nn.Module]
diff --git a/experiments/gym_cheetah_1234/src/slbo/q_function/mlp_q_function.py b/experiments/gym_cheetah_1234/src/slbo/q_function/mlp_q_function.py
new file mode 100644
index 0000000..c8eed83
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/q_function/mlp_q_function.py
@@ -0,0 +1,22 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List
+import tensorflow as tf
+from . import BaseQFunction
+import lunzi.nn as nn
+from slbo.utils.multi_layer_perceptron import MultiLayerPerceptron
+
+
+class MLPQFunction(MultiLayerPerceptron, BaseQFunction):
+    def __init__(self, dim_state: int, dim_action: int, hidden_states: List[int]):
+        super().__init__((dim_state + dim_action, *hidden_states, 1), squeeze=True)
+        self._dim_state = dim_state
+        self._dim_action = dim_action
+
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, [None, dim_state])
+            self.op_actions = tf.placeholder(tf.float32, [None, dim_action])
+
+        self.op_Q = self.forward(self.op_states, self.op_actions)
+
+    @nn.make_method(fetch='Q')
+    def get_q(self, states, actions): pass
diff --git a/experiments/gym_cheetah_1234/src/slbo/random_net.py b/experiments/gym_cheetah_1234/src/slbo/random_net.py
new file mode 100644
index 0000000..c9a94e9
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/random_net.py
@@ -0,0 +1,41 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List
+import tensorflow as tf
+from lunzi import Tensor
+import lunzi.nn as nn
+from slbo.utils.normalizer import Normalizers
+from slbo.utils.multi_layer_perceptron import MultiLayerPerceptron
+
+
+class RandomNet(MultiLayerPerceptron):
+    op_loss: Tensor
+    op_train: Tensor
+    op_grad_norm: Tensor
+
+    def __init__(self, dim_state: int, dim_action: int, normalizers: Normalizers, hidden_sizes: List[int]):
+        initializer = tf.truncated_normal_initializer(mean=0.0, stddev=1e-5)
+
+        self.dim_state = dim_state
+        self.dim_action = dim_action
+        self.hidden_sizes = hidden_sizes
+        self.op_states = tf.placeholder(tf.float32, shape=[None, self.dim_state], name='states')
+        self.op_actions = tf.placeholder(tf.float32, shape=[None, self.dim_action], name='actions')
+        super().__init__([dim_state + dim_action, *hidden_sizes[:-1]],
+                         activation=nn.ReLU,
+                         weight_initializer=initializer, build=False)
+
+        self.normalizers = normalizers
+        self.build()
+
+    def build(self):
+        self.op_features = self.forward(self.op_states, self.op_actions)
+
+    def forward(self, states, actions):
+        assert actions.shape[-1] == self.dim_action
+        inputs = tf.concat([self.normalizers.state(states), actions.clip_by_value(-1., 1.)], axis=1)
+
+        features = super().forward(inputs)
+        return features
+
+    def clone(self):
+        return RandomNet(self.dim_state, self.dim_action, self.normalizers, self.hidden_sizes)
diff --git a/experiments/gym_cheetah_1234/src/slbo/utils/OU_noise.py b/experiments/gym_cheetah_1234/src/slbo/utils/OU_noise.py
new file mode 100644
index 0000000..eae7158
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/utils/OU_noise.py
@@ -0,0 +1,36 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from slbo.policies import BasePolicy
+
+
+class OUNoise(object):
+    _policy: BasePolicy
+
+    def __init__(self, action_space, mu=0.0, theta=0.15, sigma=0.3, shape=None):
+        self.mu = mu
+        self.theta = theta
+        self.sigma = sigma
+        self.action_space = action_space
+        self._state = None
+        if shape:
+            self.shape = shape
+        else:
+            self.shape = action_space.shape
+
+        self.reset()
+
+    def reset(self):
+        self._state = np.ones(self.shape) * self.mu
+
+    def next(self):
+        delta = self.theta * (self.mu - self._state) + self.sigma * np.random.randn(*self._state.shape)
+        self._state = self._state + delta
+        return self._state
+
+    def get_actions(self, states):
+        return self._policy.get_actions(states) + self.next()
+
+    def make(self, policy: BasePolicy):
+        self._policy = policy
+        return self
+
diff --git a/experiments/gym_cheetah_1234/src/slbo/utils/__init__.py b/experiments/gym_cheetah_1234/src/slbo/utils/__init__.py
new file mode 100644
index 0000000..5c7f19c
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/utils/__init__.py
@@ -0,0 +1 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
diff --git a/experiments/gym_cheetah_1234/src/slbo/utils/average_meter.py b/experiments/gym_cheetah_1234/src/slbo/utils/average_meter.py
new file mode 100644
index 0000000..b3e285c
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/utils/average_meter.py
@@ -0,0 +1,20 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+class AverageMeter(object):
+    sum: float
+    count: float
+
+    def __init__(self, discount=1.):
+        self.discount = discount
+        self.reset()
+
+    def update(self, value, count=1):
+        self.sum = self.sum * self.discount + value * count
+        self.count = self.count * self.discount + count
+        return self.get()
+
+    def get(self):
+        return self.sum / (self.count + 1.e-8)
+
+    def reset(self):
+        self.sum = 0.
+        self.count = 0.
diff --git a/experiments/gym_cheetah_1234/src/slbo/utils/dataset.py b/experiments/gym_cheetah_1234/src/slbo/utils/dataset.py
new file mode 100644
index 0000000..c0046d2
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/utils/dataset.py
@@ -0,0 +1,28 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+import lunzi.dataset as dataset
+import gym
+
+
+def gen_dtype(env: gym.Env, fields: str):
+    dtypes = {
+        'state': ('state', 'f8', env.observation_space.shape),
+        'action': ('action', 'f8', env.action_space.shape),
+        'next_state': ('next_state', 'f8', env.observation_space.shape),
+        'reward': ('reward', 'f8'),
+        'done': ('done', 'bool'),
+        'timeout': ('timeout', 'bool'),
+        'return_': ('return_', 'f8'),
+        'advantage': ('advantage', 'f8'),
+    }
+    return [dtypes[field] for field in fields.split(' ')]
+
+
+class Dataset(dataset.Dataset):
+    def sample_multi_step(self, size: int, n_env: int, n_step=1):
+        starts = np.random.randint(0, self._len, size=size)
+        batch = []
+        for step in range(n_step):
+            batch.append(self[(starts + step * n_env) % self._len])
+        return np.concatenate(batch).reshape(n_step, size).view(np.recarray)
+
diff --git a/experiments/gym_cheetah_1234/src/slbo/utils/flags.py b/experiments/gym_cheetah_1234/src/slbo/utils/flags.py
new file mode 100644
index 0000000..60e8e11
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/utils/flags.py
@@ -0,0 +1,166 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import time
+import os
+import yaml
+from subprocess import check_output, CalledProcessError
+from lunzi.config import BaseFLAGS, expand, parse
+from lunzi.Logger import logger, FileSink
+
+
+class FLAGS(BaseFLAGS):
+    _initialized = False
+
+    use_prev = True
+    seed = 100
+    log_dir: str = None
+    run_id: str = None
+    algorithm = 'OLBO'  # possible options: OLBO, baseline, MF
+
+    class pc(BaseFLAGS):
+        bonus_scale = 0.05
+        lamb = 0.01
+
+    class slbo(BaseFLAGS):
+        n_iters = 20
+        n_policy_iters = 10
+        n_model_iters = 100
+        n_stages = 100
+        n_evaluate_iters = 10
+        opt_model = False
+        start = 'reset'  # possibly 'buffer'
+
+    class plan(BaseFLAGS):
+        max_steps = 1000
+        n_envs = None
+        n_trpo_samples = 4000
+
+        @classmethod
+        def finalize(cls):
+            if cls.n_envs is None:
+                cls.n_envs = cls.n_trpo_samples // cls.max_steps
+            assert cls.n_envs * cls.max_steps == cls.n_trpo_samples
+
+    class env(BaseFLAGS):
+        id = 'HalfCheetah-v2'
+
+    class rollout(BaseFLAGS):
+        normalizer = 'policy'
+        max_buf_size = 100000
+        n_train_samples = 2000
+        n_dev_samples = 0
+        n_test_samples = 10000
+
+        @classmethod
+        def finalize(cls):
+            cls.n_dev_samples = cls.n_dev_samples or cls.n_train_samples
+
+    class ckpt(BaseFLAGS):
+        n_save_stages = 10
+        model_load = None
+        policy_load = None
+        buf_load = None
+        buf_load_index = 0
+        base = '/tmp/mbrl/logs'
+        warm_up = None
+
+        @classmethod
+        def finalize(cls):
+            for key, value in cls.as_dict().items():
+                if isinstance(value, str):
+                    setattr(cls, key, expand(value))
+
+    class OUNoise(BaseFLAGS):
+        theta = 0.15
+        sigma = 0.3
+
+    class model(BaseFLAGS):
+        hidden_sizes = [500, 500]
+        loss = 'L2'  # possibly L1, L2, MSE, G
+        G_coef = 0.5
+        multi_step = 1
+        lr = 1e-3
+        weight_decay = 1e-5
+        validation_freq = 1
+        optimizer = 'Adam'
+        train_batch_size = 256
+        dev_batch_size = 1024
+
+    class policy(BaseFLAGS):
+        hidden_sizes = [32, 32]
+        init_std = 1.
+
+    class PPO(BaseFLAGS):
+        n_minibatches = 32
+        n_opt_epochs = 10
+        ent_coef = 0.005
+        lr = 3e-4
+        clip_range = 0.2
+
+    class TRPO(BaseFLAGS):
+        cg_damping = 0.1
+        n_cg_iters = 10
+        max_kl = 0.01
+        vf_lr = 1e-3
+        n_vf_iters = 5
+        ent_coef = 0.0
+
+    class runner(BaseFLAGS):
+        lambda_ = 0.95
+        gamma = 0.99
+        max_steps = 500
+
+    @classmethod
+    def set_seed(cls):
+        if cls.seed == 0:  # auto seed
+            cls.seed = int.from_bytes(os.urandom(3), 'little') + 1  # never use seed 0 for RNG, 0 is for `urandom`
+        logger.warning("Setting random seed to %s", cls.seed)
+
+        import numpy as np
+        import tensorflow as tf
+        import torch
+        import random
+        np.random.seed(cls.seed)
+        tf.set_random_seed(np.random.randint(2**30))
+        torch.manual_seed(np.random.randint(2**30))
+        random.seed(np.random.randint(2**30))
+        torch.cuda.manual_seed_all(np.random.randint(2**30))
+        torch.backends.cudnn.deterministic = True
+
+    @classmethod
+    def finalize(cls):
+        log_dir = cls.log_dir
+        if log_dir is None:
+            run_id = cls.run_id
+            if run_id is None:
+                run_id = time.strftime('%Y-%m-%d_%H-%M-%S')
+
+            log_dir = os.path.join(cls.ckpt.base, run_id)
+            cls.log_dir = log_dir
+
+        if not os.path.exists(log_dir):
+            os.makedirs(log_dir)
+
+        for t in range(60):
+            try:
+                cls.commit = check_output(['git', 'rev-parse', 'HEAD']).decode('utf-8').strip()
+                check_output(['git', 'add', '.'])
+                check_output(['git', 'checkout-index', '-a', '-f', f'--prefix={log_dir}/src/'])
+                break
+            except CalledProcessError:
+                pass
+            time.sleep(1)
+        else:
+            raise RuntimeError('Failed after 60 trials.')
+
+        yaml.dump(cls.as_dict(), open(os.path.join(log_dir, 'config.yml'), 'w'), default_flow_style=False)
+        open(os.path.join(log_dir, 'diff.patch'), 'w').write(
+            check_output(['git', '--no-pager', 'diff', 'HEAD']).decode('utf-8'))
+
+        logger.add_sink(FileSink(os.path.join(log_dir, 'log.json')))
+        logger.info("log_dir = %s", log_dir)
+
+        cls.set_frozen()
+
+
+parse(FLAGS)
+
diff --git a/experiments/gym_cheetah_1234/src/slbo/utils/multi_layer_perceptron.py b/experiments/gym_cheetah_1234/src/slbo/utils/multi_layer_perceptron.py
new file mode 100644
index 0000000..0fe5dfe
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/utils/multi_layer_perceptron.py
@@ -0,0 +1,51 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+
+
+class MultiLayerPerceptron(nn.Module):
+    def __init__(self, blocks, activation=nn.ReLU, squeeze=False, weight_initializer=None, build=True):
+        super().__init__()
+
+        self._blocks = blocks
+        if build:
+            self.op_inputs = tf.placeholder(tf.float32, [None, self._blocks[0]])
+
+        with self.scope:
+            kwargs = {}
+            if weight_initializer is not None:
+                kwargs['weight_initializer'] = weight_initializer
+            layers = []
+            for in_features, out_features in zip(blocks[:-1], blocks[1:]):
+                if layers:
+                    layers.append(activation())
+                layers.append(nn.Linear(in_features, out_features, **kwargs))
+            if squeeze:
+                layers.append(nn.Squeeze(axis=1))
+            self.net = nn.Sequential(*layers)
+
+        self._squeeze = squeeze
+        self._activation = activation
+
+        if build:
+            self.build()
+
+    def build(self):
+        self.op_outputs = self.forward(self.op_inputs)
+
+    def forward(self, *inputs):
+        if len(inputs) > 1:
+            inputs = tf.concat(inputs, axis=-1)
+        else:
+            inputs = inputs[0]
+        return self.net(inputs)
+
+    def fast(self, *inputs):
+        return self.net.fast(np.concatenate(inputs, axis=-1))
+
+    def clone(self):
+        return MultiLayerPerceptron(self._blocks, self._activation, self._squeeze)
+
+    def extra_repr(self):
+        return f'activation = {self._activation}, blocks = {self._blocks}, squeeze = {self._squeeze}'
diff --git a/experiments/gym_cheetah_1234/src/slbo/utils/normalizer.py b/experiments/gym_cheetah_1234/src/slbo/utils/normalizer.py
new file mode 100644
index 0000000..3bb3685
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/utils/normalizer.py
@@ -0,0 +1,66 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List
+import numpy as np
+import tensorflow as tf
+import lunzi.nn as nn
+from lunzi.Logger import logger
+from lunzi import Tensor
+from slbo.utils.np_utils import gaussian_kl
+
+
+class GaussianNormalizer(nn.Module):
+    def __init__(self, name: str, shape: List[int], eps=1e-8, verbose=False):  # batch_size x ...
+        super().__init__()
+
+        self.name = name
+        self.shape = shape
+        self.eps = eps
+        self._verbose = verbose
+
+        with self.scope:
+            self.op_mean = nn.Parameter(tf.zeros(shape, dtype=tf.float32), name='mean', trainable=False)
+            self.op_std = nn.Parameter(tf.ones(shape, dtype=tf.float32), name='std', trainable=False)
+            self.op_n = nn.Parameter(tf.zeros([], dtype=tf.int64), name='n', trainable=False)
+
+    def extra_repr(self):
+        return f'shape={self.shape}'
+
+    def forward(self, x: Tensor, inverse=False):
+        if inverse:
+            return x * self.op_std + self.op_mean
+        return (x - self.op_mean).div(self.op_std.maximum(self.eps))
+
+    def update(self, samples: np.ndarray):
+        old_mean, old_std, old_n = self.op_mean.numpy(), self.op_std.numpy(), self.op_n.numpy()
+        samples = samples - old_mean
+
+        m = samples.shape[0]
+        delta = samples.mean(axis=0)
+        new_n = old_n + m
+        new_mean = old_mean + delta * m / new_n
+        new_std = np.sqrt((old_std**2 * old_n + samples.var(axis=0) * m + delta**2 * old_n * m / new_n) / new_n)
+
+        kl_old_new = gaussian_kl(new_mean, new_std, old_mean, old_std).sum()
+        self.load_state_dict({'op_mean': new_mean, 'op_std': new_std, 'op_n': new_n})
+
+        if self._verbose:
+            logger.info("updating Normalizer<%s>, KL divergence = %.6f", self.name, kl_old_new)
+
+    def fast(self, samples: np.ndarray, inverse=False) -> np.ndarray:
+        mean, std = self.op_mean.numpy(), self.op_std.numpy()
+        if inverse:
+            return samples * std + mean
+        return (samples - mean) / np.maximum(std, self.eps)
+
+
+class Normalizers(nn.Module):
+    def __init__(self, dim_action: int, dim_state: int):
+        super().__init__()
+        self.action = GaussianNormalizer('action', [dim_action])
+        self.state = GaussianNormalizer('state', [dim_state])
+        self.diff = GaussianNormalizer('diff', [dim_state])
+
+    def forward(self):
+        pass
+
+
diff --git a/experiments/gym_cheetah_1234/src/slbo/utils/np_utils.py b/experiments/gym_cheetah_1234/src/slbo/utils/np_utils.py
new file mode 100644
index 0000000..d170ed4
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/utils/np_utils.py
@@ -0,0 +1,9 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+
+
+def gaussian_kl(mean_1: np.ndarray, std_1: np.ndarray, mean_2: np.ndarray, std_2: np.ndarray) -> np.ndarray:
+    eps = 1e-20
+    std_1 = np.maximum(std_1, eps)
+    std_2 = np.maximum(std_2, eps)
+    return np.log(std_2 / std_1) + (std_1**2 + (mean_1 - mean_2)**2) / std_2**2 / 2. - 0.5
diff --git a/experiments/gym_cheetah_1234/src/slbo/utils/pc_utils.py b/experiments/gym_cheetah_1234/src/slbo/utils/pc_utils.py
new file mode 100644
index 0000000..ebd717b
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/utils/pc_utils.py
@@ -0,0 +1,110 @@
+import torch
+import numpy as np
+import copy
+import os
+import scipy.spatial
+import scipy.signal
+
+
+def median_trick(X, args):
+    #median trick for computing the bandwith for kernel regression.
+    N = X.shape[0]
+    #print(X.shape)
+    perm = np.random.choice(N, np.min([N,args.update_size * args.buffer_width]), replace=False)
+    dsample = X[perm]
+    pd = scipy.spatial.distance.pdist(dsample)
+    sigma = np.median(pd)
+    return sigma
+
+def compute_cov_pi(phi):
+    #cov = np.zeros((phi.shape[1],phi.shape[1]))
+
+    #for i in range(len(phi)):
+    #    cov += np.outer(phi[i],phi[i])
+    cov = np.dot(phi.T,phi)
+    cov /= phi.shape[0]
+
+    #print(cov)
+
+    return cov
+
+def discount_cumsum(x, discount):
+    """
+    magic from rllab for computing discounted cumulative sums of vectors.
+    input:
+        vector x,
+        [x0,
+         x1,
+         x2]
+    output:
+        [x0 + discount * x1 + discount^2 * x2,
+         x1 + discount * x2,
+         x2]
+    """
+    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]
+
+
+
+def denormalize(ob_rms, state):
+    var = ob_rms.var
+    #print(var)
+    return torch.FloatTensor(state.data.numpy() * np.sqrt(var) + ob_rms.mean)
+
+def normalize(ob_rms, state):
+    return torch.FloatTensor((state.data.numpy() - ob_rms.mean) / np.sqrt(ob_rms.var))
+
+def soft_update_from_to(source, target, tau):
+    for target_param, param in zip(target.parameters(), source.parameters()):
+        target_param.data.copy_(
+            target_param.data * (1.0 - tau) + param.data * tau
+        )
+
+def copy_model_params_from_to(source, target):
+    for target_param, param in zip(target.parameters(), source.parameters()):
+        target_param.data.copy_(param.data)
+
+
+def save(dynamics, timestep_list, reward_list, bonus_list, loss_list, cov, sigma, args):
+
+    save_path = "trained_models/{}/{}_{}_{}_{}".format(args.env_name, args.phi_dim, args.bonus_scale, args.lamb, args.sample_size, args.plan_horizen)
+    if args.use_v_net:
+        save_path += "_vnet"
+    if args.normalize:
+        save_path += "_norm"
+    if args.no_bonus:
+        save_path += "_no_bonus"
+    try:
+        os.makedirs(save_path)
+    except OSError:
+        pass
+    torch.save(
+        dynamics, os.path.join(
+        save_path ,
+        "{}.pt".format(str(args.seed))))
+
+    np.save(
+        os.path.join(
+        save_path ,
+        "cov_{}".format(str(args.seed))),
+        [cov,sigma])
+
+
+    save_path = "data/{}/{}_{}_{}_{}_{}_{}".format(args.env_name, 
+        args.phi_dim, args.bonus_scale, args.lamb, args.buffer_width, 
+        args.plan_horizen, args.lr, args.v_lr)
+    if args.use_v_net:
+        save_path += "_vnet"
+    if args.normalize:
+        save_path += "_norm"
+    if args.no_bonus:
+        save_path += "_no_bonus"
+    try:
+        os.makedirs(save_path)
+    except OSError:
+        pass
+
+
+    np.save(save_path+"/ts_{}".format(str(args.seed)),timestep_list)
+    np.save(save_path+"/rw_{}".format(str(args.seed)),reward_list)
+    np.save(save_path+"/bonus_{}".format(str(args.seed)),bonus_list)
+    np.save(save_path+"/loss_{}".format(str(args.seed)),loss_list)
diff --git a/experiments/gym_cheetah_1234/src/slbo/utils/runner.py b/experiments/gym_cheetah_1234/src/slbo/utils/runner.py
new file mode 100644
index 0000000..388803e
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/utils/runner.py
@@ -0,0 +1,92 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from lunzi.dataset import Dataset
+from slbo.envs import BaseBatchedEnv
+from slbo.policies import BasePolicy
+from slbo.utils.dataset import Dataset, gen_dtype
+from slbo.v_function import BaseVFunction
+
+
+class Runner(object):
+    _states: np.ndarray  # [np.float]
+    _n_steps: np.ndarray
+    _returns: np.ndarray
+
+    def __init__(self, env: BaseBatchedEnv, max_steps: int, gamma=0.99, lambda_=0.95, rescale_action=False):
+        self.env = env
+        self.n_envs = env.n_envs
+        self.gamma = gamma
+        self.lambda_ = lambda_
+        self.max_steps = max_steps
+        self.rescale_action = rescale_action
+        self._dtype = gen_dtype(env, 'state action next_state reward done timeout')
+
+        self.reset()
+
+    def reset(self):
+        self.set_state(self.env.reset(), set_env_state=False)
+
+    def set_state(self, states: np.ndarray, set_env_state=True):
+        self._states = states.copy()
+        if set_env_state:
+            self.env.set_state(states)
+        self._n_steps = np.zeros(self.n_envs, 'i4')
+        self._returns = np.zeros(self.n_envs, 'f8')
+
+    def get_state(self):
+        return self._states.copy()
+
+    def run(self, policy: BasePolicy, n_samples: int):
+        ep_infos = []
+        n_steps = n_samples // self.n_envs
+        assert n_steps * self.n_envs == n_samples
+        dataset = Dataset(self._dtype, n_samples)
+
+        for T in range(n_steps):
+            unscaled_actions = policy.get_actions(self._states)
+            if self.rescale_action:
+                lo, hi = self.env.action_space.low, self.env.action_space.high
+                actions = (lo + (unscaled_actions + 1.) * 0.5 * (hi - lo))
+            else:
+                actions = unscaled_actions
+
+            next_states, rewards, dones, infos = self.env.step(actions)
+            dones = dones.astype(bool)
+            self._returns += rewards
+            self._n_steps += 1
+            timeouts = self._n_steps == self.max_steps
+
+            steps = [self._states.copy(), unscaled_actions, next_states.copy(), rewards, dones, timeouts]
+            dataset.extend(np.rec.fromarrays(steps, dtype=self._dtype))
+
+            indices = np.where(dones | timeouts)[0]
+            if len(indices) > 0:
+                next_states = next_states.copy()
+                next_states[indices] = self.env.partial_reset(indices)
+                for index in indices:
+                    infos[index]['episode'] = {'return': self._returns[index]}
+                self._n_steps[indices] = 0
+                self._returns[indices] = 0.
+
+            self._states = next_states.copy()
+            ep_infos.extend([info['episode'] for info in infos if 'episode' in info])
+
+        return dataset, ep_infos
+
+    def compute_advantage(self, vfn: BaseVFunction, samples: Dataset):
+        n_steps = len(samples) // self.n_envs
+        samples = samples.reshape((n_steps, self.n_envs))
+        use_next_vf = ~samples.done
+        use_next_adv = ~(samples.done | samples.timeout)
+
+        next_values = vfn.get_values(samples[-1].next_state)
+        values = vfn.get_values(samples.reshape(-1).state).reshape(n_steps, self.n_envs)
+        advantages = np.zeros((n_steps, self.n_envs), dtype=np.float32)
+        last_gae_lambda = 0
+
+        for t in reversed(range(n_steps)):
+            delta = samples[t].reward + self.gamma * next_values * use_next_vf[t] - values[t]
+            advantages[t] = last_gae_lambda = delta + self.gamma * self.lambda_ * last_gae_lambda * use_next_adv[t]
+            next_values = values[t]
+        return advantages.reshape(-1), values.reshape(-1)
+
diff --git a/experiments/gym_cheetah_1234/src/slbo/utils/tf_utils.py b/experiments/gym_cheetah_1234/src/slbo/utils/tf_utils.py
new file mode 100644
index 0000000..154c60a
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/utils/tf_utils.py
@@ -0,0 +1,20 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+
+
+def get_tf_config():
+    gpu_frac = 1
+
+    gpu_options = tf.GPUOptions(
+        per_process_gpu_memory_fraction=gpu_frac,
+        allow_growth=True,
+    )
+    config = tf.ConfigProto(
+        gpu_options=gpu_options,
+        log_device_placement=False,
+        allow_soft_placement=True,
+        inter_op_parallelism_threads=1,
+        intra_op_parallelism_threads=1,
+    )
+
+    return config
diff --git a/experiments/gym_cheetah_1234/src/slbo/utils/truncated_normal.py b/experiments/gym_cheetah_1234/src/slbo/utils/truncated_normal.py
new file mode 100644
index 0000000..04a0edc
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/utils/truncated_normal.py
@@ -0,0 +1,12 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+import numpy as np
+
+
+class LimitedEntNormal(tf.distributions.Normal):
+    def _entropy(self):
+        limit = 2.
+        lo, hi = (-limit - self._loc) / self._scale / np.sqrt(2), (limit - self._loc) / self._scale / np.sqrt(2)
+        return 0.5 * (self._scale.log() + np.log(2 * np.pi) / 2) * (hi.erf() - lo.erf()) + 0.5 * \
+            (tf.exp(-hi * hi) * hi - tf.exp(-lo * lo) * lo)
+
diff --git a/experiments/gym_cheetah_1234/src/slbo/v_function/__init__.py b/experiments/gym_cheetah_1234/src/slbo/v_function/__init__.py
new file mode 100644
index 0000000..7794a84
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/v_function/__init__.py
@@ -0,0 +1,13 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import abc
+from typing import Union
+import lunzi.nn as nn
+
+
+class BaseVFunction(abc.ABC):
+    @abc.abstractmethod
+    def get_values(self, states):
+        pass
+
+
+BaseNNVFunction = Union[BaseVFunction, nn.Module]  # in fact it should be Intersection
\ No newline at end of file
diff --git a/experiments/gym_cheetah_1234/src/slbo/v_function/mlp_v_function.py b/experiments/gym_cheetah_1234/src/slbo/v_function/mlp_v_function.py
new file mode 100644
index 0000000..d9749e9
--- /dev/null
+++ b/experiments/gym_cheetah_1234/src/slbo/v_function/mlp_v_function.py
@@ -0,0 +1,24 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+from baselines.common.tf_util import normc_initializer
+from slbo.utils.multi_layer_perceptron import MultiLayerPerceptron
+import lunzi.nn as nn
+from . import BaseVFunction
+
+
+class MLPVFunction(BaseVFunction, nn.Module):
+    def __init__(self, dim_state, hidden_sizes, normalizer=None):
+        super().__init__()
+        self.mlp = MultiLayerPerceptron((dim_state, *hidden_sizes, 1), activation=nn.Tanh, squeeze=True,
+                                        weight_initializer=normc_initializer(1.), build=False)
+        self.normalizer = normalizer
+        self.op_states = tf.placeholder(tf.float32, shape=[None, dim_state])
+        self.op_values = self.forward(self.op_states)
+
+    def forward(self, states):
+        states = self.normalizer(states)
+        return self.mlp(states)
+
+    @nn.make_method(fetch='values')
+    def get_values(self, states): pass
+
diff --git a/main.py b/main.py
index 6262ea2..3e25d8b 100644
--- a/main.py
+++ b/main.py
@@ -66,7 +66,7 @@ def main():
 
     policy = GaussianMLPPolicy(dim_state, dim_action, normalizer=normalizers.state, **FLAGS.policy.as_dict())
     # batched noises
-    noise = OUNoise(env.action_space, theta=FLAGS.OUNoise.theta, sigma=FLAGS.OUNoise.sigma, shape=(1, dim_action))
+    #noise = OUNoise(env.action_space, theta=FLAGS.OUNoise.theta, sigma=FLAGS.OUNoise.sigma, shape=(1, dim_action))
     vfn = MLPVFunction(dim_state, [64, 64], normalizers.state)
     model = DynamicsModel(dim_state, dim_action, normalizers, FLAGS.model.hidden_sizes)
     random_net = RandomNet(dim_state, dim_action, normalizers, FLAGS.model.hidden_sizes)
@@ -121,10 +121,10 @@ def main():
             dev_set.clear()
 
         # collect data
-        recent_train_set, ep_infos = runners['collect'].run(noise.make(policy), FLAGS.rollout.n_train_samples)
+        recent_train_set, ep_infos = runners['collect'].run(policy, FLAGS.rollout.n_train_samples)
         add_multi_step(recent_train_set, train_set)
         add_multi_step(
-            runners['dev'].run(noise.make(policy), FLAGS.rollout.n_dev_samples)[0],
+            runners['dev'].run(policy, FLAGS.rollout.n_dev_samples)[0],
             dev_set,
         )
 
@@ -149,7 +149,7 @@ def main():
         virt_env.update_cov(recent_train_set.state,recent_train_set.action)
 
         if T == 50:
-            max_ent_coef = 0.
+            virt_env.bonus_scale = 0.
 
         for i in range(FLAGS.slbo.n_iters):
             if i % FLAGS.slbo.n_evaluate_iters == 0 and i != 0:
diff --git a/run_experiments.sh b/run_experiments.sh
index d9e0d30..19a7312 100644
--- a/run_experiments.sh
+++ b/run_experiments.sh
@@ -4,6 +4,6 @@ for env_name in $1; do
     echo "=> Running environment ${env_name}"
     for random_seed in 1234 2314 2345 1235; do
         python main.py -c configs/algos/slbo_bm_200k.yml configs/env_tingwu/${env_name}.yml \
-	    -s log_dir=experiments/${env_name}_${random_seed} seed=${random_seed}
+	    -s log_dir=./experiments/${env_name}_${random_seed} seed=${random_seed}
     done
 done
diff --git a/slbo/utils/__pycache__/flags.cpython-36.pyc b/slbo/utils/__pycache__/flags.cpython-36.pyc
index b452ad4..038cf5b 100644
Binary files a/slbo/utils/__pycache__/flags.cpython-36.pyc and b/slbo/utils/__pycache__/flags.cpython-36.pyc differ
diff --git a/slbo/utils/flags.py b/slbo/utils/flags.py
index 895554b..60e8e11 100644
--- a/slbo/utils/flags.py
+++ b/slbo/utils/flags.py
@@ -30,7 +30,7 @@ class FLAGS(BaseFLAGS):
         start = 'reset'  # possibly 'buffer'
 
     class plan(BaseFLAGS):
-        max_steps = 500
+        max_steps = 1000
         n_envs = None
         n_trpo_samples = 4000
 
@@ -45,8 +45,8 @@ class FLAGS(BaseFLAGS):
 
     class rollout(BaseFLAGS):
         normalizer = 'policy'
-        max_buf_size = 200000
-        n_train_samples = 10000
+        max_buf_size = 100000
+        n_train_samples = 2000
         n_dev_samples = 0
         n_test_samples = 10000
 
