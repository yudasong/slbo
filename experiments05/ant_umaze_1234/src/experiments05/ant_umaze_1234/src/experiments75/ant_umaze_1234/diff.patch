diff --git a/configs/algos/slbo_bm_200k.yml b/configs/algos/slbo_bm_200k.yml
index 9544e89..c0e39d4 100644
--- a/configs/algos/slbo_bm_200k.yml
+++ b/configs/algos/slbo_bm_200k.yml
@@ -4,12 +4,12 @@ model:
   dev_batch_size: 512
   train_batch_size: 128
 slbo:
-  n_stages: 50
+  n_stages: 80
   n_iters: 20
   n_model_iters: 100
   n_policy_iters: 40
 TRPO:
   ent_coef: 0
 rollout:
-  n_train_samples: 4000
+  n_train_samples: 6000
 
diff --git a/configs/env_tingwu/ant_umaze.yml b/configs/env_tingwu/ant_umaze.yml
index f653d95..ab75516 100644
--- a/configs/env_tingwu/ant_umaze.yml
+++ b/configs/env_tingwu/ant_umaze.yml
@@ -5,6 +5,4 @@ runner:
   max_steps: 1000
 plan:
   max_steps: 1000
-pc: 
-  bonus_scale: 0.1  
 
diff --git a/experiments02/ant_umaze_1234/config.yml b/experiments02/ant_umaze_1234/config.yml
new file mode 100644
index 0000000..8e54a2f
--- /dev/null
+++ b/experiments02/ant_umaze_1234/config.yml
@@ -0,0 +1,76 @@
+OUNoise:
+  sigma: 0.3
+  theta: 0.15
+PPO:
+  clip_range: 0.2
+  ent_coef: 0.005
+  lr: 0.0003
+  n_minibatches: 32
+  n_opt_epochs: 10
+TRPO:
+  cg_damping: 0.1
+  ent_coef: 0
+  max_kl: 0.01
+  n_cg_iters: 10
+  n_vf_iters: 5
+  vf_lr: 0.001
+algorithm: OLBO
+ckpt:
+  base: /tmp/mbrl/logs
+  buf_load: null
+  buf_load_index: 0
+  model_load: null
+  n_save_stages: 10
+  policy_load: null
+  warm_up: null
+commit: 7315421ee85a3e3a88f7b5e93c10320b95aa84c5
+env:
+  id: AntUMaze-v1
+log_dir: ./experiments02/ant_umaze_1234
+model:
+  G_coef: 0.5
+  dev_batch_size: 512
+  hidden_sizes:
+  - 500
+  - 500
+  loss: L2
+  lr: 0.001
+  multi_step: 2
+  optimizer: Adam
+  train_batch_size: 128
+  validation_freq: 1
+  weight_decay: 1.0e-05
+pc:
+  bonus_scale: 0.3
+  bonus_stop_time: 30
+  lamb: 0.01
+plan:
+  max_steps: 1000
+  n_envs: 4
+  n_trpo_samples: 4000
+policy:
+  hidden_sizes:
+  - 32
+  - 32
+  init_std: 1.0
+rollout:
+  max_buf_size: 100000
+  n_dev_samples: 6000
+  n_test_samples: 10000
+  n_train_samples: 6000
+  normalizer: policy
+run_id: null
+runner:
+  gamma: 0.99
+  lambda_: 0.95
+  max_steps: 1000
+seed: 1234
+slbo:
+  n_evaluate_iters: 5
+  n_iters: 20
+  n_model_iters: 100
+  n_policy_iters: 40
+  n_stages: 80
+  opt_model: false
+  start: reset
+use_prev: true
diff --git a/experiments02/ant_umaze_1234/diff.patch b/experiments02/ant_umaze_1234/diff.patch
new file mode 100644
index 0000000..e69de29
diff --git a/experiments02/ant_umaze_1234/eval_real_returns.npy b/experiments02/ant_umaze_1234/eval_real_returns.npy
new file mode 100644
index 0000000..48d5e49
Binary files /dev/null and b/experiments02/ant_umaze_1234/eval_real_returns.npy differ
diff --git a/experiments02/ant_umaze_1234/final.npy b/experiments02/ant_umaze_1234/final.npy
new file mode 100644
index 0000000..bea1822
Binary files /dev/null and b/experiments02/ant_umaze_1234/final.npy differ
diff --git a/experiments02/ant_umaze_1234/log.json b/experiments02/ant_umaze_1234/log.json
new file mode 100644
index 0000000..0435f72
--- /dev/null
+++ b/experiments02/ant_umaze_1234/log.json
@@ -0,0 +1,1226 @@
+{"level": "info", "fmt": "log_dir = %s", "args": ["./experiments02/ant_umaze_1234"], "caller": "slbo/utils/flags.py:157", "time": "2021-02-04T23:40:50.354431"}
+{"level": "info", "fmt": "Enabling flattening... %s", "args": [["GaussianMLPPolicy_1/log_std:0", "GaussianMLPPolicy_1/Linear_1/weight:0", "GaussianMLPPolicy_1/Linear_1/bias:0", "GaussianMLPPolicy_1/Linear_1_2/weight:0", "GaussianMLPPolicy_1/Linear_1_2/bias:0", "GaussianMLPPolicy_1/Linear_2_1/weight:0", "GaussianMLPPolicy_1/Linear_2_1/bias:0"]], "caller": "lunzi/nn/flat_param.py:17", "time": "2021-02-04T23:40:52.807164"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [0], "caller": "main.py:129", "time": "2021-02-04T23:40:53.850868"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-04T23:41:02.252988"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["DynamicsModel", "[states actions] => [next_states]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T23:41:02.255866"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["RandomNet", "[states actions] => [features]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T23:41:02.291096"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, -7.284877287321237, 0.14332865001202508], "caller": "main.py:37", "time": "2021-02-04T23:41:06.446026"}
+{"level": "info", "fmt": "episode: %s", "args": [1.4465341592050254], "caller": "main.py:149", "time": "2021-02-04T23:41:16.002349"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["MultiStepLoss", "[states next_states_ actions masks] => [train loss grad_norm]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T23:41:17.276512"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["MultiStepLoss", "[states next_states_ actions masks] => [loss]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T23:41:19.514354"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.9418136477470398, 0.8934787511825562, 100, 0.04154036139653962], "caller": "main.py:200", "time": "2021-02-04T23:41:19.597023"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["MLPVFunction", "[states] => [values]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T23:41:21.148158"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["TRPO", "[] => [sync_old]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T23:41:21.203021"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["TRPO", "[states actions advantages ent_coef] => [loss flat_grad dist_std mean_kl dist_mean]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T23:41:21.226106"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["TRPO", "[states tangents actions] => [hessian_vec_prod]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T23:41:21.376406"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["FlatParam", "[] => [get_flat]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T23:41:21.682528"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["FlatParam", "[feed_flat] => [set_flat]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T23:41:21.733371"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["TRPO", "[states actions advantages ent_coef] => [loss mean_kl]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T23:41:21.773999"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["TRPO", "[states returns] => [train_vf vf_loss]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T23:41:21.845115"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["TRPO", "[states returns] => [vf_loss]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T23:41:22.165819"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.7860048413276672, 0.6713047623634338, 100, 0.16561025848818733], "caller": "main.py:200", "time": "2021-02-04T23:42:40.910209"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.0337914414703846, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-04T23:43:21.392524"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.043049927800893784, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-04T23:43:42.150301"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.6176711916923523, 0.5952441692352295, 100, 0.2909312271780907], "caller": "main.py:200", "time": "2021-02-04T23:44:02.105346"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.015375589020550251, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-04T23:44:53.634868"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.5566920638084412, 0.5514814257621765, 100, 0.3222561534916964], "caller": "main.py:200", "time": "2021-02-04T23:45:32.893765"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.514809250831604, 0.49304574728012085, 100, 0.3772923105577832], "caller": "main.py:200", "time": "2021-02-04T23:47:11.795298"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.4818516671657562, 0.4630093276500702, 100, 0.3703128888832759], "caller": "main.py:200", "time": "2021-02-04T23:48:41.278561"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.4599356949329376, 0.43294572830200195, 100, 0.3976968195636735], "caller": "main.py:200", "time": "2021-02-04T23:50:05.116726"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.4385826885700226, 0.42157813906669617, 100, 0.41747728283998903], "caller": "main.py:200", "time": "2021-02-04T23:51:27.495836"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.4180285632610321, 0.4044298231601715, 100, 0.4314999809429683], "caller": "main.py:200", "time": "2021-02-04T23:52:52.020397"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.3998737335205078, 0.3830729126930237, 100, 0.44684541310371867], "caller": "main.py:200", "time": "2021-02-04T23:54:11.661200"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.37786078453063965, 0.3755621910095215, 100, 0.48355998749665746], "caller": "main.py:200", "time": "2021-02-04T23:55:37.256982"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.3583447337150574, 0.3550926446914673, 100, 0.494861520181284], "caller": "main.py:200", "time": "2021-02-04T23:57:01.310805"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.34129101037979126, 0.3349471688270569, 100, 0.515805960246004], "caller": "main.py:200", "time": "2021-02-04T23:58:25.963124"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.32429271936416626, 0.30259743332862854, 100, 0.5555948510210434], "caller": "main.py:200", "time": "2021-02-04T23:59:45.825195"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.3041222393512726, 0.30217814445495605, 100, 0.5695236721065341], "caller": "main.py:200", "time": "2021-02-05T00:01:05.522490"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2898537218570709, 0.2810972332954407, 100, 0.6087744017826704], "caller": "main.py:200", "time": "2021-02-05T00:02:25.255971"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.2758485674858093, 0.26457804441452026, 100, 0.6272926207749592], "caller": "main.py:200", "time": "2021-02-05T00:03:56.050297"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.26159679889678955, 0.25655606389045715, 100, 0.6638134106329863], "caller": "main.py:200", "time": "2021-02-05T00:05:38.264378"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.24803656339645386, 0.2451026886701584, 100, 0.6629361762933599], "caller": "main.py:200", "time": "2021-02-05T00:07:10.900872"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23715320229530334, 0.2318670153617859, 100, 0.7063298832663879], "caller": "main.py:200", "time": "2021-02-05T00:08:43.387489"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [1], "caller": "main.py:129", "time": "2021-02-05T00:10:17.065749"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T00:10:25.600578"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 59.66784815162303, 0.83769214024847], "caller": "main.py:37", "time": "2021-02-05T00:10:29.595486"}
+{"level": "info", "fmt": "episode: %s", "args": [4.58624329620555], "caller": "main.py:149", "time": "2021-02-05T00:10:39.279531"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.3945019245147705, 0.373191237449646, 100, 0.5527853884740221], "caller": "main.py:200", "time": "2021-02-05T00:10:41.377140"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.3534886837005615, 0.34704163670539856, 100, 0.5814994280948881], "caller": "main.py:200", "time": "2021-02-05T00:12:14.520896"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.3369128704071045, 0.3323262333869934, 100, 0.5859061866412767], "caller": "main.py:200", "time": "2021-02-05T00:13:39.513472"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.3227706849575043, 0.3045099377632141, 100, 0.5943769037129165], "caller": "main.py:200", "time": "2021-02-05T00:15:03.601257"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.3124406337738037, 0.3011189103126526, 100, 0.5773059296030666], "caller": "main.py:200", "time": "2021-02-05T00:16:27.273159"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.3027805685997009, 0.2952074408531189, 100, 0.6022860460871179], "caller": "main.py:200", "time": "2021-02-05T00:17:54.024697"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2969760596752167, 0.28538936376571655, 100, 0.6000909134145507], "caller": "main.py:200", "time": "2021-02-05T00:19:20.475665"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.29040804505348206, 0.29642024636268616, 100, 0.6059165608276987], "caller": "main.py:200", "time": "2021-02-05T00:20:52.391944"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.28338339924812317, 0.2650022804737091, 100, 0.6075766649234686], "caller": "main.py:200", "time": "2021-02-05T00:22:27.113775"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.27689874172210693, 0.26691484451293945, 100, 0.6066067826141288], "caller": "main.py:200", "time": "2021-02-05T00:24:01.055542"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2727634310722351, 0.2568918466567993, 100, 0.6345526849588989], "caller": "main.py:200", "time": "2021-02-05T00:25:32.836366"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.26524674892425537, 0.2669631838798523, 100, 0.6208052482580222], "caller": "main.py:200", "time": "2021-02-05T00:27:02.264560"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.26112255454063416, 0.2519625425338745, 100, 0.6104279815540238], "caller": "main.py:200", "time": "2021-02-05T00:28:30.873643"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2542892396450043, 0.2667645514011383, 100, 0.6363881465156268], "caller": "main.py:200", "time": "2021-02-05T00:30:04.330224"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.25038886070251465, 0.23947253823280334, 100, 0.6618525042468069], "caller": "main.py:200", "time": "2021-02-05T00:31:38.167413"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.24788044393062592, 0.2322554588317871, 100, 0.6680479341077329], "caller": "main.py:200", "time": "2021-02-05T00:33:12.107213"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.24229389429092407, 0.24626405537128448, 100, 0.6425281225992452], "caller": "main.py:200", "time": "2021-02-05T00:34:46.847369"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2405647486448288, 0.2437303066253662, 100, 0.6863897454052228], "caller": "main.py:200", "time": "2021-02-05T00:36:18.691060"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23688623309135437, 0.23183956742286682, 100, 0.6933795588515224], "caller": "main.py:200", "time": "2021-02-05T00:37:54.501227"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2353738248348236, 0.23250848054885864, 100, 0.7301907109484043], "caller": "main.py:200", "time": "2021-02-05T00:39:25.619731"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [2], "caller": "main.py:129", "time": "2021-02-05T00:40:56.175751"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T00:41:06.189117"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 91.5205031791366, 0.6610645717411249], "caller": "main.py:37", "time": "2021-02-05T00:41:10.256551"}
+{"level": "info", "fmt": "episode: %s", "args": [2.9761238293016823], "caller": "main.py:149", "time": "2021-02-05T00:41:21.336035"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.29660630226135254, 0.28257235884666443, 100, 0.6168628709891484], "caller": "main.py:200", "time": "2021-02-05T00:41:23.819192"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2774967551231384, 0.28053098917007446, 100, 0.6173665385821003], "caller": "main.py:200", "time": "2021-02-05T00:42:49.080335"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2705373466014862, 0.2595653831958771, 100, 0.611009386716777], "caller": "main.py:200", "time": "2021-02-05T00:44:13.985371"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.26562726497650146, 0.27067774534225464, 100, 0.6172979661205021], "caller": "main.py:200", "time": "2021-02-05T00:45:38.898698"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.26249194145202637, 0.24972021579742432, 100, 0.602758825957104], "caller": "main.py:200", "time": "2021-02-05T00:47:03.870442"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2581879794597626, 0.24658995866775513, 100, 0.5987258231041137], "caller": "main.py:200", "time": "2021-02-05T00:48:29.016189"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.25295132398605347, 0.2606443762779236, 100, 0.656205359035675], "caller": "main.py:200", "time": "2021-02-05T00:49:53.688046"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2503627836704254, 0.233432799577713, 100, 0.630683917638176], "caller": "main.py:200", "time": "2021-02-05T00:51:18.898094"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.24695633351802826, 0.24885252118110657, 100, 0.6123891904337753], "caller": "main.py:200", "time": "2021-02-05T00:52:44.279425"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.24668598175048828, 0.23813524842262268, 100, 0.6238123571248899], "caller": "main.py:200", "time": "2021-02-05T00:54:09.659974"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2434948980808258, 0.23113064467906952, 100, 0.611043853400638], "caller": "main.py:200", "time": "2021-02-05T00:55:34.804312"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.24086198210716248, 0.23194433748722076, 100, 0.6064614274491386], "caller": "main.py:200", "time": "2021-02-05T00:56:59.101396"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2394019067287445, 0.2394399791955948, 100, 0.6651843150784973], "caller": "main.py:200", "time": "2021-02-05T00:58:24.117686"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.238490492105484, 0.232142835855484, 100, 0.6625386148028639], "caller": "main.py:200", "time": "2021-02-05T00:59:49.394594"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23574140667915344, 0.23185580968856812, 100, 0.6661096444059881], "caller": "main.py:200", "time": "2021-02-05T01:01:13.693672"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23412145674228668, 0.22932568192481995, 100, 0.6444987835835643], "caller": "main.py:200", "time": "2021-02-05T01:02:38.948942"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23120282590389252, 0.23356810212135315, 100, 0.6446073951672631], "caller": "main.py:200", "time": "2021-02-05T01:04:04.316391"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23055337369441986, 0.2198832631111145, 100, 0.6933301948807707], "caller": "main.py:200", "time": "2021-02-05T01:05:29.493441"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22724343836307526, 0.2299301028251648, 100, 0.6369109790880347], "caller": "main.py:200", "time": "2021-02-05T01:06:54.535505"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.22591443359851837, 0.23343002796173096, 100, 0.6394260328368677], "caller": "main.py:200", "time": "2021-02-05T01:08:19.754356"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [3], "caller": "main.py:129", "time": "2021-02-05T01:09:43.278389"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T01:09:53.061561"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 70.06077558431953, 1.85040469992654], "caller": "main.py:37", "time": "2021-02-05T01:09:57.178947"}
+{"level": "info", "fmt": "episode: %s", "args": [0.5143022048251048], "caller": "main.py:149", "time": "2021-02-05T01:10:07.840936"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2747828960418701, 0.2745357155799866, 100, 0.6652154588034126], "caller": "main.py:200", "time": "2021-02-05T01:10:09.956542"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2511478662490845, 0.23671723902225494, 100, 0.6417890947415386], "caller": "main.py:200", "time": "2021-02-05T01:11:35.709197"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.24538478255271912, 0.2603219747543335, 100, 0.6438453929972534], "caller": "main.py:200", "time": "2021-02-05T01:13:00.842102"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.24148619174957275, 0.22726598381996155, 100, 0.673624445132604], "caller": "main.py:200", "time": "2021-02-05T01:14:25.526387"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23640090227127075, 0.24437284469604492, 100, 0.6496424733942796], "caller": "main.py:200", "time": "2021-02-05T01:15:49.598878"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23429523408412933, 0.22085359692573547, 100, 0.6264325597255839], "caller": "main.py:200", "time": "2021-02-05T01:17:13.407074"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23108749091625214, 0.2580575942993164, 100, 0.666703743867961], "caller": "main.py:200", "time": "2021-02-05T01:18:46.813507"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23076403141021729, 0.21917566657066345, 100, 0.6735790544074728], "caller": "main.py:200", "time": "2021-02-05T01:20:21.124607"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.22711500525474548, 0.215532124042511, 100, 0.6678129481601475], "caller": "main.py:200", "time": "2021-02-05T01:21:52.095521"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.22666265070438385, 0.23708346486091614, 100, 0.6872190933732413], "caller": "main.py:200", "time": "2021-02-05T01:23:29.733938"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.22563436627388, 0.23131012916564941, 100, 0.658767238491462], "caller": "main.py:200", "time": "2021-02-05T01:25:01.238362"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2236941158771515, 0.22568771243095398, 100, 0.6688592996524477], "caller": "main.py:200", "time": "2021-02-05T01:26:29.569472"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22216808795928955, 0.23453032970428467, 100, 0.6570360159216927], "caller": "main.py:200", "time": "2021-02-05T01:28:03.743521"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.22058114409446716, 0.21525418758392334, 100, 0.6748221700708769], "caller": "main.py:200", "time": "2021-02-05T01:29:34.280307"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.21980172395706177, 0.21692484617233276, 100, 0.664115002684939], "caller": "main.py:200", "time": "2021-02-05T01:31:08.823627"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.21680806577205658, 0.20183688402175903, 100, 0.6549180161298055], "caller": "main.py:200", "time": "2021-02-05T01:32:42.476202"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.21572047472000122, 0.21808859705924988, 100, 0.6442843752455157], "caller": "main.py:200", "time": "2021-02-05T01:34:15.358183"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.21507346630096436, 0.19790717959403992, 100, 0.6774784546413052], "caller": "main.py:200", "time": "2021-02-05T01:35:47.950847"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.21290424466133118, 0.2217826396226883, 100, 0.6467099218913913], "caller": "main.py:200", "time": "2021-02-05T01:37:22.519723"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2122853845357895, 0.20603618025779724, 100, 0.6938286279938191], "caller": "main.py:200", "time": "2021-02-05T01:38:53.389886"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [4], "caller": "main.py:129", "time": "2021-02-05T01:40:20.591166"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T01:40:29.541609"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 74.3831195604121, 1.2968952739857256], "caller": "main.py:37", "time": "2021-02-05T01:40:34.239215"}
+{"level": "info", "fmt": "episode: %s", "args": [1.5106359654965502], "caller": "main.py:149", "time": "2021-02-05T01:40:44.203271"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.24452897906303406, 0.22790037095546722, 100, 0.6531581645550837], "caller": "main.py:200", "time": "2021-02-05T01:40:46.611997"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23275183141231537, 0.22522014379501343, 100, 0.6588502495822092], "caller": "main.py:200", "time": "2021-02-05T01:42:18.203071"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.22735166549682617, 0.20231232047080994, 100, 0.6078525286305065], "caller": "main.py:200", "time": "2021-02-05T01:43:52.706844"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.22620002925395966, 0.23698997497558594, 100, 0.6614494227701588], "caller": "main.py:200", "time": "2021-02-05T01:45:23.575288"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.22289533913135529, 0.20769917964935303, 100, 0.6291970365610351], "caller": "main.py:200", "time": "2021-02-05T01:46:56.928356"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.22102980315685272, 0.22547096014022827, 100, 0.6787353133476538], "caller": "main.py:200", "time": "2021-02-05T01:48:25.759678"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.22027505934238434, 0.20502138137817383, 100, 0.6730634706539385], "caller": "main.py:200", "time": "2021-02-05T01:50:00.241769"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.21840867400169373, 0.23849403858184814, 100, 0.6556466337382798], "caller": "main.py:200", "time": "2021-02-05T01:51:34.137242"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.21678830683231354, 0.21304991841316223, 100, 0.6389191299037976], "caller": "main.py:200", "time": "2021-02-05T01:53:05.532676"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.21280792355537415, 0.20850788056850433, 100, 0.6105457460269734], "caller": "main.py:200", "time": "2021-02-05T01:54:33.985630"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.21383340656757355, 0.20415955781936646, 100, 0.6376072346526547], "caller": "main.py:200", "time": "2021-02-05T01:56:09.717929"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.016568370163440704, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T01:57:14.768389"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.031101875007152557, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T01:57:22.180942"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2118714153766632, 0.2046220451593399, 100, 0.6422416447950898], "caller": "main.py:200", "time": "2021-02-05T01:57:45.974431"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.21353723108768463, 0.21650967001914978, 100, 0.6541571676076953], "caller": "main.py:200", "time": "2021-02-05T01:59:21.276044"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.21348094940185547, 0.21538934111595154, 100, 0.6779641913689712], "caller": "main.py:200", "time": "2021-02-05T02:00:54.474926"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.20902661979198456, 0.20525774359703064, 100, 0.6424638163400963], "caller": "main.py:200", "time": "2021-02-05T02:02:22.376146"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.21081970632076263, 0.2219056487083435, 100, 0.6492240285224189], "caller": "main.py:200", "time": "2021-02-05T02:03:50.843860"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.20884348452091217, 0.20854957401752472, 100, 0.6561691527902562], "caller": "main.py:200", "time": "2021-02-05T02:05:17.547419"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.20721761882305145, 0.205118790268898, 100, 0.6254682379335545], "caller": "main.py:200", "time": "2021-02-05T02:06:42.818820"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2074284553527832, 0.2235821783542633, 100, 0.6274740105239912], "caller": "main.py:200", "time": "2021-02-05T02:08:08.420107"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.204544797539711, 0.19173982739448547, 100, 0.6233402525755292], "caller": "main.py:200", "time": "2021-02-05T02:09:35.491716"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [5], "caller": "main.py:129", "time": "2021-02-05T02:10:59.148084"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T02:11:08.667084"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 78.58035306459414, 2.7472073325484376], "caller": "main.py:37", "time": "2021-02-05T02:11:12.597042"}
+{"level": "info", "fmt": "episode: %s", "args": [1.8108754704577583], "caller": "main.py:149", "time": "2021-02-05T02:11:22.681808"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23532453179359436, 0.22888623178005219, 100, 0.6133317624909282], "caller": "main.py:200", "time": "2021-02-05T02:11:24.787806"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2261408418416977, 0.19665667414665222, 100, 0.6005011411900835], "caller": "main.py:200", "time": "2021-02-05T02:12:52.816698"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2262032926082611, 0.24296844005584717, 100, 0.629705173608275], "caller": "main.py:200", "time": "2021-02-05T02:14:22.979195"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.22441639006137848, 0.23051390051841736, 100, 0.6432488655400758], "caller": "main.py:200", "time": "2021-02-05T02:15:47.882929"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.22182832658290863, 0.19990378618240356, 100, 0.6404158847883679], "caller": "main.py:200", "time": "2021-02-05T02:17:11.086844"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.22161762416362762, 0.2105729728937149, 100, 0.6545723884643203], "caller": "main.py:200", "time": "2021-02-05T02:18:35.726711"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.22026485204696655, 0.20275375247001648, 100, 0.6251223468155396], "caller": "main.py:200", "time": "2021-02-05T02:20:05.907735"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.21607854962348938, 0.2387051284313202, 100, 0.6222773396346565], "caller": "main.py:200", "time": "2021-02-05T02:21:35.072204"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.21614551544189453, 0.19547319412231445, 100, 0.634127885039813], "caller": "main.py:200", "time": "2021-02-05T02:22:57.659589"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.21305029094219208, 0.2125287652015686, 100, 0.60738520377761], "caller": "main.py:200", "time": "2021-02-05T02:24:21.196269"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.21438783407211304, 0.2129126787185669, 100, 0.6427594348145502], "caller": "main.py:200", "time": "2021-02-05T02:25:47.032706"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.21340304613113403, 0.2066318541765213, 100, 0.6385982453184492], "caller": "main.py:200", "time": "2021-02-05T02:27:19.184030"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.21513576805591583, 0.2066315859556198, 100, 0.6476236873217505], "caller": "main.py:200", "time": "2021-02-05T02:28:43.127411"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2107728123664856, 0.2120828479528427, 100, 0.6173658829310079], "caller": "main.py:200", "time": "2021-02-05T02:30:14.732293"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.21184591948986053, 0.22140857577323914, 100, 0.6097446598992955], "caller": "main.py:200", "time": "2021-02-05T02:31:39.470221"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.21049365401268005, 0.22231268882751465, 100, 0.6255062692731962], "caller": "main.py:200", "time": "2021-02-05T02:33:05.787492"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.21125474572181702, 0.21070195734500885, 100, 0.6173838012793185], "caller": "main.py:200", "time": "2021-02-05T02:34:36.516776"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2094743847846985, 0.21071958541870117, 100, 0.6146214630585216], "caller": "main.py:200", "time": "2021-02-05T02:36:03.611180"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.20947906374931335, 0.2017943561077118, 100, 0.6358674600084538], "caller": "main.py:200", "time": "2021-02-05T02:37:33.866718"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.20769090950489044, 0.19207385182380676, 100, 0.6322672589981099], "caller": "main.py:200", "time": "2021-02-05T02:39:01.869280"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [6], "caller": "main.py:129", "time": "2021-02-05T02:40:25.447117"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T02:40:34.476947"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 44.80890941133417, 1.0034695132065572], "caller": "main.py:37", "time": "2021-02-05T02:40:38.247756"}
+{"level": "info", "fmt": "episode: %s", "args": [0.9827420643724811], "caller": "main.py:149", "time": "2021-02-05T02:40:48.062189"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23172152042388916, 0.23798900842666626, 100, 0.6700896161124573], "caller": "main.py:200", "time": "2021-02-05T02:40:50.098575"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2216513305902481, 0.21580325067043304, 100, 0.6258292054504176], "caller": "main.py:200", "time": "2021-02-05T02:42:16.797056"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.21879777312278748, 0.22503557801246643, 100, 0.6355696206649358], "caller": "main.py:200", "time": "2021-02-05T02:43:44.819111"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.21654964983463287, 0.2132810801267624, 100, 0.6199169074869127], "caller": "main.py:200", "time": "2021-02-05T02:45:12.901988"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.21393845975399017, 0.21631349623203278, 100, 0.6473105924674268], "caller": "main.py:200", "time": "2021-02-05T02:46:39.002395"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.21327830851078033, 0.21257179975509644, 100, 0.6552593957722579], "caller": "main.py:200", "time": "2021-02-05T02:48:10.179951"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2126508206129074, 0.22732123732566833, 100, 0.6245135923595572], "caller": "main.py:200", "time": "2021-02-05T02:49:35.123359"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.21143734455108643, 0.2082347869873047, 100, 0.6160172923783359], "caller": "main.py:200", "time": "2021-02-05T02:51:00.694099"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.21138015389442444, 0.21998243033885956, 100, 0.6328821411095977], "caller": "main.py:200", "time": "2021-02-05T02:52:24.170836"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.21078073978424072, 0.22140684723854065, 100, 0.6360590496023221], "caller": "main.py:200", "time": "2021-02-05T02:53:47.895289"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.21211567521095276, 0.20849433541297913, 100, 0.6519402878824607], "caller": "main.py:200", "time": "2021-02-05T02:55:13.889409"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2110772579908371, 0.21227262914180756, 100, 0.6569983863173569], "caller": "main.py:200", "time": "2021-02-05T02:56:42.590837"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.21018581092357635, 0.23213842511177063, 100, 0.6723794423861464], "caller": "main.py:200", "time": "2021-02-05T02:58:12.434801"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.20636260509490967, 0.20916178822517395, 100, 0.6298493018116527], "caller": "main.py:200", "time": "2021-02-05T02:59:38.543322"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.20888793468475342, 0.19949431717395782, 100, 0.676013061336627], "caller": "main.py:200", "time": "2021-02-05T03:01:01.979907"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.20807725191116333, 0.20462007820606232, 100, 0.6260083913177092], "caller": "main.py:200", "time": "2021-02-05T03:02:26.793670"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.20840038359165192, 0.20557433366775513, 100, 0.6545641385854378], "caller": "main.py:200", "time": "2021-02-05T03:03:50.989874"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.20627540349960327, 0.2101355344057083, 100, 0.6468334003636118], "caller": "main.py:200", "time": "2021-02-05T03:05:17.058119"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.20570825040340424, 0.21881532669067383, 100, 0.6498606365269253], "caller": "main.py:200", "time": "2021-02-05T03:06:44.203846"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.20474494993686676, 0.20411458611488342, 100, 0.6290686010685324], "caller": "main.py:200", "time": "2021-02-05T03:08:08.342593"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [7], "caller": "main.py:129", "time": "2021-02-05T03:09:31.452279"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T03:09:40.019804"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 57.2420833683359, 9.194216763958913], "caller": "main.py:37", "time": "2021-02-05T03:09:44.270130"}
+{"level": "info", "fmt": "episode: %s", "args": [4.444304286899742], "caller": "main.py:149", "time": "2021-02-05T03:09:53.998202"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23266179859638214, 0.22155684232711792, 100, 0.6129122658951106], "caller": "main.py:200", "time": "2021-02-05T03:09:57.318173"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2271033078432083, 0.22807833552360535, 100, 0.6001989679928442], "caller": "main.py:200", "time": "2021-02-05T03:11:27.611575"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.222107395529747, 0.22563067078590393, 100, 0.6401880126551356], "caller": "main.py:200", "time": "2021-02-05T03:12:59.364707"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.22026389837265015, 0.22350040078163147, 100, 0.5926860123280071], "caller": "main.py:200", "time": "2021-02-05T03:14:27.106018"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2234170287847519, 0.2020404189825058, 100, 0.6415386106922434], "caller": "main.py:200", "time": "2021-02-05T03:15:55.028302"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.22110378742218018, 0.19976437091827393, 100, 0.6226306542131543], "caller": "main.py:200", "time": "2021-02-05T03:17:25.703109"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.21994714438915253, 0.22463132441043854, 100, 0.6399026548222555], "caller": "main.py:200", "time": "2021-02-05T03:18:53.318661"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2180846631526947, 0.2222832441329956, 100, 0.6224336900684673], "caller": "main.py:200", "time": "2021-02-05T03:20:18.654435"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.21745143830776215, 0.2288038432598114, 100, 0.6378750207382335], "caller": "main.py:200", "time": "2021-02-05T03:21:43.580093"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.21699416637420654, 0.20594245195388794, 100, 0.5988107194897749], "caller": "main.py:200", "time": "2021-02-05T03:23:12.013269"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.21410159766674042, 0.2269090861082077, 100, 0.6124237143380954], "caller": "main.py:200", "time": "2021-02-05T03:24:38.139954"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.21511636674404144, 0.2200179547071457, 100, 0.595468543410359], "caller": "main.py:200", "time": "2021-02-05T03:26:15.579520"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.21586807072162628, 0.22308503091335297, 100, 0.6635514991811958], "caller": "main.py:200", "time": "2021-02-05T03:27:43.284919"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.21366016566753387, 0.20381073653697968, 100, 0.6497512250488936], "caller": "main.py:200", "time": "2021-02-05T03:29:15.055608"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2158551663160324, 0.222601518034935, 100, 0.6403435217693855], "caller": "main.py:200", "time": "2021-02-05T03:30:45.313857"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2111324518918991, 0.20436625182628632, 100, 0.608204083977914], "caller": "main.py:200", "time": "2021-02-05T03:32:11.540830"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.2109217792749405, 0.20497769117355347, 100, 0.6000960492487672], "caller": "main.py:200", "time": "2021-02-05T03:33:38.555994"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2098643183708191, 0.2033524513244629, 100, 0.5830629836929907], "caller": "main.py:200", "time": "2021-02-05T03:35:06.568392"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.20973646640777588, 0.20178920030593872, 100, 0.617312781451006], "caller": "main.py:200", "time": "2021-02-05T03:36:37.246300"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.21052953600883484, 0.21332508325576782, 100, 0.6191328262663643], "caller": "main.py:200", "time": "2021-02-05T03:38:06.383442"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [8], "caller": "main.py:129", "time": "2021-02-05T03:39:30.152653"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T03:39:38.841056"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 35.631393631229926, 5.242544275777402], "caller": "main.py:37", "time": "2021-02-05T03:39:42.597392"}
+{"level": "info", "fmt": "episode: %s", "args": [6.401383017101238], "caller": "main.py:149", "time": "2021-02-05T03:39:52.492402"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2306661158800125, 0.24034081399440765, 100, 0.6094886573539644], "caller": "main.py:200", "time": "2021-02-05T03:39:55.094969"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.22475385665893555, 0.22509218752384186, 100, 0.6089268785106177], "caller": "main.py:200", "time": "2021-02-05T03:41:17.614213"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2267882525920868, 0.21658888459205627, 100, 0.5969994615911485], "caller": "main.py:200", "time": "2021-02-05T03:42:49.529627"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.22180595993995667, 0.2296796590089798, 100, 0.6339153587184231], "caller": "main.py:200", "time": "2021-02-05T03:44:16.853551"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2208571583032608, 0.21454650163650513, 100, 0.6002889352436592], "caller": "main.py:200", "time": "2021-02-05T03:45:43.598268"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.21912452578544617, 0.22907480597496033, 100, 0.5629650189909639], "caller": "main.py:200", "time": "2021-02-05T03:47:09.449884"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.21760110557079315, 0.20689067244529724, 100, 0.5798728867785415], "caller": "main.py:200", "time": "2021-02-05T03:48:33.659395"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.223060742020607, 0.21533970534801483, 100, 0.6064379035820107], "caller": "main.py:200", "time": "2021-02-05T03:50:04.947342"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.21714338660240173, 0.22363495826721191, 100, 0.6055146362772057], "caller": "main.py:200", "time": "2021-02-05T03:51:31.507698"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.21567915380001068, 0.22293102741241455, 100, 0.5989553263184499], "caller": "main.py:200", "time": "2021-02-05T03:52:54.719972"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.21364839375019073, 0.21759095788002014, 100, 0.5646206256182625], "caller": "main.py:200", "time": "2021-02-05T03:54:25.095209"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.21853946149349213, 0.23351994156837463, 100, 0.5744627823851235], "caller": "main.py:200", "time": "2021-02-05T03:55:52.514804"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2172238677740097, 0.22018510103225708, 100, 0.6257695134848436], "caller": "main.py:200", "time": "2021-02-05T03:57:17.147839"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.21503286063671112, 0.2144560068845749, 100, 0.574766527058822], "caller": "main.py:200", "time": "2021-02-05T03:58:39.970497"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.21534809470176697, 0.2255261242389679, 100, 0.5889720171101016], "caller": "main.py:200", "time": "2021-02-05T04:00:07.053110"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.21431298553943634, 0.22951273620128632, 100, 0.5911971052889905], "caller": "main.py:200", "time": "2021-02-05T04:01:33.592342"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.21593895554542542, 0.2148647904396057, 100, 0.5886248495586188], "caller": "main.py:200", "time": "2021-02-05T04:02:59.774623"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.21081462502479553, 0.24401280283927917, 100, 0.5687735306647871], "caller": "main.py:200", "time": "2021-02-05T04:04:26.424634"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2132093459367752, 0.21557235717773438, 100, 0.6076849313962742], "caller": "main.py:200", "time": "2021-02-05T04:05:56.913085"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.21069760620594025, 0.23244713246822357, 100, 0.575259611549072], "caller": "main.py:200", "time": "2021-02-05T04:07:31.791885"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [9], "caller": "main.py:129", "time": "2021-02-05T04:08:54.019926"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T04:09:03.096540"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 17.47303147046297, 0.4564281483096913], "caller": "main.py:37", "time": "2021-02-05T04:09:06.998567"}
+{"level": "info", "fmt": "episode: %s", "args": [6.519704613345835], "caller": "main.py:149", "time": "2021-02-05T04:09:17.060392"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.22537937760353088, 0.23145011067390442, 100, 0.6152711480240671], "caller": "main.py:200", "time": "2021-02-05T04:09:20.330409"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.22077429294586182, 0.21336954832077026, 100, 0.5489223944591917], "caller": "main.py:200", "time": "2021-02-05T04:10:47.238027"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.21789737045764923, 0.21360459923744202, 100, 0.5727830394529718], "caller": "main.py:200", "time": "2021-02-05T04:12:15.925350"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.21800994873046875, 0.21642163395881653, 100, 0.5610127996837364], "caller": "main.py:200", "time": "2021-02-05T04:13:44.090312"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.21569238603115082, 0.20989161729812622, 100, 0.5712127598548927], "caller": "main.py:200", "time": "2021-02-05T04:15:10.362500"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.21647979319095612, 0.20299780368804932, 100, 0.5322464221183681], "caller": "main.py:200", "time": "2021-02-05T04:16:31.553369"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.21699190139770508, 0.1886553168296814, 100, 0.554123981539627], "caller": "main.py:200", "time": "2021-02-05T04:17:54.959367"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.21471814811229706, 0.22292807698249817, 100, 0.5482177861976769], "caller": "main.py:200", "time": "2021-02-05T04:19:19.391586"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.21548818051815033, 0.22135424613952637, 100, 0.5999498667713077], "caller": "main.py:200", "time": "2021-02-05T04:20:48.203075"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2161298394203186, 0.20495417714118958, 100, 0.5747926523706709], "caller": "main.py:200", "time": "2021-02-05T04:22:20.492043"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.21524710953235626, 0.2274671494960785, 100, 0.5679166468410012], "caller": "main.py:200", "time": "2021-02-05T04:23:50.672593"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.21312712132930756, 0.21412143111228943, 100, 0.5641925367148782], "caller": "main.py:200", "time": "2021-02-05T04:25:18.300422"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.21399159729480743, 0.20735648274421692, 100, 0.576730337979341], "caller": "main.py:200", "time": "2021-02-05T04:26:45.828861"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.21257078647613525, 0.2178010493516922, 100, 0.5329818468752984], "caller": "main.py:200", "time": "2021-02-05T04:28:09.738826"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.21003806591033936, 0.20098596811294556, 100, 0.5434844046287647], "caller": "main.py:200", "time": "2021-02-05T04:29:36.982112"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.21278604865074158, 0.22120851278305054, 100, 0.5603062700664975], "caller": "main.py:200", "time": "2021-02-05T04:31:00.542820"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.21672077476978302, 0.21322482824325562, 100, 0.6124670913203032], "caller": "main.py:200", "time": "2021-02-05T04:32:27.953548"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.21142913401126862, 0.21537727117538452, 100, 0.5576116546430876], "caller": "main.py:200", "time": "2021-02-05T04:33:54.041470"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.21232225000858307, 0.21169130504131317, 100, 0.5592464067972744], "caller": "main.py:200", "time": "2021-02-05T04:35:20.988530"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.20882144570350647, 0.2212393879890442, 100, 0.5474956997681532], "caller": "main.py:200", "time": "2021-02-05T04:36:46.665150"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [10], "caller": "main.py:129", "time": "2021-02-05T04:38:09.020195"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T04:38:18.326296"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 23.674320199206775, 0.30751776985731116], "caller": "main.py:37", "time": "2021-02-05T04:38:22.186254"}
+{"level": "info", "fmt": "episode: %s", "args": [5.120016965577436], "caller": "main.py:149", "time": "2021-02-05T04:38:32.341071"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.22891514003276825, 0.21469643712043762, 100, 0.5450170477679738], "caller": "main.py:200", "time": "2021-02-05T04:38:34.505450"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.21947568655014038, 0.21189601719379425, 100, 0.5466309374024216], "caller": "main.py:200", "time": "2021-02-05T04:40:02.507365"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.21832691133022308, 0.22379285097122192, 100, 0.5585087334551202], "caller": "main.py:200", "time": "2021-02-05T04:41:27.767626"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2142958641052246, 0.22174349427223206, 100, 0.546953535323284], "caller": "main.py:200", "time": "2021-02-05T04:42:57.737565"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2152673900127411, 0.22087863087654114, 100, 0.5347350382270136], "caller": "main.py:200", "time": "2021-02-05T04:44:22.510069"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.21307078003883362, 0.22196656465530396, 100, 0.5401550042089087], "caller": "main.py:200", "time": "2021-02-05T04:45:46.952538"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.214826762676239, 0.21269313991069794, 100, 0.5360855143726229], "caller": "main.py:200", "time": "2021-02-05T04:47:07.692626"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.21334975957870483, 0.19789139926433563, 100, 0.5579503651895472], "caller": "main.py:200", "time": "2021-02-05T04:48:27.202585"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.21264255046844482, 0.21944624185562134, 100, 0.5511027538225295], "caller": "main.py:200", "time": "2021-02-05T04:49:49.766349"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.21094198524951935, 0.20391646027565002, 100, 0.5415101205761133], "caller": "main.py:200", "time": "2021-02-05T04:51:17.618269"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2108086347579956, 0.21364693343639374, 100, 0.5322459795538806], "caller": "main.py:200", "time": "2021-02-05T04:52:40.465750"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2102125883102417, 0.20619335770606995, 100, 0.5501493593500258], "caller": "main.py:200", "time": "2021-02-05T04:54:07.195840"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.21085676550865173, 0.20534546673297882, 100, 0.5540663629216213], "caller": "main.py:200", "time": "2021-02-05T04:55:33.878082"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.21051177382469177, 0.2061205506324768, 100, 0.5496868717120754], "caller": "main.py:200", "time": "2021-02-05T04:56:56.717689"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2089519500732422, 0.1982080042362213, 100, 0.5384019103107877], "caller": "main.py:200", "time": "2021-02-05T04:58:23.037850"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.20851051807403564, 0.21601814031600952, 100, 0.555197861437114], "caller": "main.py:200", "time": "2021-02-05T04:59:45.571920"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.2095130980014801, 0.18566244840621948, 100, 0.576974713446617], "caller": "main.py:200", "time": "2021-02-05T05:01:12.564438"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2074679434299469, 0.21707002818584442, 100, 0.5322708647910952], "caller": "main.py:200", "time": "2021-02-05T05:02:37.062272"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.20719406008720398, 0.205347940325737, 100, 0.5565472196976107], "caller": "main.py:200", "time": "2021-02-05T05:04:06.473614"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.20493918657302856, 0.20823009312152863, 100, 0.5302193805045152], "caller": "main.py:200", "time": "2021-02-05T05:05:34.005727"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [11], "caller": "main.py:129", "time": "2021-02-05T05:06:56.331462"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T05:07:05.574423"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 21.206188250055273, 0.23069245840158945], "caller": "main.py:37", "time": "2021-02-05T05:07:09.702401"}
+{"level": "info", "fmt": "episode: %s", "args": [5.194853686849164], "caller": "main.py:149", "time": "2021-02-05T05:07:19.872877"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2194519191980362, 0.2213193029165268, 100, 0.5306672828858905], "caller": "main.py:200", "time": "2021-02-05T05:07:22.558009"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.21107566356658936, 0.22039897739887238, 100, 0.5317665680711177], "caller": "main.py:200", "time": "2021-02-05T05:08:48.490839"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.21154659986495972, 0.2204236537218094, 100, 0.5195350425915936], "caller": "main.py:200", "time": "2021-02-05T05:10:17.548092"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.21208694577217102, 0.2196684628725052, 100, 0.5282643535204959], "caller": "main.py:200", "time": "2021-02-05T05:11:44.014658"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2111658900976181, 0.22315222024917603, 100, 0.5440721287779331], "caller": "main.py:200", "time": "2021-02-05T05:13:09.020129"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.21167300641536713, 0.20164406299591064, 100, 0.5385950260815852], "caller": "main.py:200", "time": "2021-02-05T05:14:36.139934"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.20964914560317993, 0.18993225693702698, 100, 0.5359848531544665], "caller": "main.py:200", "time": "2021-02-05T05:16:03.005098"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.20990799367427826, 0.19532500207424164, 100, 0.5373429843246036], "caller": "main.py:200", "time": "2021-02-05T05:17:25.349739"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.208664670586586, 0.22268924117088318, 100, 0.534354223257512], "caller": "main.py:200", "time": "2021-02-05T05:18:46.781571"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.20856937766075134, 0.2101033329963684, 100, 0.5471777602483028], "caller": "main.py:200", "time": "2021-02-05T05:20:09.158407"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2093687802553177, 0.2035926729440689, 100, 0.5379767438231364], "caller": "main.py:200", "time": "2021-02-05T05:21:33.194320"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.20713818073272705, 0.21284280717372894, 100, 0.5292600741453201], "caller": "main.py:200", "time": "2021-02-05T05:22:58.386568"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.20676393806934357, 0.2006402313709259, 100, 0.5323403402630617], "caller": "main.py:200", "time": "2021-02-05T05:24:28.839615"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.20645765960216522, 0.21134746074676514, 100, 0.5211277263835485], "caller": "main.py:200", "time": "2021-02-05T05:25:54.643839"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.01694132201373577, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T05:26:34.351497"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.20897319912910461, 0.20956504344940186, 100, 0.5363762467563218], "caller": "main.py:200", "time": "2021-02-05T05:27:26.087779"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2076064497232437, 0.1952401101589203, 100, 0.5039768841358702], "caller": "main.py:200", "time": "2021-02-05T05:28:51.581577"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.20503230392932892, 0.2211712896823883, 100, 0.5153133344134956], "caller": "main.py:200", "time": "2021-02-05T05:30:15.155155"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2053726762533188, 0.22190217673778534, 100, 0.5138558318697151], "caller": "main.py:200", "time": "2021-02-05T05:31:36.973705"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.01552261970937252, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T05:32:41.812857"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2059183567762375, 0.20675726234912872, 100, 0.5422073521667914], "caller": "main.py:200", "time": "2021-02-05T05:33:07.152469"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.20424340665340424, 0.19859987497329712, 100, 0.5304877116745704], "caller": "main.py:200", "time": "2021-02-05T05:34:34.535611"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [12], "caller": "main.py:129", "time": "2021-02-05T05:36:03.607169"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T05:36:12.810031"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 24.2131587399593, 0.3482923925999307], "caller": "main.py:37", "time": "2021-02-05T05:36:17.844332"}
+{"level": "info", "fmt": "episode: %s", "args": [0.9259464492921881], "caller": "main.py:149", "time": "2021-02-05T05:36:28.032701"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.22548311948776245, 0.20274977385997772, 100, 0.5555667420666167], "caller": "main.py:200", "time": "2021-02-05T05:36:30.671316"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.21301016211509705, 0.21441593766212463, 100, 0.5177693950612144], "caller": "main.py:200", "time": "2021-02-05T05:38:01.339759"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.20852619409561157, 0.21716037392616272, 100, 0.5208325764016025], "caller": "main.py:200", "time": "2021-02-05T05:39:31.264567"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2087157666683197, 0.21449032425880432, 100, 0.5251189568113683], "caller": "main.py:200", "time": "2021-02-05T05:40:54.655868"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.20669808983802795, 0.22340695559978485, 100, 0.5435568841748275], "caller": "main.py:200", "time": "2021-02-05T05:42:19.535212"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2067551463842392, 0.18719273805618286, 100, 0.5276250481077905], "caller": "main.py:200", "time": "2021-02-05T05:43:50.402515"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.20584571361541748, 0.2289627492427826, 100, 0.5126681819045475], "caller": "main.py:200", "time": "2021-02-05T05:45:12.813421"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.20513302087783813, 0.21575820446014404, 100, 0.5125126337492554], "caller": "main.py:200", "time": "2021-02-05T05:46:40.683920"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.20581503212451935, 0.180585578083992, 100, 0.5378762533722874], "caller": "main.py:200", "time": "2021-02-05T05:48:09.515244"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2059234082698822, 0.20440220832824707, 100, 0.5164538731654605], "caller": "main.py:200", "time": "2021-02-05T05:49:38.540662"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2024022340774536, 0.19835607707500458, 100, 0.5113361149514986], "caller": "main.py:200", "time": "2021-02-05T05:51:08.585505"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.20426736772060394, 0.2073449194431305, 100, 0.5327866333190282], "caller": "main.py:200", "time": "2021-02-05T05:52:35.001093"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.20337039232254028, 0.224775031208992, 100, 0.515551556893292], "caller": "main.py:200", "time": "2021-02-05T05:53:59.033197"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2028319239616394, 0.19459280371665955, 100, 0.5251748537492098], "caller": "main.py:200", "time": "2021-02-05T05:55:23.685073"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2011287659406662, 0.19901785254478455, 100, 0.5338448011341308], "caller": "main.py:200", "time": "2021-02-05T05:56:45.318205"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.20248766243457794, 0.2101442813873291, 100, 0.5297441002196811], "caller": "main.py:200", "time": "2021-02-05T05:58:13.982089"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.20176075398921967, 0.21889251470565796, 100, 0.5389008545336649], "caller": "main.py:200", "time": "2021-02-05T05:59:47.160438"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.20184913277626038, 0.2012616991996765, 100, 0.5218212022855597], "caller": "main.py:200", "time": "2021-02-05T06:01:09.445489"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.20182262361049652, 0.20174060761928558, 100, 0.5411788829619916], "caller": "main.py:200", "time": "2021-02-05T06:02:31.751971"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.20320522785186768, 0.20960178971290588, 100, 0.5158774491628707], "caller": "main.py:200", "time": "2021-02-05T06:03:54.107383"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [13], "caller": "main.py:129", "time": "2021-02-05T06:05:15.058640"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T06:05:24.293279"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 29.681958546134645, 0.2908149944590063], "caller": "main.py:37", "time": "2021-02-05T06:05:29.122243"}
+{"level": "info", "fmt": "episode: %s", "args": [6.7524759357775], "caller": "main.py:149", "time": "2021-02-05T06:05:39.238023"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2095405012369156, 0.20911362767219543, 100, 0.5205743014291896], "caller": "main.py:200", "time": "2021-02-05T06:05:42.527618"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2054300159215927, 0.2185230553150177, 100, 0.4995149957634183], "caller": "main.py:200", "time": "2021-02-05T06:07:09.412235"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.20568297803401947, 0.19669906795024872, 100, 0.5157714727005172], "caller": "main.py:200", "time": "2021-02-05T06:08:30.696542"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.20440734922885895, 0.1911519467830658, 100, 0.5113657035912318], "caller": "main.py:200", "time": "2021-02-05T06:09:58.214541"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.20590157806873322, 0.21855798363685608, 100, 0.508026925274591], "caller": "main.py:200", "time": "2021-02-05T06:11:27.559891"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2042624056339264, 0.20272324979305267, 100, 0.5191820045786475], "caller": "main.py:200", "time": "2021-02-05T06:13:00.612545"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2046193927526474, 0.20320093631744385, 100, 0.5197667735295282], "caller": "main.py:200", "time": "2021-02-05T06:14:26.178986"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.20240165293216705, 0.18926015496253967, 100, 0.5154958107552017], "caller": "main.py:200", "time": "2021-02-05T06:15:53.972065"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.20475049316883087, 0.19949615001678467, 100, 0.5218790378644724], "caller": "main.py:200", "time": "2021-02-05T06:17:18.577294"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.20240803062915802, 0.21669645607471466, 100, 0.509047731408236], "caller": "main.py:200", "time": "2021-02-05T06:18:45.238694"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.20410960912704468, 0.20068243145942688, 100, 0.5292561101384197], "caller": "main.py:200", "time": "2021-02-05T06:20:13.334453"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.20450054109096527, 0.18708054721355438, 100, 0.5092334523287803], "caller": "main.py:200", "time": "2021-02-05T06:21:38.312495"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.19953159987926483, 0.22308257222175598, 100, 0.49649444187682995], "caller": "main.py:200", "time": "2021-02-05T06:23:04.803413"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.20315606892108917, 0.19313105940818787, 100, 0.5099553784218095], "caller": "main.py:200", "time": "2021-02-05T06:24:29.917256"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.20002508163452148, 0.20259635150432587, 100, 0.5092688551035874], "caller": "main.py:200", "time": "2021-02-05T06:25:58.314496"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.202583447098732, 0.19959402084350586, 100, 0.5010567074513058], "caller": "main.py:200", "time": "2021-02-05T06:27:27.308928"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.20024456083774567, 0.1899884045124054, 100, 0.49077532018283704], "caller": "main.py:200", "time": "2021-02-05T06:28:56.623817"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.20124755799770355, 0.21670174598693848, 100, 0.5125475641572528], "caller": "main.py:200", "time": "2021-02-05T06:30:21.281116"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.1994333267211914, 0.19930043816566467, 100, 0.5023679658268193], "caller": "main.py:200", "time": "2021-02-05T06:31:46.401054"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.19940166175365448, 0.20769019424915314, 100, 0.5026161509249658], "caller": "main.py:200", "time": "2021-02-05T06:33:11.635331"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [14], "caller": "main.py:129", "time": "2021-02-05T06:34:34.668115"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T06:34:44.484212"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 23.462327103836426, 0.2907107316413957], "caller": "main.py:37", "time": "2021-02-05T06:34:48.562069"}
+{"level": "info", "fmt": "episode: %s", "args": [6.01904857426327], "caller": "main.py:149", "time": "2021-02-05T06:34:59.339351"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.20574608445167542, 0.20882746577262878, 100, 0.5043126141520682], "caller": "main.py:200", "time": "2021-02-05T06:35:01.795584"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.20720264315605164, 0.18711067736148834, 100, 0.5000691747165337], "caller": "main.py:200", "time": "2021-02-05T06:36:26.943874"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.20264728367328644, 0.21375177800655365, 100, 0.494338554392495], "caller": "main.py:200", "time": "2021-02-05T06:37:51.704760"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.20365816354751587, 0.19563639163970947, 100, 0.5070149123161656], "caller": "main.py:200", "time": "2021-02-05T06:39:16.139897"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.20210246741771698, 0.20433515310287476, 100, 0.49511902357157045], "caller": "main.py:200", "time": "2021-02-05T06:40:38.956495"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.20427606999874115, 0.19764499366283417, 100, 0.5045919200273415], "caller": "main.py:200", "time": "2021-02-05T06:42:02.891182"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.20247086882591248, 0.19766046106815338, 100, 0.4790767329452183], "caller": "main.py:200", "time": "2021-02-05T06:43:28.077930"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.20218515396118164, 0.21335530281066895, 100, 0.49660016948979163], "caller": "main.py:200", "time": "2021-02-05T06:44:47.697859"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.20289263129234314, 0.21053747832775116, 100, 0.4889393549668608], "caller": "main.py:200", "time": "2021-02-05T06:46:08.758697"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.20314747095108032, 0.18784214556217194, 100, 0.4923201342924848], "caller": "main.py:200", "time": "2021-02-05T06:47:34.245841"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.19941692054271698, 0.19205951690673828, 100, 0.48824260647182854], "caller": "main.py:200", "time": "2021-02-05T06:48:58.641244"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.20148807764053345, 0.19062024354934692, 100, 0.49354804332089214], "caller": "main.py:200", "time": "2021-02-05T06:50:21.895218"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2007008045911789, 0.19337664544582367, 100, 0.485826753627355], "caller": "main.py:200", "time": "2021-02-05T06:51:46.007726"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.1988503783941269, 0.21226716041564941, 100, 0.49241026754223544], "caller": "main.py:200", "time": "2021-02-05T06:53:07.742024"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.20170162618160248, 0.1979440152645111, 100, 0.5016209071372998], "caller": "main.py:200", "time": "2021-02-05T06:54:33.198834"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.1996045857667923, 0.18857093155384064, 100, 0.4929079931481503], "caller": "main.py:200", "time": "2021-02-05T06:55:58.672446"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.19979220628738403, 0.21396146714687347, 100, 0.48412121499704003], "caller": "main.py:200", "time": "2021-02-05T06:57:23.332120"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.20028595626354218, 0.22180038690567017, 100, 0.509863824793374], "caller": "main.py:200", "time": "2021-02-05T06:58:48.217471"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.1984226107597351, 0.1697971224784851, 100, 0.48911725009556994], "caller": "main.py:200", "time": "2021-02-05T07:00:13.305545"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.19995275139808655, 0.21779650449752808, 100, 0.49490652586041006], "caller": "main.py:200", "time": "2021-02-05T07:01:38.338843"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [15], "caller": "main.py:129", "time": "2021-02-05T07:03:00.033631"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T07:03:10.417407"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 12.292237312152835, 1.0849348799000047], "caller": "main.py:37", "time": "2021-02-05T07:03:14.433077"}
+{"level": "info", "fmt": "episode: %s", "args": [6.166090016823463], "caller": "main.py:149", "time": "2021-02-05T07:03:25.242173"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.21230287849903107, 0.22788366675376892, 100, 0.4844860189668636], "caller": "main.py:200", "time": "2021-02-05T07:03:27.377507"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.20552681386470795, 0.20170551538467407, 100, 0.49142282153222144], "caller": "main.py:200", "time": "2021-02-05T07:04:52.138498"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.206658273935318, 0.21557965874671936, 100, 0.4831154971831719], "caller": "main.py:200", "time": "2021-02-05T07:06:17.412730"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2035178542137146, 0.19926930963993073, 100, 0.46846582706058293], "caller": "main.py:200", "time": "2021-02-05T07:07:42.447181"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.20364782214164734, 0.20168140530586243, 100, 0.48067886854371755], "caller": "main.py:200", "time": "2021-02-05T07:09:06.710019"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.20761123299598694, 0.22913947701454163, 100, 0.49131930465553425], "caller": "main.py:200", "time": "2021-02-05T07:10:31.159983"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.20553311705589294, 0.22527021169662476, 100, 0.46591739530672555], "caller": "main.py:200", "time": "2021-02-05T07:11:56.347667"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.20337995886802673, 0.20232895016670227, 100, 0.46616122360336254], "caller": "main.py:200", "time": "2021-02-05T07:13:16.790010"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.20461587607860565, 0.19701218605041504, 100, 0.49096583510496], "caller": "main.py:200", "time": "2021-02-05T07:14:36.373621"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.20176860690116882, 0.20574550330638885, 100, 0.4813927363867973], "caller": "main.py:200", "time": "2021-02-05T07:15:57.002473"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2037794291973114, 0.22898343205451965, 100, 0.49453115070881337], "caller": "main.py:200", "time": "2021-02-05T07:17:26.918936"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.20414088666439056, 0.21371755003929138, 100, 0.48719121481076655], "caller": "main.py:200", "time": "2021-02-05T07:18:56.655904"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.20672351121902466, 0.20099060237407684, 100, 0.4985698407389843], "caller": "main.py:200", "time": "2021-02-05T07:20:24.774643"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.20189285278320312, 0.19050347805023193, 100, 0.496567633996421], "caller": "main.py:200", "time": "2021-02-05T07:21:50.180215"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.20310553908348083, 0.19971492886543274, 100, 0.48751393670929], "caller": "main.py:200", "time": "2021-02-05T07:23:15.050353"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.20075687766075134, 0.1915002465248108, 100, 0.4819389938826228], "caller": "main.py:200", "time": "2021-02-05T07:24:41.093016"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.20448189973831177, 0.19853147864341736, 100, 0.48387946064401777], "caller": "main.py:200", "time": "2021-02-05T07:26:05.609362"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2047368586063385, 0.20012158155441284, 100, 0.4913397529229921], "caller": "main.py:200", "time": "2021-02-05T07:27:27.978645"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.20179694890975952, 0.23577257990837097, 100, 0.475645588886857], "caller": "main.py:200", "time": "2021-02-05T07:28:50.654352"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.20085126161575317, 0.21587735414505005, 100, 0.4918876713022441], "caller": "main.py:200", "time": "2021-02-05T07:30:15.786792"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [16], "caller": "main.py:129", "time": "2021-02-05T07:31:38.546944"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T07:31:48.182080"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 23.222766136058027, 0.2452926407860753], "caller": "main.py:37", "time": "2021-02-05T07:31:52.913799"}
+{"level": "info", "fmt": "episode: %s", "args": [7.053807845222936], "caller": "main.py:149", "time": "2021-02-05T07:32:03.608501"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.20549704134464264, 0.2390885055065155, 100, 0.49323102677896596], "caller": "main.py:200", "time": "2021-02-05T07:32:06.605900"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.20761197805404663, 0.19804909825325012, 100, 0.46881170029720454], "caller": "main.py:200", "time": "2021-02-05T07:33:30.917607"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.20429562032222748, 0.22464120388031006, 100, 0.48922879124755925], "caller": "main.py:200", "time": "2021-02-05T07:34:53.369308"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.20646755397319794, 0.2008395791053772, 100, 0.45472743834478224], "caller": "main.py:200", "time": "2021-02-05T07:36:18.677019"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.20446859300136566, 0.18901333212852478, 100, 0.47281152452270714], "caller": "main.py:200", "time": "2021-02-05T07:37:41.995742"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.20512492954730988, 0.22939416766166687, 100, 0.47348620976243155], "caller": "main.py:200", "time": "2021-02-05T07:39:10.828741"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.20306411385536194, 0.23429343104362488, 100, 0.4801513733979726], "caller": "main.py:200", "time": "2021-02-05T07:40:37.040803"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2055957019329071, 0.20436081290245056, 100, 0.4617152320876939], "caller": "main.py:200", "time": "2021-02-05T07:42:07.068629"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2051967978477478, 0.19543471932411194, 100, 0.4861075397839813], "caller": "main.py:200", "time": "2021-02-05T07:43:33.023922"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.20333519577980042, 0.20055899024009705, 100, 0.47280807311575174], "caller": "main.py:200", "time": "2021-02-05T07:45:01.726952"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.20685596764087677, 0.2143307328224182, 100, 0.48781786556134094], "caller": "main.py:200", "time": "2021-02-05T07:46:34.571552"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.20665714144706726, 0.20074450969696045, 100, 0.4677136921414916], "caller": "main.py:200", "time": "2021-02-05T07:47:59.831878"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2045714259147644, 0.21316975355148315, 100, 0.46139413054143846], "caller": "main.py:200", "time": "2021-02-05T07:49:26.675774"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.20185716450214386, 0.21864980459213257, 100, 0.4664887189398624], "caller": "main.py:200", "time": "2021-02-05T07:50:54.957628"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.20214706659317017, 0.20826943218708038, 100, 0.47304394091405655], "caller": "main.py:200", "time": "2021-02-05T07:52:20.753460"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2038281261920929, 0.21597738564014435, 100, 0.46948527838065235], "caller": "main.py:200", "time": "2021-02-05T07:53:43.430143"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.20180082321166992, 0.18399617075920105, 100, 0.44406024868816085], "caller": "main.py:200", "time": "2021-02-05T07:55:07.113305"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2060970515012741, 0.18364739418029785, 100, 0.47162903095535863], "caller": "main.py:200", "time": "2021-02-05T07:56:29.762342"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.20245566964149475, 0.19585701823234558, 100, 0.4719525363569376], "caller": "main.py:200", "time": "2021-02-05T07:57:55.847140"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.20333024859428406, 0.19391879439353943, 100, 0.46475286628128737], "caller": "main.py:200", "time": "2021-02-05T07:59:23.712450"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [17], "caller": "main.py:129", "time": "2021-02-05T08:00:46.967088"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T08:00:56.453747"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 27.08717400590725, 0.30663099249346326], "caller": "main.py:37", "time": "2021-02-05T08:01:01.344263"}
+{"level": "info", "fmt": "episode: %s", "args": [7.126598849305634], "caller": "main.py:149", "time": "2021-02-05T08:01:12.448341"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.21029576659202576, 0.19891011714935303, 100, 0.4949781217676142], "caller": "main.py:200", "time": "2021-02-05T08:01:15.645807"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.20845302939414978, 0.22711780667304993, 100, 0.4592663922446656], "caller": "main.py:200", "time": "2021-02-05T08:02:40.826884"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.20833119750022888, 0.19982004165649414, 100, 0.47536387647405104], "caller": "main.py:200", "time": "2021-02-05T08:04:10.538573"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.20681771636009216, 0.21694111824035645, 100, 0.4638811349404894], "caller": "main.py:200", "time": "2021-02-05T08:05:35.348560"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2076413333415985, 0.22151613235473633, 100, 0.46147967065726314], "caller": "main.py:200", "time": "2021-02-05T08:07:02.096452"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.20710299909114838, 0.21779896318912506, 100, 0.4462095686347118], "caller": "main.py:200", "time": "2021-02-05T08:08:33.599647"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.204922154545784, 0.2270166277885437, 100, 0.4731228202108283], "caller": "main.py:200", "time": "2021-02-05T08:09:58.620648"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.20640963315963745, 0.19843213260173798, 100, 0.4673098068962088], "caller": "main.py:200", "time": "2021-02-05T08:11:22.415388"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.20544050633907318, 0.20079991221427917, 100, 0.45615998293368165], "caller": "main.py:200", "time": "2021-02-05T08:12:51.422196"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.20811434090137482, 0.19296003878116608, 100, 0.45692110116680945], "caller": "main.py:200", "time": "2021-02-05T08:14:20.678058"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.20489640533924103, 0.2346654236316681, 100, 0.4681592157014942], "caller": "main.py:200", "time": "2021-02-05T08:15:46.831022"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2040908932685852, 0.20114871859550476, 100, 0.45205155159482857], "caller": "main.py:200", "time": "2021-02-05T08:17:10.310846"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2039673626422882, 0.19643284380435944, 100, 0.4668641197214563], "caller": "main.py:200", "time": "2021-02-05T08:18:34.799790"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.20600053668022156, 0.21116937696933746, 100, 0.4573559304614475], "caller": "main.py:200", "time": "2021-02-05T08:20:00.902381"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2035425305366516, 0.19318616390228271, 100, 0.44672103013101383], "caller": "main.py:200", "time": "2021-02-05T08:21:27.647244"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.20325595140457153, 0.20865046977996826, 100, 0.44701379324496004], "caller": "main.py:200", "time": "2021-02-05T08:22:52.550637"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.20335768163204193, 0.20595791935920715, 100, 0.45045191521908445], "caller": "main.py:200", "time": "2021-02-05T08:24:19.698163"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2036510407924652, 0.20597006380558014, 100, 0.4680293836721645], "caller": "main.py:200", "time": "2021-02-05T08:25:49.683896"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2029491662979126, 0.18207517266273499, 100, 0.4646280532610705], "caller": "main.py:200", "time": "2021-02-05T08:27:11.713058"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.20310066640377045, 0.2081475853919983, 100, 0.44845153416679046], "caller": "main.py:200", "time": "2021-02-05T08:28:37.823322"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [18], "caller": "main.py:129", "time": "2021-02-05T08:30:03.854601"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T08:30:13.473400"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 24.76257924791187, 0.5579584643238862], "caller": "main.py:37", "time": "2021-02-05T08:30:17.310906"}
+{"level": "info", "fmt": "episode: %s", "args": [8.200175093518913], "caller": "main.py:149", "time": "2021-02-05T08:30:27.692837"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.20710119605064392, 0.21316155791282654, 100, 0.43416505153605894], "caller": "main.py:200", "time": "2021-02-05T08:30:30.998055"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.21001747250556946, 0.2095613181591034, 100, 0.4611362033621277], "caller": "main.py:200", "time": "2021-02-05T08:31:58.163950"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.20924228429794312, 0.21974217891693115, 100, 0.451179282918158], "caller": "main.py:200", "time": "2021-02-05T08:33:21.647655"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.20858009159564972, 0.1875537484884262, 100, 0.4722124263172006], "caller": "main.py:200", "time": "2021-02-05T08:34:45.100510"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.20700666308403015, 0.20641911029815674, 100, 0.442933101013713], "caller": "main.py:200", "time": "2021-02-05T08:36:10.315167"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2072085738182068, 0.18887418508529663, 100, 0.4594218164222967], "caller": "main.py:200", "time": "2021-02-05T08:37:42.844041"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.20876432955265045, 0.2514229416847229, 100, 0.45669483895503243], "caller": "main.py:200", "time": "2021-02-05T08:39:10.331587"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.20449088513851166, 0.2008139193058014, 100, 0.46010164137053955], "caller": "main.py:200", "time": "2021-02-05T08:40:41.386270"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2062341272830963, 0.2238650619983673, 100, 0.44020208861002114], "caller": "main.py:200", "time": "2021-02-05T08:42:04.979474"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.20676961541175842, 0.20841093361377716, 100, 0.4442499142440958], "caller": "main.py:200", "time": "2021-02-05T08:43:32.953959"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.20577961206436157, 0.23108983039855957, 100, 0.462012660159163], "caller": "main.py:200", "time": "2021-02-05T08:45:00.186565"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2039603441953659, 0.20999005436897278, 100, 0.4557291465541967], "caller": "main.py:200", "time": "2021-02-05T08:46:27.690153"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2073623239994049, 0.20591658353805542, 100, 0.46213111485389907], "caller": "main.py:200", "time": "2021-02-05T08:47:53.163361"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.20641058683395386, 0.20303675532341003, 100, 0.44682101841273064], "caller": "main.py:200", "time": "2021-02-05T08:49:23.675401"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.20493793487548828, 0.1971048265695572, 100, 0.4542667033814113], "caller": "main.py:200", "time": "2021-02-05T08:50:51.011209"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.20384244620800018, 0.19240549206733704, 100, 0.4518737408066918], "caller": "main.py:200", "time": "2021-02-05T08:52:17.763718"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.20592060685157776, 0.22140252590179443, 100, 0.4562286734124765], "caller": "main.py:200", "time": "2021-02-05T08:53:45.602139"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.20244380831718445, 0.21092942357063293, 100, 0.45585234965295834], "caller": "main.py:200", "time": "2021-02-05T08:55:11.776778"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2037688046693802, 0.20198895037174225, 100, 0.44768238599591936], "caller": "main.py:200", "time": "2021-02-05T08:56:38.631062"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.20121750235557556, 0.19651982188224792, 100, 0.43948666925803803], "caller": "main.py:200", "time": "2021-02-05T08:58:05.767887"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.018870944157242775, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T08:58:56.453201"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [19], "caller": "main.py:129", "time": "2021-02-05T08:59:31.670506"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T08:59:41.312799"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 17.653921956329224, 10.565575840875125], "caller": "main.py:37", "time": "2021-02-05T08:59:45.071255"}
+{"level": "info", "fmt": "episode: %s", "args": [8.635457650979149], "caller": "main.py:149", "time": "2021-02-05T08:59:55.578861"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.20761607587337494, 0.22650541365146637, 100, 0.45105350370664943], "caller": "main.py:200", "time": "2021-02-05T08:59:58.777173"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.20787805318832397, 0.2239130437374115, 100, 0.4447194206269911], "caller": "main.py:200", "time": "2021-02-05T09:01:26.952631"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2069959193468094, 0.20399580895900726, 100, 0.4411108868873083], "caller": "main.py:200", "time": "2021-02-05T09:02:55.092503"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.20813599228858948, 0.19009879231452942, 100, 0.43222714837043547], "caller": "main.py:200", "time": "2021-02-05T09:04:20.582399"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.20723527669906616, 0.1864752620458603, 100, 0.4462030937821505], "caller": "main.py:200", "time": "2021-02-05T09:05:45.844568"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.20829644799232483, 0.20122674107551575, 100, 0.45532038833118077], "caller": "main.py:200", "time": "2021-02-05T09:07:10.256540"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.20497813820838928, 0.2055489867925644, 100, 0.44874781663698715], "caller": "main.py:200", "time": "2021-02-05T09:08:32.249192"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.20789046585559845, 0.22072075307369232, 100, 0.44076086784961976], "caller": "main.py:200", "time": "2021-02-05T09:10:01.458949"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.20744778215885162, 0.19274693727493286, 100, 0.44848370815799055], "caller": "main.py:200", "time": "2021-02-05T09:11:31.550371"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2065921425819397, 0.21139273047447205, 100, 0.4390726863898849], "caller": "main.py:200", "time": "2021-02-05T09:12:59.010880"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.20880113542079926, 0.2078937143087387, 100, 0.45595425839632964], "caller": "main.py:200", "time": "2021-02-05T09:14:24.548759"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.20228581130504608, 0.19500502943992615, 100, 0.43619180555750087], "caller": "main.py:200", "time": "2021-02-05T09:15:49.354975"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.20683975517749786, 0.21051938831806183, 100, 0.4411758774077791], "caller": "main.py:200", "time": "2021-02-05T09:17:13.019417"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2028210610151291, 0.2093951255083084, 100, 0.4437185164842346], "caller": "main.py:200", "time": "2021-02-05T09:18:40.559927"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.20614492893218994, 0.22090736031532288, 100, 0.45610560114591014], "caller": "main.py:200", "time": "2021-02-05T09:20:09.437942"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.20481452345848083, 0.19611507654190063, 100, 0.43422856564947876], "caller": "main.py:200", "time": "2021-02-05T09:21:36.161567"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.2070116549730301, 0.2329661101102829, 100, 0.439017428115812], "caller": "main.py:200", "time": "2021-02-05T09:23:05.468355"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.20409783720970154, 0.20870965719223022, 100, 0.4474807703047545], "caller": "main.py:200", "time": "2021-02-05T09:24:33.311313"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.20802083611488342, 0.1961546391248703, 100, 0.43586694415025334], "caller": "main.py:200", "time": "2021-02-05T09:26:01.506164"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.20513209700584412, 0.20183761417865753, 100, 0.4371235480467456], "caller": "main.py:200", "time": "2021-02-05T09:27:29.675648"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [20], "caller": "main.py:129", "time": "2021-02-05T09:28:56.522060"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T09:29:06.195421"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 19.472664759355826, 0.26051491334042093], "caller": "main.py:37", "time": "2021-02-05T09:29:10.060826"}
+{"level": "info", "fmt": "episode: %s", "args": [7.900149925649348], "caller": "main.py:149", "time": "2021-02-05T09:29:20.529767"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.20855523645877838, 0.2130495309829712, 100, 0.4353029584449341], "caller": "main.py:200", "time": "2021-02-05T09:29:23.123044"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.20772071182727814, 0.20190021395683289, 100, 0.4358249377768521], "caller": "main.py:200", "time": "2021-02-05T09:30:48.441213"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.20558300614356995, 0.22130057215690613, 100, 0.4418958439981116], "caller": "main.py:200", "time": "2021-02-05T09:32:10.514156"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.20694665610790253, 0.21565142273902893, 100, 0.4432369622144921], "caller": "main.py:200", "time": "2021-02-05T09:33:34.842155"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.20743800699710846, 0.1808956116437912, 100, 0.4508771007802221], "caller": "main.py:200", "time": "2021-02-05T09:35:01.967592"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.20555195212364197, 0.20753642916679382, 100, 0.4280801549125791], "caller": "main.py:200", "time": "2021-02-05T09:36:26.594839"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.20688408613204956, 0.20097652077674866, 100, 0.4529537900829357], "caller": "main.py:200", "time": "2021-02-05T09:37:50.741385"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.20305994153022766, 0.20522761344909668, 100, 0.42903502310992475], "caller": "main.py:200", "time": "2021-02-05T09:39:19.559845"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2050262838602066, 0.2036084085702896, 100, 0.43308047856530024], "caller": "main.py:200", "time": "2021-02-05T09:40:46.519845"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.20596374571323395, 0.18884575366973877, 100, 0.42297934140505544], "caller": "main.py:200", "time": "2021-02-05T09:42:16.878782"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.20666028559207916, 0.210139662027359, 100, 0.4275925004054677], "caller": "main.py:200", "time": "2021-02-05T09:43:43.908534"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2069299817085266, 0.18092122673988342, 100, 0.43735003168977793], "caller": "main.py:200", "time": "2021-02-05T09:45:09.990059"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2074325680732727, 0.2108532190322876, 100, 0.4371266316930427], "caller": "main.py:200", "time": "2021-02-05T09:46:35.244367"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.20489823818206787, 0.22118690609931946, 100, 0.4310482930706037], "caller": "main.py:200", "time": "2021-02-05T09:48:08.594457"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.20435543358325958, 0.21485893428325653, 100, 0.43441679979986964], "caller": "main.py:200", "time": "2021-02-05T09:49:33.329220"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.20681928098201752, 0.21290671825408936, 100, 0.43447987224287177], "caller": "main.py:200", "time": "2021-02-05T09:50:55.601812"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.20700037479400635, 0.20083275437355042, 100, 0.4309809693263134], "caller": "main.py:200", "time": "2021-02-05T09:52:23.048829"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2061983346939087, 0.20560972392559052, 100, 0.442072021917005], "caller": "main.py:200", "time": "2021-02-05T09:53:49.712702"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.20422212779521942, 0.224481463432312, 100, 0.4255810731223818], "caller": "main.py:200", "time": "2021-02-05T09:55:15.612124"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.20447662472724915, 0.20144058763980865, 100, 0.4461032956392505], "caller": "main.py:200", "time": "2021-02-05T09:56:44.526766"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [21], "caller": "main.py:129", "time": "2021-02-05T09:58:10.891416"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T09:58:20.588641"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 20.521552722879772, 0.21098660562312163], "caller": "main.py:37", "time": "2021-02-05T09:58:26.479193"}
+{"level": "info", "fmt": "episode: %s", "args": [5.598984107855793], "caller": "main.py:149", "time": "2021-02-05T09:58:37.114053"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.20845603942871094, 0.20700842142105103, 100, 0.43711399372021226], "caller": "main.py:200", "time": "2021-02-05T09:58:39.687755"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.20808738470077515, 0.20672720670700073, 100, 0.43483005519333345], "caller": "main.py:200", "time": "2021-02-05T10:00:07.953674"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2047453373670578, 0.2127852737903595, 100, 0.42700730051300534], "caller": "main.py:200", "time": "2021-02-05T10:01:34.040176"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.20409303903579712, 0.19171284139156342, 100, 0.4225892108256229], "caller": "main.py:200", "time": "2021-02-05T10:02:59.696616"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2054242342710495, 0.1970941722393036, 100, 0.42983317609303356], "caller": "main.py:200", "time": "2021-02-05T10:04:22.726507"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2061375230550766, 0.19235850870609283, 100, 0.43608954186846993], "caller": "main.py:200", "time": "2021-02-05T10:05:48.200717"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.20712852478027344, 0.20585153996944427, 100, 0.4324318214818537], "caller": "main.py:200", "time": "2021-02-05T10:07:14.165981"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.20584779977798462, 0.21699020266532898, 100, 0.4355980431597941], "caller": "main.py:200", "time": "2021-02-05T10:08:40.775762"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.20591577887535095, 0.19045312702655792, 100, 0.43115479107359905], "caller": "main.py:200", "time": "2021-02-05T10:10:06.149172"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.20472143590450287, 0.20246964693069458, 100, 0.43044398810135337], "caller": "main.py:200", "time": "2021-02-05T10:11:36.036886"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.20515072345733643, 0.20844104886054993, 100, 0.4267382928302319], "caller": "main.py:200", "time": "2021-02-05T10:13:01.478561"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.20269130170345306, 0.2074061930179596, 100, 0.43009317423572685], "caller": "main.py:200", "time": "2021-02-05T10:14:24.174125"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.20555496215820312, 0.19132712483406067, 100, 0.42643520797995094], "caller": "main.py:200", "time": "2021-02-05T10:15:46.364208"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.20321929454803467, 0.1831161230802536, 100, 0.4228067376786857], "caller": "main.py:200", "time": "2021-02-05T10:17:10.964408"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.20334243774414062, 0.20827701687812805, 100, 0.4283993759322921], "caller": "main.py:200", "time": "2021-02-05T10:18:33.651500"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.20434680581092834, 0.21213802695274353, 100, 0.43192213352175685], "caller": "main.py:200", "time": "2021-02-05T10:19:56.658000"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.20340269804000854, 0.18816211819648743, 100, 0.41740176226495007], "caller": "main.py:200", "time": "2021-02-05T10:21:20.420577"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.20708824694156647, 0.2052897810935974, 100, 0.4446646850856529], "caller": "main.py:200", "time": "2021-02-05T10:22:44.739344"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.20547553896903992, 0.18006190657615662, 100, 0.4335523575110805], "caller": "main.py:200", "time": "2021-02-05T10:24:09.833578"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.20225247740745544, 0.1918569952249527, 100, 0.42436030204820885], "caller": "main.py:200", "time": "2021-02-05T10:25:37.096763"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [22], "caller": "main.py:129", "time": "2021-02-05T10:26:58.652544"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T10:27:08.473411"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 20.916778068590574, 0.34194555464207094], "caller": "main.py:37", "time": "2021-02-05T10:27:12.473554"}
+{"level": "info", "fmt": "episode: %s", "args": [5.410089899721636], "caller": "main.py:149", "time": "2021-02-05T10:27:23.185908"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.20752525329589844, 0.20867601037025452, 100, 0.43392346407604204], "caller": "main.py:200", "time": "2021-02-05T10:27:25.982039"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.20374391973018646, 0.21009215712547302, 100, 0.41516629125496546], "caller": "main.py:200", "time": "2021-02-05T10:28:49.022982"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2051752805709839, 0.2062891721725464, 100, 0.43627461160827], "caller": "main.py:200", "time": "2021-02-05T10:30:11.894280"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.20557299256324768, 0.21822938323020935, 100, 0.4169246106922935], "caller": "main.py:200", "time": "2021-02-05T10:31:35.790011"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2047674059867859, 0.2132643163204193, 100, 0.43765147264395615], "caller": "main.py:200", "time": "2021-02-05T10:33:07.926975"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.20401394367218018, 0.19838878512382507, 100, 0.42580946918044155], "caller": "main.py:200", "time": "2021-02-05T10:34:47.046291"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.20297059416770935, 0.18098211288452148, 100, 0.4225342398459378], "caller": "main.py:200", "time": "2021-02-05T10:36:12.878390"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.20643289387226105, 0.2195417284965515, 100, 0.42767574157014365], "caller": "main.py:200", "time": "2021-02-05T10:37:38.693741"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.20161810517311096, 0.2056683450937271, 100, 0.4226107197577344], "caller": "main.py:200", "time": "2021-02-05T10:39:06.314065"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.20388025045394897, 0.1879217028617859, 100, 0.42560389246260855], "caller": "main.py:200", "time": "2021-02-05T10:40:29.996171"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.204620361328125, 0.1958112269639969, 100, 0.4189906730828912], "caller": "main.py:200", "time": "2021-02-05T10:41:57.131040"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.20307166874408722, 0.2386128306388855, 100, 0.44160197730416745], "caller": "main.py:200", "time": "2021-02-05T10:43:27.047492"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.20504796504974365, 0.19806838035583496, 100, 0.4334302946491733], "caller": "main.py:200", "time": "2021-02-05T10:44:54.031485"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2036956250667572, 0.19955512881278992, 100, 0.4293806218671572], "caller": "main.py:200", "time": "2021-02-05T10:46:17.515313"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2032875418663025, 0.22800156474113464, 100, 0.42423039283086417], "caller": "main.py:200", "time": "2021-02-05T10:47:38.596214"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2034703493118286, 0.1964968591928482, 100, 0.4344741707605764], "caller": "main.py:200", "time": "2021-02-05T10:49:05.134026"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.20178554952144623, 0.18600772321224213, 100, 0.433483933763071], "caller": "main.py:200", "time": "2021-02-05T10:50:27.665620"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2026572972536087, 0.18738523125648499, 100, 0.4344143107099041], "caller": "main.py:200", "time": "2021-02-05T10:51:54.692606"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.20389536023139954, 0.2211359143257141, 100, 0.42721396382351245], "caller": "main.py:200", "time": "2021-02-05T10:53:20.228438"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.20393666625022888, 0.1896681785583496, 100, 0.42849934514052257], "caller": "main.py:200", "time": "2021-02-05T10:54:48.702600"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [23], "caller": "main.py:129", "time": "2021-02-05T10:56:10.260781"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T10:56:19.828456"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 22.117849258197836, 0.24924046694704008], "caller": "main.py:37", "time": "2021-02-05T10:56:23.610813"}
+{"level": "info", "fmt": "episode: %s", "args": [5.844432358635804], "caller": "main.py:149", "time": "2021-02-05T10:56:34.104086"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.20678915083408356, 0.19538149237632751, 100, 0.42804658945094687], "caller": "main.py:200", "time": "2021-02-05T10:56:36.314213"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.20322538912296295, 0.17785359919071198, 100, 0.4303853296803197], "caller": "main.py:200", "time": "2021-02-05T10:58:00.458711"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2042207270860672, 0.2009764164686203, 100, 0.43214023645371136], "caller": "main.py:200", "time": "2021-02-05T10:59:23.178086"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.20383897423744202, 0.1946508288383484, 100, 0.4282750349808618], "caller": "main.py:200", "time": "2021-02-05T11:00:45.555829"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.20235057175159454, 0.19326692819595337, 100, 0.4206079920705758], "caller": "main.py:200", "time": "2021-02-05T11:02:10.262505"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.20288802683353424, 0.21912497282028198, 100, 0.4232625555568864], "caller": "main.py:200", "time": "2021-02-05T11:03:37.391432"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.20290707051753998, 0.19317626953125, 100, 0.41253753896402584], "caller": "main.py:200", "time": "2021-02-05T11:05:02.450884"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.20379041135311127, 0.19897450506687164, 100, 0.42417624320318403], "caller": "main.py:200", "time": "2021-02-05T11:06:27.334380"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.20389480888843536, 0.20126359164714813, 100, 0.4066983913968607], "caller": "main.py:200", "time": "2021-02-05T11:07:54.091389"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2006964236497879, 0.20099739730358124, 100, 0.4169496082842633], "caller": "main.py:200", "time": "2021-02-05T11:09:20.826323"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.20075710117816925, 0.1874236762523651, 100, 0.4125189557259028], "caller": "main.py:200", "time": "2021-02-05T11:10:47.965050"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.20112618803977966, 0.19538861513137817, 100, 0.42820342536458933], "caller": "main.py:200", "time": "2021-02-05T11:12:14.647049"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.20254755020141602, 0.20725271105766296, 100, 0.4176435753285474], "caller": "main.py:200", "time": "2021-02-05T11:13:41.402925"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.20172452926635742, 0.20690631866455078, 100, 0.4140139773074031], "caller": "main.py:200", "time": "2021-02-05T11:15:06.531989"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.20299750566482544, 0.2063954472541809, 100, 0.43029957111347183], "caller": "main.py:200", "time": "2021-02-05T11:16:27.666014"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.20211735367774963, 0.21219879388809204, 100, 0.4220075925762647], "caller": "main.py:200", "time": "2021-02-05T11:17:51.258189"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.20094965398311615, 0.19579951465129852, 100, 0.4220172163422092], "caller": "main.py:200", "time": "2021-02-05T11:19:17.292193"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.20152494311332703, 0.19621452689170837, 100, 0.41505861963651464], "caller": "main.py:200", "time": "2021-02-05T11:20:46.364586"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2023029327392578, 0.2018137276172638, 100, 0.42514889861862176], "caller": "main.py:200", "time": "2021-02-05T11:22:10.550265"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.1992148458957672, 0.19855433702468872, 100, 0.42991768326036445], "caller": "main.py:200", "time": "2021-02-05T11:23:42.703628"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [24], "caller": "main.py:129", "time": "2021-02-05T11:25:06.817965"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T11:25:16.520473"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 19.65834195293378, 0.24145987969269056], "caller": "main.py:37", "time": "2021-02-05T11:25:20.308812"}
+{"level": "info", "fmt": "episode: %s", "args": [3.8177882219594905], "caller": "main.py:149", "time": "2021-02-05T11:25:30.692984"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.20286396145820618, 0.21358785033226013, 100, 0.41146068211255576], "caller": "main.py:200", "time": "2021-02-05T11:25:34.121704"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.20580890774726868, 0.2062985897064209, 100, 0.4307483809755638], "caller": "main.py:200", "time": "2021-02-05T11:27:00.034594"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.20116561651229858, 0.19085370004177094, 100, 0.41944833363868494], "caller": "main.py:200", "time": "2021-02-05T11:28:26.657918"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.20239952206611633, 0.19764956831932068, 100, 0.42593652333530174], "caller": "main.py:200", "time": "2021-02-05T11:29:56.011549"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.20063698291778564, 0.20823565125465393, 100, 0.4132805347029347], "caller": "main.py:200", "time": "2021-02-05T11:31:20.027155"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.19968238472938538, 0.19027414917945862, 100, 0.4155620744408848], "caller": "main.py:200", "time": "2021-02-05T11:32:50.516245"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.20102335512638092, 0.19805991649627686, 100, 0.4247601964645011], "caller": "main.py:200", "time": "2021-02-05T11:34:21.023226"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.20203885436058044, 0.19368740916252136, 100, 0.4271014868786003], "caller": "main.py:200", "time": "2021-02-05T11:35:49.037665"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.19864200055599213, 0.20208653807640076, 100, 0.41878997649534866], "caller": "main.py:200", "time": "2021-02-05T11:37:15.792111"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.20176631212234497, 0.19762898981571198, 100, 0.4185401996551637], "caller": "main.py:200", "time": "2021-02-05T11:38:42.887899"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2014254778623581, 0.19774630665779114, 100, 0.4282787766624372], "caller": "main.py:200", "time": "2021-02-05T11:40:08.114204"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.19941450655460358, 0.21312963962554932, 100, 0.42298837955536506], "caller": "main.py:200", "time": "2021-02-05T11:41:32.477554"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2015581727027893, 0.1994013786315918, 100, 0.4051721137356898], "caller": "main.py:200", "time": "2021-02-05T11:42:56.567856"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.20137718319892883, 0.19660253822803497, 100, 0.4074326988647247], "caller": "main.py:200", "time": "2021-02-05T11:44:21.329444"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2006186693906784, 0.21926720440387726, 100, 0.42881402756455955], "caller": "main.py:200", "time": "2021-02-05T11:45:44.455810"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.19913391768932343, 0.18534769117832184, 100, 0.423422059672452], "caller": "main.py:200", "time": "2021-02-05T11:47:09.050841"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.19947534799575806, 0.2108822613954544, 100, 0.4135880657616922], "caller": "main.py:200", "time": "2021-02-05T11:48:34.245054"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.19590406119823456, 0.2110321819782257, 100, 0.4081690319964413], "caller": "main.py:200", "time": "2021-02-05T11:50:00.816641"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.19750934839248657, 0.19399645924568176, 100, 0.4198599910316224], "caller": "main.py:200", "time": "2021-02-05T11:51:33.509550"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.1971476972103119, 0.178697407245636, 100, 0.4133062654320352], "caller": "main.py:200", "time": "2021-02-05T11:52:58.151160"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [25], "caller": "main.py:129", "time": "2021-02-05T11:54:20.734675"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T11:54:30.297217"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 19.168999480477073, 0.3589753720433337], "caller": "main.py:37", "time": "2021-02-05T11:54:34.127987"}
+{"level": "info", "fmt": "episode: %s", "args": [4.206266463132915], "caller": "main.py:149", "time": "2021-02-05T11:54:44.506937"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.19624534249305725, 0.22209258377552032, 100, 0.41255085642980666], "caller": "main.py:200", "time": "2021-02-05T11:54:47.770911"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.19662103056907654, 0.2011851966381073, 100, 0.4106334575600071], "caller": "main.py:200", "time": "2021-02-05T11:56:13.445673"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.1959928572177887, 0.17497891187667847, 100, 0.4069277649710947], "caller": "main.py:200", "time": "2021-02-05T11:57:38.613779"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.196733757853508, 0.20468655228614807, 100, 0.41346294637360126], "caller": "main.py:200", "time": "2021-02-05T11:59:04.074195"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.1931295394897461, 0.17288225889205933, 100, 0.399128052850388], "caller": "main.py:200", "time": "2021-02-05T12:00:26.741386"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.19289430975914001, 0.19518058001995087, 100, 0.4092622175403459], "caller": "main.py:200", "time": "2021-02-05T12:01:56.256371"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.19236603379249573, 0.19173011183738708, 100, 0.40039277519752103], "caller": "main.py:200", "time": "2021-02-05T12:03:20.657845"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.19556643068790436, 0.20166373252868652, 100, 0.41022650863598314], "caller": "main.py:200", "time": "2021-02-05T12:04:47.243583"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.194325789809227, 0.1990267038345337, 100, 0.40789631392453096], "caller": "main.py:200", "time": "2021-02-05T12:06:16.699465"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.19193463027477264, 0.1813075691461563, 100, 0.41356285150683864], "caller": "main.py:200", "time": "2021-02-05T12:07:41.465702"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.1933460831642151, 0.20881792902946472, 100, 0.3963304382166307], "caller": "main.py:200", "time": "2021-02-05T12:09:03.610033"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.19294506311416626, 0.17750510573387146, 100, 0.40456391449697343], "caller": "main.py:200", "time": "2021-02-05T12:10:32.563464"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.1906287521123886, 0.21008753776550293, 100, 0.4102992996163149], "caller": "main.py:200", "time": "2021-02-05T12:12:03.714611"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.19335131347179413, 0.20079848170280457, 100, 0.41591968830240983], "caller": "main.py:200", "time": "2021-02-05T12:13:32.889017"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.19644446671009064, 0.19627179205417633, 100, 0.4071437525342063], "caller": "main.py:200", "time": "2021-02-05T12:15:00.923508"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.19491828978061676, 0.19869455695152283, 100, 0.41442084487108694], "caller": "main.py:200", "time": "2021-02-05T12:16:24.863488"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.19406719505786896, 0.1736447662115097, 100, 0.4238374489121985], "caller": "main.py:200", "time": "2021-02-05T12:17:49.706657"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.19173996150493622, 0.19314241409301758, 100, 0.402875935991054], "caller": "main.py:200", "time": "2021-02-05T12:19:14.159643"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.19275827705860138, 0.18655019998550415, 100, 0.40563384558674453], "caller": "main.py:200", "time": "2021-02-05T12:20:47.072586"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.1907297968864441, 0.18408304452896118, 100, 0.4037774282290086], "caller": "main.py:200", "time": "2021-02-05T12:22:16.022740"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [26], "caller": "main.py:129", "time": "2021-02-05T12:23:42.899044"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T12:23:52.341028"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 21.627052047554123, 0.3598190260990321], "caller": "main.py:37", "time": "2021-02-05T12:23:56.105337"}
+{"level": "info", "fmt": "episode: %s", "args": [3.831571459640157], "caller": "main.py:149", "time": "2021-02-05T12:24:06.296496"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.1893519014120102, 0.18684741854667664, 100, 0.40291700955176457], "caller": "main.py:200", "time": "2021-02-05T12:24:08.998607"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.18890435993671417, 0.1816217303276062, 100, 0.4145405456009219], "caller": "main.py:200", "time": "2021-02-05T12:25:32.178345"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.18592989444732666, 0.18174120783805847, 100, 0.4045631631804261], "caller": "main.py:200", "time": "2021-02-05T12:26:54.232953"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.1877765655517578, 0.2080184668302536, 100, 0.39792688455178676], "caller": "main.py:200", "time": "2021-02-05T12:28:23.267579"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.18670979142189026, 0.1746751070022583, 100, 0.39621062602611], "caller": "main.py:200", "time": "2021-02-05T12:29:50.869925"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.18539533019065857, 0.19320301711559296, 100, 0.39608052994820403], "caller": "main.py:200", "time": "2021-02-05T12:31:17.137395"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.18574917316436768, 0.19688160717487335, 100, 0.39365297075149774], "caller": "main.py:200", "time": "2021-02-05T12:32:39.984687"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.1863546371459961, 0.17828328907489777, 100, 0.40827947731703645], "caller": "main.py:200", "time": "2021-02-05T12:34:01.968638"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.18582044541835785, 0.1903480887413025, 100, 0.39733787741263976], "caller": "main.py:200", "time": "2021-02-05T12:35:26.890542"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.1862226128578186, 0.17957700788974762, 100, 0.4020737316802402], "caller": "main.py:200", "time": "2021-02-05T12:36:52.106844"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.18466760218143463, 0.1628459393978119, 100, 0.39417924072376953], "caller": "main.py:200", "time": "2021-02-05T12:38:18.903192"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.18334805965423584, 0.18176209926605225, 100, 0.39402986462944317], "caller": "main.py:200", "time": "2021-02-05T12:39:44.304175"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.18681950867176056, 0.18910935521125793, 100, 0.4188809203682682], "caller": "main.py:200", "time": "2021-02-05T12:41:10.070651"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.18498006463050842, 0.17929832637310028, 100, 0.4049631619048467], "caller": "main.py:200", "time": "2021-02-05T12:42:39.188029"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.18241317570209503, 0.16285541653633118, 100, 0.39581429358338915], "caller": "main.py:200", "time": "2021-02-05T12:44:04.677228"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.18239493668079376, 0.19901853799819946, 100, 0.3936808237039753], "caller": "main.py:200", "time": "2021-02-05T12:45:33.426221"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.18524345755577087, 0.18790186941623688, 100, 0.40926811780413164], "caller": "main.py:200", "time": "2021-02-05T12:47:01.053346"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.18576593697071075, 0.1847902536392212, 100, 0.404457962174501], "caller": "main.py:200", "time": "2021-02-05T12:48:26.306637"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.1827160120010376, 0.17788125574588776, 100, 0.3949867599807574], "caller": "main.py:200", "time": "2021-02-05T12:49:51.891164"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.18809950351715088, 0.18629269301891327, 100, 0.40257449384478283], "caller": "main.py:200", "time": "2021-02-05T12:51:16.351155"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [27], "caller": "main.py:129", "time": "2021-02-05T12:52:34.143109"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T12:52:43.396907"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 15.55490215280049, 0.5526822354529647], "caller": "main.py:37", "time": "2021-02-05T12:52:47.193075"}
+{"level": "info", "fmt": "episode: %s", "args": [8.090867808908222], "caller": "main.py:149", "time": "2021-02-05T12:52:57.541074"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.18541213870048523, 0.17714834213256836, 100, 0.4062013437937801], "caller": "main.py:200", "time": "2021-02-05T12:52:59.980144"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.18445326387882233, 0.18411560356616974, 100, 0.40574259634728893], "caller": "main.py:200", "time": "2021-02-05T12:54:23.630889"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.18191781640052795, 0.18948540091514587, 100, 0.39866013999767064], "caller": "main.py:200", "time": "2021-02-05T12:55:49.804068"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.18438446521759033, 0.1768026053905487, 100, 0.40741955306032085], "caller": "main.py:200", "time": "2021-02-05T12:57:16.241577"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.18360714614391327, 0.17636778950691223, 100, 0.407284609634679], "caller": "main.py:200", "time": "2021-02-05T12:58:39.720821"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.183353990316391, 0.1869262158870697, 100, 0.40277201350475916], "caller": "main.py:200", "time": "2021-02-05T13:00:02.390664"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.1827675849199295, 0.19919224083423615, 100, 0.41138318564354265], "caller": "main.py:200", "time": "2021-02-05T13:01:29.703418"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.18211956322193146, 0.15534111857414246, 100, 0.39780881818131136], "caller": "main.py:200", "time": "2021-02-05T13:03:00.620556"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.18326681852340698, 0.18699878454208374, 100, 0.4116028201168445], "caller": "main.py:200", "time": "2021-02-05T13:04:27.215665"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.18416832387447357, 0.1830281913280487, 100, 0.39974987175043414], "caller": "main.py:200", "time": "2021-02-05T13:05:58.428271"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.18423865735530853, 0.1694778949022293, 100, 0.40617628331891], "caller": "main.py:200", "time": "2021-02-05T13:07:23.199090"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.1821547895669937, 0.18138819932937622, 100, 0.39288101192360253], "caller": "main.py:200", "time": "2021-02-05T13:08:46.927020"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.18293331563472748, 0.1958627849817276, 100, 0.39839736487457755], "caller": "main.py:200", "time": "2021-02-05T13:10:09.018346"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.18470507860183716, 0.20676982402801514, 100, 0.40421051706801936], "caller": "main.py:200", "time": "2021-02-05T13:11:31.865360"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.18239618837833405, 0.19238077104091644, 100, 0.39589370157336], "caller": "main.py:200", "time": "2021-02-05T13:13:01.040835"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.17919963598251343, 0.1807781159877777, 100, 0.406325224895376], "caller": "main.py:200", "time": "2021-02-05T13:14:28.803255"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.18140573799610138, 0.1682494580745697, 100, 0.4051866644215709], "caller": "main.py:200", "time": "2021-02-05T13:15:59.131421"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.17869526147842407, 0.17758166790008545, 100, 0.3970795368751175], "caller": "main.py:200", "time": "2021-02-05T13:17:24.536918"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.17902423441410065, 0.19131924211978912, 100, 0.38873749699281496], "caller": "main.py:200", "time": "2021-02-05T13:18:46.596785"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.18019698560237885, 0.20790845155715942, 100, 0.3972695749600823], "caller": "main.py:200", "time": "2021-02-05T13:20:12.345766"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [28], "caller": "main.py:129", "time": "2021-02-05T13:21:35.074621"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T13:21:44.492201"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 18.218086752649782, 0.07082593628549383], "caller": "main.py:37", "time": "2021-02-05T13:21:48.391735"}
+{"level": "info", "fmt": "episode: %s", "args": [8.503853948301689], "caller": "main.py:149", "time": "2021-02-05T13:21:58.532532"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.18485601246356964, 0.1907360553741455, 100, 0.4200414314446025], "caller": "main.py:200", "time": "2021-02-05T13:22:01.123638"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.18085302412509918, 0.18356867134571075, 100, 0.398172040541886], "caller": "main.py:200", "time": "2021-02-05T13:23:29.638189"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.18361997604370117, 0.18276067078113556, 100, 0.40769994075989685], "caller": "main.py:200", "time": "2021-02-05T13:24:52.315271"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.1842646598815918, 0.17288315296173096, 100, 0.4114886879509471], "caller": "main.py:200", "time": "2021-02-05T13:26:16.307659"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.18470723927021027, 0.17361494898796082, 100, 0.41689841147068507], "caller": "main.py:200", "time": "2021-02-05T13:27:41.610161"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.183051198720932, 0.1924295574426651, 100, 0.4107908990568404], "caller": "main.py:200", "time": "2021-02-05T13:29:06.977513"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.18172337114810944, 0.17029428482055664, 100, 0.4085626500436393], "caller": "main.py:200", "time": "2021-02-05T13:30:34.090169"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.18420524895191193, 0.18522313237190247, 100, 0.4166744669855199], "caller": "main.py:200", "time": "2021-02-05T13:31:59.734905"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.18332241475582123, 0.1979684829711914, 100, 0.40497843380692955], "caller": "main.py:200", "time": "2021-02-05T13:33:24.528755"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.18049626052379608, 0.18757036328315735, 100, 0.4034522583677244], "caller": "main.py:200", "time": "2021-02-05T13:34:50.196319"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.18074434995651245, 0.15994925796985626, 100, 0.4099083160944157], "caller": "main.py:200", "time": "2021-02-05T13:36:16.701077"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.1814202070236206, 0.18640270829200745, 100, 0.40877985979999193], "caller": "main.py:200", "time": "2021-02-05T13:37:42.899539"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.18135738372802734, 0.17342889308929443, 100, 0.402890095074419], "caller": "main.py:200", "time": "2021-02-05T13:39:07.926408"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.1813032031059265, 0.16413594782352448, 100, 0.40702363248569534], "caller": "main.py:200", "time": "2021-02-05T13:40:35.353301"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.18179143965244293, 0.19394934177398682, 100, 0.41517281259914973], "caller": "main.py:200", "time": "2021-02-05T13:42:04.209128"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.18286119401454926, 0.1684102863073349, 100, 0.4158499610008192], "caller": "main.py:200", "time": "2021-02-05T13:43:26.673770"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.1784791499376297, 0.18216869235038757, 100, 0.3994087031084242], "caller": "main.py:200", "time": "2021-02-05T13:44:48.764631"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.18044063448905945, 0.18009018898010254, 100, 0.40936824556071727], "caller": "main.py:200", "time": "2021-02-05T13:46:12.831123"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.17832216620445251, 0.20485582947731018, 100, 0.4003759353952575], "caller": "main.py:200", "time": "2021-02-05T13:47:37.120165"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.17903682589530945, 0.19207316637039185, 100, 0.41000466670466224], "caller": "main.py:200", "time": "2021-02-05T13:48:59.986281"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [29], "caller": "main.py:129", "time": "2021-02-05T13:50:25.248581"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T13:50:34.587564"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 18.387841481215098, 0.09610522933991301], "caller": "main.py:37", "time": "2021-02-05T13:50:38.352829"}
+{"level": "info", "fmt": "episode: %s", "args": [10.838279206066645], "caller": "main.py:149", "time": "2021-02-05T13:50:48.702354"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.18422052264213562, 0.18654295802116394, 100, 0.42749277408616465], "caller": "main.py:200", "time": "2021-02-05T13:50:51.363246"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.18508878350257874, 0.16868029534816742, 100, 0.4227330126935704], "caller": "main.py:200", "time": "2021-02-05T13:52:19.133779"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.18389944732189178, 0.17840901017189026, 100, 0.41763476695414337], "caller": "main.py:200", "time": "2021-02-05T13:53:46.321993"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.1826227754354477, 0.17795252799987793, 100, 0.4119315582101878], "caller": "main.py:200", "time": "2021-02-05T13:55:12.859886"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.18539047241210938, 0.179876446723938, 100, 0.4248037406377374], "caller": "main.py:200", "time": "2021-02-05T13:56:38.234780"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.1832623928785324, 0.17550775408744812, 100, 0.420800666767002], "caller": "main.py:200", "time": "2021-02-05T13:58:05.696476"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.18466471135616302, 0.19168144464492798, 100, 0.42665722723509003], "caller": "main.py:200", "time": "2021-02-05T13:59:30.682124"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.18397930264472961, 0.18582460284233093, 100, 0.4208262487825138], "caller": "main.py:200", "time": "2021-02-05T14:01:03.971570"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.18131768703460693, 0.18043628334999084, 100, 0.40714092131357976], "caller": "main.py:200", "time": "2021-02-05T14:02:28.758500"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.18355689942836761, 0.16655269265174866, 100, 0.4169845494210968], "caller": "main.py:200", "time": "2021-02-05T14:03:53.647492"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.18024973571300507, 0.1828118860721588, 100, 0.41973215456579965], "caller": "main.py:200", "time": "2021-02-05T14:05:21.070136"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.18250101804733276, 0.1735212653875351, 100, 0.4202906465110105], "caller": "main.py:200", "time": "2021-02-05T14:06:46.034659"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.18122650682926178, 0.17568838596343994, 100, 0.41649615581639027], "caller": "main.py:200", "time": "2021-02-05T14:08:11.469723"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.17758579552173615, 0.17832037806510925, 100, 0.4137951871338944], "caller": "main.py:200", "time": "2021-02-05T14:09:40.803918"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.18060055375099182, 0.1867048740386963, 100, 0.44251465345964336], "caller": "main.py:200", "time": "2021-02-05T14:11:07.544823"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.18152359127998352, 0.16808035969734192, 100, 0.4288201939630842], "caller": "main.py:200", "time": "2021-02-05T14:12:37.100147"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.17981299757957458, 0.15663328766822815, 100, 0.4070588260481995], "caller": "main.py:200", "time": "2021-02-05T14:14:04.485320"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.17686069011688232, 0.18614235520362854, 100, 0.4240068473987958], "caller": "main.py:200", "time": "2021-02-05T14:15:32.290127"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.17858582735061646, 0.17860567569732666, 100, 0.4279789757300598], "caller": "main.py:200", "time": "2021-02-05T14:17:00.540728"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.17970548570156097, 0.16593152284622192, 100, 0.41759898181553995], "caller": "main.py:200", "time": "2021-02-05T14:18:28.873434"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [30], "caller": "main.py:129", "time": "2021-02-05T14:19:53.059161"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T14:20:03.117969"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 17.47907867437462, 0.18924910476234294], "caller": "main.py:37", "time": "2021-02-05T14:20:07.103576"}
+{"level": "info", "fmt": "episode: %s", "args": [5.670603739639278], "caller": "main.py:149", "time": "2021-02-05T14:20:18.018157"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.18397140502929688, 0.17380324006080627, 100, 0.42610645498544647], "caller": "main.py:200", "time": "2021-02-05T14:20:20.780314"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.1820182353258133, 0.18570256233215332, 100, 0.43196726735086916], "caller": "main.py:200", "time": "2021-02-05T14:21:47.919873"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.1797933578491211, 0.16190052032470703, 100, 0.4359694841068128], "caller": "main.py:200", "time": "2021-02-05T14:23:14.737426"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.18523937463760376, 0.19495871663093567, 100, 0.4437106230411278], "caller": "main.py:200", "time": "2021-02-05T14:24:40.072460"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.17975330352783203, 0.18016481399536133, 100, 0.4294772943424855], "caller": "main.py:200", "time": "2021-02-05T14:26:07.532835"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.17943912744522095, 0.1695629060268402, 100, 0.4286417465973412], "caller": "main.py:200", "time": "2021-02-05T14:27:34.982684"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.1786150187253952, 0.1744830161333084, 100, 0.42833305920847514], "caller": "main.py:200", "time": "2021-02-05T14:29:02.164479"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.17788812518119812, 0.1754172295331955, 100, 0.4282300138045281], "caller": "main.py:200", "time": "2021-02-05T14:30:34.536076"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.17920070886611938, 0.14888501167297363, 100, 0.43533057029662353], "caller": "main.py:200", "time": "2021-02-05T14:32:00.773935"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.17956432700157166, 0.20105226337909698, 100, 0.43613795395304455], "caller": "main.py:200", "time": "2021-02-05T14:33:27.366230"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.17850683629512787, 0.1663493514060974, 100, 0.4434047999534661], "caller": "main.py:200", "time": "2021-02-05T14:34:53.189980"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.17914444208145142, 0.17251598834991455, 100, 0.43403065468500956], "caller": "main.py:200", "time": "2021-02-05T14:36:22.370986"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.18026985228061676, 0.1708632856607437, 100, 0.44397244210571873], "caller": "main.py:200", "time": "2021-02-05T14:37:49.472256"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.17896775901317596, 0.1766040027141571, 100, 0.4468941950351141], "caller": "main.py:200", "time": "2021-02-05T14:39:16.160107"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.17812451720237732, 0.17981994152069092, 100, 0.44011173363099193], "caller": "main.py:200", "time": "2021-02-05T14:40:41.342473"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.17872385680675507, 0.15844285488128662, 100, 0.4483358364848982], "caller": "main.py:200", "time": "2021-02-05T14:42:07.167978"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.17697156965732574, 0.1757006198167801, 100, 0.4361956882040611], "caller": "main.py:200", "time": "2021-02-05T14:43:31.937844"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.17827415466308594, 0.18199464678764343, 100, 0.436405380026569], "caller": "main.py:200", "time": "2021-02-05T14:44:54.386742"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.17665810883045197, 0.16237449645996094, 100, 0.44662632788721834], "caller": "main.py:200", "time": "2021-02-05T14:46:20.024585"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.17593540251255035, 0.1815483272075653, 100, 0.43299254472694154], "caller": "main.py:200", "time": "2021-02-05T14:47:45.871797"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [31], "caller": "main.py:129", "time": "2021-02-05T14:49:10.896173"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T14:49:20.262646"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 16.22022442893195, 0.23351130890182833], "caller": "main.py:37", "time": "2021-02-05T14:49:25.053168"}
+{"level": "info", "fmt": "episode: %s", "args": [8.554183602540352], "caller": "main.py:149", "time": "2021-02-05T14:49:35.483400"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.17961019277572632, 0.16350236535072327, 100, 0.4620422562494768], "caller": "main.py:200", "time": "2021-02-05T14:49:37.680195"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.17978672683238983, 0.16281788051128387, 100, 0.4504305228140535], "caller": "main.py:200", "time": "2021-02-05T14:51:04.173710"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.18030045926570892, 0.19557824730873108, 100, 0.4514797508265104], "caller": "main.py:200", "time": "2021-02-05T14:52:30.213421"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.18203572928905487, 0.17869693040847778, 100, 0.4548332976840089], "caller": "main.py:200", "time": "2021-02-05T14:53:52.923806"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.17658711969852448, 0.18842726945877075, 100, 0.44762908811861296], "caller": "main.py:200", "time": "2021-02-05T14:55:24.854307"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.17895358800888062, 0.15590766072273254, 100, 0.45751561577513317], "caller": "main.py:200", "time": "2021-02-05T14:56:49.184725"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.17799876630306244, 0.15657846629619598, 100, 0.45690018828114565], "caller": "main.py:200", "time": "2021-02-05T14:58:12.698560"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.17808085680007935, 0.17970728874206543, 100, 0.4643320351374734], "caller": "main.py:200", "time": "2021-02-05T14:59:38.826104"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.17709890007972717, 0.1853698492050171, 100, 0.46962681527209865], "caller": "main.py:200", "time": "2021-02-05T15:01:06.714543"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.17732612788677216, 0.168582022190094, 100, 0.46250245596313777], "caller": "main.py:200", "time": "2021-02-05T15:02:31.887604"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.17722506821155548, 0.1705106496810913, 100, 0.46336956585780353], "caller": "main.py:200", "time": "2021-02-05T15:04:02.718500"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.1774924248456955, 0.18614619970321655, 100, 0.45932136084016484], "caller": "main.py:200", "time": "2021-02-05T15:05:37.871789"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.1769055426120758, 0.17130771279335022, 100, 0.4536533873819647], "caller": "main.py:200", "time": "2021-02-05T15:07:06.609650"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.17728036642074585, 0.1654822677373886, 100, 0.4433029165420688], "caller": "main.py:200", "time": "2021-02-05T15:08:34.801524"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.17637601494789124, 0.16511189937591553, 100, 0.4659344532619889], "caller": "main.py:200", "time": "2021-02-05T15:10:03.390130"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.1766885370016098, 0.17007586359977722, 100, 0.47783781881322396], "caller": "main.py:200", "time": "2021-02-05T15:11:31.634006"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.17915746569633484, 0.18939749896526337, 100, 0.466546181691722], "caller": "main.py:200", "time": "2021-02-05T15:12:57.416151"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.17694562673568726, 0.1884787380695343, 100, 0.474161267829209], "caller": "main.py:200", "time": "2021-02-05T15:14:21.404745"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.17495523393154144, 0.18144278228282928, 100, 0.46591406489719905], "caller": "main.py:200", "time": "2021-02-05T15:15:49.326946"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.1760655641555786, 0.16706937551498413, 100, 0.4836811708920311], "caller": "main.py:200", "time": "2021-02-05T15:17:17.021980"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [32], "caller": "main.py:129", "time": "2021-02-05T15:18:42.545322"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T15:18:52.250718"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 16.569096780065784, 0.11395649201028706], "caller": "main.py:37", "time": "2021-02-05T15:18:56.682594"}
+{"level": "info", "fmt": "episode: %s", "args": [11.89437943136021], "caller": "main.py:149", "time": "2021-02-05T15:19:07.280212"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.17505702376365662, 0.17790810763835907, 100, 0.47609929854877225], "caller": "main.py:200", "time": "2021-02-05T15:19:10.362771"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.17354030907154083, 0.18171773850917816, 100, 0.477152049195735], "caller": "main.py:200", "time": "2021-02-05T15:20:40.866459"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.17454911768436432, 0.18099743127822876, 100, 0.47209548736858103], "caller": "main.py:200", "time": "2021-02-05T15:22:06.346146"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.17708776891231537, 0.17474792897701263, 100, 0.48483785892644987], "caller": "main.py:200", "time": "2021-02-05T15:23:35.879784"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.1726224720478058, 0.18245753645896912, 100, 0.47420934463765674], "caller": "main.py:200", "time": "2021-02-05T15:25:03.874717"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.1731290966272354, 0.16926154494285583, 100, 0.4733942502263651], "caller": "main.py:200", "time": "2021-02-05T15:26:32.251196"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.17242690920829773, 0.17682361602783203, 100, 0.48489477897801886], "caller": "main.py:200", "time": "2021-02-05T15:28:00.752130"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.1752544641494751, 0.16723918914794922, 100, 0.4982103597619618], "caller": "main.py:200", "time": "2021-02-05T15:29:31.167742"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.17158356308937073, 0.1825881451368332, 100, 0.48809274459964735], "caller": "main.py:200", "time": "2021-02-05T15:30:57.676139"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.17244724929332733, 0.16203437745571136, 100, 0.4787316673515333], "caller": "main.py:200", "time": "2021-02-05T15:32:24.308568"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.1720200479030609, 0.16941167414188385, 100, 0.4935031058890843], "caller": "main.py:200", "time": "2021-02-05T15:33:55.779667"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.17164988815784454, 0.17034566402435303, 100, 0.48968215281834904], "caller": "main.py:200", "time": "2021-02-05T15:35:25.603746"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.1694876253604889, 0.17338427901268005, 100, 0.4795714744445928], "caller": "main.py:200", "time": "2021-02-05T15:36:54.424899"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.16973678767681122, 0.17965713143348694, 100, 0.4868069907536074], "caller": "main.py:200", "time": "2021-02-05T15:38:27.234400"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.17170387506484985, 0.18789556622505188, 100, 0.4992343979574695], "caller": "main.py:200", "time": "2021-02-05T15:39:55.229262"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.17017477750778198, 0.17959624528884888, 100, 0.5096224081006644], "caller": "main.py:200", "time": "2021-02-05T15:41:20.876320"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.16908888518810272, 0.17069727182388306, 100, 0.49814316506172746], "caller": "main.py:200", "time": "2021-02-05T15:42:47.981325"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.17097042500972748, 0.17136907577514648, 100, 0.4892194541799561], "caller": "main.py:200", "time": "2021-02-05T15:44:17.124444"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.17061111330986023, 0.15005525946617126, 100, 0.49337677920891354], "caller": "main.py:200", "time": "2021-02-05T15:45:43.936434"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.16755017638206482, 0.16053077578544617, 100, 0.49390390510388416], "caller": "main.py:200", "time": "2021-02-05T15:47:09.610245"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [33], "caller": "main.py:129", "time": "2021-02-05T15:48:36.924211"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T15:48:46.585261"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 17.706974115718367, 0.13522453846798918], "caller": "main.py:37", "time": "2021-02-05T15:48:50.548940"}
+{"level": "info", "fmt": "episode: %s", "args": [12.448898777415412], "caller": "main.py:149", "time": "2021-02-05T15:49:01.046673"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.16745683550834656, 0.16380155086517334, 100, 0.5109723939980704], "caller": "main.py:200", "time": "2021-02-05T15:49:03.887064"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.16499745845794678, 0.15278291702270508, 100, 0.5019473975394888], "caller": "main.py:200", "time": "2021-02-05T15:50:30.588817"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.16561615467071533, 0.16314294934272766, 100, 0.5014929690455576], "caller": "main.py:200", "time": "2021-02-05T15:51:58.610113"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.16752752661705017, 0.16043727099895477, 100, 0.4993169065810612], "caller": "main.py:200", "time": "2021-02-05T15:53:25.930984"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.16785413026809692, 0.16769945621490479, 100, 0.5099358859148305], "caller": "main.py:200", "time": "2021-02-05T15:54:52.925436"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.1663026660680771, 0.16455553472042084, 100, 0.4996013158060198], "caller": "main.py:200", "time": "2021-02-05T15:56:18.366402"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.1652577966451645, 0.16642464697360992, 100, 0.5144088252864459], "caller": "main.py:200", "time": "2021-02-05T15:57:49.838143"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.16496345400810242, 0.15739014744758606, 100, 0.5024287011717], "caller": "main.py:200", "time": "2021-02-05T15:59:13.844434"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.16539110243320465, 0.18489539623260498, 100, 0.5201270436720474], "caller": "main.py:200", "time": "2021-02-05T16:00:35.694461"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.16611894965171814, 0.16304701566696167, 100, 0.5246628030609943], "caller": "main.py:200", "time": "2021-02-05T16:02:07.215852"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.16457916796207428, 0.1686747670173645, 100, 0.5150598245344087], "caller": "main.py:200", "time": "2021-02-05T16:03:34.285842"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.1643293797969818, 0.14939138293266296, 100, 0.5257402854632113], "caller": "main.py:200", "time": "2021-02-05T16:04:56.734137"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.1656329333782196, 0.17181605100631714, 100, 0.5187357192712847], "caller": "main.py:200", "time": "2021-02-05T16:06:24.599677"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.16569574177265167, 0.15604546666145325, 100, 0.5244392427277647], "caller": "main.py:200", "time": "2021-02-05T16:07:47.076742"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.1642809361219406, 0.1650693714618683, 100, 0.5162313517411893], "caller": "main.py:200", "time": "2021-02-05T16:09:11.642370"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.16294021904468536, 0.17118597030639648, 100, 0.5144797533236055], "caller": "main.py:200", "time": "2021-02-05T16:10:38.258637"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.16355398297309875, 0.15544041991233826, 100, 0.5114186709607829], "caller": "main.py:200", "time": "2021-02-05T16:12:07.386480"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.16318799555301666, 0.18283115327358246, 100, 0.5096066128698005], "caller": "main.py:200", "time": "2021-02-05T16:13:31.936048"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.16570410132408142, 0.17086274921894073, 100, 0.5268996989200121], "caller": "main.py:200", "time": "2021-02-05T16:14:57.451572"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.1640864908695221, 0.16514348983764648, 100, 0.5231522306276737], "caller": "main.py:200", "time": "2021-02-05T16:16:21.368579"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [34], "caller": "main.py:129", "time": "2021-02-05T16:17:43.841274"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T16:17:53.305193"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 15.037162500960871, 0.15385545283588228], "caller": "main.py:37", "time": "2021-02-05T16:17:57.090192"}
+{"level": "info", "fmt": "episode: %s", "args": [12.326607634672657], "caller": "main.py:149", "time": "2021-02-05T16:18:07.410394"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.16276878118515015, 0.18347889184951782, 100, 0.5396670233663672], "caller": "main.py:200", "time": "2021-02-05T16:18:10.104945"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.16535936295986176, 0.16056197881698608, 100, 0.5549235784452553], "caller": "main.py:200", "time": "2021-02-05T16:19:40.775087"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.16255173087120056, 0.1627376675605774, 100, 0.5438064640216616], "caller": "main.py:200", "time": "2021-02-05T16:21:03.876471"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.1635061502456665, 0.17676837742328644, 100, 0.5353979774774165], "caller": "main.py:200", "time": "2021-02-05T16:22:26.879971"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.16236811876296997, 0.16776913404464722, 100, 0.5547988911908389], "caller": "main.py:200", "time": "2021-02-05T16:23:49.090601"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.16334763169288635, 0.18518176674842834, 100, 0.5577205201425731], "caller": "main.py:200", "time": "2021-02-05T16:25:14.745275"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.16081959009170532, 0.15347588062286377, 100, 0.5326223298374658], "caller": "main.py:200", "time": "2021-02-05T16:26:42.605897"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.15844063460826874, 0.16167022287845612, 100, 0.5232394039107651], "caller": "main.py:200", "time": "2021-02-05T16:28:13.094942"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.16087506711483002, 0.15695007145404816, 100, 0.5486255067038274], "caller": "main.py:200", "time": "2021-02-05T16:29:38.326978"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.16168837249279022, 0.14679256081581116, 100, 0.5625643890533552], "caller": "main.py:200", "time": "2021-02-05T16:31:02.352891"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.15799793601036072, 0.16607750952243805, 100, 0.5211017414406381], "caller": "main.py:200", "time": "2021-02-05T16:32:25.751626"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.16150470077991486, 0.16013950109481812, 100, 0.5475370832730165], "caller": "main.py:200", "time": "2021-02-05T16:33:52.938020"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.16174037754535675, 0.1507706195116043, 100, 0.5413220983202346], "caller": "main.py:200", "time": "2021-02-05T16:35:16.111788"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.16072022914886475, 0.17127542197704315, 100, 0.5426444690876957], "caller": "main.py:200", "time": "2021-02-05T16:36:39.629307"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.1590816080570221, 0.15782898664474487, 100, 0.5806484013215205], "caller": "main.py:200", "time": "2021-02-05T16:38:05.393732"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.16009117662906647, 0.13972607254981995, 100, 0.545979267304182], "caller": "main.py:200", "time": "2021-02-05T16:39:37.250854"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.15727263689041138, 0.14494669437408447, 100, 0.5562564861218189], "caller": "main.py:200", "time": "2021-02-05T16:41:04.839377"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.15810203552246094, 0.1554487645626068, 100, 0.5380888407884381], "caller": "main.py:200", "time": "2021-02-05T16:42:31.547647"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.15713781118392944, 0.1630113422870636, 100, 0.5743645089290367], "caller": "main.py:200", "time": "2021-02-05T16:43:59.273613"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.15716145932674408, 0.13763931393623352, 100, 0.553720186595381], "caller": "main.py:200", "time": "2021-02-05T16:45:24.303014"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [35], "caller": "main.py:129", "time": "2021-02-05T16:46:44.494452"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T16:46:53.684926"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 17.9494064010419, 0.16402096602519928], "caller": "main.py:37", "time": "2021-02-05T16:46:57.757568"}
+{"level": "info", "fmt": "episode: %s", "args": [13.083842823340547], "caller": "main.py:149", "time": "2021-02-05T16:47:07.959551"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.16073079407215118, 0.14063206315040588, 100, 0.5510933851644647], "caller": "main.py:200", "time": "2021-02-05T16:47:10.255116"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.16037537157535553, 0.16484735906124115, 100, 0.5731983345173796], "caller": "main.py:200", "time": "2021-02-05T16:48:38.768289"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.15903455018997192, 0.16474862396717072, 100, 0.5687525316504116], "caller": "main.py:200", "time": "2021-02-05T16:50:08.855341"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.15733657777309418, 0.14897142350673676, 100, 0.5456480058482024], "caller": "main.py:200", "time": "2021-02-05T16:51:38.229751"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.1597820520401001, 0.1618148535490036, 100, 0.5929534261825794], "caller": "main.py:200", "time": "2021-02-05T16:53:02.251713"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.15845923125743866, 0.15499483048915863, 100, 0.5864655640134856], "caller": "main.py:200", "time": "2021-02-05T16:54:32.018602"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.16032959520816803, 0.1512317806482315, 100, 0.5502694931076051], "caller": "main.py:200", "time": "2021-02-05T16:55:55.661211"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.1567213535308838, 0.18217968940734863, 100, 0.5608607649242301], "caller": "main.py:200", "time": "2021-02-05T16:57:20.167975"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.1571219563484192, 0.16671036183834076, 100, 0.5714563565874848], "caller": "main.py:200", "time": "2021-02-05T16:58:46.479626"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.1564205437898636, 0.16008886694908142, 100, 0.5698385238077622], "caller": "main.py:200", "time": "2021-02-05T17:00:17.437445"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.15745213627815247, 0.16144932806491852, 100, 0.5632947259578078], "caller": "main.py:200", "time": "2021-02-05T17:01:41.883507"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.1561327576637268, 0.15331216156482697, 100, 0.5685712035921869], "caller": "main.py:200", "time": "2021-02-05T17:03:04.646674"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.1584150642156601, 0.15955963730812073, 100, 0.5904421108370663], "caller": "main.py:200", "time": "2021-02-05T17:04:31.738776"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.15458205342292786, 0.15711891651153564, 100, 0.5573030924239756], "caller": "main.py:200", "time": "2021-02-05T17:06:03.569623"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.159024178981781, 0.14748957753181458, 100, 0.6233315058919325], "caller": "main.py:200", "time": "2021-02-05T17:07:38.340143"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.15560422837734222, 0.15665920078754425, 100, 0.5917638515834323], "caller": "main.py:200", "time": "2021-02-05T17:09:10.484914"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.15402743220329285, 0.1436639130115509, 100, 0.5620651176009781], "caller": "main.py:200", "time": "2021-02-05T17:10:32.629805"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.1561068296432495, 0.16623516380786896, 100, 0.5784308212417099], "caller": "main.py:200", "time": "2021-02-05T17:11:54.838280"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.15468230843544006, 0.14645501971244812, 100, 0.5639898123773207], "caller": "main.py:200", "time": "2021-02-05T17:13:24.092879"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.15579164028167725, 0.15550899505615234, 100, 0.5998732980485905], "caller": "main.py:200", "time": "2021-02-05T17:14:50.936463"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [36], "caller": "main.py:129", "time": "2021-02-05T17:16:16.915236"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T17:16:26.142860"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 18.23246566315595, 0.18521031522224468], "caller": "main.py:37", "time": "2021-02-05T17:16:30.372904"}
+{"level": "info", "fmt": "episode: %s", "args": [8.43201993668761], "caller": "main.py:149", "time": "2021-02-05T17:16:40.615061"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.15603886544704437, 0.1681937873363495, 100, 0.5887661346204409], "caller": "main.py:200", "time": "2021-02-05T17:16:43.613268"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.1575382500886917, 0.17015808820724487, 100, 0.6160575988272683], "caller": "main.py:200", "time": "2021-02-05T17:18:11.615587"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.15577992796897888, 0.1565239429473877, 100, 0.5986010601517984], "caller": "main.py:200", "time": "2021-02-05T17:19:39.120680"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.15652146935462952, 0.1704825758934021, 100, 0.575880591513743], "caller": "main.py:200", "time": "2021-02-05T17:21:06.701717"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.15586093068122864, 0.15397872030735016, 100, 0.6084813001143419], "caller": "main.py:200", "time": "2021-02-05T17:22:30.813376"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.15204766392707825, 0.16094988584518433, 100, 0.5894357153183872], "caller": "main.py:200", "time": "2021-02-05T17:23:56.888532"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.15379586815834045, 0.15837299823760986, 100, 0.5865601315507839], "caller": "main.py:200", "time": "2021-02-05T17:25:24.892643"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.15258938074111938, 0.1409142166376114, 100, 0.5954036676288295], "caller": "main.py:200", "time": "2021-02-05T17:26:48.471421"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.15388889610767365, 0.17871609330177307, 100, 0.6224148019525862], "caller": "main.py:200", "time": "2021-02-05T17:28:13.172427"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.15417703986167908, 0.15551745891571045, 100, 0.6028065814965164], "caller": "main.py:200", "time": "2021-02-05T17:29:39.553529"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.1542772501707077, 0.14834241569042206, 100, 0.6318301224076728], "caller": "main.py:200", "time": "2021-02-05T17:31:06.917570"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.15483219921588898, 0.13535957038402557, 100, 0.5937132104637572], "caller": "main.py:200", "time": "2021-02-05T17:32:33.392634"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.15157493948936462, 0.17111165821552277, 100, 0.5788827359097479], "caller": "main.py:200", "time": "2021-02-05T17:34:01.393081"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.153750479221344, 0.14841489493846893, 100, 0.6300193249549272], "caller": "main.py:200", "time": "2021-02-05T17:35:28.184659"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.1530635803937912, 0.16569140553474426, 100, 0.6242054324717974], "caller": "main.py:200", "time": "2021-02-05T17:36:56.618695"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.15301239490509033, 0.1608687937259674, 100, 0.6004149159188671], "caller": "main.py:200", "time": "2021-02-05T17:38:20.321905"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.15271534025669098, 0.15057477355003357, 100, 0.6063123946775661], "caller": "main.py:200", "time": "2021-02-05T17:39:41.666208"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.1539148986339569, 0.14309167861938477, 100, 0.6121666114910218], "caller": "main.py:200", "time": "2021-02-05T17:41:10.160695"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.1516590416431427, 0.16557645797729492, 100, 0.6178155141451048], "caller": "main.py:200", "time": "2021-02-05T17:42:32.981903"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.018945543095469475, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T17:43:13.085964"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.1518707275390625, 0.1525948941707611, 100, 0.6015885665411176], "caller": "main.py:200", "time": "2021-02-05T17:43:59.226174"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [37], "caller": "main.py:129", "time": "2021-02-05T17:45:23.111673"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T17:45:32.353244"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 19.03400716707234, 0.09458079141028487], "caller": "main.py:37", "time": "2021-02-05T17:45:36.105147"}
+{"level": "info", "fmt": "episode: %s", "args": [9.583117544874678], "caller": "main.py:149", "time": "2021-02-05T17:45:46.391509"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.15628349781036377, 0.1638660728931427, 100, 0.608241512714597], "caller": "main.py:200", "time": "2021-02-05T17:45:49.842595"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.15362654626369476, 0.15984302759170532, 100, 0.5856771593700288], "caller": "main.py:200", "time": "2021-02-05T17:47:12.038577"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.1555958390235901, 0.17396953701972961, 100, 0.6416894319011822], "caller": "main.py:200", "time": "2021-02-05T17:48:41.046693"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.15406768023967743, 0.13808251917362213, 100, 0.5959688662886697], "caller": "main.py:200", "time": "2021-02-05T17:50:11.435509"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.15307115018367767, 0.14780321717262268, 100, 0.5914779748921195], "caller": "main.py:200", "time": "2021-02-05T17:51:35.335750"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.15491458773612976, 0.14629203081130981, 100, 0.6226781662675929], "caller": "main.py:200", "time": "2021-02-05T17:52:56.783804"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.15306776762008667, 0.1577581912279129, 100, 0.6162890603522085], "caller": "main.py:200", "time": "2021-02-05T17:54:27.941049"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.15399056673049927, 0.1596980094909668, 100, 0.6686902275012806], "caller": "main.py:200", "time": "2021-02-05T17:55:54.269806"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.1529988795518875, 0.16074104607105255, 100, 0.603580182432375], "caller": "main.py:200", "time": "2021-02-05T17:57:20.817602"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.1547670215368271, 0.1314866989850998, 100, 0.6298241122927267], "caller": "main.py:200", "time": "2021-02-05T17:58:53.797249"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.15130303800106049, 0.15498283505439758, 100, 0.6077804412830063], "caller": "main.py:200", "time": "2021-02-05T18:00:20.906694"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.1530722975730896, 0.13757243752479553, 100, 0.6091595520960092], "caller": "main.py:200", "time": "2021-02-05T18:01:44.676109"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.15338121354579926, 0.16664880514144897, 100, 0.6117073308809833], "caller": "main.py:200", "time": "2021-02-05T18:03:14.810764"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.1512027233839035, 0.16225749254226685, 100, 0.6202296366667407], "caller": "main.py:200", "time": "2021-02-05T18:04:44.682944"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.15555913746356964, 0.1435421109199524, 100, 0.6615419142776797], "caller": "main.py:200", "time": "2021-02-05T18:06:07.145040"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.15427039563655853, 0.14726218581199646, 100, 0.6375488921362383], "caller": "main.py:200", "time": "2021-02-05T18:07:29.709429"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.15332487225532532, 0.17324435710906982, 100, 0.6382591488319146], "caller": "main.py:200", "time": "2021-02-05T18:08:52.058688"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.15285950899124146, 0.14225095510482788, 100, 0.6191774790859934], "caller": "main.py:200", "time": "2021-02-05T18:10:14.407931"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.15184082090854645, 0.15003828704357147, 100, 0.6025949808352598], "caller": "main.py:200", "time": "2021-02-05T18:11:38.676631"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.15293259918689728, 0.15264657139778137, 100, 0.6788430716908629], "caller": "main.py:200", "time": "2021-02-05T18:13:04.146575"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [38], "caller": "main.py:129", "time": "2021-02-05T18:14:26.380252"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T18:14:35.889441"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 18.949088552789313, 0.07847080907005273], "caller": "main.py:37", "time": "2021-02-05T18:14:39.771037"}
+{"level": "info", "fmt": "episode: %s", "args": [11.6780435323354], "caller": "main.py:149", "time": "2021-02-05T18:14:50.092221"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.15213803946971893, 0.15590593218803406, 100, 0.6366941385824564], "caller": "main.py:200", "time": "2021-02-05T18:14:52.328710"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.15089412033557892, 0.14127126336097717, 100, 0.6318735777699883], "caller": "main.py:200", "time": "2021-02-05T18:16:17.935031"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.15100057423114777, 0.15792188048362732, 100, 0.6096320193396334], "caller": "main.py:200", "time": "2021-02-05T18:17:41.772048"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.1515776664018631, 0.15590496361255646, 100, 0.6382189165907649], "caller": "main.py:200", "time": "2021-02-05T18:19:10.086243"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.15165433287620544, 0.1712038367986679, 100, 0.6575054370699459], "caller": "main.py:200", "time": "2021-02-05T18:20:38.575352"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.15164335072040558, 0.16348674893379211, 100, 0.6176147418596635], "caller": "main.py:200", "time": "2021-02-05T18:22:04.286981"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.15045826137065887, 0.1488492488861084, 100, 0.6216968160245995], "caller": "main.py:200", "time": "2021-02-05T18:23:29.486648"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.15045872330665588, 0.1450270116329193, 100, 0.6287191378441563], "caller": "main.py:200", "time": "2021-02-05T18:24:52.277936"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.1489865779876709, 0.13644981384277344, 100, 0.6657902976204747], "caller": "main.py:200", "time": "2021-02-05T18:26:21.900397"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.15122902393341064, 0.1605420857667923, 100, 0.6168608021119285], "caller": "main.py:200", "time": "2021-02-05T18:27:49.249083"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.14894653856754303, 0.15638324618339539, 100, 0.6211016150687912], "caller": "main.py:200", "time": "2021-02-05T18:29:13.806621"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.15069027245044708, 0.1557503640651703, 100, 0.686511061420931], "caller": "main.py:200", "time": "2021-02-05T18:30:36.982920"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.1494782716035843, 0.15317794680595398, 100, 0.645747111077107], "caller": "main.py:200", "time": "2021-02-05T18:32:02.002454"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.1507234126329422, 0.14739209413528442, 100, 0.626375490662403], "caller": "main.py:200", "time": "2021-02-05T18:33:27.550920"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.14938753843307495, 0.13960295915603638, 100, 0.6404806047037513], "caller": "main.py:200", "time": "2021-02-05T18:34:51.880103"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.14956535398960114, 0.1543305367231369, 100, 0.6270901190607499], "caller": "main.py:200", "time": "2021-02-05T18:36:16.729723"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.14992651343345642, 0.15261775255203247, 100, 0.6268122782676449], "caller": "main.py:200", "time": "2021-02-05T18:37:40.792989"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.1518053114414215, 0.15924733877182007, 100, 0.6548272042811741], "caller": "main.py:200", "time": "2021-02-05T18:39:05.641837"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.14826150238513947, 0.19014254212379456, 100, 0.6103014954314282], "caller": "main.py:200", "time": "2021-02-05T18:40:32.996388"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.1487198919057846, 0.14189518988132477, 100, 0.6376884850221954], "caller": "main.py:200", "time": "2021-02-05T18:41:59.932219"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [39], "caller": "main.py:129", "time": "2021-02-05T18:43:22.370045"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T18:43:31.915345"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 18.993846161043752, 0.14861341970938424], "caller": "main.py:37", "time": "2021-02-05T18:43:35.861109"}
+{"level": "info", "fmt": "episode: %s", "args": [9.363619950575968], "caller": "main.py:149", "time": "2021-02-05T18:43:46.435237"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.14859308302402496, 0.14681580662727356, 100, 0.6137564110142164], "caller": "main.py:200", "time": "2021-02-05T18:43:48.706626"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.14920063316822052, 0.14742085337638855, 100, 0.6256606974572132], "caller": "main.py:200", "time": "2021-02-05T18:45:15.471972"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.14930953085422516, 0.14520935714244843, 100, 0.6777526652135205], "caller": "main.py:200", "time": "2021-02-05T18:46:40.406273"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.1481243073940277, 0.14201125502586365, 100, 0.6117727815493117], "caller": "main.py:200", "time": "2021-02-05T18:48:06.447422"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.14784307777881622, 0.1389731764793396, 100, 0.6395058979944639], "caller": "main.py:200", "time": "2021-02-05T18:49:31.664393"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.1463274359703064, 0.15811941027641296, 100, 0.6082934659111174], "caller": "main.py:200", "time": "2021-02-05T18:50:56.326330"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.14697904884815216, 0.16036373376846313, 100, 0.5904055928593555], "caller": "main.py:200", "time": "2021-02-05T18:52:24.168532"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.1471177339553833, 0.14253950119018555, 100, 0.6393484988169283], "caller": "main.py:200", "time": "2021-02-05T18:53:50.540266"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.1462676227092743, 0.13975748419761658, 100, 0.6540780808867598], "caller": "main.py:200", "time": "2021-02-05T18:55:16.634048"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.14694136381149292, 0.14315703511238098, 100, 0.6496898763722182], "caller": "main.py:200", "time": "2021-02-05T18:56:41.673241"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.147854745388031, 0.14955592155456543, 100, 0.6404553019360003], "caller": "main.py:200", "time": "2021-02-05T18:58:05.845597"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.1460854858160019, 0.14337998628616333, 100, 0.6040281649819976], "caller": "main.py:200", "time": "2021-02-05T18:59:30.114709"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.14711536467075348, 0.1456441879272461, 100, 0.6347478499373835], "caller": "main.py:200", "time": "2021-02-05T19:00:54.225370"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.144722580909729, 0.13783304393291473, 100, 0.6174071278550722], "caller": "main.py:200", "time": "2021-02-05T19:02:21.328269"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.14386603236198425, 0.13339018821716309, 100, 0.6153641312699074], "caller": "main.py:200", "time": "2021-02-05T19:03:46.448996"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.14410556852817535, 0.16108207404613495, 100, 0.6136148094517306], "caller": "main.py:200", "time": "2021-02-05T19:05:12.144938"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.1466231793165207, 0.15718689560890198, 100, 0.6720506154818825], "caller": "main.py:200", "time": "2021-02-05T19:06:38.561340"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.14311081171035767, 0.15506449341773987, 100, 0.629424038764477], "caller": "main.py:200", "time": "2021-02-05T19:08:05.393256"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.1444735825061798, 0.15986277163028717, 100, 0.6264909573762967], "caller": "main.py:200", "time": "2021-02-05T19:09:30.804962"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.14582590758800507, 0.13747751712799072, 100, 0.627610120114508], "caller": "main.py:200", "time": "2021-02-05T19:10:55.772153"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [40], "caller": "main.py:129", "time": "2021-02-05T19:12:19.810566"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T19:12:29.237229"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 18.67775652819425, 0.16145568232213092], "caller": "main.py:37", "time": "2021-02-05T19:12:33.227481"}
+{"level": "info", "fmt": "episode: %s", "args": [14.868754149413185], "caller": "main.py:149", "time": "2021-02-05T19:12:43.455915"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.1464328169822693, 0.1456088423728943, 100, 0.6660842048455773], "caller": "main.py:200", "time": "2021-02-05T19:12:45.662656"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.14552968740463257, 0.14433321356773376, 100, 0.6282688241568768], "caller": "main.py:200", "time": "2021-02-05T19:14:12.861321"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.14572101831436157, 0.1537698209285736, 100, 0.625219132539693], "caller": "main.py:200", "time": "2021-02-05T19:15:37.069885"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.14215756952762604, 0.14591476321220398, 100, 0.6028843989365416], "caller": "main.py:200", "time": "2021-02-05T19:17:01.099614"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.14457613229751587, 0.15497544407844543, 100, 0.6314941581452208], "caller": "main.py:200", "time": "2021-02-05T19:18:27.564480"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.14401622116565704, 0.13329969346523285, 100, 0.6234757732721556], "caller": "main.py:200", "time": "2021-02-05T19:19:55.729800"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.14482159912586212, 0.15362581610679626, 100, 0.6456802868197399], "caller": "main.py:200", "time": "2021-02-05T19:21:22.683784"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.14271214604377747, 0.15153127908706665, 100, 0.6098832916603557], "caller": "main.py:200", "time": "2021-02-05T19:22:47.998809"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.14229659736156464, 0.15315702557563782, 100, 0.6891965966727567], "caller": "main.py:200", "time": "2021-02-05T19:24:12.288869"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.14391110837459564, 0.14723040163516998, 100, 0.6472413753815954], "caller": "main.py:200", "time": "2021-02-05T19:25:39.664578"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.14413484930992126, 0.1424255520105362, 100, 0.6093275620924067], "caller": "main.py:200", "time": "2021-02-05T19:27:05.066711"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.1442677080631256, 0.1478559523820877, 100, 0.6382998537378973], "caller": "main.py:200", "time": "2021-02-05T19:28:30.090260"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.1437661349773407, 0.15200942754745483, 100, 0.6158261828998813], "caller": "main.py:200", "time": "2021-02-05T19:29:55.646679"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.1431856006383896, 0.14786094427108765, 100, 0.6176468800880767], "caller": "main.py:200", "time": "2021-02-05T19:31:21.358208"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.14422070980072021, 0.14588278532028198, 100, 0.6723285427017224], "caller": "main.py:200", "time": "2021-02-05T19:32:46.814451"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.14141178131103516, 0.14315170049667358, 100, 0.6034934934370224], "caller": "main.py:200", "time": "2021-02-05T19:34:12.491931"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.14199158549308777, 0.1431942880153656, 100, 0.6722635119481164], "caller": "main.py:200", "time": "2021-02-05T19:35:36.298799"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.145663782954216, 0.1543715000152588, 100, 0.6402083560184186], "caller": "main.py:200", "time": "2021-02-05T19:37:00.240922"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.14293108880519867, 0.1408960521221161, 100, 0.5991089218255795], "caller": "main.py:200", "time": "2021-02-05T19:38:24.090003"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.14345405995845795, 0.15792477130889893, 100, 0.6391117849306911], "caller": "main.py:200", "time": "2021-02-05T19:39:48.753536"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [41], "caller": "main.py:129", "time": "2021-02-05T19:41:10.251963"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T19:41:19.810495"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 18.980115983482754, 0.07741406102211772], "caller": "main.py:37", "time": "2021-02-05T19:41:23.740787"}
+{"level": "info", "fmt": "episode: %s", "args": [4.964985133946447], "caller": "main.py:149", "time": "2021-02-05T19:41:34.294615"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.14309045672416687, 0.13281862437725067, 100, 0.6174481188633498], "caller": "main.py:200", "time": "2021-02-05T19:41:37.158158"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.14063379168510437, 0.14995130896568298, 100, 0.6090799804912885], "caller": "main.py:200", "time": "2021-02-05T19:43:02.039758"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.1415451318025589, 0.13690724968910217, 100, 0.6527795790973198], "caller": "main.py:200", "time": "2021-02-05T19:44:25.566587"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.1414792537689209, 0.15006843209266663, 100, 0.6384567221360712], "caller": "main.py:200", "time": "2021-02-05T19:45:52.066569"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.13905847072601318, 0.1404450535774231, 100, 0.5959670236110767], "caller": "main.py:200", "time": "2021-02-05T19:47:15.972965"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.13997158408164978, 0.1490594446659088, 100, 0.6222574152920262], "caller": "main.py:200", "time": "2021-02-05T19:48:40.209502"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.14045684039592743, 0.13880300521850586, 100, 0.609526394544684], "caller": "main.py:200", "time": "2021-02-05T19:50:04.258700"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.1408408135175705, 0.1414531171321869, 100, 0.616605857549042], "caller": "main.py:200", "time": "2021-02-05T19:51:27.848633"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.14058023691177368, 0.14306727051734924, 100, 0.6222005166981293], "caller": "main.py:200", "time": "2021-02-05T19:52:51.784605"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.13754403591156006, 0.15487927198410034, 100, 0.6057165717472897], "caller": "main.py:200", "time": "2021-02-05T19:54:15.820726"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.14136779308319092, 0.14568689465522766, 100, 0.6518194102589148], "caller": "main.py:200", "time": "2021-02-05T19:55:39.102955"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.141440749168396, 0.14369811117649078, 100, 0.6752208136836987], "caller": "main.py:200", "time": "2021-02-05T19:57:03.098872"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.13881732523441315, 0.1280345767736435, 100, 0.6301066943372999], "caller": "main.py:200", "time": "2021-02-05T19:58:26.872528"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.14047332108020782, 0.13665321469306946, 100, 0.6227485927316667], "caller": "main.py:200", "time": "2021-02-05T19:59:50.866886"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.1416865438222885, 0.1359114944934845, 100, 0.6469706361838915], "caller": "main.py:200", "time": "2021-02-05T20:01:14.621433"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.13809119164943695, 0.14487528800964355, 100, 0.6273776915042064], "caller": "main.py:200", "time": "2021-02-05T20:02:41.328353"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.13938261568546295, 0.14948466420173645, 100, 0.6173015409090722], "caller": "main.py:200", "time": "2021-02-05T20:04:05.453145"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.13903623819351196, 0.13641175627708435, 100, 0.6380916788535989], "caller": "main.py:200", "time": "2021-02-05T20:05:29.851727"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.13766169548034668, 0.14612236618995667, 100, 0.6358441486318845], "caller": "main.py:200", "time": "2021-02-05T20:06:54.915249"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.13836009800434113, 0.13553130626678467, 100, 0.6225537651154714], "caller": "main.py:200", "time": "2021-02-05T20:08:19.052357"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [42], "caller": "main.py:129", "time": "2021-02-05T20:09:40.649505"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T20:09:50.221987"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 17.938087574247024, 0.05564844890363167], "caller": "main.py:37", "time": "2021-02-05T20:09:54.214299"}
+{"level": "info", "fmt": "episode: %s", "args": [4.08361509759149], "caller": "main.py:149", "time": "2021-02-05T20:10:04.629767"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.14073733985424042, 0.12830977141857147, 100, 0.6385449668126524], "caller": "main.py:200", "time": "2021-02-05T20:10:07.745639"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.1377803087234497, 0.15370477735996246, 100, 0.5878521733888238], "caller": "main.py:200", "time": "2021-02-05T20:11:32.502604"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.1374121606349945, 0.16154055297374725, 100, 0.5862357469806946], "caller": "main.py:200", "time": "2021-02-05T20:12:56.685672"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.1371416449546814, 0.14036808907985687, 100, 0.5974330630301043], "caller": "main.py:200", "time": "2021-02-05T20:14:20.577728"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.13872605562210083, 0.12254761904478073, 100, 0.6089709948884438], "caller": "main.py:200", "time": "2021-02-05T20:15:43.882676"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.1359744369983673, 0.1330808848142624, 100, 0.5915393354897238], "caller": "main.py:200", "time": "2021-02-05T20:17:07.859535"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.13764019310474396, 0.15602603554725647, 100, 0.6511611317937096], "caller": "main.py:200", "time": "2021-02-05T20:18:31.067116"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.1378808170557022, 0.1334804892539978, 100, 0.6343547838291961], "caller": "main.py:200", "time": "2021-02-05T20:19:54.867741"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.13773344457149506, 0.1509898602962494, 100, 0.6402233525470427], "caller": "main.py:200", "time": "2021-02-05T20:21:19.114116"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.13612353801727295, 0.13934817910194397, 100, 0.5993291472789343], "caller": "main.py:200", "time": "2021-02-05T20:22:42.438962"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.13774491846561432, 0.14021089673042297, 100, 0.6382727807121966], "caller": "main.py:200", "time": "2021-02-05T20:24:06.575948"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.1373550444841385, 0.14191758632659912, 100, 0.6146625303607445], "caller": "main.py:200", "time": "2021-02-05T20:25:30.860962"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.13694727420806885, 0.14028877019882202, 100, 0.6357159771879581], "caller": "main.py:200", "time": "2021-02-05T20:26:55.903116"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.13580942153930664, 0.14628475904464722, 100, 0.5922071974877382], "caller": "main.py:200", "time": "2021-02-05T20:28:19.854649"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.13414296507835388, 0.13094903528690338, 100, 0.6284601732460033], "caller": "main.py:200", "time": "2021-02-05T20:29:46.047795"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.1375882923603058, 0.14307129383087158, 100, 0.607912862836128], "caller": "main.py:200", "time": "2021-02-05T20:31:10.095536"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.1372646689414978, 0.14648041129112244, 100, 0.594089929104047], "caller": "main.py:200", "time": "2021-02-05T20:32:33.959773"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.1344330608844757, 0.1427532136440277, 100, 0.6212354245160664], "caller": "main.py:200", "time": "2021-02-05T20:33:57.770722"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.13795135915279388, 0.147400364279747, 100, 0.6300220301117301], "caller": "main.py:200", "time": "2021-02-05T20:35:21.389439"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.13822536170482635, 0.1471600979566574, 100, 0.6567018475471876], "caller": "main.py:200", "time": "2021-02-05T20:36:45.068412"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [43], "caller": "main.py:129", "time": "2021-02-05T20:38:06.869422"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T20:38:16.397300"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 18.57986175131599, 0.07817622327919645], "caller": "main.py:37", "time": "2021-02-05T20:38:20.349630"}
+{"level": "info", "fmt": "episode: %s", "args": [12.149704805659281], "caller": "main.py:149", "time": "2021-02-05T20:38:30.757815"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.13435876369476318, 0.13365983963012695, 100, 0.6231141131493204], "caller": "main.py:200", "time": "2021-02-05T20:38:32.809546"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.13492518663406372, 0.14466507732868195, 100, 0.6316254010164922], "caller": "main.py:200", "time": "2021-02-05T20:40:00.266183"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.13467064499855042, 0.14452794194221497, 100, 0.6005115034575766], "caller": "main.py:200", "time": "2021-02-05T20:41:25.442535"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.13379980623722076, 0.11821526288986206, 100, 0.6049332454195673], "caller": "main.py:200", "time": "2021-02-05T20:42:48.813473"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.13280393183231354, 0.13868337869644165, 100, 0.5647796713741098], "caller": "main.py:200", "time": "2021-02-05T20:44:12.567640"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.1342984437942505, 0.13027355074882507, 100, 0.595728522479566], "caller": "main.py:200", "time": "2021-02-05T20:45:35.449876"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.13219434022903442, 0.14046119153499603, 100, 0.5969763901232743], "caller": "main.py:200", "time": "2021-02-05T20:46:59.104540"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.13513389229774475, 0.1382291316986084, 100, 0.5847259306322928], "caller": "main.py:200", "time": "2021-02-05T20:48:22.748641"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.1342790573835373, 0.12307988107204437, 100, 0.5860057743797806], "caller": "main.py:200", "time": "2021-02-05T20:49:45.973360"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.13494873046875, 0.1490517258644104, 100, 0.600721711158285], "caller": "main.py:200", "time": "2021-02-05T20:51:09.668740"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.13456466794013977, 0.13823166489601135, 100, 0.6109152650222215], "caller": "main.py:200", "time": "2021-02-05T20:52:32.546961"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.13480590283870697, 0.11662513017654419, 100, 0.5946199708582398], "caller": "main.py:200", "time": "2021-02-05T20:53:56.448110"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.13491347432136536, 0.11911031603813171, 100, 0.6414787938549387], "caller": "main.py:200", "time": "2021-02-05T20:55:20.083935"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.13309741020202637, 0.12995117902755737, 100, 0.6226864918443743], "caller": "main.py:200", "time": "2021-02-05T20:56:43.514116"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.13404519855976105, 0.1334749311208725, 100, 0.6051645248523178], "caller": "main.py:200", "time": "2021-02-05T20:58:07.440814"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.13324400782585144, 0.14066001772880554, 100, 0.601408054768503], "caller": "main.py:200", "time": "2021-02-05T20:59:30.634595"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.13215011358261108, 0.13608185946941376, 100, 0.6089861419187957], "caller": "main.py:200", "time": "2021-02-05T21:00:54.499095"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.13234135508537292, 0.14273661375045776, 100, 0.5996875944133933], "caller": "main.py:200", "time": "2021-02-05T21:02:18.062439"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.13315948843955994, 0.11500194668769836, 100, 0.600707634627353], "caller": "main.py:200", "time": "2021-02-05T21:03:41.828340"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.1320326179265976, 0.14223375916481018, 100, 0.6214819266770677], "caller": "main.py:200", "time": "2021-02-05T21:05:05.751043"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [44], "caller": "main.py:129", "time": "2021-02-05T21:06:27.098685"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T21:06:36.611775"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 18.931676371051815, 0.11812479990230727], "caller": "main.py:37", "time": "2021-02-05T21:06:40.582552"}
+{"level": "info", "fmt": "episode: %s", "args": [10.066318529080869], "caller": "main.py:149", "time": "2021-02-05T21:06:50.939188"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.13346999883651733, 0.148898646235466, 100, 0.626911346849693], "caller": "main.py:200", "time": "2021-02-05T21:06:53.141672"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.13181859254837036, 0.13763169944286346, 100, 0.5615723224912785], "caller": "main.py:200", "time": "2021-02-05T21:08:16.944601"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.1315072774887085, 0.13686181604862213, 100, 0.5937036239507157], "caller": "main.py:200", "time": "2021-02-05T21:09:39.890836"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.13208423554897308, 0.13902811706066132, 100, 0.5960454224944116], "caller": "main.py:200", "time": "2021-02-05T21:11:03.739667"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.13256755471229553, 0.1282176971435547, 100, 0.603293935298195], "caller": "main.py:200", "time": "2021-02-05T21:12:27.148942"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.13149268925189972, 0.15323883295059204, 100, 0.5910336699490792], "caller": "main.py:200", "time": "2021-02-05T21:13:51.092995"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.13217072188854218, 0.13278518617153168, 100, 0.5860658031115976], "caller": "main.py:200", "time": "2021-02-05T21:15:14.611057"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.13136562705039978, 0.12832634150981903, 100, 0.5565870388785721], "caller": "main.py:200", "time": "2021-02-05T21:16:38.101957"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.12847676873207092, 0.1305101364850998, 100, 0.5663543226748595], "caller": "main.py:200", "time": "2021-02-05T21:17:58.289914"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.130301371216774, 0.14335845410823822, 100, 0.5940703412296364], "caller": "main.py:200", "time": "2021-02-05T21:19:21.086838"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.1313229203224182, 0.12455591559410095, 100, 0.5834352945697998], "caller": "main.py:200", "time": "2021-02-05T21:20:46.682954"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.1305990368127823, 0.14676810801029205, 100, 0.5755125313421756], "caller": "main.py:200", "time": "2021-02-05T21:22:11.207520"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.13148868083953857, 0.12971484661102295, 100, 0.6127542790157777], "caller": "main.py:200", "time": "2021-02-05T21:23:38.457116"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.12909165024757385, 0.12023821473121643, 100, 0.5584167256392916], "caller": "main.py:200", "time": "2021-02-05T21:25:06.018136"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.1300768405199051, 0.12024982273578644, 100, 0.58325711095461], "caller": "main.py:200", "time": "2021-02-05T21:26:30.549026"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.13068489730358124, 0.1187964379787445, 100, 0.6025073497884965], "caller": "main.py:200", "time": "2021-02-05T21:27:57.377149"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.13066262006759644, 0.12518201768398285, 100, 0.5669764366178973], "caller": "main.py:200", "time": "2021-02-05T21:29:19.382547"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.13133427500724792, 0.13387368619441986, 100, 0.5991013559099958], "caller": "main.py:200", "time": "2021-02-05T21:30:42.335320"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.13195309042930603, 0.11709903925657272, 100, 0.625691617068664], "caller": "main.py:200", "time": "2021-02-05T21:32:10.181844"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.13114795088768005, 0.1289709210395813, 100, 0.5959245815277222], "caller": "main.py:200", "time": "2021-02-05T21:33:36.171061"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [45], "caller": "main.py:129", "time": "2021-02-05T21:35:03.085108"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T21:35:12.503124"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 19.52435913996183, 0.08755739779930438], "caller": "main.py:37", "time": "2021-02-05T21:35:16.462829"}
+{"level": "info", "fmt": "episode: %s", "args": [15.564178799063978], "caller": "main.py:149", "time": "2021-02-05T21:35:26.712613"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.13039910793304443, 0.12773533165454865, 100, 0.5704739981319251], "caller": "main.py:200", "time": "2021-02-05T21:35:28.855073"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.12881313264369965, 0.13784006237983704, 100, 0.5514955627366748], "caller": "main.py:200", "time": "2021-02-05T21:36:54.861257"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.13028477132320404, 0.1212596744298935, 100, 0.5603503339922362], "caller": "main.py:200", "time": "2021-02-05T21:38:21.200311"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.1298355609178543, 0.12875300645828247, 100, 0.5649028118879945], "caller": "main.py:200", "time": "2021-02-05T21:39:49.536127"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.12785714864730835, 0.13944585621356964, 100, 0.5451386004141218], "caller": "main.py:200", "time": "2021-02-05T21:41:20.564608"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.1283295750617981, 0.15087890625, 100, 0.5657788994342554], "caller": "main.py:200", "time": "2021-02-05T21:42:52.443805"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.13024646043777466, 0.12154830992221832, 100, 0.571023150625347], "caller": "main.py:200", "time": "2021-02-05T21:44:20.699362"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.12785398960113525, 0.12625083327293396, 100, 0.5442142068795538], "caller": "main.py:200", "time": "2021-02-05T21:45:47.867119"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.12894827127456665, 0.11431820690631866, 100, 0.5659087675243952], "caller": "main.py:200", "time": "2021-02-05T21:47:13.219448"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.12889546155929565, 0.13587787747383118, 100, 0.5623427524600354], "caller": "main.py:200", "time": "2021-02-05T21:48:38.530514"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.12875452637672424, 0.12820973992347717, 100, 0.5745671751520656], "caller": "main.py:200", "time": "2021-02-05T21:50:07.724052"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.12807169556617737, 0.13016477227210999, 100, 0.5761841180344417], "caller": "main.py:200", "time": "2021-02-05T21:51:38.891834"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.12830063700675964, 0.13530951738357544, 100, 0.5839815514695789], "caller": "main.py:200", "time": "2021-02-05T21:53:07.103805"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.13183584809303284, 0.14293378591537476, 100, 0.5838388260615159], "caller": "main.py:200", "time": "2021-02-05T21:54:31.004881"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.12912510335445404, 0.12767696380615234, 100, 0.5711956437851008], "caller": "main.py:200", "time": "2021-02-05T21:55:53.331607"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.126296266913414, 0.12457416951656342, 100, 0.5659094005257226], "caller": "main.py:200", "time": "2021-02-05T21:57:14.101638"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.13011062145233154, 0.12063857913017273, 100, 0.5979018660781913], "caller": "main.py:200", "time": "2021-02-05T21:58:33.284301"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.1269962042570114, 0.1402040719985962, 100, 0.570789946558617], "caller": "main.py:200", "time": "2021-02-05T21:59:52.407119"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.12738355994224548, 0.1333402693271637, 100, 0.5718838998103463], "caller": "main.py:200", "time": "2021-02-05T22:01:11.496100"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.1298152506351471, 0.12241342663764954, 100, 0.5657409494549539], "caller": "main.py:200", "time": "2021-02-05T22:02:31.681688"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [46], "caller": "main.py:129", "time": "2021-02-05T22:03:52.320900"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T22:04:02.174003"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 18.889562234790795, 0.2583103685512055], "caller": "main.py:37", "time": "2021-02-05T22:04:06.126414"}
+{"level": "info", "fmt": "episode: %s", "args": [10.846287016974395], "caller": "main.py:149", "time": "2021-02-05T22:04:16.419074"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.1324796825647354, 0.13092777132987976, 100, 0.6157106211165791], "caller": "main.py:200", "time": "2021-02-05T22:04:18.482350"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.12781624495983124, 0.13161475956439972, 100, 0.5403384488280691], "caller": "main.py:200", "time": "2021-02-05T22:05:40.742149"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.12801523506641388, 0.12275135517120361, 100, 0.540206045156341], "caller": "main.py:200", "time": "2021-02-05T22:07:03.091852"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.127448171377182, 0.1280316561460495, 100, 0.5439610456876388], "caller": "main.py:200", "time": "2021-02-05T22:08:25.270340"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.12858673930168152, 0.13500350713729858, 100, 0.5726792463087561], "caller": "main.py:200", "time": "2021-02-05T22:09:47.879562"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.1302589625120163, 0.12134068459272385, 100, 0.5771121394057135], "caller": "main.py:200", "time": "2021-02-05T22:11:10.369094"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.12760765850543976, 0.11619027704000473, 100, 0.5536343591851686], "caller": "main.py:200", "time": "2021-02-05T22:12:32.321548"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.12788031995296478, 0.13033060729503632, 100, 0.5706895341898526], "caller": "main.py:200", "time": "2021-02-05T22:13:54.594402"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.12795361876487732, 0.12982997298240662, 100, 0.5584400551952775], "caller": "main.py:200", "time": "2021-02-05T22:15:17.409040"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.12871265411376953, 0.14031723141670227, 100, 0.6009470623130666], "caller": "main.py:200", "time": "2021-02-05T22:16:39.847919"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.1292271912097931, 0.13663771748542786, 100, 0.5734566518090903], "caller": "main.py:200", "time": "2021-02-05T22:18:02.135036"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.12675219774246216, 0.12159094214439392, 100, 0.5689639529016874], "caller": "main.py:200", "time": "2021-02-05T22:19:24.401554"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.12864980101585388, 0.1210736408829689, 100, 0.5771877380628372], "caller": "main.py:200", "time": "2021-02-05T22:20:47.469209"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.12790855765342712, 0.12741345167160034, 100, 0.5786322593110333], "caller": "main.py:200", "time": "2021-02-05T22:22:10.115977"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.12775592505931854, 0.11823148280382156, 100, 0.5624018600020675], "caller": "main.py:200", "time": "2021-02-05T22:23:32.978644"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.12795299291610718, 0.1270163655281067, 100, 0.5683824121383675], "caller": "main.py:200", "time": "2021-02-05T22:24:55.759757"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.12843146920204163, 0.11675874888896942, 100, 0.5501690986202341], "caller": "main.py:200", "time": "2021-02-05T22:26:18.626206"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.12613724172115326, 0.11872130632400513, 100, 0.5408524864371181], "caller": "main.py:200", "time": "2021-02-05T22:27:41.464826"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.12716293334960938, 0.14719882607460022, 100, 0.5555428221266242], "caller": "main.py:200", "time": "2021-02-05T22:29:04.457984"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.12757599353790283, 0.13121774792671204, 100, 0.5673905845909165], "caller": "main.py:200", "time": "2021-02-05T22:30:27.450133"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [47], "caller": "main.py:129", "time": "2021-02-05T22:31:47.928105"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T22:31:57.418861"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 17.480763676584168, 2.5928625249017063], "caller": "main.py:37", "time": "2021-02-05T22:32:01.376433"}
+{"level": "info", "fmt": "episode: %s", "args": [12.47629093120648], "caller": "main.py:149", "time": "2021-02-05T22:32:11.726664"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.12713713943958282, 0.12337228655815125, 100, 0.5483372410583522], "caller": "main.py:200", "time": "2021-02-05T22:32:13.795142"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.12688004970550537, 0.10907338559627533, 100, 0.5549638646286086], "caller": "main.py:200", "time": "2021-02-05T22:33:36.232739"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.12874417006969452, 0.1144128069281578, 100, 0.5724057196998396], "caller": "main.py:200", "time": "2021-02-05T22:34:58.924841"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.1278771460056305, 0.12920020520687103, 100, 0.5885055383451374], "caller": "main.py:200", "time": "2021-02-05T22:36:21.518571"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.12576736509799957, 0.12721313536167145, 100, 0.5268462037513315], "caller": "main.py:200", "time": "2021-02-05T22:37:43.808262"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.12467798590660095, 0.1316002905368805, 100, 0.5488521456169593], "caller": "main.py:200", "time": "2021-02-05T22:39:06.939680"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.12795844674110413, 0.1392330527305603, 100, 0.5495373698519486], "caller": "main.py:200", "time": "2021-02-05T22:40:29.448229"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.12581868469715118, 0.12633006274700165, 100, 0.5422100024873211], "caller": "main.py:200", "time": "2021-02-05T22:41:48.846400"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.12507174909114838, 0.13410460948944092, 100, 0.5524856054230383], "caller": "main.py:200", "time": "2021-02-05T22:43:07.043332"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.1254861205816269, 0.12146131694316864, 100, 0.5796054425256004], "caller": "main.py:200", "time": "2021-02-05T22:44:27.463490"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.1259828358888626, 0.13213002681732178, 100, 0.557966446522706], "caller": "main.py:200", "time": "2021-02-05T22:45:49.333511"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.12508240342140198, 0.14050906896591187, 100, 0.56643521988688], "caller": "main.py:200", "time": "2021-02-05T22:47:08.139535"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.12599605321884155, 0.13078059256076813, 100, 0.5662713267832967], "caller": "main.py:200", "time": "2021-02-05T22:48:26.629179"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.12810109555721283, 0.12652401626110077, 100, 0.5565129857698874], "caller": "main.py:200", "time": "2021-02-05T22:49:45.042427"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.12552106380462646, 0.1329125463962555, 100, 0.5519436931058164], "caller": "main.py:200", "time": "2021-02-05T22:51:04.946511"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.12685666978359222, 0.12339448928833008, 100, 0.5560997011701476], "caller": "main.py:200", "time": "2021-02-05T22:52:24.581581"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.12572841346263885, 0.12635201215744019, 100, 0.5526633288903704], "caller": "main.py:200", "time": "2021-02-05T22:53:43.406086"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.12418286502361298, 0.12786415219306946, 100, 0.5461083205988277], "caller": "main.py:200", "time": "2021-02-05T22:55:06.118408"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.12501434981822968, 0.14277909696102142, 100, 0.5481194123077636], "caller": "main.py:200", "time": "2021-02-05T22:56:31.899217"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.12447892129421234, 0.13454917073249817, 100, 0.562386783007173], "caller": "main.py:200", "time": "2021-02-05T22:57:53.228796"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [48], "caller": "main.py:129", "time": "2021-02-05T22:59:14.215179"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T22:59:23.369100"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 19.984070511773762, 0.04499266769031467], "caller": "main.py:37", "time": "2021-02-05T22:59:27.089998"}
+{"level": "info", "fmt": "episode: %s", "args": [15.126450305161423], "caller": "main.py:149", "time": "2021-02-05T22:59:37.008965"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.12703920900821686, 0.11853233724832535, 100, 0.5456341233303892], "caller": "main.py:200", "time": "2021-02-05T22:59:39.279179"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.12603828310966492, 0.13134142756462097, 100, 0.541903691535642], "caller": "main.py:200", "time": "2021-02-05T23:01:01.602452"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.12586717307567596, 0.12560616433620453, 100, 0.5478267469454348], "caller": "main.py:200", "time": "2021-02-05T23:02:31.319549"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.12565015256404877, 0.1287255585193634, 100, 0.5376512732564194], "caller": "main.py:200", "time": "2021-02-05T23:03:58.618081"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.12562762200832367, 0.10970274358987808, 100, 0.531126599258716], "caller": "main.py:200", "time": "2021-02-05T23:05:24.032821"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.1266167163848877, 0.12795361876487732, 100, 0.5431889146023202], "caller": "main.py:200", "time": "2021-02-05T23:06:49.382978"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.12616315484046936, 0.12938275933265686, 100, 0.5337259941759441], "caller": "main.py:200", "time": "2021-02-05T23:08:09.664187"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.12666967511177063, 0.11305791139602661, 100, 0.5470932015229542], "caller": "main.py:200", "time": "2021-02-05T23:09:29.556622"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.12457778304815292, 0.12339778989553452, 100, 0.5761886581202338], "caller": "main.py:200", "time": "2021-02-05T23:10:47.844229"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.1263524293899536, 0.13911202549934387, 100, 0.5554767777960339], "caller": "main.py:200", "time": "2021-02-05T23:12:06.001537"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.12449391186237335, 0.11943113803863525, 100, 0.5201299249605755], "caller": "main.py:200", "time": "2021-02-05T23:13:24.683992"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.12785468995571136, 0.13533951342105865, 100, 0.5620891218815429], "caller": "main.py:200", "time": "2021-02-05T23:14:43.145909"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.1258900910615921, 0.13079272210597992, 100, 0.549007952754433], "caller": "main.py:200", "time": "2021-02-05T23:16:01.743317"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.1264682114124298, 0.12744954228401184, 100, 0.5633936669859161], "caller": "main.py:200", "time": "2021-02-05T23:17:21.378636"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.12563982605934143, 0.13825279474258423, 100, 0.5414789464528292], "caller": "main.py:200", "time": "2021-02-05T23:18:42.572670"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.1255548596382141, 0.12034881114959717, 100, 0.5428572126680158], "caller": "main.py:200", "time": "2021-02-05T23:20:03.012435"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.12566974759101868, 0.13369476795196533, 100, 0.5551231309216415], "caller": "main.py:200", "time": "2021-02-05T23:21:25.634554"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.1257888376712799, 0.1303040087223053, 100, 0.5513888459846332], "caller": "main.py:200", "time": "2021-02-05T23:22:45.634887"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.12441299110651016, 0.128257155418396, 100, 0.5496613987730635], "caller": "main.py:200", "time": "2021-02-05T23:24:07.528523"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.1254102885723114, 0.11938510835170746, 100, 0.5421192848140285], "caller": "main.py:200", "time": "2021-02-05T23:25:28.856127"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [49], "caller": "main.py:129", "time": "2021-02-05T23:26:48.146725"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T23:26:57.056207"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 19.51085091342092, 0.03916824022773515], "caller": "main.py:37", "time": "2021-02-05T23:27:00.842043"}
+{"level": "info", "fmt": "episode: %s", "args": [15.38432686863803], "caller": "main.py:149", "time": "2021-02-05T23:27:10.811182"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.12901560962200165, 0.11405520886182785, 100, 0.5428636651688353], "caller": "main.py:200", "time": "2021-02-05T23:27:12.955752"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.12527580559253693, 0.1325850784778595, 100, 0.545985771064996], "caller": "main.py:200", "time": "2021-02-05T23:28:32.583880"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.12527652084827423, 0.12437957525253296, 100, 0.5431735327296678], "caller": "main.py:200", "time": "2021-02-05T23:29:53.442393"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.1281537562608719, 0.11950528621673584, 100, 0.5581037511863505], "caller": "main.py:200", "time": "2021-02-05T23:31:14.213889"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.12481024116277695, 0.1268218457698822, 100, 0.5453743388703474], "caller": "main.py:200", "time": "2021-02-05T23:32:34.745400"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.12688304483890533, 0.13190990686416626, 100, 0.5493310954617486], "caller": "main.py:200", "time": "2021-02-05T23:33:54.713874"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.12643346190452576, 0.11945541203022003, 100, 0.5480715271282488], "caller": "main.py:200", "time": "2021-02-05T23:35:16.978976"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.12626101076602936, 0.11439090222120285, 100, 0.5205108138398367], "caller": "main.py:200", "time": "2021-02-05T23:36:36.918935"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.125532329082489, 0.13412949442863464, 100, 0.5452812271764976], "caller": "main.py:200", "time": "2021-02-05T23:37:57.264723"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.12699981033802032, 0.11757286638021469, 100, 0.5482666584301091], "caller": "main.py:200", "time": "2021-02-05T23:39:17.792879"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.12700402736663818, 0.12770286202430725, 100, 0.5403501063044932], "caller": "main.py:200", "time": "2021-02-05T23:40:37.798881"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.12411576509475708, 0.118194580078125, 100, 0.5270889642307575], "caller": "main.py:200", "time": "2021-02-05T23:41:58.656426"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.12366225570440292, 0.11182991415262222, 100, 0.5413774683530164], "caller": "main.py:200", "time": "2021-02-05T23:43:17.338681"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.1266610473394394, 0.12295617163181305, 100, 0.5657549455196155], "caller": "main.py:200", "time": "2021-02-05T23:44:35.675680"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.1252298504114151, 0.11288648843765259, 100, 0.5384765150604598], "caller": "main.py:200", "time": "2021-02-05T23:45:54.486027"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.12595872581005096, 0.13194265961647034, 100, 0.5531720390362699], "caller": "main.py:200", "time": "2021-02-05T23:47:14.355372"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.12551474571228027, 0.14190822839736938, 100, 0.5406933834608119], "caller": "main.py:200", "time": "2021-02-05T23:48:35.156877"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.1251673698425293, 0.11317723989486694, 100, 0.544455002491865], "caller": "main.py:200", "time": "2021-02-05T23:49:54.518550"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.12654174864292145, 0.13759446144104004, 100, 0.5286831098266255], "caller": "main.py:200", "time": "2021-02-05T23:51:14.172479"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.12559455633163452, 0.1272668093442917, 100, 0.5575071232953465], "caller": "main.py:200", "time": "2021-02-05T23:52:34.864476"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T23:54:02.737535"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 19.319123512213707, 0.15047368629751307], "caller": "main.py:37", "time": "2021-02-05T23:54:06.519452"}
diff --git a/experiments02/ant_umaze_1234/src/.gitignore b/experiments02/ant_umaze_1234/src/.gitignore
new file mode 100644
index 0000000..01e12c7
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/.gitignore
@@ -0,0 +1,132 @@
+# Byte-compiled / optimized / DLL files
+__pycache__/
+*.py[cod]
+*$py.class
+
+# C extensions
+*.so
+
+# Distribution / packaging
+.Python
+build/
+develop-eggs/
+dist/
+downloads/
+eggs/
+.eggs/
+lib/
+lib64/
+parts/
+sdist/
+var/
+wheels/
+pip-wheel-metadata/
+share/python-wheels/
+*.egg-info/
+.installed.cfg
+*.egg
+MANIFEST
+vendor/
+rllab/
+tmp/
+log/
+experiments/
+# PyInstaller
+#  Usually these files are written by a python script from a template
+#  before PyInstaller builds the exe, so as to inject date/other infos into it.
+*.manifest
+*.spec
+
+# Installer logs
+pip-log.txt
+pip-delete-this-directory.txt
+
+# Unit test / coverage reports
+htmlcov/
+.tox/
+.nox/
+.coverage
+.coverage.*
+.cache
+nosetests.xml
+coverage.xml
+*.cover
+*.py,cover
+.hypothesis/
+.pytest_cache/
+
+# Translations
+*.mo
+*.pot
+
+# Django stuff:
+*.log
+local_settings.py
+db.sqlite3
+db.sqlite3-journal
+
+# Flask stuff:
+instance/
+.webassets-cache
+
+# Scrapy stuff:
+.scrapy
+
+# Sphinx documentation
+docs/_build/
+
+# PyBuilder
+target/
+
+# Jupyter Notebook
+.ipynb_checkpoints
+
+# IPython
+profile_default/
+ipython_config.py
+
+# pyenv
+.python-version
+
+# pipenv
+#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
+#   However, in case of collaboration, if having platform-specific dependencies or dependencies
+#   having no cross-platform support, pipenv may install dependencies that don't work, or not
+#   install all needed dependencies.
+#Pipfile.lock
+
+# PEP 582; used by e.g. github.com/David-OConnor/pyflow
+__pypackages__/
+
+# Celery stuff
+celerybeat-schedule
+celerybeat.pid
+
+# SageMath parsed files
+*.sage.py
+
+# Environments
+.env
+.venv
+venv/
+ENV/
+env.bak/
+venv.bak/
+
+# Spyder project settings
+.spyderproject
+.spyproject
+
+# Rope project settings
+.ropeproject
+
+# mkdocs documentation
+/site
+
+# mypy
+.mypy_cache/
+.dmypy.json
+dmypy.json
+
+# Pyre type checker
+.pyre/
diff --git a/experiments02/ant_umaze_1234/src/CODE_OF_CONDUCT.md b/experiments02/ant_umaze_1234/src/CODE_OF_CONDUCT.md
new file mode 100644
index 0000000..0d31b1f
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/CODE_OF_CONDUCT.md
@@ -0,0 +1,5 @@
+# Code of Conduct
+
+Facebook has adopted a Code of Conduct that we expect project participants to adhere to.
+Please read the [full text](https://code.fb.com/codeofconduct/)
+so that you can understand what actions will and will not be tolerated.
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/CONTRIBUTING.md b/experiments02/ant_umaze_1234/src/CONTRIBUTING.md
new file mode 100644
index 0000000..07780f7
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/CONTRIBUTING.md
@@ -0,0 +1,31 @@
+# Contributing to SLBO
+We want to make contributing to this project as easy and transparent as
+possible.
+
+## Pull Requests
+We actively welcome your pull requests.
+
+1. Fork the repo and create your branch from `master`.
+2. Ensure the examples still run.
+3. If you haven't already, complete the Contributor License Agreement ("CLA").
+
+## Contributor License Agreement ("CLA")
+In order to accept your pull request, we need you to submit a CLA. You only need
+to do this once to work on any of Facebook's open source projects.
+
+Complete your CLA here: <https://code.facebook.com/cla>
+
+## Issues
+We use GitHub issues to track public bugs. Please ensure your description is
+clear and has sufficient instructions to be able to reproduce the issue.
+
+Facebook has a [bounty program](https://www.facebook.com/whitehat/) for the safe
+disclosure of security bugs. In those cases, please go through the process
+outlined on that page and do not file a public issue.
+
+## Coding Style  
+We try to follow the PEP style guidelines and encourage you to as well.
+
+## License
+By contributing to SparseConvNet, you agree that your contributions will be licensed
+under the LICENSE file in the root directory of this source tree.
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/LICENSE b/experiments02/ant_umaze_1234/src/LICENSE
new file mode 100644
index 0000000..1fe4148
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/LICENSE
@@ -0,0 +1,407 @@
+Attribution-NonCommercial 4.0 International
+
+=======================================================================
+
+Creative Commons Corporation ("Creative Commons") is not a law firm and
+does not provide legal services or legal advice. Distribution of
+Creative Commons public licenses does not create a lawyer-client or
+other relationship. Creative Commons makes its licenses and related
+information available on an "as-is" basis. Creative Commons gives no
+warranties regarding its licenses, any material licensed under their
+terms and conditions, or any related information. Creative Commons
+disclaims all liability for damages resulting from their use to the
+fullest extent possible.
+
+Using Creative Commons Public Licenses
+
+Creative Commons public licenses provide a standard set of terms and
+conditions that creators and other rights holders may use to share
+original works of authorship and other material subject to copyright
+and certain other rights specified in the public license below. The
+following considerations are for informational purposes only, are not
+exhaustive, and do not form part of our licenses.
+
+     Considerations for licensors: Our public licenses are
+     intended for use by those authorized to give the public
+     permission to use material in ways otherwise restricted by
+     copyright and certain other rights. Our licenses are
+     irrevocable. Licensors should read and understand the terms
+     and conditions of the license they choose before applying it.
+     Licensors should also secure all rights necessary before
+     applying our licenses so that the public can reuse the
+     material as expected. Licensors should clearly mark any
+     material not subject to the license. This includes other CC-
+     licensed material, or material used under an exception or
+     limitation to copyright. More considerations for licensors:
+	wiki.creativecommons.org/Considerations_for_licensors
+
+     Considerations for the public: By using one of our public
+     licenses, a licensor grants the public permission to use the
+     licensed material under specified terms and conditions. If
+     the licensor's permission is not necessary for any reason--for
+     example, because of any applicable exception or limitation to
+     copyright--then that use is not regulated by the license. Our
+     licenses grant only permissions under copyright and certain
+     other rights that a licensor has authority to grant. Use of
+     the licensed material may still be restricted for other
+     reasons, including because others have copyright or other
+     rights in the material. A licensor may make special requests,
+     such as asking that all changes be marked or described.
+     Although not required by our licenses, you are encouraged to
+     respect those requests where reasonable. More_considerations
+     for the public:
+	wiki.creativecommons.org/Considerations_for_licensees
+
+=======================================================================
+
+Creative Commons Attribution-NonCommercial 4.0 International Public
+License
+
+By exercising the Licensed Rights (defined below), You accept and agree
+to be bound by the terms and conditions of this Creative Commons
+Attribution-NonCommercial 4.0 International Public License ("Public
+License"). To the extent this Public License may be interpreted as a
+contract, You are granted the Licensed Rights in consideration of Your
+acceptance of these terms and conditions, and the Licensor grants You
+such rights in consideration of benefits the Licensor receives from
+making the Licensed Material available under these terms and
+conditions.
+
+
+Section 1 -- Definitions.
+
+  a. Adapted Material means material subject to Copyright and Similar
+     Rights that is derived from or based upon the Licensed Material
+     and in which the Licensed Material is translated, altered,
+     arranged, transformed, or otherwise modified in a manner requiring
+     permission under the Copyright and Similar Rights held by the
+     Licensor. For purposes of this Public License, where the Licensed
+     Material is a musical work, performance, or sound recording,
+     Adapted Material is always produced where the Licensed Material is
+     synched in timed relation with a moving image.
+
+  b. Adapter's License means the license You apply to Your Copyright
+     and Similar Rights in Your contributions to Adapted Material in
+     accordance with the terms and conditions of this Public License.
+
+  c. Copyright and Similar Rights means copyright and/or similar rights
+     closely related to copyright including, without limitation,
+     performance, broadcast, sound recording, and Sui Generis Database
+     Rights, without regard to how the rights are labeled or
+     categorized. For purposes of this Public License, the rights
+     specified in Section 2(b)(1)-(2) are not Copyright and Similar
+     Rights.
+  d. Effective Technological Measures means those measures that, in the
+     absence of proper authority, may not be circumvented under laws
+     fulfilling obligations under Article 11 of the WIPO Copyright
+     Treaty adopted on December 20, 1996, and/or similar international
+     agreements.
+
+  e. Exceptions and Limitations means fair use, fair dealing, and/or
+     any other exception or limitation to Copyright and Similar Rights
+     that applies to Your use of the Licensed Material.
+
+  f. Licensed Material means the artistic or literary work, database,
+     or other material to which the Licensor applied this Public
+     License.
+
+  g. Licensed Rights means the rights granted to You subject to the
+     terms and conditions of this Public License, which are limited to
+     all Copyright and Similar Rights that apply to Your use of the
+     Licensed Material and that the Licensor has authority to license.
+
+  h. Licensor means the individual(s) or entity(ies) granting rights
+     under this Public License.
+
+  i. NonCommercial means not primarily intended for or directed towards
+     commercial advantage or monetary compensation. For purposes of
+     this Public License, the exchange of the Licensed Material for
+     other material subject to Copyright and Similar Rights by digital
+     file-sharing or similar means is NonCommercial provided there is
+     no payment of monetary compensation in connection with the
+     exchange.
+
+  j. Share means to provide material to the public by any means or
+     process that requires permission under the Licensed Rights, such
+     as reproduction, public display, public performance, distribution,
+     dissemination, communication, or importation, and to make material
+     available to the public including in ways that members of the
+     public may access the material from a place and at a time
+     individually chosen by them.
+
+  k. Sui Generis Database Rights means rights other than copyright
+     resulting from Directive 96/9/EC of the European Parliament and of
+     the Council of 11 March 1996 on the legal protection of databases,
+     as amended and/or succeeded, as well as other essentially
+     equivalent rights anywhere in the world.
+
+  l. You means the individual or entity exercising the Licensed Rights
+     under this Public License. Your has a corresponding meaning.
+
+
+Section 2 -- Scope.
+
+  a. License grant.
+
+       1. Subject to the terms and conditions of this Public License,
+          the Licensor hereby grants You a worldwide, royalty-free,
+          non-sublicensable, non-exclusive, irrevocable license to
+          exercise the Licensed Rights in the Licensed Material to:
+
+            a. reproduce and Share the Licensed Material, in whole or
+               in part, for NonCommercial purposes only; and
+
+            b. produce, reproduce, and Share Adapted Material for
+               NonCommercial purposes only.
+
+       2. Exceptions and Limitations. For the avoidance of doubt, where
+          Exceptions and Limitations apply to Your use, this Public
+          License does not apply, and You do not need to comply with
+          its terms and conditions.
+
+       3. Term. The term of this Public License is specified in Section
+          6(a).
+
+       4. Media and formats; technical modifications allowed. The
+          Licensor authorizes You to exercise the Licensed Rights in
+          all media and formats whether now known or hereafter created,
+          and to make technical modifications necessary to do so. The
+          Licensor waives and/or agrees not to assert any right or
+          authority to forbid You from making technical modifications
+          necessary to exercise the Licensed Rights, including
+          technical modifications necessary to circumvent Effective
+          Technological Measures. For purposes of this Public License,
+          simply making modifications authorized by this Section 2(a)
+          (4) never produces Adapted Material.
+
+       5. Downstream recipients.
+
+            a. Offer from the Licensor -- Licensed Material. Every
+               recipient of the Licensed Material automatically
+               receives an offer from the Licensor to exercise the
+               Licensed Rights under the terms and conditions of this
+               Public License.
+
+            b. No downstream restrictions. You may not offer or impose
+               any additional or different terms or conditions on, or
+               apply any Effective Technological Measures to, the
+               Licensed Material if doing so restricts exercise of the
+               Licensed Rights by any recipient of the Licensed
+               Material.
+
+       6. No endorsement. Nothing in this Public License constitutes or
+          may be construed as permission to assert or imply that You
+          are, or that Your use of the Licensed Material is, connected
+          with, or sponsored, endorsed, or granted official status by,
+          the Licensor or others designated to receive attribution as
+          provided in Section 3(a)(1)(A)(i).
+
+  b. Other rights.
+
+       1. Moral rights, such as the right of integrity, are not
+          licensed under this Public License, nor are publicity,
+          privacy, and/or other similar personality rights; however, to
+          the extent possible, the Licensor waives and/or agrees not to
+          assert any such rights held by the Licensor to the limited
+          extent necessary to allow You to exercise the Licensed
+          Rights, but not otherwise.
+
+       2. Patent and trademark rights are not licensed under this
+          Public License.
+
+       3. To the extent possible, the Licensor waives any right to
+          collect royalties from You for the exercise of the Licensed
+          Rights, whether directly or through a collecting society
+          under any voluntary or waivable statutory or compulsory
+          licensing scheme. In all other cases the Licensor expressly
+          reserves any right to collect such royalties, including when
+          the Licensed Material is used other than for NonCommercial
+          purposes.
+
+
+Section 3 -- License Conditions.
+
+Your exercise of the Licensed Rights is expressly made subject to the
+following conditions.
+
+  a. Attribution.
+
+       1. If You Share the Licensed Material (including in modified
+          form), You must:
+
+            a. retain the following if it is supplied by the Licensor
+               with the Licensed Material:
+
+                 i. identification of the creator(s) of the Licensed
+                    Material and any others designated to receive
+                    attribution, in any reasonable manner requested by
+                    the Licensor (including by pseudonym if
+                    designated);
+
+                ii. a copyright notice;
+
+               iii. a notice that refers to this Public License;
+
+                iv. a notice that refers to the disclaimer of
+                    warranties;
+
+                 v. a URI or hyperlink to the Licensed Material to the
+                    extent reasonably practicable;
+
+            b. indicate if You modified the Licensed Material and
+               retain an indication of any previous modifications; and
+
+            c. indicate the Licensed Material is licensed under this
+               Public License, and include the text of, or the URI or
+               hyperlink to, this Public License.
+
+       2. You may satisfy the conditions in Section 3(a)(1) in any
+          reasonable manner based on the medium, means, and context in
+          which You Share the Licensed Material. For example, it may be
+          reasonable to satisfy the conditions by providing a URI or
+          hyperlink to a resource that includes the required
+          information.
+
+       3. If requested by the Licensor, You must remove any of the
+          information required by Section 3(a)(1)(A) to the extent
+          reasonably practicable.
+
+       4. If You Share Adapted Material You produce, the Adapter's
+          License You apply must not prevent recipients of the Adapted
+          Material from complying with this Public License.
+
+
+Section 4 -- Sui Generis Database Rights.
+
+Where the Licensed Rights include Sui Generis Database Rights that
+apply to Your use of the Licensed Material:
+
+  a. for the avoidance of doubt, Section 2(a)(1) grants You the right
+     to extract, reuse, reproduce, and Share all or a substantial
+     portion of the contents of the database for NonCommercial purposes
+     only;
+
+  b. if You include all or a substantial portion of the database
+     contents in a database in which You have Sui Generis Database
+     Rights, then the database in which You have Sui Generis Database
+     Rights (but not its individual contents) is Adapted Material; and
+
+  c. You must comply with the conditions in Section 3(a) if You Share
+     all or a substantial portion of the contents of the database.
+
+For the avoidance of doubt, this Section 4 supplements and does not
+replace Your obligations under this Public License where the Licensed
+Rights include other Copyright and Similar Rights.
+
+
+Section 5 -- Disclaimer of Warranties and Limitation of Liability.
+
+  a. UNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE
+     EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS
+     AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF
+     ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS,
+     IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION,
+     WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR
+     PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS,
+     ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT
+     KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT
+     ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU.
+
+  b. TO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE
+     TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION,
+     NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT,
+     INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES,
+     COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR
+     USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN
+     ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR
+     DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR
+     IN PART, THIS LIMITATION MAY NOT APPLY TO YOU.
+
+  c. The disclaimer of warranties and limitation of liability provided
+     above shall be interpreted in a manner that, to the extent
+     possible, most closely approximates an absolute disclaimer and
+     waiver of all liability.
+
+
+Section 6 -- Term and Termination.
+
+  a. This Public License applies for the term of the Copyright and
+     Similar Rights licensed here. However, if You fail to comply with
+     this Public License, then Your rights under this Public License
+     terminate automatically.
+
+  b. Where Your right to use the Licensed Material has terminated under
+     Section 6(a), it reinstates:
+
+       1. automatically as of the date the violation is cured, provided
+          it is cured within 30 days of Your discovery of the
+          violation; or
+
+       2. upon express reinstatement by the Licensor.
+
+     For the avoidance of doubt, this Section 6(b) does not affect any
+     right the Licensor may have to seek remedies for Your violations
+     of this Public License.
+
+  c. For the avoidance of doubt, the Licensor may also offer the
+     Licensed Material under separate terms or conditions or stop
+     distributing the Licensed Material at any time; however, doing so
+     will not terminate this Public License.
+
+  d. Sections 1, 5, 6, 7, and 8 survive termination of this Public
+     License.
+
+
+Section 7 -- Other Terms and Conditions.
+
+  a. The Licensor shall not be bound by any additional or different
+     terms or conditions communicated by You unless expressly agreed.
+
+  b. Any arrangements, understandings, or agreements regarding the
+     Licensed Material not stated herein are separate from and
+     independent of the terms and conditions of this Public License.
+
+
+Section 8 -- Interpretation.
+
+  a. For the avoidance of doubt, this Public License does not, and
+     shall not be interpreted to, reduce, limit, restrict, or impose
+     conditions on any use of the Licensed Material that could lawfully
+     be made without permission under this Public License.
+
+  b. To the extent possible, if any provision of this Public License is
+     deemed unenforceable, it shall be automatically reformed to the
+     minimum extent necessary to make it enforceable. If the provision
+     cannot be reformed, it shall be severed from this Public License
+     without affecting the enforceability of the remaining terms and
+     conditions.
+
+  c. No term or condition of this Public License will be waived and no
+     failure to comply consented to unless expressly agreed to by the
+     Licensor.
+
+  d. Nothing in this Public License constitutes or may be interpreted
+     as a limitation upon, or waiver of, any privileges and immunities
+     that apply to the Licensor or You, including from the legal
+     processes of any jurisdiction or authority.
+
+=======================================================================
+
+Creative Commons is not a party to its public
+licenses. Notwithstanding, Creative Commons may elect to apply one of
+its public licenses to material it publishes and in those instances
+will be considered the “Licensor.” The text of the Creative Commons
+public licenses is dedicated to the public domain under the CC0 Public
+Domain Dedication. Except for the limited purpose of indicating that
+material is shared under a Creative Commons public license or as
+otherwise permitted by the Creative Commons policies published at
+creativecommons.org/policies, Creative Commons does not authorize the
+use of the trademark "Creative Commons" or any other trademark or logo
+of Creative Commons without its prior written consent including,
+without limitation, in connection with any unauthorized modifications
+to any of its public licenses or any other arrangements,
+understandings, or agreements concerning use of licensed material. For
+the avoidance of doubt, this paragraph does not form part of the
+public licenses.
+
+Creative Commons may be contacted at creativecommons.org.
diff --git a/experiments02/ant_umaze_1234/src/README.md b/experiments02/ant_umaze_1234/src/README.md
new file mode 100644
index 0000000..ec09da9
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/README.md
@@ -0,0 +1,29 @@
+# Stochastic Lower Bound Optimization
+
+This is the TensorFlow implementation for the paper [
+Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees](https://arxiv.org/abs/1807.03858).
+A PyTorch version will be released later.  
+
+
+## Requirements
+1. OpenAI Baselines
+2. rllab (commit number `b3a2899`)
+3. MuJoCo (1.5)
+4. TensorFlow (>= 1.9)
+5. NumPy (>= 1.14.5)
+6. Python 3.6
+
+## Run
+
+Before running, please make sure that `rllab` and `baselines` are available 
+
+```bash
+python main.py -c configs/algos/slbo.yml configs/envs/half_cheetah.yml -s log_dir=/tmp
+```
+
+If you want to change hyper-parameters, you can either modify a corresponding `yml` file or 
+change it temporarily by appending `model.hidden_sizes='[1000,1000]'` in the command line.
+
+## License
+
+See [LICENSE](LICENSE) for additional details.
diff --git a/experiments02/ant_umaze_1234/src/check_result.py b/experiments02/ant_umaze_1234/src/check_result.py
new file mode 100644
index 0000000..d1c8161
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/check_result.py
@@ -0,0 +1,12 @@
+import numpy as np
+
+seed_list = ['1234', '1235', '2314', '2345']
+env_name = 'gym_reacher'
+lasts = []
+for seed in seed_list:
+    cur = np.load('experiments/'+env_name+'_'+seed+'/eval_real_returns.npy')
+    lasts.append(cur[-1])
+
+print(lasts)
+print(np.mean(lasts))
+print(np.std(lasts))
diff --git a/experiments02/ant_umaze_1234/src/configs/algos/mb_trpo.yml b/experiments02/ant_umaze_1234/src/configs/algos/mb_trpo.yml
new file mode 100644
index 0000000..cce36ec
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/algos/mb_trpo.yml
@@ -0,0 +1,12 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 2
+  dev_batch_size: 512
+  train_batch_size: 128
+slbo:
+  n_stages: 100
+  n_iters: 1
+  n_model_iters: 2000
+  n_policy_iters: 200
+TRPO:
+  ent_coef: 0.005
diff --git a/experiments02/ant_umaze_1234/src/configs/algos/mf.yml b/experiments02/ant_umaze_1234/src/configs/algos/mf.yml
new file mode 100644
index 0000000..5d5e317
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/algos/mf.yml
@@ -0,0 +1,11 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+algorithm: MF
+rollout:
+  n_train_samples: 10000
+  n_dev_samples: 128
+  n_test_samples: 10000
+slbo:
+  n_stages: 100
+  n_iters: 1
+  n_policy_iters: 20
+  n_model_iters: 1
diff --git a/experiments02/ant_umaze_1234/src/configs/algos/slbo.yml b/experiments02/ant_umaze_1234/src/configs/algos/slbo.yml
new file mode 100644
index 0000000..2b5eecb
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/algos/slbo.yml
@@ -0,0 +1,12 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 2
+  dev_batch_size: 512
+  train_batch_size: 128
+slbo:
+  n_stages: 100
+  n_iters: 20
+  n_model_iters: 100
+  n_policy_iters: 40
+TRPO:
+  ent_coef: 0
diff --git a/experiments02/ant_umaze_1234/src/configs/algos/slbo_bm_1m.yml b/experiments02/ant_umaze_1234/src/configs/algos/slbo_bm_1m.yml
new file mode 100644
index 0000000..dcf3777
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/algos/slbo_bm_1m.yml
@@ -0,0 +1,12 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 2
+  dev_batch_size: 512
+  train_batch_size: 128
+slbo:
+  n_stages: 100
+  n_iters: 20
+  n_model_iters: 100
+  n_policy_iters: 40
+TRPO:
+  ent_coef: 0.005
diff --git a/experiments02/ant_umaze_1234/src/configs/algos/slbo_bm_200k.yml b/experiments02/ant_umaze_1234/src/configs/algos/slbo_bm_200k.yml
new file mode 100644
index 0000000..c0e39d4
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/algos/slbo_bm_200k.yml
@@ -0,0 +1,15 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 2
+  dev_batch_size: 512
+  train_batch_size: 128
+slbo:
+  n_stages: 80
+  n_iters: 20
+  n_model_iters: 100
+  n_policy_iters: 40
+TRPO:
+  ent_coef: 0
+rollout:
+  n_train_samples: 6000
+
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/ant_umaze.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/ant_umaze.yml
new file mode 100644
index 0000000..ab75516
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/ant_umaze.yml
@@ -0,0 +1,8 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: AntUMaze-v1
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
+
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/fetch_push.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/fetch_push.yml
new file mode 100644
index 0000000..43a36d0
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/fetch_push.yml
@@ -0,0 +1,8 @@
+env:
+  id: FetchPush
+runner:
+  max_steps: 50
+plan:
+  max_steps: 50
+pc:
+  bonus_scale: 0.1
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_acrobot.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_acrobot.yml
new file mode 100644
index 0000000..fe01e4a
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_acrobot.yml
@@ -0,0 +1,6 @@
+env:
+  id: Acrobot
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_ant.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_ant.yml
new file mode 100644
index 0000000..07d4c3e
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_ant.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Ant
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_ant_depth_search.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_ant_depth_search.yml
new file mode 100644
index 0000000..1d50248
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_ant_depth_search.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Ant
+runner:
+  max_steps: ENV_LENGTH
+plan:
+  max_steps: PLAN_LENGTH
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cartpole.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cartpole.yml
new file mode 100644
index 0000000..a608f7c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cartpole.yml
@@ -0,0 +1,6 @@
+env:
+  id: CartPole
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cartpoleO001.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cartpoleO001.yml
new file mode 100644
index 0000000..f83770b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cartpoleO001.yml
@@ -0,0 +1,6 @@
+env:
+  id: CartPoleO001
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cartpoleO01.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cartpoleO01.yml
new file mode 100644
index 0000000..3f7674f
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cartpoleO01.yml
@@ -0,0 +1,6 @@
+env:
+  id: CartPoleO01
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cheetah.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cheetah.yml
new file mode 100644
index 0000000..ae2c095
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cheetah.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetah
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahA003.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahA003.yml
new file mode 100644
index 0000000..a87b6be
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahA003.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetahA003
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahA01.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahA01.yml
new file mode 100644
index 0000000..5312055
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahA01.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetahA01
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahO001.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahO001.yml
new file mode 100644
index 0000000..960e36e
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahO001.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetahO001
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahO01.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahO01.yml
new file mode 100644
index 0000000..4a5d67d
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahO01.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetahO01
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cheetah_depth_search.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cheetah_depth_search.yml
new file mode 100644
index 0000000..c147ec4
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_cheetah_depth_search.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetah
+runner:
+  max_steps: ENV_LENGTH
+plan:
+  max_steps: PLAN_LENGTH
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_fant.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_fant.yml
new file mode 100644
index 0000000..6773c7b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_fant.yml
@@ -0,0 +1,6 @@
+env:
+  id: FixedAnt
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_fhopper.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_fhopper.yml
new file mode 100644
index 0000000..2efc44e
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_fhopper.yml
@@ -0,0 +1,6 @@
+env:
+  id: FixedHopper
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_fswimmer.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_fswimmer.yml
new file mode 100644
index 0000000..888b87b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_fswimmer.yml
@@ -0,0 +1,6 @@
+env:
+  id: FixedSwimmer
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_fwalker2d.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_fwalker2d.yml
new file mode 100644
index 0000000..ee63b73
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_fwalker2d.yml
@@ -0,0 +1,6 @@
+env:
+  id: FixedWalker
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_hopper.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_hopper.yml
new file mode 100644
index 0000000..a061802
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_hopper.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Hopper
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_humanoid.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_humanoid.yml
new file mode 100644
index 0000000..426bdfb
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_humanoid.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: gym_humanoid
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_invertedPendulum.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_invertedPendulum.yml
new file mode 100644
index 0000000..d1062d9
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_invertedPendulum.yml
@@ -0,0 +1,6 @@
+env:
+  id: InvertedPendulum
+runner:
+  max_steps: 100
+plan:
+  max_steps: 100
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_mountain.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_mountain.yml
new file mode 100644
index 0000000..6c335d5
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_mountain.yml
@@ -0,0 +1,11 @@
+env:
+  id: MountainCar
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
+pc:
+  bonus_scale: 2
+  bonus_stop_time: 50
+rollout:
+  n_train_samples: 1000
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_nostopslimhumanoid.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_nostopslimhumanoid.yml
new file mode 100644
index 0000000..dabe168
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_nostopslimhumanoid.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: gym_nostopslimhumanoid
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_pendulum.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_pendulum.yml
new file mode 100644
index 0000000..c0caff5
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_pendulum.yml
@@ -0,0 +1,6 @@
+env:
+  id: Pendulum
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_pendulumO001.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_pendulumO001.yml
new file mode 100644
index 0000000..1ebd722
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_pendulumO001.yml
@@ -0,0 +1,6 @@
+env:
+  id: PendulumO001
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_pendulumO01.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_pendulumO01.yml
new file mode 100644
index 0000000..4658d7f
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_pendulumO01.yml
@@ -0,0 +1,6 @@
+env:
+  id: PendulumO01
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_reacher.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_reacher.yml
new file mode 100644
index 0000000..427ad36
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_reacher.yml
@@ -0,0 +1,6 @@
+env:
+  id: Reacher
+runner:
+  max_steps: 50
+plan:
+  max_steps: 50
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_slimhumanoid.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_slimhumanoid.yml
new file mode 100644
index 0000000..ecd0976
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_slimhumanoid.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: gym_slimhumanoid
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_swimmer.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_swimmer.yml
new file mode 100644
index 0000000..b9ba125
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_swimmer.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Swimmer
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_walker2d.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_walker2d.yml
new file mode 100644
index 0000000..8403547
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/gym_walker2d.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Walker2D
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/hand_egg.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/hand_egg.yml
new file mode 100644
index 0000000..d4f5160
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/hand_egg.yml
@@ -0,0 +1,8 @@
+env:
+  id: HandEgg
+runner:
+  max_steps: 100
+plan:
+  max_steps: 100
+pc:
+  bonus_scale: 1
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/hand_reach.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/hand_reach.yml
new file mode 100644
index 0000000..40ae08b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/hand_reach.yml
@@ -0,0 +1,8 @@
+env:
+  id: HandReach
+runner:
+  max_steps: 50
+plan:
+  max_steps: 50
+pc:
+  bonus_scale: 0.5
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/point_fall.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/point_fall.yml
new file mode 100644
index 0000000..ac26219
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/point_fall.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: PointFall-v1
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/point_push.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/point_push.yml
new file mode 100644
index 0000000..22d4f65
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/point_push.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: PointPush-v1
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/configs/env_tingwu/point_umaze.yml b/experiments02/ant_umaze_1234/src/configs/env_tingwu/point_umaze.yml
new file mode 100644
index 0000000..b459aa6
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/env_tingwu/point_umaze.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: PointUMaze-v1
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/configs/envs/acrobot.yml b/experiments02/ant_umaze_1234/src/configs/envs/acrobot.yml
new file mode 100644
index 0000000..4ca200d
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/envs/acrobot.yml
@@ -0,0 +1,4 @@
+env:
+  id: Acrobot
+runner:
+  max_steps: 200
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/configs/envs/ant.yml b/experiments02/ant_umaze_1234/src/configs/envs/ant.yml
new file mode 100644
index 0000000..f24b431
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/envs/ant.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Ant
+runner:
+  max_steps: 1000
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/configs/envs/cartpole.yml b/experiments02/ant_umaze_1234/src/configs/envs/cartpole.yml
new file mode 100644
index 0000000..9b49dbe
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/envs/cartpole.yml
@@ -0,0 +1,4 @@
+env:
+  id: CartPole
+runner:
+  max_steps: 200
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/configs/envs/fswimmer.yml b/experiments02/ant_umaze_1234/src/configs/envs/fswimmer.yml
new file mode 100644
index 0000000..2aee16a
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/envs/fswimmer.yml
@@ -0,0 +1,4 @@
+env:
+  id: FixedSwimmer
+runner:
+  max_steps: 1000
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/configs/envs/half_cheetah.yml b/experiments02/ant_umaze_1234/src/configs/envs/half_cheetah.yml
new file mode 100644
index 0000000..36cfffa
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/envs/half_cheetah.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetah
+runner:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/configs/envs/half_cheetah_short.yml b/experiments02/ant_umaze_1234/src/configs/envs/half_cheetah_short.yml
new file mode 100644
index 0000000..e0e21ed
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/envs/half_cheetah_short.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetah-v2
+plan:
+  max_steps: 200
+runner:
+  max_steps: 200
diff --git a/experiments02/ant_umaze_1234/src/configs/envs/hopper.yml b/experiments02/ant_umaze_1234/src/configs/envs/hopper.yml
new file mode 100644
index 0000000..7b34a11
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/envs/hopper.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Hopper
+runner:
+  max_steps: 1000
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/configs/envs/humanoid.yml b/experiments02/ant_umaze_1234/src/configs/envs/humanoid.yml
new file mode 100644
index 0000000..1eb1236
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/envs/humanoid.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: POMDPHumanoid-v2
+runner:
+  max_steps: 500
diff --git a/experiments02/ant_umaze_1234/src/configs/envs/inverted_pendulum.yml b/experiments02/ant_umaze_1234/src/configs/envs/inverted_pendulum.yml
new file mode 100644
index 0000000..1d60732
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/envs/inverted_pendulum.yml
@@ -0,0 +1,4 @@
+env:
+  id: InvertedPendulum
+runner:
+  max_steps: 100
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/configs/envs/mountain.yml b/experiments02/ant_umaze_1234/src/configs/envs/mountain.yml
new file mode 100644
index 0000000..e4cb7f6
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/envs/mountain.yml
@@ -0,0 +1,4 @@
+env:
+  id: MountainCar
+runner:
+  max_steps: 200
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/configs/envs/pendulum.yml b/experiments02/ant_umaze_1234/src/configs/envs/pendulum.yml
new file mode 100644
index 0000000..8ea91c3
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/envs/pendulum.yml
@@ -0,0 +1,4 @@
+env:
+  id: Pendulum
+runner:
+  max_steps: 200
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/configs/envs/reacher.yml b/experiments02/ant_umaze_1234/src/configs/envs/reacher.yml
new file mode 100644
index 0000000..bfaf65b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/envs/reacher.yml
@@ -0,0 +1,4 @@
+env:
+  id: Reacher
+runner:
+  max_steps: 50
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/configs/envs/swimmer.yml b/experiments02/ant_umaze_1234/src/configs/envs/swimmer.yml
new file mode 100644
index 0000000..abe2842
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/envs/swimmer.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Swimmer
+runner:
+  max_steps: 1000
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/configs/envs/walker.yml b/experiments02/ant_umaze_1234/src/configs/envs/walker.yml
new file mode 100644
index 0000000..e42fa53
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/envs/walker.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Walker2D
+runner:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/configs/multi_step/1.yml b/experiments02/ant_umaze_1234/src/configs/multi_step/1.yml
new file mode 100644
index 0000000..e102872
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/multi_step/1.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 1
+  dev_batch_size: 1024
+  train_batch_size: 256
diff --git a/experiments02/ant_umaze_1234/src/configs/multi_step/2.yml b/experiments02/ant_umaze_1234/src/configs/multi_step/2.yml
new file mode 100644
index 0000000..c866952
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/multi_step/2.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 2
+  dev_batch_size: 512
+  train_batch_size: 128
diff --git a/experiments02/ant_umaze_1234/src/configs/multi_step/4.yml b/experiments02/ant_umaze_1234/src/configs/multi_step/4.yml
new file mode 100644
index 0000000..16a1e5c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/multi_step/4.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 4
+  dev_batch_size: 256
+  train_batch_size: 64
diff --git a/experiments02/ant_umaze_1234/src/configs/multi_step/8.yml b/experiments02/ant_umaze_1234/src/configs/multi_step/8.yml
new file mode 100644
index 0000000..7262945
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/configs/multi_step/8.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 8
+  dev_batch_size: 128
+  train_batch_size: 32
diff --git a/experiments02/ant_umaze_1234/src/cpu_requirements.txt b/experiments02/ant_umaze_1234/src/cpu_requirements.txt
new file mode 100644
index 0000000..141cfbb
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/cpu_requirements.txt
@@ -0,0 +1,8 @@
+tensorflow==1.13.1
+pyyaml==5.1
+termcolor==1.1.0
+gym
+mujoco-py
+json_tricks==3.13.1
+baselines==0.1.5
+
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/config.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/config.yml
new file mode 100644
index 0000000..cad5fe6
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/config.yml
@@ -0,0 +1,76 @@
+OUNoise:
+  sigma: 0.3
+  theta: 0.15
+PPO:
+  clip_range: 0.2
+  ent_coef: 0.005
+  lr: 0.0003
+  n_minibatches: 32
+  n_opt_epochs: 10
+TRPO:
+  cg_damping: 0.1
+  ent_coef: 0
+  max_kl: 0.01
+  n_cg_iters: 10
+  n_vf_iters: 5
+  vf_lr: 0.001
+algorithm: OLBO
+ckpt:
+  base: /tmp/mbrl/logs
+  buf_load: null
+  buf_load_index: 0
+  model_load: null
+  n_save_stages: 10
+  policy_load: null
+  warm_up: null
+commit: 7315421ee85a3e3a88f7b5e93c10320b95aa84c5
+env:
+  id: AntUMaze-v1
+log_dir: ./experiments05/ant_umaze_1234
+model:
+  G_coef: 0.5
+  dev_batch_size: 512
+  hidden_sizes:
+  - 500
+  - 500
+  loss: L2
+  lr: 0.001
+  multi_step: 2
+  optimizer: Adam
+  train_batch_size: 128
+  validation_freq: 1
+  weight_decay: 1.0e-05
+pc:
+  bonus_scale: 0.5
+  bonus_stop_time: 30
+  lamb: 0.01
+plan:
+  max_steps: 1000
+  n_envs: 4
+  n_trpo_samples: 4000
+policy:
+  hidden_sizes:
+  - 32
+  - 32
+  init_std: 1.0
+rollout:
+  max_buf_size: 100000
+  n_dev_samples: 4000
+  n_test_samples: 10000
+  n_train_samples: 4000
+  normalizer: policy
+run_id: null
+runner:
+  gamma: 0.99
+  lambda_: 0.95
+  max_steps: 1000
+seed: 1234
+slbo:
+  n_evaluate_iters: 5
+  n_iters: 20
+  n_model_iters: 100
+  n_policy_iters: 40
+  n_stages: 50
+  opt_model: false
+  start: reset
+use_prev: true
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/diff.patch b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/diff.patch
new file mode 100644
index 0000000..cf39b3b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/diff.patch
@@ -0,0 +1,156 @@
+diff --git a/configs/env_tingwu/ant_umaze.yml b/configs/env_tingwu/ant_umaze.yml
+index f653d95..ab75516 100644
+--- a/configs/env_tingwu/ant_umaze.yml
++++ b/configs/env_tingwu/ant_umaze.yml
+@@ -5,6 +5,4 @@ runner:
+   max_steps: 1000
+ plan:
+   max_steps: 1000
+-pc: 
+-  bonus_scale: 0.1  
+ 
+diff --git a/lunzi/__pycache__/Logger.cpython-36.pyc b/lunzi/__pycache__/Logger.cpython-36.pyc
+index c62e4dc..cd65096 100644
+Binary files a/lunzi/__pycache__/Logger.cpython-36.pyc and b/lunzi/__pycache__/Logger.cpython-36.pyc differ
+diff --git a/lunzi/__pycache__/__init__.cpython-36.pyc b/lunzi/__pycache__/__init__.cpython-36.pyc
+index d0052a1..bb94071 100644
+Binary files a/lunzi/__pycache__/__init__.cpython-36.pyc and b/lunzi/__pycache__/__init__.cpython-36.pyc differ
+diff --git a/lunzi/__pycache__/config.cpython-36.pyc b/lunzi/__pycache__/config.cpython-36.pyc
+index 38e50cc..151d0cd 100644
+Binary files a/lunzi/__pycache__/config.cpython-36.pyc and b/lunzi/__pycache__/config.cpython-36.pyc differ
+diff --git a/lunzi/__pycache__/dataset.cpython-36.pyc b/lunzi/__pycache__/dataset.cpython-36.pyc
+index f5f173f..673cfda 100644
+Binary files a/lunzi/__pycache__/dataset.cpython-36.pyc and b/lunzi/__pycache__/dataset.cpython-36.pyc differ
+diff --git a/lunzi/__pycache__/stubs.cpython-36.pyc b/lunzi/__pycache__/stubs.cpython-36.pyc
+index 40d1d88..ac6d28a 100644
+Binary files a/lunzi/__pycache__/stubs.cpython-36.pyc and b/lunzi/__pycache__/stubs.cpython-36.pyc differ
+diff --git a/lunzi/nn/__pycache__/__init__.cpython-36.pyc b/lunzi/nn/__pycache__/__init__.cpython-36.pyc
+index 2082353..d28b7c9 100644
+Binary files a/lunzi/nn/__pycache__/__init__.cpython-36.pyc and b/lunzi/nn/__pycache__/__init__.cpython-36.pyc differ
+diff --git a/lunzi/nn/__pycache__/container.cpython-36.pyc b/lunzi/nn/__pycache__/container.cpython-36.pyc
+index fd7039b..f16264f 100644
+Binary files a/lunzi/nn/__pycache__/container.cpython-36.pyc and b/lunzi/nn/__pycache__/container.cpython-36.pyc differ
+diff --git a/lunzi/nn/__pycache__/flat_param.cpython-36.pyc b/lunzi/nn/__pycache__/flat_param.cpython-36.pyc
+index dc8e6f9..792725a 100644
+Binary files a/lunzi/nn/__pycache__/flat_param.cpython-36.pyc and b/lunzi/nn/__pycache__/flat_param.cpython-36.pyc differ
+diff --git a/lunzi/nn/__pycache__/layers.cpython-36.pyc b/lunzi/nn/__pycache__/layers.cpython-36.pyc
+index 3f22fba..9fd6751 100644
+Binary files a/lunzi/nn/__pycache__/layers.cpython-36.pyc and b/lunzi/nn/__pycache__/layers.cpython-36.pyc differ
+diff --git a/lunzi/nn/__pycache__/loss.cpython-36.pyc b/lunzi/nn/__pycache__/loss.cpython-36.pyc
+index 9fa218d..10a1d20 100644
+Binary files a/lunzi/nn/__pycache__/loss.cpython-36.pyc and b/lunzi/nn/__pycache__/loss.cpython-36.pyc differ
+diff --git a/lunzi/nn/__pycache__/module.cpython-36.pyc b/lunzi/nn/__pycache__/module.cpython-36.pyc
+index c0f0f4a..9088e26 100644
+Binary files a/lunzi/nn/__pycache__/module.cpython-36.pyc and b/lunzi/nn/__pycache__/module.cpython-36.pyc differ
+diff --git a/lunzi/nn/__pycache__/parameter.cpython-36.pyc b/lunzi/nn/__pycache__/parameter.cpython-36.pyc
+index 5191416..2131727 100644
+Binary files a/lunzi/nn/__pycache__/parameter.cpython-36.pyc and b/lunzi/nn/__pycache__/parameter.cpython-36.pyc differ
+diff --git a/lunzi/nn/__pycache__/patch.cpython-36.pyc b/lunzi/nn/__pycache__/patch.cpython-36.pyc
+index d535500..78a52be 100644
+Binary files a/lunzi/nn/__pycache__/patch.cpython-36.pyc and b/lunzi/nn/__pycache__/patch.cpython-36.pyc differ
+diff --git a/lunzi/nn/__pycache__/utils.cpython-36.pyc b/lunzi/nn/__pycache__/utils.cpython-36.pyc
+index 1c808e8..f1860e7 100644
+Binary files a/lunzi/nn/__pycache__/utils.cpython-36.pyc and b/lunzi/nn/__pycache__/utils.cpython-36.pyc differ
+diff --git a/run2.sh b/run2.sh
+new file mode 100644
+index 0000000..7440dc5
+--- /dev/null
++++ b/run2.sh
+@@ -0,0 +1,10 @@
++#!/usr/bin/env bash
++
++for env_name in $1; do
++    echo "=> Running environment ${env_name}"
++    #for random_seed in 1234 2314 2345 1235; do
++    for random_seed in 1234; do
++        python main.py -c configs/algos/slbo_bm_200k.yml configs/env_tingwu/${env_name}.yml \
++	    -s pc.bonus_scale=0.2 log_dir=./experiments02/${env_name}_${random_seed} seed=${random_seed}
++    done
++done
+diff --git a/run_experiments.sh b/run_experiments.sh
+index 37ce016..e3f2ad7 100644
+--- a/run_experiments.sh
++++ b/run_experiments.sh
+@@ -3,8 +3,8 @@
+ for env_name in $1; do
+     echo "=> Running environment ${env_name}"
+     #for random_seed in 1234 2314 2345 1235; do
+-    for random_seed in 19; do
++    for random_seed in 1234; do
+         python main.py -c configs/algos/slbo_bm_200k.yml configs/env_tingwu/${env_name}.yml \
+-	    -s log_dir=./experiments/${env_name}_${random_seed} seed=${random_seed}
++	    -s pc.bonus_scale=0.5 log_dir=./experiments05/${env_name}_${random_seed} seed=${random_seed}
+     done
+ done
+diff --git a/slbo/__pycache__/__init__.cpython-36.pyc b/slbo/__pycache__/__init__.cpython-36.pyc
+index 069fa36..6a33858 100644
+Binary files a/slbo/__pycache__/__init__.cpython-36.pyc and b/slbo/__pycache__/__init__.cpython-36.pyc differ
+diff --git a/slbo/__pycache__/dynamics_model.cpython-36.pyc b/slbo/__pycache__/dynamics_model.cpython-36.pyc
+index d9193cb..91423ce 100644
+Binary files a/slbo/__pycache__/dynamics_model.cpython-36.pyc and b/slbo/__pycache__/dynamics_model.cpython-36.pyc differ
+diff --git a/slbo/__pycache__/partial_envs.cpython-36.pyc b/slbo/__pycache__/partial_envs.cpython-36.pyc
+index f20520d..7ad21fc 100644
+Binary files a/slbo/__pycache__/partial_envs.cpython-36.pyc and b/slbo/__pycache__/partial_envs.cpython-36.pyc differ
+diff --git a/slbo/__pycache__/random_net.cpython-36.pyc b/slbo/__pycache__/random_net.cpython-36.pyc
+index 7688576..6e866b0 100644
+Binary files a/slbo/__pycache__/random_net.cpython-36.pyc and b/slbo/__pycache__/random_net.cpython-36.pyc differ
+diff --git a/slbo/algos/__pycache__/TRPO.cpython-36.pyc b/slbo/algos/__pycache__/TRPO.cpython-36.pyc
+index 99cdd6a..9ba8329 100644
+Binary files a/slbo/algos/__pycache__/TRPO.cpython-36.pyc and b/slbo/algos/__pycache__/TRPO.cpython-36.pyc differ
+diff --git a/slbo/algos/__pycache__/__init__.cpython-36.pyc b/slbo/algos/__pycache__/__init__.cpython-36.pyc
+index 09f2f9c..9c74708 100644
+Binary files a/slbo/algos/__pycache__/__init__.cpython-36.pyc and b/slbo/algos/__pycache__/__init__.cpython-36.pyc differ
+diff --git a/slbo/loss/__pycache__/__init__.cpython-36.pyc b/slbo/loss/__pycache__/__init__.cpython-36.pyc
+index f1c5979..cc5457b 100644
+Binary files a/slbo/loss/__pycache__/__init__.cpython-36.pyc and b/slbo/loss/__pycache__/__init__.cpython-36.pyc differ
+diff --git a/slbo/loss/__pycache__/multi_step_loss.cpython-36.pyc b/slbo/loss/__pycache__/multi_step_loss.cpython-36.pyc
+index 53c9a3d..695e469 100644
+Binary files a/slbo/loss/__pycache__/multi_step_loss.cpython-36.pyc and b/slbo/loss/__pycache__/multi_step_loss.cpython-36.pyc differ
+diff --git a/slbo/policies/__pycache__/__init__.cpython-36.pyc b/slbo/policies/__pycache__/__init__.cpython-36.pyc
+index a98e999..0f2ff6d 100644
+Binary files a/slbo/policies/__pycache__/__init__.cpython-36.pyc and b/slbo/policies/__pycache__/__init__.cpython-36.pyc differ
+diff --git a/slbo/policies/__pycache__/gaussian_mlp_policy.cpython-36.pyc b/slbo/policies/__pycache__/gaussian_mlp_policy.cpython-36.pyc
+index b355a90..3f0a156 100644
+Binary files a/slbo/policies/__pycache__/gaussian_mlp_policy.cpython-36.pyc and b/slbo/policies/__pycache__/gaussian_mlp_policy.cpython-36.pyc differ
+diff --git a/slbo/utils/__pycache__/OU_noise.cpython-36.pyc b/slbo/utils/__pycache__/OU_noise.cpython-36.pyc
+index 5819e5d..bdfb380 100644
+Binary files a/slbo/utils/__pycache__/OU_noise.cpython-36.pyc and b/slbo/utils/__pycache__/OU_noise.cpython-36.pyc differ
+diff --git a/slbo/utils/__pycache__/__init__.cpython-36.pyc b/slbo/utils/__pycache__/__init__.cpython-36.pyc
+index 0e85b1e..9df4fd9 100644
+Binary files a/slbo/utils/__pycache__/__init__.cpython-36.pyc and b/slbo/utils/__pycache__/__init__.cpython-36.pyc differ
+diff --git a/slbo/utils/__pycache__/average_meter.cpython-36.pyc b/slbo/utils/__pycache__/average_meter.cpython-36.pyc
+index 1bc0297..7fa8339 100644
+Binary files a/slbo/utils/__pycache__/average_meter.cpython-36.pyc and b/slbo/utils/__pycache__/average_meter.cpython-36.pyc differ
+diff --git a/slbo/utils/__pycache__/dataset.cpython-36.pyc b/slbo/utils/__pycache__/dataset.cpython-36.pyc
+index 0cc5d73..46a212c 100644
+Binary files a/slbo/utils/__pycache__/dataset.cpython-36.pyc and b/slbo/utils/__pycache__/dataset.cpython-36.pyc differ
+diff --git a/slbo/utils/__pycache__/flags.cpython-36.pyc b/slbo/utils/__pycache__/flags.cpython-36.pyc
+index dfd7feb..193bff4 100644
+Binary files a/slbo/utils/__pycache__/flags.cpython-36.pyc and b/slbo/utils/__pycache__/flags.cpython-36.pyc differ
+diff --git a/slbo/utils/__pycache__/multi_layer_perceptron.cpython-36.pyc b/slbo/utils/__pycache__/multi_layer_perceptron.cpython-36.pyc
+index ae136c1..eceaef9 100644
+Binary files a/slbo/utils/__pycache__/multi_layer_perceptron.cpython-36.pyc and b/slbo/utils/__pycache__/multi_layer_perceptron.cpython-36.pyc differ
+diff --git a/slbo/utils/__pycache__/normalizer.cpython-36.pyc b/slbo/utils/__pycache__/normalizer.cpython-36.pyc
+index 8c67648..574fc2b 100644
+Binary files a/slbo/utils/__pycache__/normalizer.cpython-36.pyc and b/slbo/utils/__pycache__/normalizer.cpython-36.pyc differ
+diff --git a/slbo/utils/__pycache__/np_utils.cpython-36.pyc b/slbo/utils/__pycache__/np_utils.cpython-36.pyc
+index e1f3150..d9d1d69 100644
+Binary files a/slbo/utils/__pycache__/np_utils.cpython-36.pyc and b/slbo/utils/__pycache__/np_utils.cpython-36.pyc differ
+diff --git a/slbo/utils/__pycache__/pc_utils.cpython-36.pyc b/slbo/utils/__pycache__/pc_utils.cpython-36.pyc
+index 0e894f3..b76bb46 100644
+Binary files a/slbo/utils/__pycache__/pc_utils.cpython-36.pyc and b/slbo/utils/__pycache__/pc_utils.cpython-36.pyc differ
+diff --git a/slbo/utils/__pycache__/runner.cpython-36.pyc b/slbo/utils/__pycache__/runner.cpython-36.pyc
+index 2678071..cce1028 100644
+Binary files a/slbo/utils/__pycache__/runner.cpython-36.pyc and b/slbo/utils/__pycache__/runner.cpython-36.pyc differ
+diff --git a/slbo/utils/__pycache__/tf_utils.cpython-36.pyc b/slbo/utils/__pycache__/tf_utils.cpython-36.pyc
+index 47606c4..59378e0 100644
+Binary files a/slbo/utils/__pycache__/tf_utils.cpython-36.pyc and b/slbo/utils/__pycache__/tf_utils.cpython-36.pyc differ
+diff --git a/slbo/utils/__pycache__/truncated_normal.cpython-36.pyc b/slbo/utils/__pycache__/truncated_normal.cpython-36.pyc
+index 6d8d1b4..a615a74 100644
+Binary files a/slbo/utils/__pycache__/truncated_normal.cpython-36.pyc and b/slbo/utils/__pycache__/truncated_normal.cpython-36.pyc differ
+diff --git a/slbo/v_function/__pycache__/__init__.cpython-36.pyc b/slbo/v_function/__pycache__/__init__.cpython-36.pyc
+index a38ef07..5932be7 100644
+Binary files a/slbo/v_function/__pycache__/__init__.cpython-36.pyc and b/slbo/v_function/__pycache__/__init__.cpython-36.pyc differ
+diff --git a/slbo/v_function/__pycache__/mlp_v_function.cpython-36.pyc b/slbo/v_function/__pycache__/mlp_v_function.cpython-36.pyc
+index dba380d..06cd147 100644
+Binary files a/slbo/v_function/__pycache__/mlp_v_function.cpython-36.pyc and b/slbo/v_function/__pycache__/mlp_v_function.cpython-36.pyc differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/final.npy b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/final.npy
new file mode 100644
index 0000000..e06d831
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/final.npy differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/log.json b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/log.json
new file mode 100644
index 0000000..ba20205
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/log.json
@@ -0,0 +1,1245 @@
+{"level": "info", "fmt": "log_dir = %s", "args": ["./experiments05/ant_umaze_1234"], "caller": "slbo/utils/flags.py:157", "time": "2021-02-04T22:06:16.576636"}
+{"level": "info", "fmt": "Enabling flattening... %s", "args": [["GaussianMLPPolicy_1/log_std:0", "GaussianMLPPolicy_1/Linear_1/weight:0", "GaussianMLPPolicy_1/Linear_1/bias:0", "GaussianMLPPolicy_1/Linear_1_2/weight:0", "GaussianMLPPolicy_1/Linear_1_2/bias:0", "GaussianMLPPolicy_1/Linear_2_1/weight:0", "GaussianMLPPolicy_1/Linear_2_1/bias:0"]], "caller": "lunzi/nn/flat_param.py:17", "time": "2021-02-04T22:06:18.960629"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [0], "caller": "main.py:129", "time": "2021-02-04T22:06:19.998601"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-04T22:06:28.500010"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["DynamicsModel", "[states actions] => [next_states]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:28.502822"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["RandomNet", "[states actions] => [features]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:28.538762"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, -7.284889785537853, 0.1432991688976139], "caller": "main.py:37", "time": "2021-02-04T22:06:33.022952"}
+{"level": "info", "fmt": "episode: %s", "args": [1.5049363137840386], "caller": "main.py:149", "time": "2021-02-04T22:06:42.796739"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["MultiStepLoss", "[states next_states_ actions masks] => [train loss grad_norm]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:43.195288"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["MultiStepLoss", "[states next_states_ actions masks] => [loss]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:45.545221"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.9378997683525085, 0.8658612370491028, 100, 0.0445103133518442], "caller": "main.py:200", "time": "2021-02-04T22:06:45.625833"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["MLPVFunction", "[states] => [values]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:47.284993"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["TRPO", "[] => [sync_old]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:47.342512"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["TRPO", "[states actions advantages ent_coef] => [loss flat_grad dist_std mean_kl dist_mean]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:47.366344"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["TRPO", "[states tangents actions] => [hessian_vec_prod]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:47.508742"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["FlatParam", "[] => [get_flat]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:47.808253"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["FlatParam", "[feed_flat] => [set_flat]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:47.854704"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["TRPO", "[states actions advantages ent_coef] => [loss mean_kl]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:47.886948"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["TRPO", "[states returns] => [train_vf vf_loss]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:47.948213"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["TRPO", "[states returns] => [vf_loss]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:48.262897"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.016324544325470924, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-04T22:07:43.593863"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.7674996852874756, 0.6594032049179077, 100, 0.18592303832996326], "caller": "main.py:200", "time": "2021-02-04T22:08:10.697304"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.6140119433403015, 0.5865762829780579, 100, 0.29068448287322], "caller": "main.py:200", "time": "2021-02-04T22:09:35.164936"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.5615626573562622, 0.5586785078048706, 100, 0.3232737593029521], "caller": "main.py:200", "time": "2021-02-04T22:10:59.071856"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.5196434259414673, 0.5130995512008667, 100, 0.3634198417896201], "caller": "main.py:200", "time": "2021-02-04T22:12:25.863775"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.48745059967041016, 0.47261008620262146, 100, 0.381774088106125], "caller": "main.py:200", "time": "2021-02-04T22:13:49.000563"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.015538932755589485, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-04T22:14:04.970451"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.46413275599479675, 0.43267542123794556, 100, 0.40544601227758975], "caller": "main.py:200", "time": "2021-02-04T22:15:12.233352"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.4409376084804535, 0.43018847703933716, 100, 0.4347987842125016], "caller": "main.py:200", "time": "2021-02-04T22:16:35.703169"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.4228054881095886, 0.40959271788597107, 100, 0.4525770934963645], "caller": "main.py:200", "time": "2021-02-04T22:17:59.243764"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.4062086045742035, 0.39805346727371216, 100, 0.464379822863394], "caller": "main.py:200", "time": "2021-02-04T22:19:22.427830"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.38758179545402527, 0.3727763295173645, 100, 0.4922852912052919], "caller": "main.py:200", "time": "2021-02-04T22:20:45.706091"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.36915135383605957, 0.36561858654022217, 100, 0.5079268761961365], "caller": "main.py:200", "time": "2021-02-04T22:22:09.004981"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.3513016402721405, 0.3548884391784668, 100, 0.5363482841312979], "caller": "main.py:200", "time": "2021-02-04T22:23:32.073457"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.3346582353115082, 0.3185774087905884, 100, 0.5705401512409921], "caller": "main.py:200", "time": "2021-02-04T22:24:55.240207"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.3141838014125824, 0.3186214864253998, 100, 0.5858541428456963], "caller": "main.py:200", "time": "2021-02-04T22:26:18.546528"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2984384596347809, 0.29204872250556946, 100, 0.6008404158945059], "caller": "main.py:200", "time": "2021-02-04T22:27:41.838724"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.28569743037223816, 0.27806591987609863, 100, 0.672443496517648], "caller": "main.py:200", "time": "2021-02-04T22:29:05.087587"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.27058354020118713, 0.26003047823905945, 100, 0.7046934481750561], "caller": "main.py:200", "time": "2021-02-04T22:30:28.263191"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2567311227321625, 0.24795295298099518, 100, 0.6876876067427548], "caller": "main.py:200", "time": "2021-02-04T22:31:51.223327"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.24300675094127655, 0.24058246612548828, 100, 0.6691345238016474], "caller": "main.py:200", "time": "2021-02-04T22:33:14.495438"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [1], "caller": "main.py:129", "time": "2021-02-04T22:34:35.655817"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-04T22:34:44.369043"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 27.192308183330308, 24.760283378785978], "caller": "main.py:37", "time": "2021-02-04T22:34:48.267922"}
+{"level": "info", "fmt": "episode: %s", "args": [1.489976007330652], "caller": "main.py:149", "time": "2021-02-04T22:34:58.121211"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.3885601758956909, 0.350123792886734, 100, 0.5928953191045098], "caller": "main.py:200", "time": "2021-02-04T22:35:00.283383"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.34407249093055725, 0.34708529710769653, 100, 0.5578546842374846], "caller": "main.py:200", "time": "2021-02-04T22:36:23.252016"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.32253608107566833, 0.3255731463432312, 100, 0.5708341010834157], "caller": "main.py:200", "time": "2021-02-04T22:37:46.110918"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.313480943441391, 0.30371248722076416, 100, 0.6192356213308033], "caller": "main.py:200", "time": "2021-02-04T22:39:09.192702"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2979777455329895, 0.29178136587142944, 100, 0.5731983556770285], "caller": "main.py:200", "time": "2021-02-04T22:40:31.981213"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2904617190361023, 0.2777021527290344, 100, 0.6256044807404524], "caller": "main.py:200", "time": "2021-02-04T22:41:54.926837"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.28052788972854614, 0.2641128897666931, 100, 0.6070984631169711], "caller": "main.py:200", "time": "2021-02-04T22:43:17.806912"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2707897424697876, 0.27002131938934326, 100, 0.5908373644357169], "caller": "main.py:200", "time": "2021-02-04T22:44:40.655936"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2660534381866455, 0.2636871635913849, 100, 0.6013584124440604], "caller": "main.py:200", "time": "2021-02-04T22:46:03.514508"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.25895369052886963, 0.25655531883239746, 100, 0.6216240509722881], "caller": "main.py:200", "time": "2021-02-04T22:47:26.590334"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.25077924132347107, 0.2553086280822754, 100, 0.6017815517777431], "caller": "main.py:200", "time": "2021-02-04T22:48:49.516891"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.24886669218540192, 0.23380886018276215, 100, 0.6649544762900109], "caller": "main.py:200", "time": "2021-02-04T22:50:12.335035"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.24325209856033325, 0.23574472963809967, 100, 0.6566666015325962], "caller": "main.py:200", "time": "2021-02-04T22:51:35.301970"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23818263411521912, 0.23670199513435364, 100, 0.6738321166556774], "caller": "main.py:200", "time": "2021-02-04T22:52:58.254151"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23493073880672455, 0.2335895597934723, 100, 0.6719666754527466], "caller": "main.py:200", "time": "2021-02-04T22:54:21.252748"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2304995208978653, 0.23062093555927277, 100, 0.692142946650909], "caller": "main.py:200", "time": "2021-02-04T22:55:44.285615"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22534342110157013, 0.23156443238258362, 100, 0.6872498794702879], "caller": "main.py:200", "time": "2021-02-04T22:57:07.401667"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22134967148303986, 0.22219376266002655, 100, 0.648625861639487], "caller": "main.py:200", "time": "2021-02-04T22:58:30.390541"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.21619480848312378, 0.2310514897108078, 100, 0.6517196726147292], "caller": "main.py:200", "time": "2021-02-04T22:59:53.255547"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.21469898521900177, 0.2038353532552719, 100, 0.6756238823453481], "caller": "main.py:200", "time": "2021-02-04T23:01:16.160010"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [2], "caller": "main.py:129", "time": "2021-02-04T23:02:37.002820"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-04T23:02:45.523438"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 13.267047479582477, 1.9088918151755696], "caller": "main.py:37", "time": "2021-02-04T23:02:49.424144"}
+{"level": "info", "fmt": "episode: %s", "args": [1.487764008239687], "caller": "main.py:149", "time": "2021-02-04T23:02:58.913970"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.3144923150539398, 0.2991337478160858, 100, 0.6635317563347088], "caller": "main.py:200", "time": "2021-02-04T23:03:01.012835"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.28729701042175293, 0.3148777484893799, 100, 0.6241637703152196], "caller": "main.py:200", "time": "2021-02-04T23:04:23.954885"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.27259671688079834, 0.2707923650741577, 100, 0.5935929482581686], "caller": "main.py:200", "time": "2021-02-04T23:05:46.955610"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2694573998451233, 0.2647353410720825, 100, 0.5981501745579524], "caller": "main.py:200", "time": "2021-02-04T23:07:09.871475"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.26527607440948486, 0.258899450302124, 100, 0.6539499264301571], "caller": "main.py:200", "time": "2021-02-04T23:08:32.990118"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2576935589313507, 0.2569398283958435, 100, 0.6086306899177365], "caller": "main.py:200", "time": "2021-02-04T23:09:55.835832"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.25381457805633545, 0.24574750661849976, 100, 0.6171740683296058], "caller": "main.py:200", "time": "2021-02-04T23:11:18.832240"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.022534703835844994, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-04T23:11:26.694358"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2511337697505951, 0.25088873505592346, 100, 0.6123427557332909], "caller": "main.py:200", "time": "2021-02-04T23:12:41.665803"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.24774698913097382, 0.2308853566646576, 100, 0.6431975745511588], "caller": "main.py:200", "time": "2021-02-04T23:14:04.341207"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.24619396030902863, 0.24259039759635925, 100, 0.651422682339376], "caller": "main.py:200", "time": "2021-02-04T23:15:27.195008"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.24316495656967163, 0.265470027923584, 100, 0.6222608702752602], "caller": "main.py:200", "time": "2021-02-04T23:16:50.085719"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2408212423324585, 0.23947125673294067, 100, 0.6285275407997075], "caller": "main.py:200", "time": "2021-02-04T23:18:12.936986"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23787333071231842, 0.23752161860466003, 100, 0.6805508797680274], "caller": "main.py:200", "time": "2021-02-04T23:19:35.890560"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23594336211681366, 0.23638111352920532, 100, 0.6598031651313921], "caller": "main.py:200", "time": "2021-02-04T23:20:58.793084"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23323023319244385, 0.2264213263988495, 100, 0.6622032528453474], "caller": "main.py:200", "time": "2021-02-04T23:22:21.709722"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22890880703926086, 0.22115200757980347, 100, 0.6395868932561249], "caller": "main.py:200", "time": "2021-02-04T23:23:44.646134"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22666774690151215, 0.2260974496603012, 100, 0.6247618656725779], "caller": "main.py:200", "time": "2021-02-04T23:25:07.563841"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22472494840621948, 0.22024323046207428, 100, 0.6225398712727757], "caller": "main.py:200", "time": "2021-02-04T23:26:30.493156"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22352708876132965, 0.21198055148124695, 100, 0.6530370154323808], "caller": "main.py:200", "time": "2021-02-04T23:27:53.313411"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.22073757648468018, 0.2294287532567978, 100, 0.662848564320083], "caller": "main.py:200", "time": "2021-02-04T23:29:16.214194"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.015084369108080864, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-04T23:29:34.103943"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [3], "caller": "main.py:129", "time": "2021-02-04T23:30:36.847258"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-04T23:30:45.167955"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 47.932426984214715, 40.372009165803384], "caller": "main.py:37", "time": "2021-02-04T23:30:49.084754"}
+{"level": "info", "fmt": "episode: %s", "args": [-0.7433383015969701], "caller": "main.py:149", "time": "2021-02-04T23:30:58.423775"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2656884491443634, 0.2537093758583069, 100, 0.6232987427088188], "caller": "main.py:200", "time": "2021-02-04T23:31:00.517908"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.016556521877646446, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-04T23:31:42.621634"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2495730221271515, 0.2373039424419403, 100, 0.6469324394413607], "caller": "main.py:200", "time": "2021-02-04T23:32:23.377174"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.24598462879657745, 0.23024287819862366, 100, 0.6409468358114165], "caller": "main.py:200", "time": "2021-02-04T23:33:46.352348"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2429787665605545, 0.24423713982105255, 100, 0.6559720181762851], "caller": "main.py:200", "time": "2021-02-04T23:35:09.392199"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2384742945432663, 0.19784434139728546, 100, 0.6408335500314748], "caller": "main.py:200", "time": "2021-02-04T23:36:32.369834"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23314964771270752, 0.21497824788093567, 100, 0.6509717588965804], "caller": "main.py:200", "time": "2021-02-04T23:37:55.184743"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23205643892288208, 0.2285858690738678, 100, 0.663094974093885], "caller": "main.py:200", "time": "2021-02-04T23:39:18.300826"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23327691853046417, 0.24604706466197968, 100, 0.692660083701486], "caller": "main.py:200", "time": "2021-02-04T23:40:41.044261"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2292214035987854, 0.2330201268196106, 100, 0.6445126246761493], "caller": "main.py:200", "time": "2021-02-04T23:42:05.080223"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.227254256606102, 0.22167238593101501, 100, 0.661100497775725], "caller": "main.py:200", "time": "2021-02-04T23:43:29.647808"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.22531892359256744, 0.23452788591384888, 100, 0.6399301677302346], "caller": "main.py:200", "time": "2021-02-04T23:44:53.588470"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2212097942829132, 0.22138234972953796, 100, 0.6430232112956754], "caller": "main.py:200", "time": "2021-02-04T23:46:17.752448"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22144100069999695, 0.21072830259799957, 100, 0.6399233638600342], "caller": "main.py:200", "time": "2021-02-04T23:47:42.395341"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.22005149722099304, 0.21922963857650757, 100, 0.6763345169344723], "caller": "main.py:200", "time": "2021-02-04T23:49:07.661406"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.21930217742919922, 0.233090341091156, 100, 0.6710715168043452], "caller": "main.py:200", "time": "2021-02-04T23:50:33.401670"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22003845870494843, 0.22990548610687256, 100, 0.6849718516380077], "caller": "main.py:200", "time": "2021-02-04T23:51:58.923680"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.2167053073644638, 0.21754032373428345, 100, 0.6560989966332464], "caller": "main.py:200", "time": "2021-02-04T23:53:23.968621"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2148575782775879, 0.22517520189285278, 100, 0.6717091243787443], "caller": "main.py:200", "time": "2021-02-04T23:54:48.929845"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.21590504050254822, 0.2071198970079422, 100, 0.6604776447350921], "caller": "main.py:200", "time": "2021-02-04T23:56:14.541895"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.21348434686660767, 0.21213486790657043, 100, 0.6755247026006376], "caller": "main.py:200", "time": "2021-02-04T23:57:39.628160"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [4], "caller": "main.py:129", "time": "2021-02-04T23:59:02.374247"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-04T23:59:11.794486"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 47.5697844388983, 8.45114845500459], "caller": "main.py:37", "time": "2021-02-04T23:59:15.830087"}
+{"level": "info", "fmt": "episode: %s", "args": [-0.08707215349711947], "caller": "main.py:149", "time": "2021-02-04T23:59:26.105679"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.28529617190361023, 0.2699047923088074, 100, 0.6425333731724029], "caller": "main.py:200", "time": "2021-02-04T23:59:28.146791"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.25237181782722473, 0.27006638050079346, 100, 0.6509869917576209], "caller": "main.py:200", "time": "2021-02-05T00:00:52.611066"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2490644007921219, 0.21160146594047546, 100, 0.6431614606929394], "caller": "main.py:200", "time": "2021-02-05T00:02:16.985384"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2455822378396988, 0.2233978509902954, 100, 0.6878601347712257], "caller": "main.py:200", "time": "2021-02-05T00:03:41.167851"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.24017281830310822, 0.2073362171649933, 100, 0.6459101157734599], "caller": "main.py:200", "time": "2021-02-05T00:05:04.980196"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.01629333756864071, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T00:05:43.592875"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.24119457602500916, 0.23819997906684875, 100, 0.6432261037183312], "caller": "main.py:200", "time": "2021-02-05T00:06:28.952071"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23444733023643494, 0.22679953277111053, 100, 0.6725821780485866], "caller": "main.py:200", "time": "2021-02-05T00:07:52.628698"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23471219837665558, 0.24003352224826813, 100, 0.6270547621815191], "caller": "main.py:200", "time": "2021-02-05T00:09:16.770194"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23340551555156708, 0.23040629923343658, 100, 0.6539878200830693], "caller": "main.py:200", "time": "2021-02-05T00:10:40.424363"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2307969331741333, 0.21976877748966217, 100, 0.6609324424797977], "caller": "main.py:200", "time": "2021-02-05T00:12:05.175998"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.22864414751529694, 0.20312675833702087, 100, 0.643386617238556], "caller": "main.py:200", "time": "2021-02-05T00:13:31.007471"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22849096357822418, 0.23248913884162903, 100, 0.6442771189760615], "caller": "main.py:200", "time": "2021-02-05T00:14:56.249584"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.11905625462532043, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T00:15:16.554236"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22401410341262817, 0.23796740174293518, 100, 0.6648567023208473], "caller": "main.py:200", "time": "2021-02-05T00:16:20.959983"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2227412611246109, 0.21277067065238953, 100, 0.6710423564239847], "caller": "main.py:200", "time": "2021-02-05T00:17:46.208251"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22126063704490662, 0.22996008396148682, 100, 0.6321452336871288], "caller": "main.py:200", "time": "2021-02-05T00:19:11.215023"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22360236942768097, 0.20850680768489838, 100, 0.6653822349836632], "caller": "main.py:200", "time": "2021-02-05T00:20:35.425638"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22139175236225128, 0.21116870641708374, 100, 0.6491831558293613], "caller": "main.py:200", "time": "2021-02-05T00:21:59.360574"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22132712602615356, 0.20492494106292725, 100, 0.6365446376163992], "caller": "main.py:200", "time": "2021-02-05T00:23:23.652327"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.21965672075748444, 0.21849045157432556, 100, 0.6556047248184728], "caller": "main.py:200", "time": "2021-02-05T00:24:48.230219"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2139768749475479, 0.2538764476776123, 100, 0.6541388895453535], "caller": "main.py:200", "time": "2021-02-05T00:26:12.415637"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [5], "caller": "main.py:129", "time": "2021-02-05T00:27:34.654171"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T00:27:44.067842"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 71.39646059703323, 27.294946375464257], "caller": "main.py:37", "time": "2021-02-05T00:27:48.078799"}
+{"level": "info", "fmt": "episode: %s", "args": [2.2144787578124445], "caller": "main.py:149", "time": "2021-02-05T00:27:58.510817"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.25690239667892456, 0.2550359070301056, 100, 0.6484734683580424], "caller": "main.py:200", "time": "2021-02-05T00:28:00.551082"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2512354552745819, 0.23482318222522736, 100, 0.6363460748751395], "caller": "main.py:200", "time": "2021-02-05T00:29:24.865786"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2473858892917633, 0.256070077419281, 100, 0.6414153578236034], "caller": "main.py:200", "time": "2021-02-05T00:30:49.093594"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2443704456090927, 0.24331316351890564, 100, 0.6328067871295409], "caller": "main.py:200", "time": "2021-02-05T00:32:13.367040"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.24306240677833557, 0.22519540786743164, 100, 0.6352494182547468], "caller": "main.py:200", "time": "2021-02-05T00:33:37.748737"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2415125072002411, 0.21591416001319885, 100, 0.6393087732152593], "caller": "main.py:200", "time": "2021-02-05T00:35:02.285519"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23792116343975067, 0.25856488943099976, 100, 0.6170594009139982], "caller": "main.py:200", "time": "2021-02-05T00:36:26.670380"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23853355646133423, 0.26366886496543884, 100, 0.6307867293919156], "caller": "main.py:200", "time": "2021-02-05T00:37:50.972133"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.028958823531866074, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T00:38:54.334711"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2396746277809143, 0.24119020998477936, 100, 0.6517470752541155], "caller": "main.py:200", "time": "2021-02-05T00:39:15.239974"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23309510946273804, 0.2499125599861145, 100, 0.6181801092006485], "caller": "main.py:200", "time": "2021-02-05T00:40:39.644400"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23509331047534943, 0.21861311793327332, 100, 0.6366631203253184], "caller": "main.py:200", "time": "2021-02-05T00:42:05.209308"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2332783341407776, 0.2452094405889511, 100, 0.6322601532303836], "caller": "main.py:200", "time": "2021-02-05T00:43:30.972233"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23395492136478424, 0.23399657011032104, 100, 0.6183639811851119], "caller": "main.py:200", "time": "2021-02-05T00:44:56.542394"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.22825662791728973, 0.2198822796344757, 100, 0.6019069301480156], "caller": "main.py:200", "time": "2021-02-05T00:46:21.929313"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22814065217971802, 0.2401348501443863, 100, 0.6354995324572211], "caller": "main.py:200", "time": "2021-02-05T00:47:47.841328"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22969715297222137, 0.2128194272518158, 100, 0.6258110090464392], "caller": "main.py:200", "time": "2021-02-05T00:49:13.226694"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22897331416606903, 0.25535422563552856, 100, 0.6400332626060369], "caller": "main.py:200", "time": "2021-02-05T00:50:38.611967"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2271210253238678, 0.23252001404762268, 100, 0.6326220338904045], "caller": "main.py:200", "time": "2021-02-05T00:52:04.423472"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2274535596370697, 0.22818778455257416, 100, 0.6181319501377909], "caller": "main.py:200", "time": "2021-02-05T00:53:30.433158"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.22706808149814606, 0.24610774219036102, 100, 0.6464491026947164], "caller": "main.py:200", "time": "2021-02-05T00:54:56.077081"}
+{"level": "info", "fmt": "Surrogate didn't improve, shrinking step... %.6f => %.6f", "args": [-3.051757957450718e-08, -0.01571696437895298], "caller": "slbo/algos/TRPO.py:157", "time": "2021-02-05T00:55:04.229753"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.015451893210411072, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T00:55:25.178124"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.018789203837513924, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T00:55:43.953030"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [6], "caller": "main.py:129", "time": "2021-02-05T00:56:19.437729"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T00:56:28.764539"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 136.51673785451956, 11.694227744727254], "caller": "main.py:37", "time": "2021-02-05T00:56:32.840211"}
+{"level": "info", "fmt": "episode: %s", "args": [3.6217575034117115], "caller": "main.py:149", "time": "2021-02-05T00:56:43.304498"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2546501159667969, 0.24717777967453003, 100, 0.6139195119720706], "caller": "main.py:200", "time": "2021-02-05T00:56:45.382651"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2498742640018463, 0.2441970407962799, 100, 0.607142258225762], "caller": "main.py:200", "time": "2021-02-05T00:58:11.116696"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2473253309726715, 0.2449156939983368, 100, 0.5861406194577586], "caller": "main.py:200", "time": "2021-02-05T00:59:37.131977"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.24532003700733185, 0.25969821214675903, 100, 0.60262457752162], "caller": "main.py:200", "time": "2021-02-05T01:01:02.063764"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.24587509036064148, 0.2324397712945938, 100, 0.6281653305260011], "caller": "main.py:200", "time": "2021-02-05T01:02:27.728158"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2449197918176651, 0.21796435117721558, 100, 0.6051476474991281], "caller": "main.py:200", "time": "2021-02-05T01:03:53.111304"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.24188388884067535, 0.24208447337150574, 100, 0.6332119547687559], "caller": "main.py:200", "time": "2021-02-05T01:05:18.779830"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2419341653585434, 0.2251308709383011, 100, 0.5960266145467779], "caller": "main.py:200", "time": "2021-02-05T01:06:44.648018"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23830875754356384, 0.2722820043563843, 100, 0.6172737207391493], "caller": "main.py:200", "time": "2021-02-05T01:08:10.376450"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23798123002052307, 0.23688046634197235, 100, 0.5920388793353274], "caller": "main.py:200", "time": "2021-02-05T01:09:36.172002"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23926259577274323, 0.23515667021274567, 100, 0.6275042569009794], "caller": "main.py:200", "time": "2021-02-05T01:11:01.467436"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23478169739246368, 0.22522060573101044, 100, 0.5979346042273541], "caller": "main.py:200", "time": "2021-02-05T01:12:27.479881"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23851048946380615, 0.2303755134344101, 100, 0.6139878510814846], "caller": "main.py:200", "time": "2021-02-05T01:13:52.875639"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2373509258031845, 0.22519919276237488, 100, 0.6562608330785619], "caller": "main.py:200", "time": "2021-02-05T01:15:17.519687"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23446351289749146, 0.24777008593082428, 100, 0.5962043752670131], "caller": "main.py:200", "time": "2021-02-05T01:16:42.118260"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23261785507202148, 0.23913981020450592, 100, 0.6180589880921287], "caller": "main.py:200", "time": "2021-02-05T01:18:06.254920"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23437218368053436, 0.244974285364151, 100, 0.6461721449444237], "caller": "main.py:200", "time": "2021-02-05T01:19:30.282964"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23298360407352448, 0.24794146418571472, 100, 0.612424463270457], "caller": "main.py:200", "time": "2021-02-05T01:20:54.671182"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23008038103580475, 0.2533778250217438, 100, 0.6188328304267986], "caller": "main.py:200", "time": "2021-02-05T01:22:18.900541"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2296837419271469, 0.22406411170959473, 100, 0.5935192253592158], "caller": "main.py:200", "time": "2021-02-05T01:23:43.097294"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [7], "caller": "main.py:129", "time": "2021-02-05T01:25:05.359471"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T01:25:15.182537"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 139.8140261080573, 4.518970035887739], "caller": "main.py:37", "time": "2021-02-05T01:25:19.394568"}
+{"level": "info", "fmt": "episode: %s", "args": [4.135310105596485], "caller": "main.py:149", "time": "2021-02-05T01:25:30.177535"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2561134994029999, 0.25994595885276794, 100, 0.6046202277532587], "caller": "main.py:200", "time": "2021-02-05T01:25:32.192019"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2497568279504776, 0.23927614092826843, 100, 0.5928111844662087], "caller": "main.py:200", "time": "2021-02-05T01:26:56.423594"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.24596314132213593, 0.2588469386100769, 100, 0.583161360265113], "caller": "main.py:200", "time": "2021-02-05T01:28:20.662612"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.24499107897281647, 0.2503383755683899, 100, 0.583274904729213], "caller": "main.py:200", "time": "2021-02-05T01:29:44.847844"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.24114812910556793, 0.2564353048801422, 100, 0.5839961125862728], "caller": "main.py:200", "time": "2021-02-05T01:31:08.906064"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.24337728321552277, 0.2416534721851349, 100, 0.588257506430928], "caller": "main.py:200", "time": "2021-02-05T01:32:33.254828"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2448645979166031, 0.22732630372047424, 100, 0.587286156655234], "caller": "main.py:200", "time": "2021-02-05T01:33:57.881815"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.24099348485469818, 0.2605285346508026, 100, 0.5976805808542525], "caller": "main.py:200", "time": "2021-02-05T01:35:22.298907"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.018181294202804565, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T01:35:52.879023"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.24151098728179932, 0.25365400314331055, 100, 0.5922492610815985], "caller": "main.py:200", "time": "2021-02-05T01:36:46.551328"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.015179076232016087, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T01:37:58.422316"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.24082595109939575, 0.2583504915237427, 100, 0.5881023877271013], "caller": "main.py:200", "time": "2021-02-05T01:38:11.196762"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23825474083423615, 0.23263993859291077, 100, 0.597797863125626], "caller": "main.py:200", "time": "2021-02-05T01:39:35.396018"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23738840222358704, 0.24960659444332123, 100, 0.5840041639816883], "caller": "main.py:200", "time": "2021-02-05T01:40:58.892513"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23952694237232208, 0.22409279644489288, 100, 0.5866982250819018], "caller": "main.py:200", "time": "2021-02-05T01:42:23.366124"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23632682859897614, 0.24172060191631317, 100, 0.5999051349755243], "caller": "main.py:200", "time": "2021-02-05T01:43:47.399527"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23285011947155, 0.2527998685836792, 100, 0.5915127914552454], "caller": "main.py:200", "time": "2021-02-05T01:45:11.420897"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2346242070198059, 0.22603186964988708, 100, 0.6107030724868423], "caller": "main.py:200", "time": "2021-02-05T01:46:35.405940"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23699365556240082, 0.20847022533416748, 100, 0.5998176916833922], "caller": "main.py:200", "time": "2021-02-05T01:47:59.622041"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23287010192871094, 0.22177228331565857, 100, 0.5692973338988351], "caller": "main.py:200", "time": "2021-02-05T01:49:24.184153"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23394416272640228, 0.21613772213459015, 100, 0.6304995983208536], "caller": "main.py:200", "time": "2021-02-05T01:50:48.506470"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.020929701626300812, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T01:51:39.473435"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23125287890434265, 0.22990241646766663, 100, 0.5780823757666432], "caller": "main.py:200", "time": "2021-02-05T01:52:12.582247"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [8], "caller": "main.py:129", "time": "2021-02-05T01:53:34.465002"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T01:53:44.093999"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 88.55253829951161, 87.39300517162971], "caller": "main.py:37", "time": "2021-02-05T01:53:48.335951"}
+{"level": "info", "fmt": "episode: %s", "args": [3.8922143871487687], "caller": "main.py:149", "time": "2021-02-05T01:53:59.115312"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.253539115190506, 0.22343987226486206, 100, 0.5863700574050089], "caller": "main.py:200", "time": "2021-02-05T01:54:01.169165"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.24836164712905884, 0.2521657645702362, 100, 0.5813989674463614], "caller": "main.py:200", "time": "2021-02-05T01:55:25.717506"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.016245270147919655, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T01:56:37.220630"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.24847206473350525, 0.24373896420001984, 100, 0.5740000622890584], "caller": "main.py:200", "time": "2021-02-05T01:56:50.024475"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.24503283202648163, 0.23863857984542847, 100, 0.5974337088464303], "caller": "main.py:200", "time": "2021-02-05T01:58:14.396029"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.24531850218772888, 0.2348083108663559, 100, 0.5731535988472943], "caller": "main.py:200", "time": "2021-02-05T01:59:38.806482"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.24348850548267365, 0.24612316489219666, 100, 0.5851369237314644], "caller": "main.py:200", "time": "2021-02-05T02:01:03.128141"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2452690154314041, 0.2326495349407196, 100, 0.5687832664874638], "caller": "main.py:200", "time": "2021-02-05T02:02:27.663006"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.24529756605625153, 0.24180975556373596, 100, 0.603822085498986], "caller": "main.py:200", "time": "2021-02-05T02:03:52.674321"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.24310949444770813, 0.23828907310962677, 100, 0.5785147851127037], "caller": "main.py:200", "time": "2021-02-05T02:05:16.952472"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23852764070034027, 0.24449196457862854, 100, 0.5643907033832735], "caller": "main.py:200", "time": "2021-02-05T02:06:41.661180"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.24083974957466125, 0.23168952763080597, 100, 0.5675576298861382], "caller": "main.py:200", "time": "2021-02-05T02:08:05.775454"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23995639383792877, 0.22847026586532593, 100, 0.5822544237389051], "caller": "main.py:200", "time": "2021-02-05T02:09:30.158408"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23781640827655792, 0.2398337870836258, 100, 0.5658086707622249], "caller": "main.py:200", "time": "2021-02-05T02:10:54.427820"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2397816777229309, 0.24923667311668396, 100, 0.5814821594372055], "caller": "main.py:200", "time": "2021-02-05T02:12:18.393826"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23905906081199646, 0.2645703852176666, 100, 0.5894206138855884], "caller": "main.py:200", "time": "2021-02-05T02:13:42.743996"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23872387409210205, 0.2437550127506256, 100, 0.5740603714607291], "caller": "main.py:200", "time": "2021-02-05T02:15:06.978934"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.015456756576895714, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T02:16:19.066351"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.24034756422042847, 0.22683954238891602, 100, 0.5613427650367155], "caller": "main.py:200", "time": "2021-02-05T02:16:31.731813"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.015493057668209076, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T02:16:45.950359"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23633815348148346, 0.2204608917236328, 100, 0.5766333132405621], "caller": "main.py:200", "time": "2021-02-05T02:17:56.096867"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2375219762325287, 0.2395864576101303, 100, 0.5669500344424734], "caller": "main.py:200", "time": "2021-02-05T02:19:20.418073"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23295754194259644, 0.2422870397567749, 100, 0.5990837552544418], "caller": "main.py:200", "time": "2021-02-05T02:20:44.899150"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [9], "caller": "main.py:129", "time": "2021-02-05T02:22:06.496961"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T02:22:16.052393"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 9, 125.6131081194575, 7.950406983491776], "caller": "main.py:37", "time": "2021-02-05T02:22:20.073575"}
+{"level": "info", "fmt": "episode: %s", "args": [3.5405883202678954], "caller": "main.py:149", "time": "2021-02-05T02:22:30.389257"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.24905410408973694, 0.2457980513572693, 100, 0.5800446003095416], "caller": "main.py:200", "time": "2021-02-05T02:22:32.449680"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2463855892419815, 0.22956515848636627, 100, 0.5549776681702675], "caller": "main.py:200", "time": "2021-02-05T02:23:57.020694"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.015467201359570026, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T02:24:46.135826"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.24582207202911377, 0.25118064880371094, 100, 0.5890726458437264], "caller": "main.py:200", "time": "2021-02-05T02:25:21.395487"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.01569010689854622, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T02:25:33.293702"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.24758435785770416, 0.25752806663513184, 100, 0.5623095589313866], "caller": "main.py:200", "time": "2021-02-05T02:26:45.401406"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.244784876704216, 0.2173251211643219, 100, 0.5781899839061474], "caller": "main.py:200", "time": "2021-02-05T02:28:09.994430"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.24188640713691711, 0.2296314239501953, 100, 0.5653117042214612], "caller": "main.py:200", "time": "2021-02-05T02:29:34.689744"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.24323700368404388, 0.20339763164520264, 100, 0.5615577995215573], "caller": "main.py:200", "time": "2021-02-05T02:30:58.688968"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.017334330826997757, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T02:31:55.875214"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2464331090450287, 0.25175321102142334, 100, 0.5770685368199253], "caller": "main.py:200", "time": "2021-02-05T02:32:22.926969"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2391011118888855, 0.23390552401542664, 100, 0.5653912075788636], "caller": "main.py:200", "time": "2021-02-05T02:33:47.528885"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2409404218196869, 0.20986053347587585, 100, 0.5752314602753473], "caller": "main.py:200", "time": "2021-02-05T02:35:11.637064"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.017908740788698196, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T02:35:50.609471"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.24237947165966034, 0.24372419714927673, 100, 0.5785158389228232], "caller": "main.py:200", "time": "2021-02-05T02:36:36.103919"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2400665432214737, 0.21777789294719696, 100, 0.5976022767423028], "caller": "main.py:200", "time": "2021-02-05T02:38:00.566957"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2381182163953781, 0.2502570152282715, 100, 0.5699236452009575], "caller": "main.py:200", "time": "2021-02-05T02:39:24.897391"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2401796132326126, 0.21370282769203186, 100, 0.5730757083915793], "caller": "main.py:200", "time": "2021-02-05T02:40:48.696319"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23973819613456726, 0.25155067443847656, 100, 0.5649843462736674], "caller": "main.py:200", "time": "2021-02-05T02:42:13.115858"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2440023422241211, 0.258251816034317, 100, 0.5599292444622871], "caller": "main.py:200", "time": "2021-02-05T02:43:37.853070"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23790033161640167, 0.25619447231292725, 100, 0.568924965801567], "caller": "main.py:200", "time": "2021-02-05T02:45:02.882139"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2372003197669983, 0.23275581002235413, 100, 0.5709963428403155], "caller": "main.py:200", "time": "2021-02-05T02:46:27.548411"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23719394207000732, 0.21050511300563812, 100, 0.5694762694266187], "caller": "main.py:200", "time": "2021-02-05T02:47:52.218789"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23509658873081207, 0.2360454797744751, 100, 0.5652700763375541], "caller": "main.py:200", "time": "2021-02-05T02:49:16.310048"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [10], "caller": "main.py:129", "time": "2021-02-05T02:50:38.348384"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T02:50:47.688769"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 11.263155409933916, 0.3202181712846762], "caller": "main.py:37", "time": "2021-02-05T02:50:51.722002"}
+{"level": "info", "fmt": "episode: %s", "args": [5.225637714164765], "caller": "main.py:149", "time": "2021-02-05T02:51:02.118462"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.24933385848999023, 0.22712016105651855, 100, 0.5348168938817219], "caller": "main.py:200", "time": "2021-02-05T02:51:04.162140"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.24523530900478363, 0.2574393153190613, 100, 0.5553703873794225], "caller": "main.py:200", "time": "2021-02-05T02:52:28.211503"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.24492484331130981, 0.260459840297699, 100, 0.5427248081022179], "caller": "main.py:200", "time": "2021-02-05T02:53:52.421728"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.24384020268917084, 0.24149054288864136, 100, 0.5544856124561863], "caller": "main.py:200", "time": "2021-02-05T02:55:16.231559"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.24312061071395874, 0.25843894481658936, 100, 0.5625468634996535], "caller": "main.py:200", "time": "2021-02-05T02:56:40.810561"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.24364332854747772, 0.2466900646686554, 100, 0.549537045602681], "caller": "main.py:200", "time": "2021-02-05T02:58:05.463829"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.24341653287410736, 0.23161321878433228, 100, 0.5380850123821046], "caller": "main.py:200", "time": "2021-02-05T02:59:30.120038"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.020135419443249702, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T02:59:48.232277"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.24109594523906708, 0.22654855251312256, 100, 0.5348744087876457], "caller": "main.py:200", "time": "2021-02-05T03:00:54.322108"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2425806075334549, 0.2536276578903198, 100, 0.5351762875379398], "caller": "main.py:200", "time": "2021-02-05T03:02:18.582045"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2435699701309204, 0.23969092965126038, 100, 0.5359992575109448], "caller": "main.py:200", "time": "2021-02-05T03:03:43.132311"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.24210457503795624, 0.24198272824287415, 100, 0.5398506825622488], "caller": "main.py:200", "time": "2021-02-05T03:05:07.890619"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23934511840343475, 0.2345769703388214, 100, 0.5390978905019489], "caller": "main.py:200", "time": "2021-02-05T03:06:32.093428"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2399994432926178, 0.24925497174263, 100, 0.5466625263740332], "caller": "main.py:200", "time": "2021-02-05T03:07:56.403596"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2412506490945816, 0.2507460117340088, 100, 0.553088582400803], "caller": "main.py:200", "time": "2021-02-05T03:09:21.095898"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.24003919959068298, 0.24060767889022827, 100, 0.5654713203937588], "caller": "main.py:200", "time": "2021-02-05T03:10:44.652338"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.24188318848609924, 0.23756691813468933, 100, 0.5469116979290506], "caller": "main.py:200", "time": "2021-02-05T03:12:09.535505"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.2372056245803833, 0.2245379537343979, 100, 0.52646969849567], "caller": "main.py:200", "time": "2021-02-05T03:13:33.635652"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2389972060918808, 0.25206875801086426, 100, 0.5344039397897374], "caller": "main.py:200", "time": "2021-02-05T03:14:58.370612"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23647023737430573, 0.23285701870918274, 100, 0.5281276177831858], "caller": "main.py:200", "time": "2021-02-05T03:16:22.435685"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23651131987571716, 0.22310933470726013, 100, 0.5284125405021591], "caller": "main.py:200", "time": "2021-02-05T03:17:47.276076"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [11], "caller": "main.py:129", "time": "2021-02-05T03:19:09.482703"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T03:19:19.041102"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 85.63172215988948, 5.959366426256553], "caller": "main.py:37", "time": "2021-02-05T03:19:23.076384"}
+{"level": "info", "fmt": "episode: %s", "args": [4.4062887063878575], "caller": "main.py:149", "time": "2021-02-05T03:19:33.524642"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.25241827964782715, 0.2648758590221405, 100, 0.5299149658746094], "caller": "main.py:200", "time": "2021-02-05T03:19:35.578474"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.24533672630786896, 0.227889746427536, 100, 0.534048566168786], "caller": "main.py:200", "time": "2021-02-05T03:21:00.032674"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.24379630386829376, 0.25789421796798706, 100, 0.5375286307393418], "caller": "main.py:200", "time": "2021-02-05T03:22:24.329267"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.24161136150360107, 0.25691524147987366, 100, 0.5459577834060213], "caller": "main.py:200", "time": "2021-02-05T03:23:49.081648"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2444019466638565, 0.25747594237327576, 100, 0.5390114008841329], "caller": "main.py:200", "time": "2021-02-05T03:25:13.097501"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.24572782218456268, 0.22652515769004822, 100, 0.5313931023543161], "caller": "main.py:200", "time": "2021-02-05T03:26:37.340100"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2414115071296692, 0.2402125597000122, 100, 0.5279275530048779], "caller": "main.py:200", "time": "2021-02-05T03:28:02.351830"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.24199028313159943, 0.25564759969711304, 100, 0.5226727988793314], "caller": "main.py:200", "time": "2021-02-05T03:29:26.777640"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.24424071609973907, 0.23686924576759338, 100, 0.5538157745641977], "caller": "main.py:200", "time": "2021-02-05T03:30:50.906691"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.24205555021762848, 0.25030580163002014, 100, 0.5426893993669462], "caller": "main.py:200", "time": "2021-02-05T03:32:15.413585"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.24466800689697266, 0.26697665452957153, 100, 0.552499392871443], "caller": "main.py:200", "time": "2021-02-05T03:33:39.885007"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2400943785905838, 0.2581492066383362, 100, 0.5180592581111694], "caller": "main.py:200", "time": "2021-02-05T03:35:04.633509"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2377762347459793, 0.2515753507614136, 100, 0.5204582219795475], "caller": "main.py:200", "time": "2021-02-05T03:36:28.871706"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23807333409786224, 0.2624308466911316, 100, 0.533871942705173], "caller": "main.py:200", "time": "2021-02-05T03:37:52.747405"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.24054567515850067, 0.23535126447677612, 100, 0.5253694730471716], "caller": "main.py:200", "time": "2021-02-05T03:39:16.772160"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.24227501451969147, 0.22701257467269897, 100, 0.5417482643777787], "caller": "main.py:200", "time": "2021-02-05T03:40:40.583069"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23715631663799286, 0.21524301171302795, 100, 0.5459024458577709], "caller": "main.py:200", "time": "2021-02-05T03:42:04.700207"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23818954825401306, 0.2377251833677292, 100, 0.5447362285307696], "caller": "main.py:200", "time": "2021-02-05T03:43:29.517131"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23806962370872498, 0.23591193556785583, 100, 0.5366494377910883], "caller": "main.py:200", "time": "2021-02-05T03:44:53.854810"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2360640913248062, 0.2150774896144867, 100, 0.5389069836792865], "caller": "main.py:200", "time": "2021-02-05T03:46:18.218977"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [12], "caller": "main.py:129", "time": "2021-02-05T03:47:40.769117"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T03:47:50.254388"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 11.189342261381967, 0.6760811701034309], "caller": "main.py:37", "time": "2021-02-05T03:47:54.496397"}
+{"level": "info", "fmt": "episode: %s", "args": [0.5328876993732647], "caller": "main.py:149", "time": "2021-02-05T03:48:05.110950"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2515033483505249, 0.21494406461715698, 100, 0.5177868651825963], "caller": "main.py:200", "time": "2021-02-05T03:48:07.173586"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.24477031826972961, 0.22443941235542297, 100, 0.5448008328131423], "caller": "main.py:200", "time": "2021-02-05T03:49:31.252254"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2429143488407135, 0.22828984260559082, 100, 0.522679384596532], "caller": "main.py:200", "time": "2021-02-05T03:50:55.696041"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23806734383106232, 0.24920453131198883, 100, 0.5162591349562324], "caller": "main.py:200", "time": "2021-02-05T03:52:20.634927"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2411928027868271, 0.247531458735466, 100, 0.5180969735461938], "caller": "main.py:200", "time": "2021-02-05T03:53:44.950500"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23803992569446564, 0.23578707873821259, 100, 0.5119425326073828], "caller": "main.py:200", "time": "2021-02-05T03:55:10.106658"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23909364640712738, 0.2399097979068756, 100, 0.5028430088851672], "caller": "main.py:200", "time": "2021-02-05T03:56:34.462524"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2378530502319336, 0.2504572868347168, 100, 0.5469548904348828], "caller": "main.py:200", "time": "2021-02-05T03:57:59.160895"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.24005012214183807, 0.22631457448005676, 100, 0.5195754369573771], "caller": "main.py:200", "time": "2021-02-05T03:59:23.815819"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23848681151866913, 0.24351046979427338, 100, 0.5298279866046367], "caller": "main.py:200", "time": "2021-02-05T04:00:48.245883"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2332642376422882, 0.2573530673980713, 100, 0.5077650618045397], "caller": "main.py:200", "time": "2021-02-05T04:02:12.490455"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23668457567691803, 0.23947648704051971, 100, 0.529945661372596], "caller": "main.py:200", "time": "2021-02-05T04:03:36.789432"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2376016229391098, 0.26157575845718384, 100, 0.5273033809134562], "caller": "main.py:200", "time": "2021-02-05T04:05:01.526491"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23611308634281158, 0.24506834149360657, 100, 0.5296745350427242], "caller": "main.py:200", "time": "2021-02-05T04:06:26.117598"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23046736419200897, 0.218243807554245, 100, 0.5083927756039536], "caller": "main.py:200", "time": "2021-02-05T04:07:50.580767"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23397499322891235, 0.2504982054233551, 100, 0.539999282306077], "caller": "main.py:200", "time": "2021-02-05T04:09:14.647813"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23375195264816284, 0.2379341423511505, 100, 0.515980113991161], "caller": "main.py:200", "time": "2021-02-05T04:10:39.074613"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23357468843460083, 0.23717711865901947, 100, 0.5265748720834585], "caller": "main.py:200", "time": "2021-02-05T04:12:03.596183"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23349276185035706, 0.24050670862197876, 100, 0.5126517208898016], "caller": "main.py:200", "time": "2021-02-05T04:13:28.076350"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2310056835412979, 0.21687093377113342, 100, 0.5163893073280837], "caller": "main.py:200", "time": "2021-02-05T04:14:52.752482"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [13], "caller": "main.py:129", "time": "2021-02-05T04:16:14.907389"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T04:16:24.354396"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 18.82341581272284, 1.4422721413589146], "caller": "main.py:37", "time": "2021-02-05T04:16:28.405108"}
+{"level": "info", "fmt": "episode: %s", "args": [2.6244859576066673], "caller": "main.py:149", "time": "2021-02-05T04:16:38.621789"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.24924756586551666, 0.23691189289093018, 100, 0.5110208475078731], "caller": "main.py:200", "time": "2021-02-05T04:16:40.672976"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2432815432548523, 0.25454097986221313, 100, 0.49660386378567445], "caller": "main.py:200", "time": "2021-02-05T04:18:04.994709"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.24208784103393555, 0.22772693634033203, 100, 0.5100921809163217], "caller": "main.py:200", "time": "2021-02-05T04:19:29.396469"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.24017666280269623, 0.23231709003448486, 100, 0.5158924337705658], "caller": "main.py:200", "time": "2021-02-05T04:20:53.627468"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23981986939907074, 0.24187330901622772, 100, 0.5223041259720158], "caller": "main.py:200", "time": "2021-02-05T04:22:17.777196"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2411426603794098, 0.2267467975616455, 100, 0.525544334299462], "caller": "main.py:200", "time": "2021-02-05T04:23:42.111707"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23795433342456818, 0.232161745429039, 100, 0.518706341631994], "caller": "main.py:200", "time": "2021-02-05T04:25:06.927214"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2392052710056305, 0.2410559207201004, 100, 0.5176649364311399], "caller": "main.py:200", "time": "2021-02-05T04:26:31.545434"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23857130110263824, 0.2706950604915619, 100, 0.5121817966783465], "caller": "main.py:200", "time": "2021-02-05T04:27:56.035908"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23658007383346558, 0.25420814752578735, 100, 0.5211870017126556], "caller": "main.py:200", "time": "2021-02-05T04:29:20.081861"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23655787110328674, 0.2211454212665558, 100, 0.5032708921525271], "caller": "main.py:200", "time": "2021-02-05T04:30:44.408138"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2370852679014206, 0.23860886693000793, 100, 0.5074354665964046], "caller": "main.py:200", "time": "2021-02-05T04:32:08.674195"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23756422102451324, 0.2589957118034363, 100, 0.5177195175964482], "caller": "main.py:200", "time": "2021-02-05T04:33:32.777521"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.233832448720932, 0.2221498042345047, 100, 0.5059107189864649], "caller": "main.py:200", "time": "2021-02-05T04:34:57.452690"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23256249725818634, 0.22339260578155518, 100, 0.49993251527316884], "caller": "main.py:200", "time": "2021-02-05T04:36:21.800640"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2349739819765091, 0.2602747976779938, 100, 0.5019751029704705], "caller": "main.py:200", "time": "2021-02-05T04:37:45.800727"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23570579290390015, 0.2128116488456726, 100, 0.5314061411683827], "caller": "main.py:200", "time": "2021-02-05T04:39:09.630329"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23447303473949432, 0.2510994076728821, 100, 0.5256547307442485], "caller": "main.py:200", "time": "2021-02-05T04:40:33.597769"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23311756551265717, 0.2643945813179016, 100, 0.5105426462974222], "caller": "main.py:200", "time": "2021-02-05T04:41:58.397797"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23467139899730682, 0.2314663529396057, 100, 0.5277484365769973], "caller": "main.py:200", "time": "2021-02-05T04:43:23.886654"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [14], "caller": "main.py:129", "time": "2021-02-05T04:44:47.557671"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T04:44:56.897777"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 91.56676955723033, 13.419996211006787], "caller": "main.py:37", "time": "2021-02-05T04:45:00.956879"}
+{"level": "info", "fmt": "episode: %s", "args": [3.41369583617426], "caller": "main.py:149", "time": "2021-02-05T04:45:11.427928"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23886606097221375, 0.235527902841568, 100, 0.5094543733683394], "caller": "main.py:200", "time": "2021-02-05T04:45:13.507565"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23911406099796295, 0.21654824912548065, 100, 0.503934398541125], "caller": "main.py:200", "time": "2021-02-05T04:46:38.747929"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2362968474626541, 0.24300456047058105, 100, 0.5122026353562276], "caller": "main.py:200", "time": "2021-02-05T04:48:03.060611"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2348414957523346, 0.23777830600738525, 100, 0.5094810538974544], "caller": "main.py:200", "time": "2021-02-05T04:49:27.735214"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23345227539539337, 0.23192834854125977, 100, 0.49194535965768416], "caller": "main.py:200", "time": "2021-02-05T04:50:51.979865"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2364008128643036, 0.2220446765422821, 100, 0.5093232470241486], "caller": "main.py:200", "time": "2021-02-05T04:52:16.025716"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2345947027206421, 0.20215454697608948, 100, 0.4800924875852191], "caller": "main.py:200", "time": "2021-02-05T04:53:40.294209"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23348921537399292, 0.25329291820526123, 100, 0.5092735307899464], "caller": "main.py:200", "time": "2021-02-05T04:55:04.440854"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2327483743429184, 0.21947702765464783, 100, 0.5074549263208289], "caller": "main.py:200", "time": "2021-02-05T04:56:29.012129"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23554208874702454, 0.21928875148296356, 100, 0.49720309490953873], "caller": "main.py:200", "time": "2021-02-05T04:57:53.431341"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2320355921983719, 0.22462373971939087, 100, 0.5089302566138599], "caller": "main.py:200", "time": "2021-02-05T04:59:17.905153"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23384010791778564, 0.2517027258872986, 100, 0.5182106783467881], "caller": "main.py:200", "time": "2021-02-05T05:00:42.125702"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23119299113750458, 0.22643741965293884, 100, 0.495405487070611], "caller": "main.py:200", "time": "2021-02-05T05:02:06.568844"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.230971559882164, 0.2261505126953125, 100, 0.48972789431681474], "caller": "main.py:200", "time": "2021-02-05T05:03:30.685764"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.01686042547225952, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T05:04:36.006313"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.016561081632971764, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T05:04:37.995773"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23465225100517273, 0.22139891982078552, 100, 0.5060727542132706], "caller": "main.py:200", "time": "2021-02-05T05:04:54.687817"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23483304679393768, 0.22966304421424866, 100, 0.5076963585107462], "caller": "main.py:200", "time": "2021-02-05T05:06:19.304040"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.230827197432518, 0.24103735387325287, 100, 0.497536061356382], "caller": "main.py:200", "time": "2021-02-05T05:07:43.349252"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23072892427444458, 0.21429011225700378, 100, 0.49874498213310514], "caller": "main.py:200", "time": "2021-02-05T05:09:07.845440"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23117703199386597, 0.2353840470314026, 100, 0.506705076643818], "caller": "main.py:200", "time": "2021-02-05T05:10:32.183704"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2288881242275238, 0.23827148973941803, 100, 0.5036924093458024], "caller": "main.py:200", "time": "2021-02-05T05:11:56.519963"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [15], "caller": "main.py:129", "time": "2021-02-05T05:13:18.735773"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T05:13:28.027153"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 11.453584602286728, 0.20539113666074518], "caller": "main.py:37", "time": "2021-02-05T05:13:31.911390"}
+{"level": "info", "fmt": "episode: %s", "args": [7.4268915145181404], "caller": "main.py:149", "time": "2021-02-05T05:13:42.125575"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23891639709472656, 0.22890569269657135, 100, 0.48877179379343855], "caller": "main.py:200", "time": "2021-02-05T05:13:44.206490"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.025204673409461975, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T05:14:27.524724"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23955067992210388, 0.23774927854537964, 100, 0.5097321301188953], "caller": "main.py:200", "time": "2021-02-05T05:15:08.699585"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2384500503540039, 0.25526756048202515, 100, 0.48753717536819274], "caller": "main.py:200", "time": "2021-02-05T05:16:32.816271"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2345035970211029, 0.21408134698867798, 100, 0.48730273271694435], "caller": "main.py:200", "time": "2021-02-05T05:17:57.186340"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23491880297660828, 0.21782457828521729, 100, 0.48404910917209987], "caller": "main.py:200", "time": "2021-02-05T05:19:21.798152"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.233291894197464, 0.22658461332321167, 100, 0.4775457730411775], "caller": "main.py:200", "time": "2021-02-05T05:20:45.950542"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23734605312347412, 0.2349460870027542, 100, 0.4808020600195735], "caller": "main.py:200", "time": "2021-02-05T05:22:10.338591"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23496030271053314, 0.2222604751586914, 100, 0.48992744142878286], "caller": "main.py:200", "time": "2021-02-05T05:23:34.407124"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23416930437088013, 0.26103299856185913, 100, 0.47672855640650885], "caller": "main.py:200", "time": "2021-02-05T05:24:58.979986"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2355758249759674, 0.2250482738018036, 100, 0.491945232997814], "caller": "main.py:200", "time": "2021-02-05T05:26:23.355193"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2340431958436966, 0.2562577724456787, 100, 0.4767081030726562], "caller": "main.py:200", "time": "2021-02-05T05:27:47.616674"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23304826021194458, 0.24584999680519104, 100, 0.48654894227884454], "caller": "main.py:200", "time": "2021-02-05T05:29:11.817358"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.016263088211417198, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T05:29:19.829141"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2366795688867569, 0.24710440635681152, 100, 0.5014514460658255], "caller": "main.py:200", "time": "2021-02-05T05:30:36.186705"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23368015885353088, 0.23180612921714783, 100, 0.49089777583935323], "caller": "main.py:200", "time": "2021-02-05T05:32:00.212673"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23129817843437195, 0.25586360692977905, 100, 0.48633534396791844], "caller": "main.py:200", "time": "2021-02-05T05:33:24.359437"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23265472054481506, 0.22961172461509705, 100, 0.4775468044995552], "caller": "main.py:200", "time": "2021-02-05T05:34:48.926653"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23433057963848114, 0.22930845618247986, 100, 0.4875885197032668], "caller": "main.py:200", "time": "2021-02-05T05:36:12.412931"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23070505261421204, 0.22964432835578918, 100, 0.46407739515432167], "caller": "main.py:200", "time": "2021-02-05T05:37:36.059034"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23228776454925537, 0.2271398901939392, 100, 0.4877114605415914], "caller": "main.py:200", "time": "2021-02-05T05:39:00.053217"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2312116026878357, 0.25577878952026367, 100, 0.4820994597191317], "caller": "main.py:200", "time": "2021-02-05T05:40:24.265329"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [16], "caller": "main.py:129", "time": "2021-02-05T05:41:46.517249"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T05:41:55.850548"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 25.065238425284143, 0.5792627061726003], "caller": "main.py:37", "time": "2021-02-05T05:41:59.977562"}
+{"level": "info", "fmt": "episode: %s", "args": [3.985361198958518], "caller": "main.py:149", "time": "2021-02-05T05:42:10.647104"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.24025161564350128, 0.26333311200141907, 100, 0.5170236747939931], "caller": "main.py:200", "time": "2021-02-05T05:42:12.710147"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23790797591209412, 0.2280895709991455, 100, 0.46864045436464263], "caller": "main.py:200", "time": "2021-02-05T05:43:36.752029"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23451891541481018, 0.22655585408210754, 100, 0.4753780087353258], "caller": "main.py:200", "time": "2021-02-05T05:45:01.031404"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23855631053447723, 0.22669316828250885, 100, 0.4842498656623104], "caller": "main.py:200", "time": "2021-02-05T05:46:24.807846"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23838888108730316, 0.22859713435173035, 100, 0.47619896526343053], "caller": "main.py:200", "time": "2021-02-05T05:47:48.675652"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.016243992373347282, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T05:48:47.742427"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23376336693763733, 0.2072775810956955, 100, 0.4563882600804751], "caller": "main.py:200", "time": "2021-02-05T05:49:12.586784"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23402950167655945, 0.24270538985729218, 100, 0.4703045880324221], "caller": "main.py:200", "time": "2021-02-05T05:50:36.632287"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23294197022914886, 0.22113165259361267, 100, 0.4756015759230896], "caller": "main.py:200", "time": "2021-02-05T05:52:00.360528"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23153960704803467, 0.2677220106124878, 100, 0.4779888796328347], "caller": "main.py:200", "time": "2021-02-05T05:53:24.353629"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23349694907665253, 0.2504303753376007, 100, 0.45583146209926856], "caller": "main.py:200", "time": "2021-02-05T05:54:48.945961"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23394544422626495, 0.21827740967273712, 100, 0.4725258343939324], "caller": "main.py:200", "time": "2021-02-05T05:56:13.182649"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23416203260421753, 0.2744644284248352, 100, 0.4807754355188246], "caller": "main.py:200", "time": "2021-02-05T05:57:38.293281"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23501595854759216, 0.2465403974056244, 100, 0.48403556044029733], "caller": "main.py:200", "time": "2021-02-05T05:59:02.753728"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2312059849500656, 0.2330743968486786, 100, 0.46670614506107555], "caller": "main.py:200", "time": "2021-02-05T06:00:27.104523"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23236827552318573, 0.21455501019954681, 100, 0.4724990611795949], "caller": "main.py:200", "time": "2021-02-05T06:01:51.478992"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23072011768817902, 0.20852775871753693, 100, 0.4804426723238201], "caller": "main.py:200", "time": "2021-02-05T06:03:15.534774"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23141290247440338, 0.2099551260471344, 100, 0.47681363220215084], "caller": "main.py:200", "time": "2021-02-05T06:04:39.777595"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2330247312784195, 0.23137208819389343, 100, 0.4681860050090858], "caller": "main.py:200", "time": "2021-02-05T06:06:03.406477"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2297554761171341, 0.2378171682357788, 100, 0.4785816907404109], "caller": "main.py:200", "time": "2021-02-05T06:07:28.258744"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23352240025997162, 0.23339585959911346, 100, 0.4723723384265596], "caller": "main.py:200", "time": "2021-02-05T06:08:52.677242"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [17], "caller": "main.py:129", "time": "2021-02-05T06:10:15.130956"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T06:10:24.509238"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 13.151267625546446, 1.1674214710377575], "caller": "main.py:37", "time": "2021-02-05T06:10:28.579585"}
+{"level": "info", "fmt": "episode: %s", "args": [6.361894630352007], "caller": "main.py:149", "time": "2021-02-05T06:10:38.942348"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2340031862258911, 0.2409648299217224, 100, 0.460775562478718], "caller": "main.py:200", "time": "2021-02-05T06:10:41.023104"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23603001236915588, 0.2121797800064087, 100, 0.4718091517215035], "caller": "main.py:200", "time": "2021-02-05T06:12:05.178690"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23292867839336395, 0.22598688304424286, 100, 0.46211433316616424], "caller": "main.py:200", "time": "2021-02-05T06:13:29.180837"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23613639175891876, 0.24162890017032623, 100, 0.45539984162068164], "caller": "main.py:200", "time": "2021-02-05T06:14:53.112015"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23332369327545166, 0.25787824392318726, 100, 0.4539393400691996], "caller": "main.py:200", "time": "2021-02-05T06:16:17.852799"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23532316088676453, 0.23272372782230377, 100, 0.45803509350011024], "caller": "main.py:200", "time": "2021-02-05T06:17:41.768439"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2331160306930542, 0.24416548013687134, 100, 0.46992711330721343], "caller": "main.py:200", "time": "2021-02-05T06:19:06.276909"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.22992555797100067, 0.2131688892841339, 100, 0.46056539054079654], "caller": "main.py:200", "time": "2021-02-05T06:20:31.342999"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23451000452041626, 0.20407795906066895, 100, 0.4706508472091186], "caller": "main.py:200", "time": "2021-02-05T06:21:55.800406"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23527327179908752, 0.22056610882282257, 100, 0.46581905717959915], "caller": "main.py:200", "time": "2021-02-05T06:23:20.011747"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23198404908180237, 0.2381836622953415, 100, 0.45983206148317635], "caller": "main.py:200", "time": "2021-02-05T06:24:44.468037"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2338002622127533, 0.26579520106315613, 100, 0.47079243893683803], "caller": "main.py:200", "time": "2021-02-05T06:26:09.324985"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23447303473949432, 0.24053707718849182, 100, 0.46107379193463605], "caller": "main.py:200", "time": "2021-02-05T06:27:34.019540"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23031164705753326, 0.2078428864479065, 100, 0.4469660365134547], "caller": "main.py:200", "time": "2021-02-05T06:28:58.926141"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23225905001163483, 0.26054278016090393, 100, 0.4625957431926664], "caller": "main.py:200", "time": "2021-02-05T06:30:23.691610"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23172885179519653, 0.22829553484916687, 100, 0.4614923882022944], "caller": "main.py:200", "time": "2021-02-05T06:31:48.834340"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.2300747036933899, 0.22779832780361176, 100, 0.47626630897527683], "caller": "main.py:200", "time": "2021-02-05T06:33:14.121116"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22698825597763062, 0.2417001724243164, 100, 0.452276017024589], "caller": "main.py:200", "time": "2021-02-05T06:34:39.712053"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22937501966953278, 0.23394030332565308, 100, 0.4447970348151776], "caller": "main.py:200", "time": "2021-02-05T06:36:05.008397"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23091286420822144, 0.23570109903812408, 100, 0.45287754918576467], "caller": "main.py:200", "time": "2021-02-05T06:37:30.536882"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [18], "caller": "main.py:129", "time": "2021-02-05T06:38:54.253419"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T06:39:03.561044"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 22.591292451709577, 0.5066565546767223], "caller": "main.py:37", "time": "2021-02-05T06:39:07.622659"}
+{"level": "info", "fmt": "episode: %s", "args": [5.976062145430522], "caller": "main.py:149", "time": "2021-02-05T06:39:17.812316"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23678892850875854, 0.24572071433067322, 100, 0.4555732756397531], "caller": "main.py:200", "time": "2021-02-05T06:39:19.867024"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2353256344795227, 0.21990510821342468, 100, 0.4620146289005797], "caller": "main.py:200", "time": "2021-02-05T06:40:44.770408"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23664043843746185, 0.21498650312423706, 100, 0.4563225706955444], "caller": "main.py:200", "time": "2021-02-05T06:42:10.670834"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2338596135377884, 0.2312772572040558, 100, 0.4592355805176217], "caller": "main.py:200", "time": "2021-02-05T06:43:36.585366"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23109443485736847, 0.21659594774246216, 100, 0.45571222092124475], "caller": "main.py:200", "time": "2021-02-05T06:45:01.247250"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2330709844827652, 0.22419197857379913, 100, 0.4392592256822017], "caller": "main.py:200", "time": "2021-02-05T06:46:26.249443"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23462063074111938, 0.2433680146932602, 100, 0.4509230172183202], "caller": "main.py:200", "time": "2021-02-05T06:47:51.755178"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23405911028385162, 0.23898372054100037, 100, 0.457219464435155], "caller": "main.py:200", "time": "2021-02-05T06:49:16.641839"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23065179586410522, 0.2443445324897766, 100, 0.4577004450101916], "caller": "main.py:200", "time": "2021-02-05T06:50:41.440650"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23051807284355164, 0.23562215268611908, 100, 0.45325575376984856], "caller": "main.py:200", "time": "2021-02-05T06:52:07.094728"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2334284782409668, 0.24470052123069763, 100, 0.4477801662235707], "caller": "main.py:200", "time": "2021-02-05T06:53:32.089679"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2318325936794281, 0.2591647803783417, 100, 0.4474077340513095], "caller": "main.py:200", "time": "2021-02-05T06:54:57.564113"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22810064256191254, 0.2502260208129883, 100, 0.4496019193080481], "caller": "main.py:200", "time": "2021-02-05T06:56:23.513295"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2298877090215683, 0.22838044166564941, 100, 0.4577901687760327], "caller": "main.py:200", "time": "2021-02-05T06:57:48.873591"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23073039948940277, 0.21686507761478424, 100, 0.4480628573446438], "caller": "main.py:200", "time": "2021-02-05T06:59:14.672994"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2319490760564804, 0.23545992374420166, 100, 0.448813447609366], "caller": "main.py:200", "time": "2021-02-05T07:00:40.646248"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23144912719726562, 0.22021140158176422, 100, 0.449950697973079], "caller": "main.py:200", "time": "2021-02-05T07:02:06.424119"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23167894780635834, 0.23121041059494019, 100, 0.44263144756897616], "caller": "main.py:200", "time": "2021-02-05T07:03:31.328928"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2294718623161316, 0.22808435559272766, 100, 0.44457138712728816], "caller": "main.py:200", "time": "2021-02-05T07:04:56.828166"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2302701324224472, 0.23662367463111877, 100, 0.44259444083317817], "caller": "main.py:200", "time": "2021-02-05T07:06:22.345712"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [19], "caller": "main.py:129", "time": "2021-02-05T07:07:46.019125"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T07:07:55.420245"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 16.575898185210274, 3.0381880915286703], "caller": "main.py:37", "time": "2021-02-05T07:07:59.487268"}
+{"level": "info", "fmt": "episode: %s", "args": [6.801214068929713], "caller": "main.py:149", "time": "2021-02-05T07:08:09.828968"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.235059916973114, 0.2576184868812561, 100, 0.4515701269651885], "caller": "main.py:200", "time": "2021-02-05T07:08:11.905869"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2339038848876953, 0.2510763704776764, 100, 0.4475181898027175], "caller": "main.py:200", "time": "2021-02-05T07:09:37.355904"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23485197126865387, 0.23424509167671204, 100, 0.4572478055496732], "caller": "main.py:200", "time": "2021-02-05T07:11:03.091671"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23379234969615936, 0.2054198980331421, 100, 0.45768446470690566], "caller": "main.py:200", "time": "2021-02-05T07:12:28.448803"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23196735978126526, 0.23525120317935944, 100, 0.4272540342380516], "caller": "main.py:200", "time": "2021-02-05T07:13:52.989967"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23106621205806732, 0.23004399240016937, 100, 0.4548404702089372], "caller": "main.py:200", "time": "2021-02-05T07:15:17.490902"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23141609132289886, 0.22950252890586853, 100, 0.44996280457050136], "caller": "main.py:200", "time": "2021-02-05T07:16:41.697252"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23460924625396729, 0.23333299160003662, 100, 0.4517458426500616], "caller": "main.py:200", "time": "2021-02-05T07:18:05.622490"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23318763077259064, 0.21843907237052917, 100, 0.4524909567380456], "caller": "main.py:200", "time": "2021-02-05T07:19:30.024859"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2295101135969162, 0.24244031310081482, 100, 0.44473647023760415], "caller": "main.py:200", "time": "2021-02-05T07:20:54.611727"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.22887387871742249, 0.2252350151538849, 100, 0.4403767612136108], "caller": "main.py:200", "time": "2021-02-05T07:22:19.398363"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22893011569976807, 0.2130654752254486, 100, 0.4400187640942199], "caller": "main.py:200", "time": "2021-02-05T07:23:43.738687"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23231498897075653, 0.2196507453918457, 100, 0.44438503409948626], "caller": "main.py:200", "time": "2021-02-05T07:25:08.131308"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.226996511220932, 0.23967714607715607, 100, 0.4489899149088097], "caller": "main.py:200", "time": "2021-02-05T07:26:32.685513"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22925955057144165, 0.22958122193813324, 100, 0.44343450780248883], "caller": "main.py:200", "time": "2021-02-05T07:27:57.227510"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22911719977855682, 0.21450024843215942, 100, 0.43392856444549494], "caller": "main.py:200", "time": "2021-02-05T07:29:21.579913"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22940796613693237, 0.24871481955051422, 100, 0.442663846269687], "caller": "main.py:200", "time": "2021-02-05T07:30:45.930277"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2320886105298996, 0.2444828301668167, 100, 0.45582908506603514], "caller": "main.py:200", "time": "2021-02-05T07:32:09.774532"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23058828711509705, 0.2167779952287674, 100, 0.4496486699131498], "caller": "main.py:200", "time": "2021-02-05T07:33:33.972474"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23110555112361908, 0.23248390853405, 100, 0.44461273367727183], "caller": "main.py:200", "time": "2021-02-05T07:34:58.573966"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [20], "caller": "main.py:129", "time": "2021-02-05T07:36:21.104228"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T07:36:30.468602"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 35.50797603535172, 11.696723458647924], "caller": "main.py:37", "time": "2021-02-05T07:36:34.712406"}
+{"level": "info", "fmt": "episode: %s", "args": [4.954166266683951], "caller": "main.py:149", "time": "2021-02-05T07:36:45.244356"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23200687766075134, 0.24967879056930542, 100, 0.45397842462291427], "caller": "main.py:200", "time": "2021-02-05T07:36:47.323613"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23126055300235748, 0.22798220813274384, 100, 0.44671917195621313], "caller": "main.py:200", "time": "2021-02-05T07:38:11.698454"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23137202858924866, 0.20931392908096313, 100, 0.44132664169428215], "caller": "main.py:200", "time": "2021-02-05T07:39:35.568495"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23272937536239624, 0.21460652351379395, 100, 0.4390347024359359], "caller": "main.py:200", "time": "2021-02-05T07:41:00.056802"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23037250339984894, 0.2335653454065323, 100, 0.4434593867812156], "caller": "main.py:200", "time": "2021-02-05T07:42:24.175737"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23159682750701904, 0.2372710257768631, 100, 0.4469195755869266], "caller": "main.py:200", "time": "2021-02-05T07:43:48.464765"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.22598280012607574, 0.2419876605272293, 100, 0.4370839973847088], "caller": "main.py:200", "time": "2021-02-05T07:45:13.238346"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.22913362085819244, 0.2262241095304489, 100, 0.43579488243275033], "caller": "main.py:200", "time": "2021-02-05T07:46:37.437884"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2270323783159256, 0.23754096031188965, 100, 0.4389278637923957], "caller": "main.py:200", "time": "2021-02-05T07:48:02.173673"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.22833533585071564, 0.2510738670825958, 100, 0.4593681665795629], "caller": "main.py:200", "time": "2021-02-05T07:49:26.996687"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.22879809141159058, 0.2241210639476776, 100, 0.43778590078744395], "caller": "main.py:200", "time": "2021-02-05T07:50:51.050156"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22978082299232483, 0.20631016790866852, 100, 0.4411116504228078], "caller": "main.py:200", "time": "2021-02-05T07:52:15.371840"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23067785799503326, 0.24106743931770325, 100, 0.4416084253346386], "caller": "main.py:200", "time": "2021-02-05T07:53:39.894324"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23025988042354584, 0.23235741257667542, 100, 0.45072956557534943], "caller": "main.py:200", "time": "2021-02-05T07:55:04.020343"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22742120921611786, 0.26399409770965576, 100, 0.44182778324238875], "caller": "main.py:200", "time": "2021-02-05T07:56:28.576112"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.226710706949234, 0.2117944359779358, 100, 0.4477048766165302], "caller": "main.py:200", "time": "2021-02-05T07:57:53.343429"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22990193963050842, 0.2326594740152359, 100, 0.45201874460230107], "caller": "main.py:200", "time": "2021-02-05T07:59:17.354609"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22767825424671173, 0.23734712600708008, 100, 0.43725691433348685], "caller": "main.py:200", "time": "2021-02-05T08:00:41.488942"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22730767726898193, 0.2334766387939453, 100, 0.4256877526215205], "caller": "main.py:200", "time": "2021-02-05T08:02:05.313755"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2275833636522293, 0.21819943189620972, 100, 0.43759902711306997], "caller": "main.py:200", "time": "2021-02-05T08:03:29.411032"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [21], "caller": "main.py:129", "time": "2021-02-05T08:04:51.945383"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T08:05:01.397839"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 11.678316187586628, 0.8241494184373047], "caller": "main.py:37", "time": "2021-02-05T08:05:05.420591"}
+{"level": "info", "fmt": "episode: %s", "args": [0.7405185857933296], "caller": "main.py:149", "time": "2021-02-05T08:05:15.902418"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2329593151807785, 0.24336454272270203, 100, 0.4505266826894368], "caller": "main.py:200", "time": "2021-02-05T08:05:17.964751"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.22805869579315186, 0.25074946880340576, 100, 0.445466201558866], "caller": "main.py:200", "time": "2021-02-05T08:06:41.793054"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.22591181099414825, 0.23801615834236145, 100, 0.4421076917206208], "caller": "main.py:200", "time": "2021-02-05T08:08:06.477789"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2277001440525055, 0.2154579758644104, 100, 0.4349544584316175], "caller": "main.py:200", "time": "2021-02-05T08:09:30.524203"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.22770191729068756, 0.24047306180000305, 100, 0.4454101776631311], "caller": "main.py:200", "time": "2021-02-05T08:10:55.145713"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.22394296526908875, 0.23222608864307404, 100, 0.4421647125040395], "caller": "main.py:200", "time": "2021-02-05T08:12:19.538660"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.226701557636261, 0.22540119290351868, 100, 0.43667385752602716], "caller": "main.py:200", "time": "2021-02-05T08:13:43.817967"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2242944836616516, 0.20997388660907745, 100, 0.4257047745159737], "caller": "main.py:200", "time": "2021-02-05T08:15:08.088091"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2249254733324051, 0.2349366843700409, 100, 0.4411407994782858], "caller": "main.py:200", "time": "2021-02-05T08:16:32.457535"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.22629916667938232, 0.25230658054351807, 100, 0.44670336331430416], "caller": "main.py:200", "time": "2021-02-05T08:17:56.903873"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2267710119485855, 0.21963194012641907, 100, 0.4420914986287531], "caller": "main.py:200", "time": "2021-02-05T08:19:21.273463"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22553569078445435, 0.2366010695695877, 100, 0.4445815830978225], "caller": "main.py:200", "time": "2021-02-05T08:20:45.900155"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22712768614292145, 0.23524415493011475, 100, 0.4424032461200816], "caller": "main.py:200", "time": "2021-02-05T08:22:10.107624"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.22481849789619446, 0.2332494556903839, 100, 0.4573072552223662], "caller": "main.py:200", "time": "2021-02-05T08:23:35.138130"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22663724422454834, 0.22033077478408813, 100, 0.4414046316897539], "caller": "main.py:200", "time": "2021-02-05T08:24:59.874916"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22284114360809326, 0.24398528039455414, 100, 0.43693393494289795], "caller": "main.py:200", "time": "2021-02-05T08:26:24.743544"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22525501251220703, 0.2440469115972519, 100, 0.43492664034785716], "caller": "main.py:200", "time": "2021-02-05T08:27:49.213017"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2248300015926361, 0.21128100156784058, 100, 0.43459627236550263], "caller": "main.py:200", "time": "2021-02-05T08:29:13.499469"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2244674265384674, 0.19695927202701569, 100, 0.4352485620540246], "caller": "main.py:200", "time": "2021-02-05T08:30:37.422945"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2258348912000656, 0.22057051956653595, 100, 0.4496342607883], "caller": "main.py:200", "time": "2021-02-05T08:32:02.107271"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [22], "caller": "main.py:129", "time": "2021-02-05T08:33:24.288118"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T08:33:33.745374"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 13.646150891386148, 0.842932619768964], "caller": "main.py:37", "time": "2021-02-05T08:33:37.758700"}
+{"level": "info", "fmt": "episode: %s", "args": [7.495536238869844], "caller": "main.py:149", "time": "2021-02-05T08:33:48.196832"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23146165907382965, 0.2181902825832367, 100, 0.4400997525013468], "caller": "main.py:200", "time": "2021-02-05T08:33:50.246089"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2274530977010727, 0.2222861647605896, 100, 0.43325717623669596], "caller": "main.py:200", "time": "2021-02-05T08:35:14.319316"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.22841982543468475, 0.25896620750427246, 100, 0.44349357868766437], "caller": "main.py:200", "time": "2021-02-05T08:36:38.603080"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23159055411815643, 0.23844605684280396, 100, 0.43897746945998184], "caller": "main.py:200", "time": "2021-02-05T08:38:02.749225"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.22758233547210693, 0.23871368169784546, 100, 0.4212396233852671], "caller": "main.py:200", "time": "2021-02-05T08:39:27.332666"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.22957351803779602, 0.20552703738212585, 100, 0.4257119863799676], "caller": "main.py:200", "time": "2021-02-05T08:40:51.923725"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.227495938539505, 0.23718777298927307, 100, 0.43351626332239396], "caller": "main.py:200", "time": "2021-02-05T08:42:16.401375"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.22724592685699463, 0.2320299744606018, 100, 0.4378466274896512], "caller": "main.py:200", "time": "2021-02-05T08:43:41.479854"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.22853054106235504, 0.21745902299880981, 100, 0.432450433032183], "caller": "main.py:200", "time": "2021-02-05T08:45:05.995232"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.22813375294208527, 0.23637905716896057, 100, 0.42335310365496465], "caller": "main.py:200", "time": "2021-02-05T08:46:29.854553"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.22510850429534912, 0.21295496821403503, 100, 0.4336928450627556], "caller": "main.py:200", "time": "2021-02-05T08:47:54.331363"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22726497054100037, 0.2387245148420334, 100, 0.43300602550944906], "caller": "main.py:200", "time": "2021-02-05T08:49:19.054967"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2270524948835373, 0.20198146998882294, 100, 0.43288614060126174], "caller": "main.py:200", "time": "2021-02-05T08:50:43.795536"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2265285700559616, 0.2256505936384201, 100, 0.42524836357155754], "caller": "main.py:200", "time": "2021-02-05T08:52:08.558787"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2282010316848755, 0.22593548893928528, 100, 0.4417589661037238], "caller": "main.py:200", "time": "2021-02-05T08:53:33.687801"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22643284499645233, 0.22619250416755676, 100, 0.43485948141040964], "caller": "main.py:200", "time": "2021-02-05T08:54:58.175010"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22714722156524658, 0.22351756691932678, 100, 0.4322143324777404], "caller": "main.py:200", "time": "2021-02-05T08:56:23.196312"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22618955373764038, 0.22516003251075745, 100, 0.4376068985024582], "caller": "main.py:200", "time": "2021-02-05T08:57:47.865482"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2276214361190796, 0.22527214884757996, 100, 0.4364525720040652], "caller": "main.py:200", "time": "2021-02-05T08:59:12.533597"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2256065011024475, 0.23036065697669983, 100, 0.44000161106954816], "caller": "main.py:200", "time": "2021-02-05T09:00:36.196322"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [23], "caller": "main.py:129", "time": "2021-02-05T09:01:59.273442"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T09:02:08.809749"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 14.787321611438987, 1.5307683208637288], "caller": "main.py:37", "time": "2021-02-05T09:02:12.720756"}
+{"level": "info", "fmt": "episode: %s", "args": [7.9897001042202], "caller": "main.py:149", "time": "2021-02-05T09:02:23.366138"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2296789586544037, 0.22227759659290314, 100, 0.4259324451855066], "caller": "main.py:200", "time": "2021-02-05T09:02:25.471773"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.22773799300193787, 0.22642311453819275, 100, 0.4269938361217751], "caller": "main.py:200", "time": "2021-02-05T09:03:50.052160"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23173849284648895, 0.21476605534553528, 100, 0.4289557224083145], "caller": "main.py:200", "time": "2021-02-05T09:05:14.513368"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.22872401773929596, 0.21574732661247253, 100, 0.4191789629636249], "caller": "main.py:200", "time": "2021-02-05T09:06:38.897607"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.22903625667095184, 0.25186383724212646, 100, 0.427203625099854], "caller": "main.py:200", "time": "2021-02-05T09:08:03.326012"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2311437427997589, 0.2246394157409668, 100, 0.4329810389443385], "caller": "main.py:200", "time": "2021-02-05T09:09:27.817044"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.22891083359718323, 0.21505534648895264, 100, 0.4241666623525838], "caller": "main.py:200", "time": "2021-02-05T09:10:53.066694"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.22808845341205597, 0.24990767240524292, 100, 0.4356497180026234], "caller": "main.py:200", "time": "2021-02-05T09:12:17.791395"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2279646098613739, 0.20408862829208374, 100, 0.4239628907614177], "caller": "main.py:200", "time": "2021-02-05T09:13:42.922025"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2261800318956375, 0.2387159764766693, 100, 0.4183436679421744], "caller": "main.py:200", "time": "2021-02-05T09:15:07.300921"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23035933077335358, 0.21731385588645935, 100, 0.4316625675127382], "caller": "main.py:200", "time": "2021-02-05T09:16:31.730990"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22864408791065216, 0.23575985431671143, 100, 0.42866403813843856], "caller": "main.py:200", "time": "2021-02-05T09:17:55.986255"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22640617191791534, 0.23659692704677582, 100, 0.4224291613275577], "caller": "main.py:200", "time": "2021-02-05T09:19:20.690875"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23104728758335114, 0.23005177080631256, 100, 0.4218358432824777], "caller": "main.py:200", "time": "2021-02-05T09:20:44.811581"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2274480015039444, 0.23568496108055115, 100, 0.41976290907238456], "caller": "main.py:200", "time": "2021-02-05T09:22:09.530854"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2264994978904724, 0.22596633434295654, 100, 0.44431072826710766], "caller": "main.py:200", "time": "2021-02-05T09:23:34.285274"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22854375839233398, 0.22115981578826904, 100, 0.4283836993146713], "caller": "main.py:200", "time": "2021-02-05T09:24:58.282555"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22585396468639374, 0.20178104937076569, 100, 0.4179365950404894], "caller": "main.py:200", "time": "2021-02-05T09:26:22.806094"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22730234265327454, 0.21419626474380493, 100, 0.4266398757269512], "caller": "main.py:200", "time": "2021-02-05T09:27:47.103522"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.22416910529136658, 0.20088323950767517, 100, 0.4225049536997768], "caller": "main.py:200", "time": "2021-02-05T09:29:11.327784"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [24], "caller": "main.py:129", "time": "2021-02-05T09:30:33.645752"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T09:30:43.177271"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 16.75103532874253, 3.9489207613781168], "caller": "main.py:37", "time": "2021-02-05T09:30:47.392210"}
+{"level": "info", "fmt": "episode: %s", "args": [5.900852249399076], "caller": "main.py:149", "time": "2021-02-05T09:30:57.670285"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.22900953888893127, 0.27509045600891113, 100, 0.4177231987893091], "caller": "main.py:200", "time": "2021-02-05T09:30:59.702051"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23160240054130554, 0.22372379899024963, 100, 0.4212767836030254], "caller": "main.py:200", "time": "2021-02-05T09:32:24.561611"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2299179583787918, 0.2303573489189148, 100, 0.4205876698669417], "caller": "main.py:200", "time": "2021-02-05T09:33:49.260847"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2287469059228897, 0.22462165355682373, 100, 0.42782956178678405], "caller": "main.py:200", "time": "2021-02-05T09:35:14.162256"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.22891603410243988, 0.22504167258739471, 100, 0.4204216271218449], "caller": "main.py:200", "time": "2021-02-05T09:36:38.848223"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.22728127241134644, 0.2097894549369812, 100, 0.4230411925488862], "caller": "main.py:200", "time": "2021-02-05T09:38:03.600618"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.22944797575473785, 0.24224016070365906, 100, 0.4119073274319729], "caller": "main.py:200", "time": "2021-02-05T09:39:28.200316"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2270640730857849, 0.21584001183509827, 100, 0.42673471416736], "caller": "main.py:200", "time": "2021-02-05T09:40:53.535297"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.22915852069854736, 0.2426147758960724, 100, 0.43800278689057554], "caller": "main.py:200", "time": "2021-02-05T09:42:18.051178"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.22880497574806213, 0.2463330626487732, 100, 0.4217616739447356], "caller": "main.py:200", "time": "2021-02-05T09:43:42.760416"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2265554964542389, 0.21464776992797852, 100, 0.4245857256226339], "caller": "main.py:200", "time": "2021-02-05T09:45:06.968245"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22774738073349, 0.2462335228919983, 100, 0.4232103192383034], "caller": "main.py:200", "time": "2021-02-05T09:46:31.346817"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2273605912923813, 0.23120859265327454, 100, 0.43475142713013937], "caller": "main.py:200", "time": "2021-02-05T09:47:55.170977"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.224029079079628, 0.2174343317747116, 100, 0.411934139985376], "caller": "main.py:200", "time": "2021-02-05T09:49:19.214888"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22686201333999634, 0.20543088018894196, 100, 0.4160663929165576], "caller": "main.py:200", "time": "2021-02-05T09:50:43.391757"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22835072875022888, 0.24601656198501587, 100, 0.44281969036204055], "caller": "main.py:200", "time": "2021-02-05T09:52:07.946370"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22853542864322662, 0.20579226315021515, 100, 0.4137307950440571], "caller": "main.py:200", "time": "2021-02-05T09:53:32.367886"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22480060160160065, 0.23445820808410645, 100, 0.42250672991819094], "caller": "main.py:200", "time": "2021-02-05T09:54:56.512440"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22941723465919495, 0.22701774537563324, 100, 0.4239478328400096], "caller": "main.py:200", "time": "2021-02-05T09:56:20.708633"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.22432689368724823, 0.21706244349479675, 100, 0.41671932961349856], "caller": "main.py:200", "time": "2021-02-05T09:57:44.819640"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [25], "caller": "main.py:129", "time": "2021-02-05T09:59:06.557949"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T09:59:15.695398"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 16.3812468206838, 1.4244652225023984], "caller": "main.py:37", "time": "2021-02-05T09:59:19.634018"}
+{"level": "info", "fmt": "episode: %s", "args": [8.50590461653519], "caller": "main.py:149", "time": "2021-02-05T09:59:30.032019"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.22846826910972595, 0.24267610907554626, 100, 0.41908556457143376], "caller": "main.py:200", "time": "2021-02-05T09:59:32.079547"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2289559692144394, 0.21861332654953003, 100, 0.4243272152118741], "caller": "main.py:200", "time": "2021-02-05T10:00:56.726452"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.22599942982196808, 0.20319610834121704, 100, 0.4104893636293002], "caller": "main.py:200", "time": "2021-02-05T10:02:21.199092"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2259712666273117, 0.229896679520607, 100, 0.41839934315073746], "caller": "main.py:200", "time": "2021-02-05T10:03:45.506357"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.22949257493019104, 0.2119535505771637, 100, 0.4146450998964513], "caller": "main.py:200", "time": "2021-02-05T10:05:10.063254"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2281280755996704, 0.24138349294662476, 100, 0.4111149799412646], "caller": "main.py:200", "time": "2021-02-05T10:06:34.800400"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.22875478863716125, 0.2389909029006958, 100, 0.42499280835908537], "caller": "main.py:200", "time": "2021-02-05T10:07:59.554896"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.22700923681259155, 0.23604990541934967, 100, 0.4210035693224474], "caller": "main.py:200", "time": "2021-02-05T10:09:24.607377"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2254626303911209, 0.21443504095077515, 100, 0.40838811065643066], "caller": "main.py:200", "time": "2021-02-05T10:10:49.131774"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2253580093383789, 0.22282874584197998, 100, 0.4131205570284664], "caller": "main.py:200", "time": "2021-02-05T10:12:13.282174"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.22705651819705963, 0.2334049642086029, 100, 0.40926069076737015], "caller": "main.py:200", "time": "2021-02-05T10:13:37.886222"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22620481252670288, 0.2126861810684204, 100, 0.40500274505141487], "caller": "main.py:200", "time": "2021-02-05T10:15:02.221105"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22564832866191864, 0.22062227129936218, 100, 0.4231458064490604], "caller": "main.py:200", "time": "2021-02-05T10:16:26.845031"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2277829349040985, 0.21216678619384766, 100, 0.4147589745702833], "caller": "main.py:200", "time": "2021-02-05T10:17:51.286822"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22410330176353455, 0.22810602188110352, 100, 0.4143692043009284], "caller": "main.py:200", "time": "2021-02-05T10:19:15.984912"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22607174515724182, 0.22563862800598145, 100, 0.41305404927176304], "caller": "main.py:200", "time": "2021-02-05T10:20:40.304892"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.2249542772769928, 0.23390032351016998, 100, 0.41753046866056326], "caller": "main.py:200", "time": "2021-02-05T10:22:04.518181"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2251235395669937, 0.2272515594959259, 100, 0.41293341274900514], "caller": "main.py:200", "time": "2021-02-05T10:23:28.789063"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2215394228696823, 0.23634155094623566, 100, 0.40192043032203256], "caller": "main.py:200", "time": "2021-02-05T10:24:53.567802"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.22681652009487152, 0.23759952187538147, 100, 0.41267661477968953], "caller": "main.py:200", "time": "2021-02-05T10:26:18.526006"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [26], "caller": "main.py:129", "time": "2021-02-05T10:27:40.729666"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T10:27:50.225197"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 21.384929069126127, 2.4757067411925124], "caller": "main.py:37", "time": "2021-02-05T10:27:54.300445"}
+{"level": "info", "fmt": "episode: %s", "args": [7.895967041952188], "caller": "main.py:149", "time": "2021-02-05T10:28:04.841765"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23329991102218628, 0.22833693027496338, 100, 0.4220623680527149], "caller": "main.py:200", "time": "2021-02-05T10:28:06.996864"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23172339797019958, 0.23100614547729492, 100, 0.40140476848641365], "caller": "main.py:200", "time": "2021-02-05T10:29:31.278788"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2287292331457138, 0.20986971259117126, 100, 0.420338766830849], "caller": "main.py:200", "time": "2021-02-05T10:30:55.518094"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.22908878326416016, 0.23271626234054565, 100, 0.40362275954024995], "caller": "main.py:200", "time": "2021-02-05T10:32:20.328142"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.22901897132396698, 0.21118101477622986, 100, 0.4191519692121017], "caller": "main.py:200", "time": "2021-02-05T10:33:44.632411"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.22908951342105865, 0.20597173273563385, 100, 0.41529044564385764], "caller": "main.py:200", "time": "2021-02-05T10:35:08.494269"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23022235929965973, 0.20681267976760864, 100, 0.4091203826256758], "caller": "main.py:200", "time": "2021-02-05T10:36:33.287465"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23036514222621918, 0.2261263132095337, 100, 0.41862295981035635], "caller": "main.py:200", "time": "2021-02-05T10:37:57.617723"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2267182320356369, 0.2392967790365219, 100, 0.4269524520208653], "caller": "main.py:200", "time": "2021-02-05T10:39:22.240096"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23092392086982727, 0.23132500052452087, 100, 0.4138744058791181], "caller": "main.py:200", "time": "2021-02-05T10:40:46.719596"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2277345061302185, 0.24620458483695984, 100, 0.41361489738858165], "caller": "main.py:200", "time": "2021-02-05T10:42:11.052093"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22896669805049896, 0.23505553603172302, 100, 0.4173951890647249], "caller": "main.py:200", "time": "2021-02-05T10:43:35.788154"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23045678436756134, 0.23032410442829132, 100, 0.4195325469551171], "caller": "main.py:200", "time": "2021-02-05T10:45:00.576550"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23094232380390167, 0.2291749119758606, 100, 0.41659733350879347], "caller": "main.py:200", "time": "2021-02-05T10:46:24.993796"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22946952283382416, 0.2070923149585724, 100, 0.41184596266204343], "caller": "main.py:200", "time": "2021-02-05T10:47:49.505333"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22914983332157135, 0.22693291306495667, 100, 0.41179357464963945], "caller": "main.py:200", "time": "2021-02-05T10:49:14.100705"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.230529323220253, 0.22341805696487427, 100, 0.4097350674457895], "caller": "main.py:200", "time": "2021-02-05T10:50:38.628013"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22557421028614044, 0.22160880267620087, 100, 0.41909192796330935], "caller": "main.py:200", "time": "2021-02-05T10:52:03.337872"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22521667182445526, 0.21596431732177734, 100, 0.4132467937056236], "caller": "main.py:200", "time": "2021-02-05T10:53:28.003274"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2251421958208084, 0.23713986575603485, 100, 0.4093473034687775], "caller": "main.py:200", "time": "2021-02-05T10:54:52.190476"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [27], "caller": "main.py:129", "time": "2021-02-05T10:56:14.311704"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T10:56:23.166765"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 16.469842913766026, 0.09926198037178974], "caller": "main.py:37", "time": "2021-02-05T10:56:27.111106"}
+{"level": "info", "fmt": "episode: %s", "args": [6.4530090679476775], "caller": "main.py:149", "time": "2021-02-05T10:56:36.838545"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23150356113910675, 0.23053812980651855, 100, 0.4171253275454152], "caller": "main.py:200", "time": "2021-02-05T10:56:38.881956"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23342232406139374, 0.23725609481334686, 100, 0.4240858834557833], "caller": "main.py:200", "time": "2021-02-05T10:58:03.030095"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23197823762893677, 0.23863019049167633, 100, 0.4262582617571865], "caller": "main.py:200", "time": "2021-02-05T10:59:27.579552"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23243603110313416, 0.21856436133384705, 100, 0.4165192022503136], "caller": "main.py:200", "time": "2021-02-05T11:00:52.076103"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23066078126430511, 0.25932490825653076, 100, 0.4037162559820817], "caller": "main.py:200", "time": "2021-02-05T11:02:16.829212"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23150569200515747, 0.23925775289535522, 100, 0.4088419663497256], "caller": "main.py:200", "time": "2021-02-05T11:03:41.322846"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2295193076133728, 0.23605120182037354, 100, 0.4078849261591246], "caller": "main.py:200", "time": "2021-02-05T11:05:05.551972"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.22871188819408417, 0.24074429273605347, 100, 0.4220809688742081], "caller": "main.py:200", "time": "2021-02-05T11:06:29.695068"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.22955270111560822, 0.23456184566020966, 100, 0.42274019207303287], "caller": "main.py:200", "time": "2021-02-05T11:07:54.085462"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.22809921205043793, 0.21796569228172302, 100, 0.4144427930894304], "caller": "main.py:200", "time": "2021-02-05T11:09:19.464274"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2308286875486374, 0.21301540732383728, 100, 0.4151793610634473], "caller": "main.py:200", "time": "2021-02-05T11:10:43.731066"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22956305742263794, 0.19841308891773224, 100, 0.4059887873720446], "caller": "main.py:200", "time": "2021-02-05T11:12:08.391923"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22883471846580505, 0.2482808530330658, 100, 0.4158454325379328], "caller": "main.py:200", "time": "2021-02-05T11:13:33.259111"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.22951200604438782, 0.22678108513355255, 100, 0.4141053592744617], "caller": "main.py:200", "time": "2021-02-05T11:14:57.768216"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22774146497249603, 0.24708041548728943, 100, 0.4124291714613473], "caller": "main.py:200", "time": "2021-02-05T11:16:22.435084"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23062698543071747, 0.2055921107530594, 100, 0.4074673246929874], "caller": "main.py:200", "time": "2021-02-05T11:17:46.859204"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.2290666252374649, 0.23233084380626678, 100, 0.4134982019249359], "caller": "main.py:200", "time": "2021-02-05T11:19:11.523058"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2289571315050125, 0.23866552114486694, 100, 0.416087848502512], "caller": "main.py:200", "time": "2021-02-05T11:20:35.582599"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22857166826725006, 0.225029855966568, 100, 0.42010896380514945], "caller": "main.py:200", "time": "2021-02-05T11:22:00.056773"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23065416514873505, 0.23923705518245697, 100, 0.4240023025446321], "caller": "main.py:200", "time": "2021-02-05T11:23:24.467596"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [28], "caller": "main.py:129", "time": "2021-02-05T11:24:46.817523"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T11:24:56.062007"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 16.684407853856555, 1.3412064134297486], "caller": "main.py:37", "time": "2021-02-05T11:25:00.092112"}
+{"level": "info", "fmt": "episode: %s", "args": [8.503659082553982], "caller": "main.py:149", "time": "2021-02-05T11:25:10.055582"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23239833116531372, 0.24972394108772278, 100, 0.413342157560976], "caller": "main.py:200", "time": "2021-02-05T11:25:12.090043"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23339633643627167, 0.22776781022548676, 100, 0.41854795332537853], "caller": "main.py:200", "time": "2021-02-05T11:26:35.721152"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2340412735939026, 0.23336540162563324, 100, 0.4145519020738898], "caller": "main.py:200", "time": "2021-02-05T11:27:59.796825"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23419791460037231, 0.22607091069221497, 100, 0.40377140219942237], "caller": "main.py:200", "time": "2021-02-05T11:29:24.407799"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23655575513839722, 0.23231931030750275, 100, 0.41740736361144226], "caller": "main.py:200", "time": "2021-02-05T11:30:48.524239"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23474888503551483, 0.20632554590702057, 100, 0.4167611580670352], "caller": "main.py:200", "time": "2021-02-05T11:32:13.449658"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23529760539531708, 0.245508074760437, 100, 0.42162671293572357], "caller": "main.py:200", "time": "2021-02-05T11:33:37.975492"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23239010572433472, 0.21756422519683838, 100, 0.420489930468472], "caller": "main.py:200", "time": "2021-02-05T11:35:02.272829"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2352842092514038, 0.21713164448738098, 100, 0.43001732404700227], "caller": "main.py:200", "time": "2021-02-05T11:36:26.765907"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2345941960811615, 0.245948925614357, 100, 0.4147019585552362], "caller": "main.py:200", "time": "2021-02-05T11:37:50.822348"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2350664585828781, 0.24084067344665527, 100, 0.4213339811142158], "caller": "main.py:200", "time": "2021-02-05T11:39:15.201492"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23126445710659027, 0.23010146617889404, 100, 0.4159559982599105], "caller": "main.py:200", "time": "2021-02-05T11:40:39.984054"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23539108037948608, 0.25763484835624695, 100, 0.42377742286259057], "caller": "main.py:200", "time": "2021-02-05T11:42:04.405351"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23336774110794067, 0.23329317569732666, 100, 0.4098870002813532], "caller": "main.py:200", "time": "2021-02-05T11:43:28.652708"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2304171323776245, 0.2293580174446106, 100, 0.41469353731299996], "caller": "main.py:200", "time": "2021-02-05T11:44:52.872375"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23218156397342682, 0.25721579790115356, 100, 0.427809523301259], "caller": "main.py:200", "time": "2021-02-05T11:46:17.194362"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23411718010902405, 0.23872634768486023, 100, 0.4327312305136918], "caller": "main.py:200", "time": "2021-02-05T11:47:41.558991"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23090137541294098, 0.23732367157936096, 100, 0.4145799055280954], "caller": "main.py:200", "time": "2021-02-05T11:49:06.050880"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2310263067483902, 0.24542877078056335, 100, 0.4092729749866339], "caller": "main.py:200", "time": "2021-02-05T11:50:30.758690"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23053394258022308, 0.24504883587360382, 100, 0.4130575689260366], "caller": "main.py:200", "time": "2021-02-05T11:51:55.582638"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [29], "caller": "main.py:129", "time": "2021-02-05T11:53:17.720796"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T11:53:26.711047"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 17.67494603701322, 0.8739401003799816], "caller": "main.py:37", "time": "2021-02-05T11:53:30.741301"}
+{"level": "info", "fmt": "episode: %s", "args": [8.661982128020187], "caller": "main.py:149", "time": "2021-02-05T11:53:40.996868"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2381761521100998, 0.23598472774028778, 100, 0.4222311517174014], "caller": "main.py:200", "time": "2021-02-05T11:53:43.070720"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23682613670825958, 0.2304438054561615, 100, 0.421128698843328], "caller": "main.py:200", "time": "2021-02-05T11:55:06.805024"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2360369861125946, 0.2586919665336609, 100, 0.42265974815433593], "caller": "main.py:200", "time": "2021-02-05T11:56:31.498841"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23618778586387634, 0.24411262571811676, 100, 0.41642680342801525], "caller": "main.py:200", "time": "2021-02-05T11:57:55.772239"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23721590638160706, 0.24810925126075745, 100, 0.4376433294573877], "caller": "main.py:200", "time": "2021-02-05T11:59:20.268434"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23670722544193268, 0.23183277249336243, 100, 0.4099792655892473], "caller": "main.py:200", "time": "2021-02-05T12:00:44.667506"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2345290333032608, 0.23624348640441895, 100, 0.41669586743717796], "caller": "main.py:200", "time": "2021-02-05T12:02:09.111457"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23672974109649658, 0.2091682404279709, 100, 0.4167809795916532], "caller": "main.py:200", "time": "2021-02-05T12:03:33.518430"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23708535730838776, 0.24444802105426788, 100, 0.4118888464158157], "caller": "main.py:200", "time": "2021-02-05T12:04:58.015219"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2346968650817871, 0.23961228132247925, 100, 0.41664938386088485], "caller": "main.py:200", "time": "2021-02-05T12:06:22.799334"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23421983420848846, 0.24035978317260742, 100, 0.42020185824006906], "caller": "main.py:200", "time": "2021-02-05T12:07:47.262738"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2352622002363205, 0.22993043065071106, 100, 0.4154114821137819], "caller": "main.py:200", "time": "2021-02-05T12:09:11.627562"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2370234876871109, 0.24542613327503204, 100, 0.4202005496200932], "caller": "main.py:200", "time": "2021-02-05T12:10:35.672454"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23560455441474915, 0.24280783534049988, 100, 0.41709310438038316], "caller": "main.py:200", "time": "2021-02-05T12:12:00.553355"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2339141070842743, 0.251403272151947, 100, 0.4289647334385107], "caller": "main.py:200", "time": "2021-02-05T12:13:25.078846"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23480769991874695, 0.2288515567779541, 100, 0.419814060884456], "caller": "main.py:200", "time": "2021-02-05T12:14:49.662666"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23552098870277405, 0.23452982306480408, 100, 0.41811583514800776], "caller": "main.py:200", "time": "2021-02-05T12:16:13.852735"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2345278412103653, 0.20782023668289185, 100, 0.4268315252234874], "caller": "main.py:200", "time": "2021-02-05T12:17:38.201763"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2350294142961502, 0.21054431796073914, 100, 0.4229721888476831], "caller": "main.py:200", "time": "2021-02-05T12:19:02.867732"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23374561965465546, 0.24860715866088867, 100, 0.41840132917750117], "caller": "main.py:200", "time": "2021-02-05T12:20:26.821685"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [30], "caller": "main.py:129", "time": "2021-02-05T12:21:48.854460"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T12:21:57.912694"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 14.12015962914338, 0.8329277295227031], "caller": "main.py:37", "time": "2021-02-05T12:22:01.899295"}
+{"level": "info", "fmt": "episode: %s", "args": [6.495148597439268], "caller": "main.py:149", "time": "2021-02-05T12:22:12.159595"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.24087347090244293, 0.23570066690444946, 100, 0.44291278894286845], "caller": "main.py:200", "time": "2021-02-05T12:22:14.254656"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23839399218559265, 0.24759666621685028, 100, 0.4167204796851194], "caller": "main.py:200", "time": "2021-02-05T12:23:38.820248"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.24042661488056183, 0.2576863765716553, 100, 0.42316026206553314], "caller": "main.py:200", "time": "2021-02-05T12:25:02.760070"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.24062447249889374, 0.2318393737077713, 100, 0.42783335860265587], "caller": "main.py:200", "time": "2021-02-05T12:26:27.160149"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23700514435768127, 0.23862163722515106, 100, 0.4222817128474432], "caller": "main.py:200", "time": "2021-02-05T12:27:51.348986"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23731762170791626, 0.24572665989398956, 100, 0.425904621439305], "caller": "main.py:200", "time": "2021-02-05T12:29:15.909664"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2386789470911026, 0.2255786955356598, 100, 0.41363421495592817], "caller": "main.py:200", "time": "2021-02-05T12:30:40.809362"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23940858244895935, 0.23716990649700165, 100, 0.4196282228407849], "caller": "main.py:200", "time": "2021-02-05T12:32:04.947898"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23519867658615112, 0.2258324921131134, 100, 0.42837205077894397], "caller": "main.py:200", "time": "2021-02-05T12:33:29.345358"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23893900215625763, 0.24659866094589233, 100, 0.41242582525658994], "caller": "main.py:200", "time": "2021-02-05T12:34:53.645737"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2348192036151886, 0.22183822095394135, 100, 0.4172090580642247], "caller": "main.py:200", "time": "2021-02-05T12:36:18.171291"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23468193411827087, 0.25687962770462036, 100, 0.41769120450611297], "caller": "main.py:200", "time": "2021-02-05T12:37:42.772134"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23422767221927643, 0.2751650810241699, 100, 0.41792973603599254], "caller": "main.py:200", "time": "2021-02-05T12:39:07.228924"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23627272248268127, 0.2229863703250885, 100, 0.40623720463028234], "caller": "main.py:200", "time": "2021-02-05T12:40:31.746330"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23384620249271393, 0.24161316454410553, 100, 0.4131784599606277], "caller": "main.py:200", "time": "2021-02-05T12:41:56.569131"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2361495941877365, 0.24747774004936218, 100, 0.41806205655924134], "caller": "main.py:200", "time": "2021-02-05T12:43:21.470662"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23527920246124268, 0.23463165760040283, 100, 0.41901475484949413], "caller": "main.py:200", "time": "2021-02-05T12:44:45.761954"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23700012266635895, 0.2550811171531677, 100, 0.4318387049004731], "caller": "main.py:200", "time": "2021-02-05T12:46:10.188695"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2351173758506775, 0.222858265042305, 100, 0.43174424584362825], "caller": "main.py:200", "time": "2021-02-05T12:47:34.389529"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2320203185081482, 0.2480468600988388, 100, 0.44091161425472936], "caller": "main.py:200", "time": "2021-02-05T12:48:58.547547"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [31], "caller": "main.py:129", "time": "2021-02-05T12:50:21.122723"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T12:50:30.384272"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 16.697821233148765, 0.30912444907498643], "caller": "main.py:37", "time": "2021-02-05T12:50:34.394165"}
+{"level": "info", "fmt": "episode: %s", "args": [9.169208715600298], "caller": "main.py:149", "time": "2021-02-05T12:50:44.747463"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2377040833234787, 0.22912120819091797, 100, 0.4209805997785158], "caller": "main.py:200", "time": "2021-02-05T12:50:46.800747"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23814602196216583, 0.24404509365558624, 100, 0.42376475598150737], "caller": "main.py:200", "time": "2021-02-05T12:52:11.288214"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2378983497619629, 0.2189067006111145, 100, 0.428547434763969], "caller": "main.py:200", "time": "2021-02-05T12:53:35.000764"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2393874228000641, 0.2398446500301361, 100, 0.4260482903888947], "caller": "main.py:200", "time": "2021-02-05T12:54:59.082277"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23936985433101654, 0.2397918552160263, 100, 0.43385960634184506], "caller": "main.py:200", "time": "2021-02-05T12:56:23.021147"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23854666948318481, 0.23105022311210632, 100, 0.4286573198009034], "caller": "main.py:200", "time": "2021-02-05T12:57:47.320796"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.237352192401886, 0.2207583636045456, 100, 0.4213483792122063], "caller": "main.py:200", "time": "2021-02-05T12:59:11.400784"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23783451318740845, 0.23574337363243103, 100, 0.4272542154361717], "caller": "main.py:200", "time": "2021-02-05T13:00:35.544690"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23485563695430756, 0.23206695914268494, 100, 0.42615252460748165], "caller": "main.py:200", "time": "2021-02-05T13:01:59.901128"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23651722073554993, 0.24865156412124634, 100, 0.41357602413333266], "caller": "main.py:200", "time": "2021-02-05T13:03:24.218038"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2384539395570755, 0.23909595608711243, 100, 0.42830428685435207], "caller": "main.py:200", "time": "2021-02-05T13:04:48.461475"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23857122659683228, 0.24559161067008972, 100, 0.42500417943519125], "caller": "main.py:200", "time": "2021-02-05T13:06:12.631152"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23505067825317383, 0.24092169106006622, 100, 0.41728905673622524], "caller": "main.py:200", "time": "2021-02-05T13:07:37.234523"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2341098189353943, 0.22506234049797058, 100, 0.42034740354387606], "caller": "main.py:200", "time": "2021-02-05T13:09:02.040913"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23649783432483673, 0.24484096467494965, 100, 0.4175079369127475], "caller": "main.py:200", "time": "2021-02-05T13:10:26.993488"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23523801565170288, 0.23190897703170776, 100, 0.43320647563178544], "caller": "main.py:200", "time": "2021-02-05T13:11:51.953413"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23464426398277283, 0.2470315843820572, 100, 0.4191250803647533], "caller": "main.py:200", "time": "2021-02-05T13:13:15.997424"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23636837303638458, 0.21350640058517456, 100, 0.41733471598028116], "caller": "main.py:200", "time": "2021-02-05T13:14:40.140620"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23347193002700806, 0.21690112352371216, 100, 0.42662587817217357], "caller": "main.py:200", "time": "2021-02-05T13:16:04.355843"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2360200732946396, 0.21441778540611267, 100, 0.4228243621760976], "caller": "main.py:200", "time": "2021-02-05T13:17:29.395140"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [32], "caller": "main.py:129", "time": "2021-02-05T13:18:51.662160"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.125, 0.33071891388307384], "caller": "main.py:37", "time": "2021-02-05T13:19:01.013242"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 17.099058986130874, 1.7798287210530355], "caller": "main.py:37", "time": "2021-02-05T13:19:05.053803"}
+{"level": "info", "fmt": "episode: %s", "args": [8.565944982148569], "caller": "main.py:149", "time": "2021-02-05T13:19:15.315175"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23948675394058228, 0.23987612128257751, 100, 0.42646844978836207], "caller": "main.py:200", "time": "2021-02-05T13:19:17.365887"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.24067042768001556, 0.25892701745033264, 100, 0.44055053557520385], "caller": "main.py:200", "time": "2021-02-05T13:20:41.664596"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23807695508003235, 0.22899705171585083, 100, 0.42106666977963253], "caller": "main.py:200", "time": "2021-02-05T13:22:05.537730"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2392447143793106, 0.24144624173641205, 100, 0.4186602171836856], "caller": "main.py:200", "time": "2021-02-05T13:23:29.904634"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23904016613960266, 0.21896162629127502, 100, 0.42560269142901647], "caller": "main.py:200", "time": "2021-02-05T13:24:54.663434"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23877044022083282, 0.2499300241470337, 100, 0.4275844841767927], "caller": "main.py:200", "time": "2021-02-05T13:26:19.066096"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23693706095218658, 0.24342837929725647, 100, 0.4273141294290475], "caller": "main.py:200", "time": "2021-02-05T13:27:43.790929"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2399091273546219, 0.25587087869644165, 100, 0.4236603754335175], "caller": "main.py:200", "time": "2021-02-05T13:29:08.237148"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23627637326717377, 0.25549060106277466, 100, 0.42418322558929555], "caller": "main.py:200", "time": "2021-02-05T13:30:33.224667"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23902390897274017, 0.20502103865146637, 100, 0.42355473961170875], "caller": "main.py:200", "time": "2021-02-05T13:31:57.809784"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2399158924818039, 0.2382073551416397, 100, 0.4336112463040663], "caller": "main.py:200", "time": "2021-02-05T13:33:22.194398"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.240621417760849, 0.23510566353797913, 100, 0.42685991253160527], "caller": "main.py:200", "time": "2021-02-05T13:34:46.854647"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23872441053390503, 0.2289065271615982, 100, 0.4315379246637821], "caller": "main.py:200", "time": "2021-02-05T13:36:11.415894"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23564963042736053, 0.23947851359844208, 100, 0.4248603757791044], "caller": "main.py:200", "time": "2021-02-05T13:37:35.841108"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23709522187709808, 0.25841057300567627, 100, 0.42342515255746904], "caller": "main.py:200", "time": "2021-02-05T13:39:00.621741"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2372925579547882, 0.2379949986934662, 100, 0.43167001600717175], "caller": "main.py:200", "time": "2021-02-05T13:40:25.108156"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23735913634300232, 0.25828880071640015, 100, 0.4309478002355689], "caller": "main.py:200", "time": "2021-02-05T13:41:49.368863"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2367773950099945, 0.21956667304039001, 100, 0.4325681665107174], "caller": "main.py:200", "time": "2021-02-05T13:43:13.721830"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23755569756031036, 0.2184622883796692, 100, 0.4298303043412486], "caller": "main.py:200", "time": "2021-02-05T13:44:38.381074"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23451197147369385, 0.2360258847475052, 100, 0.4252517526916591], "caller": "main.py:200", "time": "2021-02-05T13:46:03.151867"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [33], "caller": "main.py:129", "time": "2021-02-05T13:47:25.842634"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T13:47:34.810358"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 15.886760522581632, 1.1767567972316233], "caller": "main.py:37", "time": "2021-02-05T13:47:38.766004"}
+{"level": "info", "fmt": "episode: %s", "args": [7.719284116312], "caller": "main.py:149", "time": "2021-02-05T13:47:48.940601"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23749251663684845, 0.26370951533317566, 100, 0.42872104819302115], "caller": "main.py:200", "time": "2021-02-05T13:47:51.007474"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23904304206371307, 0.257124125957489, 100, 0.4240120741300956], "caller": "main.py:200", "time": "2021-02-05T13:49:15.180386"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23855793476104736, 0.24073252081871033, 100, 0.42927595104454724], "caller": "main.py:200", "time": "2021-02-05T13:50:39.224698"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23926374316215515, 0.23413440585136414, 100, 0.4359536328475828], "caller": "main.py:200", "time": "2021-02-05T13:52:03.074670"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23968790471553802, 0.24098461866378784, 100, 0.44108751113772265], "caller": "main.py:200", "time": "2021-02-05T13:53:27.752219"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2387489676475525, 0.25625818967819214, 100, 0.4318105727002348], "caller": "main.py:200", "time": "2021-02-05T13:54:52.394826"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2387760728597641, 0.22157692909240723, 100, 0.42476598258539205], "caller": "main.py:200", "time": "2021-02-05T13:56:16.923092"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.238718643784523, 0.23324137926101685, 100, 0.44272000487191915], "caller": "main.py:200", "time": "2021-02-05T13:57:41.327498"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2367924153804779, 0.22596576809883118, 100, 0.4319116028711422], "caller": "main.py:200", "time": "2021-02-05T13:59:05.813711"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23771192133426666, 0.21937647461891174, 100, 0.42305089290679926], "caller": "main.py:200", "time": "2021-02-05T14:00:29.955446"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23766852915287018, 0.23832356929779053, 100, 0.43651761348650725], "caller": "main.py:200", "time": "2021-02-05T14:01:54.484419"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23686212301254272, 0.2618093490600586, 100, 0.4242093851738152], "caller": "main.py:200", "time": "2021-02-05T14:03:18.532927"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23533739149570465, 0.23121842741966248, 100, 0.4188086410938118], "caller": "main.py:200", "time": "2021-02-05T14:04:42.818848"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23593096435070038, 0.22437205910682678, 100, 0.4359389477532277], "caller": "main.py:200", "time": "2021-02-05T14:06:07.395909"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23821483552455902, 0.2103254497051239, 100, 0.4307230090664248], "caller": "main.py:200", "time": "2021-02-05T14:07:31.775786"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23536552488803864, 0.23153045773506165, 100, 0.4308979728387042], "caller": "main.py:200", "time": "2021-02-05T14:08:56.264529"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23494167625904083, 0.22412732243537903, 100, 0.4251875417999419], "caller": "main.py:200", "time": "2021-02-05T14:10:20.958052"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2359275370836258, 0.2462773323059082, 100, 0.4354424178164869], "caller": "main.py:200", "time": "2021-02-05T14:11:45.518425"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23791499435901642, 0.22724346816539764, 100, 0.42970363225216135], "caller": "main.py:200", "time": "2021-02-05T14:13:10.680238"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2339869886636734, 0.2379150092601776, 100, 0.43236031140533804], "caller": "main.py:200", "time": "2021-02-05T14:14:34.812700"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [34], "caller": "main.py:129", "time": "2021-02-05T14:15:57.364073"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T14:16:06.372003"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 16.192045044128847, 0.7575395368910914], "caller": "main.py:37", "time": "2021-02-05T14:16:10.387689"}
+{"level": "info", "fmt": "episode: %s", "args": [6.395255066637142], "caller": "main.py:149", "time": "2021-02-05T14:16:20.470112"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23781335353851318, 0.23270709812641144, 100, 0.4347409334344045], "caller": "main.py:200", "time": "2021-02-05T14:16:22.515395"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23825393617153168, 0.23006302118301392, 100, 0.4344273003501387], "caller": "main.py:200", "time": "2021-02-05T14:17:46.942636"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.237502321600914, 0.2277696579694748, 100, 0.43311202581366054], "caller": "main.py:200", "time": "2021-02-05T14:19:11.514237"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2376207411289215, 0.2404472678899765, 100, 0.4429929229174126], "caller": "main.py:200", "time": "2021-02-05T14:20:35.241102"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23481245338916779, 0.2037070393562317, 100, 0.43196579213591113], "caller": "main.py:200", "time": "2021-02-05T14:21:59.265012"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23767055571079254, 0.22123804688453674, 100, 0.4385248896041082], "caller": "main.py:200", "time": "2021-02-05T14:23:23.399045"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23719264566898346, 0.24608291685581207, 100, 0.4276111384798639], "caller": "main.py:200", "time": "2021-02-05T14:24:47.631237"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23622705042362213, 0.2545086741447449, 100, 0.42928492362974763], "caller": "main.py:200", "time": "2021-02-05T14:26:12.198994"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23601047694683075, 0.2600865364074707, 100, 0.42611467118770474], "caller": "main.py:200", "time": "2021-02-05T14:27:36.854485"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23404468595981598, 0.21421319246292114, 100, 0.43548491533216105], "caller": "main.py:200", "time": "2021-02-05T14:29:01.465800"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23602552711963654, 0.24709810316562653, 100, 0.4299552431272659], "caller": "main.py:200", "time": "2021-02-05T14:30:26.119870"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23526757955551147, 0.2240152359008789, 100, 0.4319307842418755], "caller": "main.py:200", "time": "2021-02-05T14:31:51.010692"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2372729331254959, 0.22626934945583344, 100, 0.4417975943838827], "caller": "main.py:200", "time": "2021-02-05T14:33:15.310482"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23424343764781952, 0.22903117537498474, 100, 0.42993096198312614], "caller": "main.py:200", "time": "2021-02-05T14:34:40.025759"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2365448772907257, 0.2350548505783081, 100, 0.4305069025920468], "caller": "main.py:200", "time": "2021-02-05T14:36:04.269262"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.236988827586174, 0.2287195324897766, 100, 0.4285960372853837], "caller": "main.py:200", "time": "2021-02-05T14:37:28.218347"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23330436646938324, 0.24983106553554535, 100, 0.43563556458169966], "caller": "main.py:200", "time": "2021-02-05T14:38:52.586121"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23299342393875122, 0.23864655196666718, 100, 0.4219574874217554], "caller": "main.py:200", "time": "2021-02-05T14:40:16.932503"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2366781085729599, 0.2608482837677002, 100, 0.44517964418250994], "caller": "main.py:200", "time": "2021-02-05T14:41:41.146047"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2360677868127823, 0.22395992279052734, 100, 0.44664738322798314], "caller": "main.py:200", "time": "2021-02-05T14:43:05.510929"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [35], "caller": "main.py:129", "time": "2021-02-05T14:44:27.543353"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T14:44:36.949433"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 16.07003529814674, 1.4102033693818403], "caller": "main.py:37", "time": "2021-02-05T14:44:41.190623"}
+{"level": "info", "fmt": "episode: %s", "args": [7.796053246965693], "caller": "main.py:149", "time": "2021-02-05T14:44:51.761336"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2365526705980301, 0.24877668917179108, 100, 0.4317367097303669], "caller": "main.py:200", "time": "2021-02-05T14:44:53.875604"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23518867790699005, 0.2669808566570282, 100, 0.4393281870641071], "caller": "main.py:200", "time": "2021-02-05T14:46:18.325217"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2349936068058014, 0.2309490442276001, 100, 0.4435668202791106], "caller": "main.py:200", "time": "2021-02-05T14:47:42.542378"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23637159168720245, 0.24333137273788452, 100, 0.43340994174914715], "caller": "main.py:200", "time": "2021-02-05T14:49:07.331679"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23605068027973175, 0.2603378891944885, 100, 0.45242935474162993], "caller": "main.py:200", "time": "2021-02-05T14:50:31.092440"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23492220044136047, 0.23333224654197693, 100, 0.43906125630518067], "caller": "main.py:200", "time": "2021-02-05T14:51:55.813310"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23721659183502197, 0.22387422621250153, 100, 0.44958124066863825], "caller": "main.py:200", "time": "2021-02-05T14:53:20.595855"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23519177734851837, 0.26265671849250793, 100, 0.4389218303122289], "caller": "main.py:200", "time": "2021-02-05T14:54:44.889657"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2324686199426651, 0.24487073719501495, 100, 0.43499790217102735], "caller": "main.py:200", "time": "2021-02-05T14:56:09.270113"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2343348115682602, 0.23988091945648193, 100, 0.43775589580926705], "caller": "main.py:200", "time": "2021-02-05T14:57:33.605840"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23576343059539795, 0.21531075239181519, 100, 0.4470671706944672], "caller": "main.py:200", "time": "2021-02-05T14:58:58.071053"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.235397607088089, 0.23561596870422363, 100, 0.44364880229566583], "caller": "main.py:200", "time": "2021-02-05T15:00:22.451206"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23462079465389252, 0.22523468732833862, 100, 0.4437022062692399], "caller": "main.py:200", "time": "2021-02-05T15:01:46.922047"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23248769342899323, 0.23762226104736328, 100, 0.4392147603191805], "caller": "main.py:200", "time": "2021-02-05T15:03:11.671476"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23266635835170746, 0.23227518796920776, 100, 0.45691894288262236], "caller": "main.py:200", "time": "2021-02-05T15:04:36.522940"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23156234622001648, 0.26443108916282654, 100, 0.44116029109119514], "caller": "main.py:200", "time": "2021-02-05T15:06:01.182974"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23231396079063416, 0.23749807476997375, 100, 0.432681173936968], "caller": "main.py:200", "time": "2021-02-05T15:07:25.357948"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2337011843919754, 0.2353040724992752, 100, 0.44144203807469506], "caller": "main.py:200", "time": "2021-02-05T15:08:50.514927"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.01744777150452137, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T15:09:04.588706"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23156818747520447, 0.23994848132133484, 100, 0.4446518089922666], "caller": "main.py:200", "time": "2021-02-05T15:10:15.294081"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2324523776769638, 0.22867780923843384, 100, 0.43622030224014186], "caller": "main.py:200", "time": "2021-02-05T15:11:39.888415"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [36], "caller": "main.py:129", "time": "2021-02-05T15:13:02.186232"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T15:13:11.354617"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 11, 47.7793342786651, 43.29094637235998], "caller": "main.py:37", "time": "2021-02-05T15:13:15.375009"}
+{"level": "info", "fmt": "episode: %s", "args": [6.381818275124899], "caller": "main.py:149", "time": "2021-02-05T15:13:25.645080"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23447206616401672, 0.25502774119377136, 100, 0.4389031278648164], "caller": "main.py:200", "time": "2021-02-05T15:13:27.697276"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23463214933872223, 0.22088029980659485, 100, 0.4422418269072796], "caller": "main.py:200", "time": "2021-02-05T15:14:52.439801"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23609282076358795, 0.23534086346626282, 100, 0.44238919104682173], "caller": "main.py:200", "time": "2021-02-05T15:16:17.168400"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23479114472866058, 0.22984054684638977, 100, 0.4486183639912214], "caller": "main.py:200", "time": "2021-02-05T15:17:42.303831"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2357025146484375, 0.22867605090141296, 100, 0.4437407838854508], "caller": "main.py:200", "time": "2021-02-05T15:19:06.538565"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23482657968997955, 0.23247504234313965, 100, 0.44627387489386067], "caller": "main.py:200", "time": "2021-02-05T15:20:31.311711"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2337576448917389, 0.2553703486919403, 100, 0.44064461614255435], "caller": "main.py:200", "time": "2021-02-05T15:21:56.533278"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2337253987789154, 0.24719248712062836, 100, 0.44526715959626234], "caller": "main.py:200", "time": "2021-02-05T15:23:21.277578"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2331402599811554, 0.24935288727283478, 100, 0.4487931111006171], "caller": "main.py:200", "time": "2021-02-05T15:24:46.153578"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2303377389907837, 0.24499808251857758, 100, 0.44641821290558786], "caller": "main.py:200", "time": "2021-02-05T15:26:11.362364"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23437154293060303, 0.2196826934814453, 100, 0.4396262818135279], "caller": "main.py:200", "time": "2021-02-05T15:27:36.429765"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.234903484582901, 0.21552646160125732, 100, 0.4560560094854155], "caller": "main.py:200", "time": "2021-02-05T15:29:00.747854"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2311568260192871, 0.2392803132534027, 100, 0.44523297007399937], "caller": "main.py:200", "time": "2021-02-05T15:30:25.021057"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2343774288892746, 0.23209825158119202, 100, 0.4414717241680225], "caller": "main.py:200", "time": "2021-02-05T15:31:49.992852"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23242951929569244, 0.22001047432422638, 100, 0.44852951045319395], "caller": "main.py:200", "time": "2021-02-05T15:33:14.445809"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23309946060180664, 0.23485936224460602, 100, 0.44256120766744056], "caller": "main.py:200", "time": "2021-02-05T15:34:40.167213"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23405028879642487, 0.2377026379108429, 100, 0.4399806636131904], "caller": "main.py:200", "time": "2021-02-05T15:36:05.087798"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23154667019844055, 0.23546873033046722, 100, 0.45059944625163195], "caller": "main.py:200", "time": "2021-02-05T15:37:30.529509"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23077325522899628, 0.21887142956256866, 100, 0.44336399222947115], "caller": "main.py:200", "time": "2021-02-05T15:38:55.023889"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2315177321434021, 0.21584421396255493, 100, 0.44704461808502577], "caller": "main.py:200", "time": "2021-02-05T15:40:19.411496"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [37], "caller": "main.py:129", "time": "2021-02-05T15:41:41.930513"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T15:41:51.122922"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 9, 35.357172018840316, 39.786886946678024], "caller": "main.py:37", "time": "2021-02-05T15:41:55.320568"}
+{"level": "info", "fmt": "episode: %s", "args": [6.821140146659908], "caller": "main.py:149", "time": "2021-02-05T15:42:05.659104"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23729945719242096, 0.2485862374305725, 100, 0.4377822371879532], "caller": "main.py:200", "time": "2021-02-05T15:42:07.730142"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.24083834886550903, 0.23290413618087769, 100, 0.45639808441604923], "caller": "main.py:200", "time": "2021-02-05T15:43:33.129303"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23600073158740997, 0.2510356307029724, 100, 0.44584267194581034], "caller": "main.py:200", "time": "2021-02-05T15:44:57.496314"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23666322231292725, 0.25105157494544983, 100, 0.4489169474987294], "caller": "main.py:200", "time": "2021-02-05T15:46:22.452411"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23867517709732056, 0.2212589979171753, 100, 0.4496200441864528], "caller": "main.py:200", "time": "2021-02-05T15:47:46.898588"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23725463449954987, 0.2463301718235016, 100, 0.44295683766942273], "caller": "main.py:200", "time": "2021-02-05T15:49:10.702061"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2360650897026062, 0.20841102302074432, 100, 0.45246503885036055], "caller": "main.py:200", "time": "2021-02-05T15:50:35.238066"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2341122180223465, 0.23713630437850952, 100, 0.4481073420791746], "caller": "main.py:200", "time": "2021-02-05T15:52:00.116404"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2355661243200302, 0.23870781064033508, 100, 0.4505618050243904], "caller": "main.py:200", "time": "2021-02-05T15:53:25.209439"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23613151907920837, 0.23676055669784546, 100, 0.45018335099486817], "caller": "main.py:200", "time": "2021-02-05T15:54:50.299281"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23620253801345825, 0.22988729178905487, 100, 0.4595228579180866], "caller": "main.py:200", "time": "2021-02-05T15:56:14.960139"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2347993403673172, 0.25713762640953064, 100, 0.4628220027222344], "caller": "main.py:200", "time": "2021-02-05T15:57:39.978815"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23726676404476166, 0.23476187884807587, 100, 0.45196552421103287], "caller": "main.py:200", "time": "2021-02-05T15:59:04.716921"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23566505312919617, 0.2243373990058899, 100, 0.4432498561892718], "caller": "main.py:200", "time": "2021-02-05T16:00:29.263138"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23520460724830627, 0.23808325827121735, 100, 0.45351276899809434], "caller": "main.py:200", "time": "2021-02-05T16:01:53.688232"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2337217777967453, 0.2389431893825531, 100, 0.4425728675280503], "caller": "main.py:200", "time": "2021-02-05T16:03:18.380476"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23605051636695862, 0.23428596556186676, 100, 0.46343779171839244], "caller": "main.py:200", "time": "2021-02-05T16:04:42.779370"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23371444642543793, 0.2260482907295227, 100, 0.4534006732248855], "caller": "main.py:200", "time": "2021-02-05T16:06:07.403679"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23334650695323944, 0.2171487808227539, 100, 0.4525491636538952], "caller": "main.py:200", "time": "2021-02-05T16:07:31.570184"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23317846655845642, 0.22572065889835358, 100, 0.45201565052519105], "caller": "main.py:200", "time": "2021-02-05T16:08:56.246592"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [38], "caller": "main.py:129", "time": "2021-02-05T16:10:18.931983"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T16:10:28.124588"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 12.66067063209116, 1.311320977982055], "caller": "main.py:37", "time": "2021-02-05T16:10:32.196172"}
+{"level": "info", "fmt": "episode: %s", "args": [7.445363677587325], "caller": "main.py:149", "time": "2021-02-05T16:10:42.255784"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23890022933483124, 0.23938998579978943, 100, 0.4657935887109356], "caller": "main.py:200", "time": "2021-02-05T16:10:44.319486"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23700255155563354, 0.23058485984802246, 100, 0.45863689507428024], "caller": "main.py:200", "time": "2021-02-05T16:12:08.140097"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23692908883094788, 0.22706934809684753, 100, 0.44248439277754037], "caller": "main.py:200", "time": "2021-02-05T16:13:32.681377"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23685938119888306, 0.2705398201942444, 100, 0.45962886507683226], "caller": "main.py:200", "time": "2021-02-05T16:14:57.272046"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2374970018863678, 0.2398618459701538, 100, 0.4673483660349798], "caller": "main.py:200", "time": "2021-02-05T16:16:21.550742"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23822666704654694, 0.2499352991580963, 100, 0.4634103124870376], "caller": "main.py:200", "time": "2021-02-05T16:17:46.033078"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.24027858674526215, 0.22413185238838196, 100, 0.4552451788923875], "caller": "main.py:200", "time": "2021-02-05T16:19:09.571044"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23918001353740692, 0.24901127815246582, 100, 0.4630592521562049], "caller": "main.py:200", "time": "2021-02-05T16:20:33.680601"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2364547848701477, 0.21402013301849365, 100, 0.46073687191170526], "caller": "main.py:200", "time": "2021-02-05T16:21:58.335095"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2374955117702484, 0.22699925303459167, 100, 0.4589631086128943], "caller": "main.py:200", "time": "2021-02-05T16:23:22.497531"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23571506142616272, 0.23066388070583344, 100, 0.4506577696945621], "caller": "main.py:200", "time": "2021-02-05T16:24:46.770558"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23716305196285248, 0.24088333547115326, 100, 0.44996294464141656], "caller": "main.py:200", "time": "2021-02-05T16:26:10.887105"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23452675342559814, 0.24013462662696838, 100, 0.4544199585460192], "caller": "main.py:200", "time": "2021-02-05T16:27:35.193150"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23638513684272766, 0.220822274684906, 100, 0.4642534496916004], "caller": "main.py:200", "time": "2021-02-05T16:28:59.042992"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2367001324892044, 0.23808611929416656, 100, 0.4628039633764949], "caller": "main.py:200", "time": "2021-02-05T16:30:23.114480"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2377249300479889, 0.2499973475933075, 100, 0.46370454545152384], "caller": "main.py:200", "time": "2021-02-05T16:31:47.282548"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23765848577022552, 0.24133643507957458, 100, 0.46034740204976], "caller": "main.py:200", "time": "2021-02-05T16:33:11.859854"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23551064729690552, 0.24532556533813477, 100, 0.4618444472089502], "caller": "main.py:200", "time": "2021-02-05T16:34:36.528212"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23487350344657898, 0.22199775278568268, 100, 0.4485794609336501], "caller": "main.py:200", "time": "2021-02-05T16:36:00.873248"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2369215190410614, 0.21715672314167023, 100, 0.4638096850646649], "caller": "main.py:200", "time": "2021-02-05T16:37:25.416794"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [39], "caller": "main.py:129", "time": "2021-02-05T16:38:47.344284"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 9, 0.1111111111111111, 0.31426968052735443], "caller": "main.py:37", "time": "2021-02-05T16:38:56.326384"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 12, 46.60995600016309, 41.61974942089652], "caller": "main.py:37", "time": "2021-02-05T16:39:00.390423"}
+{"level": "info", "fmt": "episode: %s", "args": [7.831795865859383], "caller": "main.py:149", "time": "2021-02-05T16:39:10.613499"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23940563201904297, 0.24000173807144165, 100, 0.4556088554403553], "caller": "main.py:200", "time": "2021-02-05T16:39:12.705442"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.24010291695594788, 0.22536233067512512, 100, 0.46268804247520795], "caller": "main.py:200", "time": "2021-02-05T16:40:37.202070"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23819111287593842, 0.2453872263431549, 100, 0.4549729492805411], "caller": "main.py:200", "time": "2021-02-05T16:42:02.127035"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23886407911777496, 0.24499860405921936, 100, 0.46671989972692574], "caller": "main.py:200", "time": "2021-02-05T16:43:26.619434"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2359938770532608, 0.24747754633426666, 100, 0.473377745104182], "caller": "main.py:200", "time": "2021-02-05T16:44:51.346530"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23762211203575134, 0.26559343934059143, 100, 0.4572366266985467], "caller": "main.py:200", "time": "2021-02-05T16:46:15.551122"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2367493063211441, 0.24643544852733612, 100, 0.46333090092078805], "caller": "main.py:200", "time": "2021-02-05T16:47:39.493770"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23652799427509308, 0.2459586262702942, 100, 0.46399085397848805], "caller": "main.py:200", "time": "2021-02-05T16:49:04.354159"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23747214674949646, 0.22770676016807556, 100, 0.4616109358802763], "caller": "main.py:200", "time": "2021-02-05T16:50:28.811933"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23804622888565063, 0.2379261702299118, 100, 0.46881674016794306], "caller": "main.py:200", "time": "2021-02-05T16:51:52.957039"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2366502583026886, 0.2281028777360916, 100, 0.4605443790094456], "caller": "main.py:200", "time": "2021-02-05T16:53:17.335690"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.24046926200389862, 0.22899511456489563, 100, 0.4729809140686077], "caller": "main.py:200", "time": "2021-02-05T16:54:41.778009"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2344689518213272, 0.24827739596366882, 100, 0.4585713779467729], "caller": "main.py:200", "time": "2021-02-05T16:56:05.940041"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2360691875219345, 0.2483687698841095, 100, 0.4530075516843784], "caller": "main.py:200", "time": "2021-02-05T16:57:30.139462"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2335922122001648, 0.21956270933151245, 100, 0.46115696693815633], "caller": "main.py:200", "time": "2021-02-05T16:58:54.753625"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23474366962909698, 0.2425473928451538, 100, 0.46162178899201917], "caller": "main.py:200", "time": "2021-02-05T17:00:18.969066"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23636573553085327, 0.24002552032470703, 100, 0.47494047249574556], "caller": "main.py:200", "time": "2021-02-05T17:01:43.030969"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2347142994403839, 0.22413673996925354, 100, 0.4643352904451475], "caller": "main.py:200", "time": "2021-02-05T17:03:07.415895"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2357916682958603, 0.23858897387981415, 100, 0.4632125779523169], "caller": "main.py:200", "time": "2021-02-05T17:04:32.363631"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23276594281196594, 0.22114039957523346, 100, 0.463919931901793], "caller": "main.py:200", "time": "2021-02-05T17:05:56.460084"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [40], "caller": "main.py:129", "time": "2021-02-05T17:07:18.078165"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T17:07:27.329245"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 15.124017847207309, 0.28528974452359573], "caller": "main.py:37", "time": "2021-02-05T17:07:31.255131"}
+{"level": "info", "fmt": "episode: %s", "args": [9.697748136440897], "caller": "main.py:149", "time": "2021-02-05T17:07:41.385241"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2365410029888153, 0.23173415660858154, 100, 0.4638031962050121], "caller": "main.py:200", "time": "2021-02-05T17:07:43.438689"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23551255464553833, 0.23486334085464478, 100, 0.44146309907790116], "caller": "main.py:200", "time": "2021-02-05T17:09:07.376226"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23637953400611877, 0.24855856597423553, 100, 0.46553566570395], "caller": "main.py:200", "time": "2021-02-05T17:10:31.772476"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23681625723838806, 0.24269446730613708, 100, 0.4728387307647889], "caller": "main.py:200", "time": "2021-02-05T17:11:55.848486"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23484741151332855, 0.23005159199237823, 100, 0.4580779978174849], "caller": "main.py:200", "time": "2021-02-05T17:13:20.594428"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2333139032125473, 0.23746713995933533, 100, 0.465999935520779], "caller": "main.py:200", "time": "2021-02-05T17:14:45.282273"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23630741238594055, 0.22891543805599213, 100, 0.46809616769639645], "caller": "main.py:200", "time": "2021-02-05T17:16:09.311914"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23384201526641846, 0.24054472148418427, 100, 0.45685986067256434], "caller": "main.py:200", "time": "2021-02-05T17:17:33.200397"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23456571996212006, 0.2068147361278534, 100, 0.463514721645257], "caller": "main.py:200", "time": "2021-02-05T17:18:57.465298"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23193418979644775, 0.25503289699554443, 100, 0.45326829965350285], "caller": "main.py:200", "time": "2021-02-05T17:20:22.389380"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23169326782226562, 0.23497025668621063, 100, 0.4574880983968057], "caller": "main.py:200", "time": "2021-02-05T17:21:47.383778"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23387950658798218, 0.2411385178565979, 100, 0.4659276970755043], "caller": "main.py:200", "time": "2021-02-05T17:23:12.013347"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23092979192733765, 0.2197592556476593, 100, 0.4552060314557726], "caller": "main.py:200", "time": "2021-02-05T17:24:36.703172"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23433426022529602, 0.2350253462791443, 100, 0.46855393107020243], "caller": "main.py:200", "time": "2021-02-05T17:26:01.177761"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23441879451274872, 0.25800663232803345, 100, 0.47044169331852975], "caller": "main.py:200", "time": "2021-02-05T17:27:25.321513"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23138310015201569, 0.2278004288673401, 100, 0.47250546540065214], "caller": "main.py:200", "time": "2021-02-05T17:28:49.368077"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23155656456947327, 0.250644713640213, 100, 0.4659938376675959], "caller": "main.py:200", "time": "2021-02-05T17:30:13.811586"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2288067489862442, 0.23767447471618652, 100, 0.45564381147835], "caller": "main.py:200", "time": "2021-02-05T17:31:38.263373"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23200878500938416, 0.274647057056427, 100, 0.47133394385631283], "caller": "main.py:200", "time": "2021-02-05T17:33:02.767657"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23171889781951904, 0.24073174595832825, 100, 0.4693618452079619], "caller": "main.py:200", "time": "2021-02-05T17:34:27.165881"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [41], "caller": "main.py:129", "time": "2021-02-05T17:35:49.734219"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T17:35:58.928262"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 14.610597435197008, 0.20168119447474012], "caller": "main.py:37", "time": "2021-02-05T17:36:02.858771"}
+{"level": "info", "fmt": "episode: %s", "args": [8.994818925302278], "caller": "main.py:149", "time": "2021-02-05T17:36:13.044759"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23364654183387756, 0.2309800535440445, 100, 0.4646813427937266], "caller": "main.py:200", "time": "2021-02-05T17:36:15.121470"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23508639633655548, 0.22453373670578003, 100, 0.4582451217908378], "caller": "main.py:200", "time": "2021-02-05T17:37:39.036139"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23268760740756989, 0.2241072654724121, 100, 0.4593551626342136], "caller": "main.py:200", "time": "2021-02-05T17:39:03.300531"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23521193861961365, 0.23345908522605896, 100, 0.46309759284395285], "caller": "main.py:200", "time": "2021-02-05T17:40:27.963135"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2312266230583191, 0.25381773710250854, 100, 0.465148411644674], "caller": "main.py:200", "time": "2021-02-05T17:41:52.264102"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23395231366157532, 0.23021820187568665, 100, 0.45632604296412543], "caller": "main.py:200", "time": "2021-02-05T17:43:16.356661"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23484255373477936, 0.2140742689371109, 100, 0.4673682477602891], "caller": "main.py:200", "time": "2021-02-05T17:44:40.917930"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23192815482616425, 0.21908962726593018, 100, 0.45590506578893175], "caller": "main.py:200", "time": "2021-02-05T17:46:04.774127"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23490121960639954, 0.23499897122383118, 100, 0.4631168198122394], "caller": "main.py:200", "time": "2021-02-05T17:47:29.212155"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23557274043560028, 0.23344430327415466, 100, 0.4626564684047575], "caller": "main.py:200", "time": "2021-02-05T17:48:52.965645"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23555535078048706, 0.23915982246398926, 100, 0.47032128865784], "caller": "main.py:200", "time": "2021-02-05T17:50:16.811453"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23265983164310455, 0.23585115373134613, 100, 0.46894380594570895], "caller": "main.py:200", "time": "2021-02-05T17:51:41.243802"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23307280242443085, 0.2441948503255844, 100, 0.4674197235236049], "caller": "main.py:200", "time": "2021-02-05T17:53:05.724150"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23254962265491486, 0.24308297038078308, 100, 0.46505317862151685], "caller": "main.py:200", "time": "2021-02-05T17:54:29.711804"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2321709394454956, 0.23377324640750885, 100, 0.4609811898604991], "caller": "main.py:200", "time": "2021-02-05T17:55:54.368558"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2336745709180832, 0.2340405434370041, 100, 0.46743837173079067], "caller": "main.py:200", "time": "2021-02-05T17:57:18.813142"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.2315642535686493, 0.21046295762062073, 100, 0.47278398836891655], "caller": "main.py:200", "time": "2021-02-05T17:58:43.219020"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23300492763519287, 0.22849087417125702, 100, 0.4623856710925249], "caller": "main.py:200", "time": "2021-02-05T18:00:07.351219"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23181341588497162, 0.23458531498908997, 100, 0.4607338165776144], "caller": "main.py:200", "time": "2021-02-05T18:01:31.292111"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2319052517414093, 0.2307671308517456, 100, 0.4746281381966893], "caller": "main.py:200", "time": "2021-02-05T18:02:55.681504"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [42], "caller": "main.py:129", "time": "2021-02-05T18:04:18.102392"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T18:04:27.421708"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 16.116155259408565, 0.1274442548976258], "caller": "main.py:37", "time": "2021-02-05T18:04:31.405825"}
+{"level": "info", "fmt": "episode: %s", "args": [9.431063333938603], "caller": "main.py:149", "time": "2021-02-05T18:04:41.573099"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23416437208652496, 0.2291576862335205, 100, 0.4671823659072041], "caller": "main.py:200", "time": "2021-02-05T18:04:43.693955"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23463822901248932, 0.23889963328838348, 100, 0.4669704731831881], "caller": "main.py:200", "time": "2021-02-05T18:06:08.001388"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23466309905052185, 0.23367956280708313, 100, 0.4673080107102387], "caller": "main.py:200", "time": "2021-02-05T18:07:32.117974"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23281075060367584, 0.22789326310157776, 100, 0.45964034671955417], "caller": "main.py:200", "time": "2021-02-05T18:08:56.286841"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23283201456069946, 0.22092163562774658, 100, 0.4670510774383795], "caller": "main.py:200", "time": "2021-02-05T18:10:20.986294"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23235748708248138, 0.21843545138835907, 100, 0.47269287998717746], "caller": "main.py:200", "time": "2021-02-05T18:11:45.557433"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23447531461715698, 0.22478915750980377, 100, 0.46682204480271355], "caller": "main.py:200", "time": "2021-02-05T18:13:09.720901"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23181818425655365, 0.2377874255180359, 100, 0.45957107777768186], "caller": "main.py:200", "time": "2021-02-05T18:14:34.215797"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23140497505664825, 0.23559217154979706, 100, 0.4631696584357578], "caller": "main.py:200", "time": "2021-02-05T18:15:58.223265"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2355061173439026, 0.23316287994384766, 100, 0.4896620720135262], "caller": "main.py:200", "time": "2021-02-05T18:17:23.364242"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23194879293441772, 0.21628467738628387, 100, 0.4619773432150442], "caller": "main.py:200", "time": "2021-02-05T18:18:48.082579"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23048175871372223, 0.22827085852622986, 100, 0.4604702636139071], "caller": "main.py:200", "time": "2021-02-05T18:20:12.026566"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23188123106956482, 0.24102386832237244, 100, 0.4682589074501033], "caller": "main.py:200", "time": "2021-02-05T18:21:36.579017"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23481091856956482, 0.2174474000930786, 100, 0.4677760383018972], "caller": "main.py:200", "time": "2021-02-05T18:23:00.862949"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22840501368045807, 0.23471394181251526, 100, 0.47372134770186114], "caller": "main.py:200", "time": "2021-02-05T18:24:25.147086"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23129074275493622, 0.22378644347190857, 100, 0.47538288379922156], "caller": "main.py:200", "time": "2021-02-05T18:25:49.439168"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23150911927223206, 0.23688039183616638, 100, 0.4801199778434332], "caller": "main.py:200", "time": "2021-02-05T18:27:14.084411"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22986909747123718, 0.2253774106502533, 100, 0.4696335845716051], "caller": "main.py:200", "time": "2021-02-05T18:28:38.643773"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22855894267559052, 0.21949489414691925, 100, 0.46261659736775507], "caller": "main.py:200", "time": "2021-02-05T18:30:03.461292"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23176822066307068, 0.22483499348163605, 100, 0.47336183751856276], "caller": "main.py:200", "time": "2021-02-05T18:31:27.648942"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [43], "caller": "main.py:129", "time": "2021-02-05T18:32:49.926868"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T18:32:59.225855"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 10, 32.23234062640965, 37.73759662851746], "caller": "main.py:37", "time": "2021-02-05T18:33:03.469957"}
+{"level": "info", "fmt": "episode: %s", "args": [7.061682403152367], "caller": "main.py:149", "time": "2021-02-05T18:33:13.987653"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23501363396644592, 0.24171724915504456, 100, 0.4791563596842354], "caller": "main.py:200", "time": "2021-02-05T18:33:16.058267"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23186944425106049, 0.2336975634098053, 100, 0.46517771358606586], "caller": "main.py:200", "time": "2021-02-05T18:34:40.441045"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.22913645207881927, 0.20746156573295593, 100, 0.46805175746767164], "caller": "main.py:200", "time": "2021-02-05T18:36:04.779333"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23197656869888306, 0.2278231382369995, 100, 0.47495843346137717], "caller": "main.py:200", "time": "2021-02-05T18:37:29.917690"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23107802867889404, 0.2408502995967865, 100, 0.47539059395804567], "caller": "main.py:200", "time": "2021-02-05T18:38:56.197463"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23261617124080658, 0.2184026539325714, 100, 0.46323307986395507], "caller": "main.py:200", "time": "2021-02-05T18:40:21.464726"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23077671229839325, 0.23495712876319885, 100, 0.4567836156030682], "caller": "main.py:200", "time": "2021-02-05T18:41:46.895969"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2289884388446808, 0.23075240850448608, 100, 0.4672619437704125], "caller": "main.py:200", "time": "2021-02-05T18:43:13.039202"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23088832199573517, 0.22697916626930237, 100, 0.46794995303242864], "caller": "main.py:200", "time": "2021-02-05T18:44:39.566697"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2297772765159607, 0.2300540804862976, 100, 0.4635061275495509], "caller": "main.py:200", "time": "2021-02-05T18:46:05.019663"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23051989078521729, 0.21583767235279083, 100, 0.463618773473631], "caller": "main.py:200", "time": "2021-02-05T18:47:30.697229"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22972959280014038, 0.2265666127204895, 100, 0.4616295203104922], "caller": "main.py:200", "time": "2021-02-05T18:48:57.156782"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22795400023460388, 0.20594000816345215, 100, 0.46383491839300484], "caller": "main.py:200", "time": "2021-02-05T18:50:24.083160"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.228134885430336, 0.2269469052553177, 100, 0.47926149065470297], "caller": "main.py:200", "time": "2021-02-05T18:51:49.467285"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22915731370449066, 0.217448890209198, 100, 0.4680880528220343], "caller": "main.py:200", "time": "2021-02-05T18:53:14.636464"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22820226848125458, 0.2241438627243042, 100, 0.46676346744399677], "caller": "main.py:200", "time": "2021-02-05T18:54:38.888222"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22893396019935608, 0.2208709418773651, 100, 0.469509198618668], "caller": "main.py:200", "time": "2021-02-05T18:56:03.882108"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22945378720760345, 0.2362232804298401, 100, 0.460065672351607], "caller": "main.py:200", "time": "2021-02-05T18:57:27.990035"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2289971113204956, 0.22503507137298584, 100, 0.4686274471410378], "caller": "main.py:200", "time": "2021-02-05T18:58:53.347071"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.22823798656463623, 0.21780389547348022, 100, 0.474462799084424], "caller": "main.py:200", "time": "2021-02-05T19:00:19.684043"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [44], "caller": "main.py:129", "time": "2021-02-05T19:01:42.295972"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T19:01:51.466764"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 14.258784939982974, 0.24517499736323603], "caller": "main.py:37", "time": "2021-02-05T19:01:55.484815"}
+{"level": "info", "fmt": "episode: %s", "args": [8.406760120541438], "caller": "main.py:149", "time": "2021-02-05T19:02:05.678494"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23213092982769012, 0.23817282915115356, 100, 0.4635558262003829], "caller": "main.py:200", "time": "2021-02-05T19:02:07.730572"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2316037118434906, 0.22696052491664886, 100, 0.46802279318659235], "caller": "main.py:200", "time": "2021-02-05T19:03:32.375010"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.22950096428394318, 0.2125900685787201, 100, 0.4568561848541214], "caller": "main.py:200", "time": "2021-02-05T19:04:57.129866"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.22905586659908295, 0.2141202688217163, 100, 0.4549707850358895], "caller": "main.py:200", "time": "2021-02-05T19:06:22.816055"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.22969721257686615, 0.2127482295036316, 100, 0.4507078852798842], "caller": "main.py:200", "time": "2021-02-05T19:07:47.717828"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23025457561016083, 0.22138462960720062, 100, 0.44630853946514243], "caller": "main.py:200", "time": "2021-02-05T19:09:12.812544"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.22852087020874023, 0.22912491858005524, 100, 0.47040582234923783], "caller": "main.py:200", "time": "2021-02-05T19:10:37.437411"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.22803257405757904, 0.22517292201519012, 100, 0.46268190766714506], "caller": "main.py:200", "time": "2021-02-05T19:12:02.938898"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.22869230806827545, 0.2344784438610077, 100, 0.46490285570740836], "caller": "main.py:200", "time": "2021-02-05T19:13:29.623064"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.22820283472537994, 0.21168017387390137, 100, 0.4689333715565956], "caller": "main.py:200", "time": "2021-02-05T19:14:55.762921"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2270478457212448, 0.2522195279598236, 100, 0.46559837783687447], "caller": "main.py:200", "time": "2021-02-05T19:16:22.065784"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22895774245262146, 0.2275615930557251, 100, 0.45045422430302284], "caller": "main.py:200", "time": "2021-02-05T19:17:48.345885"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22638103365898132, 0.2203412652015686, 100, 0.4709950768476607], "caller": "main.py:200", "time": "2021-02-05T19:19:15.220494"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2282118797302246, 0.21506839990615845, 100, 0.45094802971145126], "caller": "main.py:200", "time": "2021-02-05T19:20:42.458195"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22585946321487427, 0.21138951182365417, 100, 0.4668118640313637], "caller": "main.py:200", "time": "2021-02-05T19:22:06.879043"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22909626364707947, 0.24506303668022156, 100, 0.46538613821382596], "caller": "main.py:200", "time": "2021-02-05T19:23:32.192052"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22779367864131927, 0.22811123728752136, 100, 0.45699875946243307], "caller": "main.py:200", "time": "2021-02-05T19:24:58.191789"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22845982015132904, 0.23889517784118652, 100, 0.4600446214811905], "caller": "main.py:200", "time": "2021-02-05T19:26:23.427902"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22960838675498962, 0.22347328066825867, 100, 0.4790293901679708], "caller": "main.py:200", "time": "2021-02-05T19:27:48.457412"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.22553662955760956, 0.21772021055221558, 100, 0.46328476096527205], "caller": "main.py:200", "time": "2021-02-05T19:29:12.897589"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [45], "caller": "main.py:129", "time": "2021-02-05T19:30:35.518025"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T19:30:44.538925"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 11, 40.13436685365309, 40.18736587977387], "caller": "main.py:37", "time": "2021-02-05T19:30:48.560234"}
+{"level": "info", "fmt": "episode: %s", "args": [8.62976318372353], "caller": "main.py:149", "time": "2021-02-05T19:30:58.642302"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23011822998523712, 0.232692152261734, 100, 0.4524687030458977], "caller": "main.py:200", "time": "2021-02-05T19:31:00.698884"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23076528310775757, 0.21686731278896332, 100, 0.4510462623383564], "caller": "main.py:200", "time": "2021-02-05T19:32:25.737647"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23004254698753357, 0.21855789422988892, 100, 0.4584873318213693], "caller": "main.py:200", "time": "2021-02-05T19:33:50.212251"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23001764714717865, 0.23706457018852234, 100, 0.46331836248771435], "caller": "main.py:200", "time": "2021-02-05T19:35:15.335619"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.22957977652549744, 0.24067118763923645, 100, 0.45802524472663175], "caller": "main.py:200", "time": "2021-02-05T19:36:40.187297"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.22943958640098572, 0.21898028254508972, 100, 0.4679334133395516], "caller": "main.py:200", "time": "2021-02-05T19:38:04.490048"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.22930634021759033, 0.24478605389595032, 100, 0.458701540781881], "caller": "main.py:200", "time": "2021-02-05T19:39:28.848508"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23049387335777283, 0.22885125875473022, 100, 0.45989406491687457], "caller": "main.py:200", "time": "2021-02-05T19:40:53.354139"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.22954364120960236, 0.2177695333957672, 100, 0.460505422307694], "caller": "main.py:200", "time": "2021-02-05T19:42:17.609497"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23029354214668274, 0.22158978879451752, 100, 0.46216199780849343], "caller": "main.py:200", "time": "2021-02-05T19:43:42.294804"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.22750791907310486, 0.249235138297081, 100, 0.4531095954322701], "caller": "main.py:200", "time": "2021-02-05T19:45:07.239887"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22906608879566193, 0.2325197011232376, 100, 0.4636426800025783], "caller": "main.py:200", "time": "2021-02-05T19:46:31.545670"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22804033756256104, 0.23286661505699158, 100, 0.4657698663603406], "caller": "main.py:200", "time": "2021-02-05T19:47:55.880718"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23212066292762756, 0.21859483420848846, 100, 0.46731715942716434], "caller": "main.py:200", "time": "2021-02-05T19:49:20.399416"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2280958592891693, 0.2006756067276001, 100, 0.46467684711339485], "caller": "main.py:200", "time": "2021-02-05T19:50:44.962924"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22788649797439575, 0.23170270025730133, 100, 0.46039297725126677], "caller": "main.py:200", "time": "2021-02-05T19:52:09.529653"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22822493314743042, 0.23586460947990417, 100, 0.46138194466983856], "caller": "main.py:200", "time": "2021-02-05T19:53:34.107485"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2271745800971985, 0.22622612118721008, 100, 0.44730476613340064], "caller": "main.py:200", "time": "2021-02-05T19:54:58.651869"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2303430140018463, 0.23525357246398926, 100, 0.4697997730500583], "caller": "main.py:200", "time": "2021-02-05T19:56:23.209726"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.22983571887016296, 0.22176602482795715, 100, 0.461194014503136], "caller": "main.py:200", "time": "2021-02-05T19:57:47.508916"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [46], "caller": "main.py:129", "time": "2021-02-05T19:59:10.119994"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T19:59:19.273285"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 14.813534787165244, 0.28962878770687583], "caller": "main.py:37", "time": "2021-02-05T19:59:23.254469"}
+{"level": "info", "fmt": "episode: %s", "args": [7.955724235104869], "caller": "main.py:149", "time": "2021-02-05T19:59:33.356646"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23124147951602936, 0.24199455976486206, 100, 0.45598742510288837], "caller": "main.py:200", "time": "2021-02-05T19:59:35.406396"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23225905001163483, 0.24713754653930664, 100, 0.45438319708333447], "caller": "main.py:200", "time": "2021-02-05T20:00:59.704077"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23367109894752502, 0.23191742599010468, 100, 0.46011726792025875], "caller": "main.py:200", "time": "2021-02-05T20:02:24.356441"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23435162007808685, 0.2255796492099762, 100, 0.46175262982996157], "caller": "main.py:200", "time": "2021-02-05T20:03:48.854892"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23386016488075256, 0.23463553190231323, 100, 0.457032225980355], "caller": "main.py:200", "time": "2021-02-05T20:05:13.572685"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23244932293891907, 0.22502173483371735, 100, 0.4566225939532468], "caller": "main.py:200", "time": "2021-02-05T20:06:38.087195"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23458778858184814, 0.2175971418619156, 100, 0.45944935883457494], "caller": "main.py:200", "time": "2021-02-05T20:08:02.663606"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23241685330867767, 0.2350248098373413, 100, 0.45650989766324185], "caller": "main.py:200", "time": "2021-02-05T20:09:27.122864"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23207446932792664, 0.2272978574037552, 100, 0.455430918886418], "caller": "main.py:200", "time": "2021-02-05T20:10:51.147600"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2322351634502411, 0.2205696702003479, 100, 0.44634530003375755], "caller": "main.py:200", "time": "2021-02-05T20:12:16.158460"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2311272770166397, 0.264732301235199, 100, 0.45203510428915095], "caller": "main.py:200", "time": "2021-02-05T20:13:40.830620"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23432961106300354, 0.2439839392900467, 100, 0.45667372997008737], "caller": "main.py:200", "time": "2021-02-05T20:15:05.298487"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23189692199230194, 0.22299537062644958, 100, 0.45470782871467635], "caller": "main.py:200", "time": "2021-02-05T20:16:29.684980"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23097367584705353, 0.22457081079483032, 100, 0.46671206916671915], "caller": "main.py:200", "time": "2021-02-05T20:17:54.379560"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23255066573619843, 0.24191905558109283, 100, 0.4566012277742595], "caller": "main.py:200", "time": "2021-02-05T20:19:18.947973"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23191513121128082, 0.26025325059890747, 100, 0.46062200303239853], "caller": "main.py:200", "time": "2021-02-05T20:20:43.230244"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23106732964515686, 0.23045578598976135, 100, 0.45903049941241], "caller": "main.py:200", "time": "2021-02-05T20:22:07.545736"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23064184188842773, 0.23573477566242218, 100, 0.4516975184823381], "caller": "main.py:200", "time": "2021-02-05T20:23:32.147649"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23027195036411285, 0.23115095496177673, 100, 0.4599777715938262], "caller": "main.py:200", "time": "2021-02-05T20:24:56.690207"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23138432204723358, 0.21993502974510193, 100, 0.45969882215432845], "caller": "main.py:200", "time": "2021-02-05T20:26:21.167537"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [47], "caller": "main.py:129", "time": "2021-02-05T20:27:43.609465"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 9, 0.1111111111111111, 0.31426968052735443], "caller": "main.py:37", "time": "2021-02-05T20:27:52.677692"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 10, 32.97890659917072, 36.70491684517064], "caller": "main.py:37", "time": "2021-02-05T20:27:56.713515"}
+{"level": "info", "fmt": "episode: %s", "args": [8.73831902074591], "caller": "main.py:149", "time": "2021-02-05T20:28:06.971571"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23065166175365448, 0.23906102776527405, 100, 0.4540774878405443], "caller": "main.py:200", "time": "2021-02-05T20:28:09.011243"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2307441085577011, 0.24170057475566864, 100, 0.45598073656767557], "caller": "main.py:200", "time": "2021-02-05T20:29:33.818564"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23038333654403687, 0.23406276106834412, 100, 0.45088257576486734], "caller": "main.py:200", "time": "2021-02-05T20:30:58.242586"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.22911608219146729, 0.23017892241477966, 100, 0.4508431085493333], "caller": "main.py:200", "time": "2021-02-05T20:32:22.531082"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2312256395816803, 0.24091270565986633, 100, 0.46020629788803474], "caller": "main.py:200", "time": "2021-02-05T20:33:47.094717"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.229876309633255, 0.21086038649082184, 100, 0.4439142635020754], "caller": "main.py:200", "time": "2021-02-05T20:35:11.668702"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.22842366993427277, 0.23558300733566284, 100, 0.4462212934643844], "caller": "main.py:200", "time": "2021-02-05T20:36:35.850977"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.22938168048858643, 0.231635183095932, 100, 0.4469598865062074], "caller": "main.py:200", "time": "2021-02-05T20:38:00.229170"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.22722621262073517, 0.21877889335155487, 100, 0.44054767008190654], "caller": "main.py:200", "time": "2021-02-05T20:39:24.700291"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.22950123250484467, 0.23072856664657593, 100, 0.46004531647134855], "caller": "main.py:200", "time": "2021-02-05T20:40:49.511907"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23145227134227753, 0.22434483468532562, 100, 0.45679420556030437], "caller": "main.py:200", "time": "2021-02-05T20:42:13.916880"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2284998893737793, 0.2257922738790512, 100, 0.46221815849688813], "caller": "main.py:200", "time": "2021-02-05T20:43:38.275278"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22702130675315857, 0.22987332940101624, 100, 0.44866223449988785], "caller": "main.py:200", "time": "2021-02-05T20:45:02.671929"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.22855013608932495, 0.22451722621917725, 100, 0.45741283710197145], "caller": "main.py:200", "time": "2021-02-05T20:46:26.959128"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22433950006961823, 0.23093745112419128, 100, 0.45217687572487436], "caller": "main.py:200", "time": "2021-02-05T20:47:51.347839"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22967976331710815, 0.2380084991455078, 100, 0.4621173807516513], "caller": "main.py:200", "time": "2021-02-05T20:49:15.717032"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23036788403987885, 0.2280646562576294, 100, 0.45943514580900646], "caller": "main.py:200", "time": "2021-02-05T20:50:39.998999"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22758251428604126, 0.2241745889186859, 100, 0.46527213360186837], "caller": "main.py:200", "time": "2021-02-05T20:52:04.488847"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22946380078792572, 0.23824399709701538, 100, 0.45161248172773844], "caller": "main.py:200", "time": "2021-02-05T20:53:29.285742"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.22718115150928497, 0.21703898906707764, 100, 0.4475149195938823], "caller": "main.py:200", "time": "2021-02-05T20:54:53.748122"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [48], "caller": "main.py:129", "time": "2021-02-05T20:56:15.929288"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T20:56:25.241142"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 16.51380972265385, 0.1820444452392426], "caller": "main.py:37", "time": "2021-02-05T20:56:29.485854"}
+{"level": "info", "fmt": "episode: %s", "args": [8.890210054481447], "caller": "main.py:149", "time": "2021-02-05T20:56:39.801727"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.22970624268054962, 0.23037654161453247, 100, 0.45432604010330246], "caller": "main.py:200", "time": "2021-02-05T20:56:41.867865"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.22949573397636414, 0.21296051144599915, 100, 0.4488744955806085], "caller": "main.py:200", "time": "2021-02-05T20:58:06.111392"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.22544075548648834, 0.24675486981868744, 100, 0.4543122723244304], "caller": "main.py:200", "time": "2021-02-05T20:59:30.663241"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.22649045288562775, 0.22680403292179108, 100, 0.45500608618477745], "caller": "main.py:200", "time": "2021-02-05T21:00:55.029328"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.224647656083107, 0.22210437059402466, 100, 0.45442345435843495], "caller": "main.py:200", "time": "2021-02-05T21:02:19.220923"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.22553607821464539, 0.24753545224666595, 100, 0.4499808129218255], "caller": "main.py:200", "time": "2021-02-05T21:03:43.736917"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.22608008980751038, 0.2315455973148346, 100, 0.45481855566721807], "caller": "main.py:200", "time": "2021-02-05T21:05:08.274649"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.22711843252182007, 0.21715010702610016, 100, 0.458205540432409], "caller": "main.py:200", "time": "2021-02-05T21:06:32.538903"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.22481755912303925, 0.2304503321647644, 100, 0.4449699437173286], "caller": "main.py:200", "time": "2021-02-05T21:07:56.482087"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2265903502702713, 0.22031566500663757, 100, 0.4492558812645837], "caller": "main.py:200", "time": "2021-02-05T21:09:20.728933"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.22514083981513977, 0.23209263384342194, 100, 0.461561665488817], "caller": "main.py:200", "time": "2021-02-05T21:10:45.363606"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22505861520767212, 0.25201570987701416, 100, 0.4446360930355802], "caller": "main.py:200", "time": "2021-02-05T21:12:09.891111"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22550366818904877, 0.23932233452796936, 100, 0.4546886720798707], "caller": "main.py:200", "time": "2021-02-05T21:13:34.020904"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.22784537076950073, 0.23966604471206665, 100, 0.4507768493439986], "caller": "main.py:200", "time": "2021-02-05T21:14:58.251918"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22616051137447357, 0.20994681119918823, 100, 0.44935030604875775], "caller": "main.py:200", "time": "2021-02-05T21:16:22.990847"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22391757369041443, 0.24089692533016205, 100, 0.4466719483882552], "caller": "main.py:200", "time": "2021-02-05T21:17:47.639776"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.2258712202310562, 0.22992601990699768, 100, 0.45492082740287604], "caller": "main.py:200", "time": "2021-02-05T21:19:11.829994"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22392091155052185, 0.25136464834213257, 100, 0.4556815835377868], "caller": "main.py:200", "time": "2021-02-05T21:20:35.931966"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22696909308433533, 0.19530263543128967, 100, 0.44611323530981223], "caller": "main.py:200", "time": "2021-02-05T21:22:00.606079"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.22636108100414276, 0.21151208877563477, 100, 0.44498257274946235], "caller": "main.py:200", "time": "2021-02-05T21:23:24.805560"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [49], "caller": "main.py:129", "time": "2021-02-05T21:24:46.700269"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T21:24:55.737022"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 10, 33.454236625118604, 36.58440751537775], "caller": "main.py:37", "time": "2021-02-05T21:24:59.778858"}
+{"level": "info", "fmt": "episode: %s", "args": [8.612110514550421], "caller": "main.py:149", "time": "2021-02-05T21:25:09.867361"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.22677846252918243, 0.2141023576259613, 100, 0.4489411282090427], "caller": "main.py:200", "time": "2021-02-05T21:25:11.932657"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.22743871808052063, 0.20570001006126404, 100, 0.44619335587331127], "caller": "main.py:200", "time": "2021-02-05T21:26:35.880346"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.22545818984508514, 0.2284492552280426, 100, 0.44411145772069127], "caller": "main.py:200", "time": "2021-02-05T21:28:00.203846"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.22451549768447876, 0.22394105792045593, 100, 0.44977468635114454], "caller": "main.py:200", "time": "2021-02-05T21:29:24.319056"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.224447101354599, 0.2260664701461792, 100, 0.4450153538139694], "caller": "main.py:200", "time": "2021-02-05T21:30:48.868265"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.22404465079307556, 0.22218182682991028, 100, 0.4518319215918751], "caller": "main.py:200", "time": "2021-02-05T21:32:13.051485"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2244257777929306, 0.21632204949855804, 100, 0.4567461853762691], "caller": "main.py:200", "time": "2021-02-05T21:33:37.747845"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2258124202489853, 0.22551798820495605, 100, 0.44655383880441235], "caller": "main.py:200", "time": "2021-02-05T21:35:02.435210"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.22594498097896576, 0.21023118495941162, 100, 0.45169191326154395], "caller": "main.py:200", "time": "2021-02-05T21:36:26.547961"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.22501353919506073, 0.20948831737041473, 100, 0.4483976199774572], "caller": "main.py:200", "time": "2021-02-05T21:37:50.911234"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2277127206325531, 0.2167503535747528, 100, 0.4553521027705697], "caller": "main.py:200", "time": "2021-02-05T21:39:15.747912"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22520712018013, 0.22566339373588562, 100, 0.4574934711594852], "caller": "main.py:200", "time": "2021-02-05T21:40:40.142691"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22380632162094116, 0.20672887563705444, 100, 0.44236133243190295], "caller": "main.py:200", "time": "2021-02-05T21:42:04.240015"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.22627809643745422, 0.22971618175506592, 100, 0.44045560295945635], "caller": "main.py:200", "time": "2021-02-05T21:43:28.696580"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2249078005552292, 0.20753884315490723, 100, 0.4522518157506733], "caller": "main.py:200", "time": "2021-02-05T21:44:52.978090"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22444534301757812, 0.22223594784736633, 100, 0.4453567680271158], "caller": "main.py:200", "time": "2021-02-05T21:46:17.195860"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.2239004522562027, 0.22277352213859558, 100, 0.44363299573991943], "caller": "main.py:200", "time": "2021-02-05T21:47:41.729496"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22574563324451447, 0.22675205767154694, 100, 0.45525534357048325], "caller": "main.py:200", "time": "2021-02-05T21:49:06.124808"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22572964429855347, 0.19745349884033203, 100, 0.4531894102239415], "caller": "main.py:200", "time": "2021-02-05T21:50:30.510158"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.22378362715244293, 0.23681676387786865, 100, 0.45859046663228353], "caller": "main.py:200", "time": "2021-02-05T21:51:55.342569"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T21:53:27.123728"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 15.773014155492273, 0.31361383936605874], "caller": "main.py:37", "time": "2021-02-05T21:53:31.109118"}
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/.gitignore b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/.gitignore
new file mode 100644
index 0000000..01e12c7
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/.gitignore
@@ -0,0 +1,132 @@
+# Byte-compiled / optimized / DLL files
+__pycache__/
+*.py[cod]
+*$py.class
+
+# C extensions
+*.so
+
+# Distribution / packaging
+.Python
+build/
+develop-eggs/
+dist/
+downloads/
+eggs/
+.eggs/
+lib/
+lib64/
+parts/
+sdist/
+var/
+wheels/
+pip-wheel-metadata/
+share/python-wheels/
+*.egg-info/
+.installed.cfg
+*.egg
+MANIFEST
+vendor/
+rllab/
+tmp/
+log/
+experiments/
+# PyInstaller
+#  Usually these files are written by a python script from a template
+#  before PyInstaller builds the exe, so as to inject date/other infos into it.
+*.manifest
+*.spec
+
+# Installer logs
+pip-log.txt
+pip-delete-this-directory.txt
+
+# Unit test / coverage reports
+htmlcov/
+.tox/
+.nox/
+.coverage
+.coverage.*
+.cache
+nosetests.xml
+coverage.xml
+*.cover
+*.py,cover
+.hypothesis/
+.pytest_cache/
+
+# Translations
+*.mo
+*.pot
+
+# Django stuff:
+*.log
+local_settings.py
+db.sqlite3
+db.sqlite3-journal
+
+# Flask stuff:
+instance/
+.webassets-cache
+
+# Scrapy stuff:
+.scrapy
+
+# Sphinx documentation
+docs/_build/
+
+# PyBuilder
+target/
+
+# Jupyter Notebook
+.ipynb_checkpoints
+
+# IPython
+profile_default/
+ipython_config.py
+
+# pyenv
+.python-version
+
+# pipenv
+#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
+#   However, in case of collaboration, if having platform-specific dependencies or dependencies
+#   having no cross-platform support, pipenv may install dependencies that don't work, or not
+#   install all needed dependencies.
+#Pipfile.lock
+
+# PEP 582; used by e.g. github.com/David-OConnor/pyflow
+__pypackages__/
+
+# Celery stuff
+celerybeat-schedule
+celerybeat.pid
+
+# SageMath parsed files
+*.sage.py
+
+# Environments
+.env
+.venv
+venv/
+ENV/
+env.bak/
+venv.bak/
+
+# Spyder project settings
+.spyderproject
+.spyproject
+
+# Rope project settings
+.ropeproject
+
+# mkdocs documentation
+/site
+
+# mypy
+.mypy_cache/
+.dmypy.json
+dmypy.json
+
+# Pyre type checker
+.pyre/
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/CODE_OF_CONDUCT.md b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/CODE_OF_CONDUCT.md
new file mode 100644
index 0000000..0d31b1f
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/CODE_OF_CONDUCT.md
@@ -0,0 +1,5 @@
+# Code of Conduct
+
+Facebook has adopted a Code of Conduct that we expect project participants to adhere to.
+Please read the [full text](https://code.fb.com/codeofconduct/)
+so that you can understand what actions will and will not be tolerated.
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/CONTRIBUTING.md b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/CONTRIBUTING.md
new file mode 100644
index 0000000..07780f7
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/CONTRIBUTING.md
@@ -0,0 +1,31 @@
+# Contributing to SLBO
+We want to make contributing to this project as easy and transparent as
+possible.
+
+## Pull Requests
+We actively welcome your pull requests.
+
+1. Fork the repo and create your branch from `master`.
+2. Ensure the examples still run.
+3. If you haven't already, complete the Contributor License Agreement ("CLA").
+
+## Contributor License Agreement ("CLA")
+In order to accept your pull request, we need you to submit a CLA. You only need
+to do this once to work on any of Facebook's open source projects.
+
+Complete your CLA here: <https://code.facebook.com/cla>
+
+## Issues
+We use GitHub issues to track public bugs. Please ensure your description is
+clear and has sufficient instructions to be able to reproduce the issue.
+
+Facebook has a [bounty program](https://www.facebook.com/whitehat/) for the safe
+disclosure of security bugs. In those cases, please go through the process
+outlined on that page and do not file a public issue.
+
+## Coding Style  
+We try to follow the PEP style guidelines and encourage you to as well.
+
+## License
+By contributing to SparseConvNet, you agree that your contributions will be licensed
+under the LICENSE file in the root directory of this source tree.
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/LICENSE b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/LICENSE
new file mode 100644
index 0000000..1fe4148
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/LICENSE
@@ -0,0 +1,407 @@
+Attribution-NonCommercial 4.0 International
+
+=======================================================================
+
+Creative Commons Corporation ("Creative Commons") is not a law firm and
+does not provide legal services or legal advice. Distribution of
+Creative Commons public licenses does not create a lawyer-client or
+other relationship. Creative Commons makes its licenses and related
+information available on an "as-is" basis. Creative Commons gives no
+warranties regarding its licenses, any material licensed under their
+terms and conditions, or any related information. Creative Commons
+disclaims all liability for damages resulting from their use to the
+fullest extent possible.
+
+Using Creative Commons Public Licenses
+
+Creative Commons public licenses provide a standard set of terms and
+conditions that creators and other rights holders may use to share
+original works of authorship and other material subject to copyright
+and certain other rights specified in the public license below. The
+following considerations are for informational purposes only, are not
+exhaustive, and do not form part of our licenses.
+
+     Considerations for licensors: Our public licenses are
+     intended for use by those authorized to give the public
+     permission to use material in ways otherwise restricted by
+     copyright and certain other rights. Our licenses are
+     irrevocable. Licensors should read and understand the terms
+     and conditions of the license they choose before applying it.
+     Licensors should also secure all rights necessary before
+     applying our licenses so that the public can reuse the
+     material as expected. Licensors should clearly mark any
+     material not subject to the license. This includes other CC-
+     licensed material, or material used under an exception or
+     limitation to copyright. More considerations for licensors:
+	wiki.creativecommons.org/Considerations_for_licensors
+
+     Considerations for the public: By using one of our public
+     licenses, a licensor grants the public permission to use the
+     licensed material under specified terms and conditions. If
+     the licensor's permission is not necessary for any reason--for
+     example, because of any applicable exception or limitation to
+     copyright--then that use is not regulated by the license. Our
+     licenses grant only permissions under copyright and certain
+     other rights that a licensor has authority to grant. Use of
+     the licensed material may still be restricted for other
+     reasons, including because others have copyright or other
+     rights in the material. A licensor may make special requests,
+     such as asking that all changes be marked or described.
+     Although not required by our licenses, you are encouraged to
+     respect those requests where reasonable. More_considerations
+     for the public:
+	wiki.creativecommons.org/Considerations_for_licensees
+
+=======================================================================
+
+Creative Commons Attribution-NonCommercial 4.0 International Public
+License
+
+By exercising the Licensed Rights (defined below), You accept and agree
+to be bound by the terms and conditions of this Creative Commons
+Attribution-NonCommercial 4.0 International Public License ("Public
+License"). To the extent this Public License may be interpreted as a
+contract, You are granted the Licensed Rights in consideration of Your
+acceptance of these terms and conditions, and the Licensor grants You
+such rights in consideration of benefits the Licensor receives from
+making the Licensed Material available under these terms and
+conditions.
+
+
+Section 1 -- Definitions.
+
+  a. Adapted Material means material subject to Copyright and Similar
+     Rights that is derived from or based upon the Licensed Material
+     and in which the Licensed Material is translated, altered,
+     arranged, transformed, or otherwise modified in a manner requiring
+     permission under the Copyright and Similar Rights held by the
+     Licensor. For purposes of this Public License, where the Licensed
+     Material is a musical work, performance, or sound recording,
+     Adapted Material is always produced where the Licensed Material is
+     synched in timed relation with a moving image.
+
+  b. Adapter's License means the license You apply to Your Copyright
+     and Similar Rights in Your contributions to Adapted Material in
+     accordance with the terms and conditions of this Public License.
+
+  c. Copyright and Similar Rights means copyright and/or similar rights
+     closely related to copyright including, without limitation,
+     performance, broadcast, sound recording, and Sui Generis Database
+     Rights, without regard to how the rights are labeled or
+     categorized. For purposes of this Public License, the rights
+     specified in Section 2(b)(1)-(2) are not Copyright and Similar
+     Rights.
+  d. Effective Technological Measures means those measures that, in the
+     absence of proper authority, may not be circumvented under laws
+     fulfilling obligations under Article 11 of the WIPO Copyright
+     Treaty adopted on December 20, 1996, and/or similar international
+     agreements.
+
+  e. Exceptions and Limitations means fair use, fair dealing, and/or
+     any other exception or limitation to Copyright and Similar Rights
+     that applies to Your use of the Licensed Material.
+
+  f. Licensed Material means the artistic or literary work, database,
+     or other material to which the Licensor applied this Public
+     License.
+
+  g. Licensed Rights means the rights granted to You subject to the
+     terms and conditions of this Public License, which are limited to
+     all Copyright and Similar Rights that apply to Your use of the
+     Licensed Material and that the Licensor has authority to license.
+
+  h. Licensor means the individual(s) or entity(ies) granting rights
+     under this Public License.
+
+  i. NonCommercial means not primarily intended for or directed towards
+     commercial advantage or monetary compensation. For purposes of
+     this Public License, the exchange of the Licensed Material for
+     other material subject to Copyright and Similar Rights by digital
+     file-sharing or similar means is NonCommercial provided there is
+     no payment of monetary compensation in connection with the
+     exchange.
+
+  j. Share means to provide material to the public by any means or
+     process that requires permission under the Licensed Rights, such
+     as reproduction, public display, public performance, distribution,
+     dissemination, communication, or importation, and to make material
+     available to the public including in ways that members of the
+     public may access the material from a place and at a time
+     individually chosen by them.
+
+  k. Sui Generis Database Rights means rights other than copyright
+     resulting from Directive 96/9/EC of the European Parliament and of
+     the Council of 11 March 1996 on the legal protection of databases,
+     as amended and/or succeeded, as well as other essentially
+     equivalent rights anywhere in the world.
+
+  l. You means the individual or entity exercising the Licensed Rights
+     under this Public License. Your has a corresponding meaning.
+
+
+Section 2 -- Scope.
+
+  a. License grant.
+
+       1. Subject to the terms and conditions of this Public License,
+          the Licensor hereby grants You a worldwide, royalty-free,
+          non-sublicensable, non-exclusive, irrevocable license to
+          exercise the Licensed Rights in the Licensed Material to:
+
+            a. reproduce and Share the Licensed Material, in whole or
+               in part, for NonCommercial purposes only; and
+
+            b. produce, reproduce, and Share Adapted Material for
+               NonCommercial purposes only.
+
+       2. Exceptions and Limitations. For the avoidance of doubt, where
+          Exceptions and Limitations apply to Your use, this Public
+          License does not apply, and You do not need to comply with
+          its terms and conditions.
+
+       3. Term. The term of this Public License is specified in Section
+          6(a).
+
+       4. Media and formats; technical modifications allowed. The
+          Licensor authorizes You to exercise the Licensed Rights in
+          all media and formats whether now known or hereafter created,
+          and to make technical modifications necessary to do so. The
+          Licensor waives and/or agrees not to assert any right or
+          authority to forbid You from making technical modifications
+          necessary to exercise the Licensed Rights, including
+          technical modifications necessary to circumvent Effective
+          Technological Measures. For purposes of this Public License,
+          simply making modifications authorized by this Section 2(a)
+          (4) never produces Adapted Material.
+
+       5. Downstream recipients.
+
+            a. Offer from the Licensor -- Licensed Material. Every
+               recipient of the Licensed Material automatically
+               receives an offer from the Licensor to exercise the
+               Licensed Rights under the terms and conditions of this
+               Public License.
+
+            b. No downstream restrictions. You may not offer or impose
+               any additional or different terms or conditions on, or
+               apply any Effective Technological Measures to, the
+               Licensed Material if doing so restricts exercise of the
+               Licensed Rights by any recipient of the Licensed
+               Material.
+
+       6. No endorsement. Nothing in this Public License constitutes or
+          may be construed as permission to assert or imply that You
+          are, or that Your use of the Licensed Material is, connected
+          with, or sponsored, endorsed, or granted official status by,
+          the Licensor or others designated to receive attribution as
+          provided in Section 3(a)(1)(A)(i).
+
+  b. Other rights.
+
+       1. Moral rights, such as the right of integrity, are not
+          licensed under this Public License, nor are publicity,
+          privacy, and/or other similar personality rights; however, to
+          the extent possible, the Licensor waives and/or agrees not to
+          assert any such rights held by the Licensor to the limited
+          extent necessary to allow You to exercise the Licensed
+          Rights, but not otherwise.
+
+       2. Patent and trademark rights are not licensed under this
+          Public License.
+
+       3. To the extent possible, the Licensor waives any right to
+          collect royalties from You for the exercise of the Licensed
+          Rights, whether directly or through a collecting society
+          under any voluntary or waivable statutory or compulsory
+          licensing scheme. In all other cases the Licensor expressly
+          reserves any right to collect such royalties, including when
+          the Licensed Material is used other than for NonCommercial
+          purposes.
+
+
+Section 3 -- License Conditions.
+
+Your exercise of the Licensed Rights is expressly made subject to the
+following conditions.
+
+  a. Attribution.
+
+       1. If You Share the Licensed Material (including in modified
+          form), You must:
+
+            a. retain the following if it is supplied by the Licensor
+               with the Licensed Material:
+
+                 i. identification of the creator(s) of the Licensed
+                    Material and any others designated to receive
+                    attribution, in any reasonable manner requested by
+                    the Licensor (including by pseudonym if
+                    designated);
+
+                ii. a copyright notice;
+
+               iii. a notice that refers to this Public License;
+
+                iv. a notice that refers to the disclaimer of
+                    warranties;
+
+                 v. a URI or hyperlink to the Licensed Material to the
+                    extent reasonably practicable;
+
+            b. indicate if You modified the Licensed Material and
+               retain an indication of any previous modifications; and
+
+            c. indicate the Licensed Material is licensed under this
+               Public License, and include the text of, or the URI or
+               hyperlink to, this Public License.
+
+       2. You may satisfy the conditions in Section 3(a)(1) in any
+          reasonable manner based on the medium, means, and context in
+          which You Share the Licensed Material. For example, it may be
+          reasonable to satisfy the conditions by providing a URI or
+          hyperlink to a resource that includes the required
+          information.
+
+       3. If requested by the Licensor, You must remove any of the
+          information required by Section 3(a)(1)(A) to the extent
+          reasonably practicable.
+
+       4. If You Share Adapted Material You produce, the Adapter's
+          License You apply must not prevent recipients of the Adapted
+          Material from complying with this Public License.
+
+
+Section 4 -- Sui Generis Database Rights.
+
+Where the Licensed Rights include Sui Generis Database Rights that
+apply to Your use of the Licensed Material:
+
+  a. for the avoidance of doubt, Section 2(a)(1) grants You the right
+     to extract, reuse, reproduce, and Share all or a substantial
+     portion of the contents of the database for NonCommercial purposes
+     only;
+
+  b. if You include all or a substantial portion of the database
+     contents in a database in which You have Sui Generis Database
+     Rights, then the database in which You have Sui Generis Database
+     Rights (but not its individual contents) is Adapted Material; and
+
+  c. You must comply with the conditions in Section 3(a) if You Share
+     all or a substantial portion of the contents of the database.
+
+For the avoidance of doubt, this Section 4 supplements and does not
+replace Your obligations under this Public License where the Licensed
+Rights include other Copyright and Similar Rights.
+
+
+Section 5 -- Disclaimer of Warranties and Limitation of Liability.
+
+  a. UNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE
+     EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS
+     AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF
+     ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS,
+     IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION,
+     WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR
+     PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS,
+     ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT
+     KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT
+     ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU.
+
+  b. TO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE
+     TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION,
+     NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT,
+     INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES,
+     COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR
+     USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN
+     ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR
+     DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR
+     IN PART, THIS LIMITATION MAY NOT APPLY TO YOU.
+
+  c. The disclaimer of warranties and limitation of liability provided
+     above shall be interpreted in a manner that, to the extent
+     possible, most closely approximates an absolute disclaimer and
+     waiver of all liability.
+
+
+Section 6 -- Term and Termination.
+
+  a. This Public License applies for the term of the Copyright and
+     Similar Rights licensed here. However, if You fail to comply with
+     this Public License, then Your rights under this Public License
+     terminate automatically.
+
+  b. Where Your right to use the Licensed Material has terminated under
+     Section 6(a), it reinstates:
+
+       1. automatically as of the date the violation is cured, provided
+          it is cured within 30 days of Your discovery of the
+          violation; or
+
+       2. upon express reinstatement by the Licensor.
+
+     For the avoidance of doubt, this Section 6(b) does not affect any
+     right the Licensor may have to seek remedies for Your violations
+     of this Public License.
+
+  c. For the avoidance of doubt, the Licensor may also offer the
+     Licensed Material under separate terms or conditions or stop
+     distributing the Licensed Material at any time; however, doing so
+     will not terminate this Public License.
+
+  d. Sections 1, 5, 6, 7, and 8 survive termination of this Public
+     License.
+
+
+Section 7 -- Other Terms and Conditions.
+
+  a. The Licensor shall not be bound by any additional or different
+     terms or conditions communicated by You unless expressly agreed.
+
+  b. Any arrangements, understandings, or agreements regarding the
+     Licensed Material not stated herein are separate from and
+     independent of the terms and conditions of this Public License.
+
+
+Section 8 -- Interpretation.
+
+  a. For the avoidance of doubt, this Public License does not, and
+     shall not be interpreted to, reduce, limit, restrict, or impose
+     conditions on any use of the Licensed Material that could lawfully
+     be made without permission under this Public License.
+
+  b. To the extent possible, if any provision of this Public License is
+     deemed unenforceable, it shall be automatically reformed to the
+     minimum extent necessary to make it enforceable. If the provision
+     cannot be reformed, it shall be severed from this Public License
+     without affecting the enforceability of the remaining terms and
+     conditions.
+
+  c. No term or condition of this Public License will be waived and no
+     failure to comply consented to unless expressly agreed to by the
+     Licensor.
+
+  d. Nothing in this Public License constitutes or may be interpreted
+     as a limitation upon, or waiver of, any privileges and immunities
+     that apply to the Licensor or You, including from the legal
+     processes of any jurisdiction or authority.
+
+=======================================================================
+
+Creative Commons is not a party to its public
+licenses. Notwithstanding, Creative Commons may elect to apply one of
+its public licenses to material it publishes and in those instances
+will be considered the “Licensor.” The text of the Creative Commons
+public licenses is dedicated to the public domain under the CC0 Public
+Domain Dedication. Except for the limited purpose of indicating that
+material is shared under a Creative Commons public license or as
+otherwise permitted by the Creative Commons policies published at
+creativecommons.org/policies, Creative Commons does not authorize the
+use of the trademark "Creative Commons" or any other trademark or logo
+of Creative Commons without its prior written consent including,
+without limitation, in connection with any unauthorized modifications
+to any of its public licenses or any other arrangements,
+understandings, or agreements concerning use of licensed material. For
+the avoidance of doubt, this paragraph does not form part of the
+public licenses.
+
+Creative Commons may be contacted at creativecommons.org.
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/README.md b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/README.md
new file mode 100644
index 0000000..ec09da9
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/README.md
@@ -0,0 +1,29 @@
+# Stochastic Lower Bound Optimization
+
+This is the TensorFlow implementation for the paper [
+Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees](https://arxiv.org/abs/1807.03858).
+A PyTorch version will be released later.  
+
+
+## Requirements
+1. OpenAI Baselines
+2. rllab (commit number `b3a2899`)
+3. MuJoCo (1.5)
+4. TensorFlow (>= 1.9)
+5. NumPy (>= 1.14.5)
+6. Python 3.6
+
+## Run
+
+Before running, please make sure that `rllab` and `baselines` are available 
+
+```bash
+python main.py -c configs/algos/slbo.yml configs/envs/half_cheetah.yml -s log_dir=/tmp
+```
+
+If you want to change hyper-parameters, you can either modify a corresponding `yml` file or 
+change it temporarily by appending `model.hidden_sizes='[1000,1000]'` in the command line.
+
+## License
+
+See [LICENSE](LICENSE) for additional details.
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/check_result.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/check_result.py
new file mode 100644
index 0000000..d1c8161
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/check_result.py
@@ -0,0 +1,12 @@
+import numpy as np
+
+seed_list = ['1234', '1235', '2314', '2345']
+env_name = 'gym_reacher'
+lasts = []
+for seed in seed_list:
+    cur = np.load('experiments/'+env_name+'_'+seed+'/eval_real_returns.npy')
+    lasts.append(cur[-1])
+
+print(lasts)
+print(np.mean(lasts))
+print(np.std(lasts))
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/algos/mb_trpo.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/algos/mb_trpo.yml
new file mode 100644
index 0000000..cce36ec
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/algos/mb_trpo.yml
@@ -0,0 +1,12 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 2
+  dev_batch_size: 512
+  train_batch_size: 128
+slbo:
+  n_stages: 100
+  n_iters: 1
+  n_model_iters: 2000
+  n_policy_iters: 200
+TRPO:
+  ent_coef: 0.005
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/algos/mf.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/algos/mf.yml
new file mode 100644
index 0000000..5d5e317
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/algos/mf.yml
@@ -0,0 +1,11 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+algorithm: MF
+rollout:
+  n_train_samples: 10000
+  n_dev_samples: 128
+  n_test_samples: 10000
+slbo:
+  n_stages: 100
+  n_iters: 1
+  n_policy_iters: 20
+  n_model_iters: 1
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/algos/slbo.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/algos/slbo.yml
new file mode 100644
index 0000000..2b5eecb
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/algos/slbo.yml
@@ -0,0 +1,12 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 2
+  dev_batch_size: 512
+  train_batch_size: 128
+slbo:
+  n_stages: 100
+  n_iters: 20
+  n_model_iters: 100
+  n_policy_iters: 40
+TRPO:
+  ent_coef: 0
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/algos/slbo_bm_1m.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/algos/slbo_bm_1m.yml
new file mode 100644
index 0000000..dcf3777
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/algos/slbo_bm_1m.yml
@@ -0,0 +1,12 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 2
+  dev_batch_size: 512
+  train_batch_size: 128
+slbo:
+  n_stages: 100
+  n_iters: 20
+  n_model_iters: 100
+  n_policy_iters: 40
+TRPO:
+  ent_coef: 0.005
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/algos/slbo_bm_200k.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/algos/slbo_bm_200k.yml
new file mode 100644
index 0000000..9544e89
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/algos/slbo_bm_200k.yml
@@ -0,0 +1,15 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 2
+  dev_batch_size: 512
+  train_batch_size: 128
+slbo:
+  n_stages: 50
+  n_iters: 20
+  n_model_iters: 100
+  n_policy_iters: 40
+TRPO:
+  ent_coef: 0
+rollout:
+  n_train_samples: 4000
+
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/ant_umaze.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/ant_umaze.yml
new file mode 100644
index 0000000..ab75516
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/ant_umaze.yml
@@ -0,0 +1,8 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: AntUMaze-v1
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
+
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/fetch_push.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/fetch_push.yml
new file mode 100644
index 0000000..43a36d0
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/fetch_push.yml
@@ -0,0 +1,8 @@
+env:
+  id: FetchPush
+runner:
+  max_steps: 50
+plan:
+  max_steps: 50
+pc:
+  bonus_scale: 0.1
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_acrobot.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_acrobot.yml
new file mode 100644
index 0000000..fe01e4a
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_acrobot.yml
@@ -0,0 +1,6 @@
+env:
+  id: Acrobot
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_ant.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_ant.yml
new file mode 100644
index 0000000..07d4c3e
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_ant.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Ant
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_ant_depth_search.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_ant_depth_search.yml
new file mode 100644
index 0000000..1d50248
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_ant_depth_search.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Ant
+runner:
+  max_steps: ENV_LENGTH
+plan:
+  max_steps: PLAN_LENGTH
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cartpole.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cartpole.yml
new file mode 100644
index 0000000..a608f7c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cartpole.yml
@@ -0,0 +1,6 @@
+env:
+  id: CartPole
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cartpoleO001.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cartpoleO001.yml
new file mode 100644
index 0000000..f83770b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cartpoleO001.yml
@@ -0,0 +1,6 @@
+env:
+  id: CartPoleO001
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cartpoleO01.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cartpoleO01.yml
new file mode 100644
index 0000000..3f7674f
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cartpoleO01.yml
@@ -0,0 +1,6 @@
+env:
+  id: CartPoleO01
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetah.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetah.yml
new file mode 100644
index 0000000..ae2c095
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetah.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetah
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahA003.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahA003.yml
new file mode 100644
index 0000000..a87b6be
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahA003.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetahA003
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahA01.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahA01.yml
new file mode 100644
index 0000000..5312055
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahA01.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetahA01
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahO001.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahO001.yml
new file mode 100644
index 0000000..960e36e
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahO001.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetahO001
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahO01.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahO01.yml
new file mode 100644
index 0000000..4a5d67d
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahO01.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetahO01
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetah_depth_search.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetah_depth_search.yml
new file mode 100644
index 0000000..c147ec4
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetah_depth_search.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetah
+runner:
+  max_steps: ENV_LENGTH
+plan:
+  max_steps: PLAN_LENGTH
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_fant.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_fant.yml
new file mode 100644
index 0000000..6773c7b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_fant.yml
@@ -0,0 +1,6 @@
+env:
+  id: FixedAnt
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_fhopper.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_fhopper.yml
new file mode 100644
index 0000000..2efc44e
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_fhopper.yml
@@ -0,0 +1,6 @@
+env:
+  id: FixedHopper
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_fswimmer.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_fswimmer.yml
new file mode 100644
index 0000000..888b87b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_fswimmer.yml
@@ -0,0 +1,6 @@
+env:
+  id: FixedSwimmer
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_fwalker2d.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_fwalker2d.yml
new file mode 100644
index 0000000..ee63b73
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_fwalker2d.yml
@@ -0,0 +1,6 @@
+env:
+  id: FixedWalker
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_hopper.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_hopper.yml
new file mode 100644
index 0000000..a061802
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_hopper.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Hopper
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_humanoid.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_humanoid.yml
new file mode 100644
index 0000000..426bdfb
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_humanoid.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: gym_humanoid
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_invertedPendulum.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_invertedPendulum.yml
new file mode 100644
index 0000000..d1062d9
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_invertedPendulum.yml
@@ -0,0 +1,6 @@
+env:
+  id: InvertedPendulum
+runner:
+  max_steps: 100
+plan:
+  max_steps: 100
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_mountain.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_mountain.yml
new file mode 100644
index 0000000..6c335d5
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_mountain.yml
@@ -0,0 +1,11 @@
+env:
+  id: MountainCar
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
+pc:
+  bonus_scale: 2
+  bonus_stop_time: 50
+rollout:
+  n_train_samples: 1000
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_nostopslimhumanoid.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_nostopslimhumanoid.yml
new file mode 100644
index 0000000..dabe168
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_nostopslimhumanoid.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: gym_nostopslimhumanoid
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_pendulum.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_pendulum.yml
new file mode 100644
index 0000000..c0caff5
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_pendulum.yml
@@ -0,0 +1,6 @@
+env:
+  id: Pendulum
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_pendulumO001.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_pendulumO001.yml
new file mode 100644
index 0000000..1ebd722
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_pendulumO001.yml
@@ -0,0 +1,6 @@
+env:
+  id: PendulumO001
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_pendulumO01.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_pendulumO01.yml
new file mode 100644
index 0000000..4658d7f
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_pendulumO01.yml
@@ -0,0 +1,6 @@
+env:
+  id: PendulumO01
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_reacher.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_reacher.yml
new file mode 100644
index 0000000..427ad36
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_reacher.yml
@@ -0,0 +1,6 @@
+env:
+  id: Reacher
+runner:
+  max_steps: 50
+plan:
+  max_steps: 50
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_slimhumanoid.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_slimhumanoid.yml
new file mode 100644
index 0000000..ecd0976
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_slimhumanoid.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: gym_slimhumanoid
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_swimmer.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_swimmer.yml
new file mode 100644
index 0000000..b9ba125
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_swimmer.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Swimmer
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_walker2d.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_walker2d.yml
new file mode 100644
index 0000000..8403547
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_walker2d.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Walker2D
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/hand_egg.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/hand_egg.yml
new file mode 100644
index 0000000..d4f5160
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/hand_egg.yml
@@ -0,0 +1,8 @@
+env:
+  id: HandEgg
+runner:
+  max_steps: 100
+plan:
+  max_steps: 100
+pc:
+  bonus_scale: 1
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/hand_reach.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/hand_reach.yml
new file mode 100644
index 0000000..40ae08b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/hand_reach.yml
@@ -0,0 +1,8 @@
+env:
+  id: HandReach
+runner:
+  max_steps: 50
+plan:
+  max_steps: 50
+pc:
+  bonus_scale: 0.5
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/point_fall.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/point_fall.yml
new file mode 100644
index 0000000..ac26219
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/point_fall.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: PointFall-v1
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/point_push.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/point_push.yml
new file mode 100644
index 0000000..22d4f65
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/point_push.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: PointPush-v1
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/point_umaze.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/point_umaze.yml
new file mode 100644
index 0000000..b459aa6
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/env_tingwu/point_umaze.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: PointUMaze-v1
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/acrobot.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/acrobot.yml
new file mode 100644
index 0000000..4ca200d
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/acrobot.yml
@@ -0,0 +1,4 @@
+env:
+  id: Acrobot
+runner:
+  max_steps: 200
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/ant.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/ant.yml
new file mode 100644
index 0000000..f24b431
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/ant.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Ant
+runner:
+  max_steps: 1000
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/cartpole.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/cartpole.yml
new file mode 100644
index 0000000..9b49dbe
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/cartpole.yml
@@ -0,0 +1,4 @@
+env:
+  id: CartPole
+runner:
+  max_steps: 200
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/fswimmer.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/fswimmer.yml
new file mode 100644
index 0000000..2aee16a
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/fswimmer.yml
@@ -0,0 +1,4 @@
+env:
+  id: FixedSwimmer
+runner:
+  max_steps: 1000
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/half_cheetah.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/half_cheetah.yml
new file mode 100644
index 0000000..36cfffa
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/half_cheetah.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetah
+runner:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/half_cheetah_short.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/half_cheetah_short.yml
new file mode 100644
index 0000000..e0e21ed
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/half_cheetah_short.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetah-v2
+plan:
+  max_steps: 200
+runner:
+  max_steps: 200
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/hopper.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/hopper.yml
new file mode 100644
index 0000000..7b34a11
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/hopper.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Hopper
+runner:
+  max_steps: 1000
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/humanoid.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/humanoid.yml
new file mode 100644
index 0000000..1eb1236
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/humanoid.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: POMDPHumanoid-v2
+runner:
+  max_steps: 500
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/inverted_pendulum.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/inverted_pendulum.yml
new file mode 100644
index 0000000..1d60732
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/inverted_pendulum.yml
@@ -0,0 +1,4 @@
+env:
+  id: InvertedPendulum
+runner:
+  max_steps: 100
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/mountain.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/mountain.yml
new file mode 100644
index 0000000..e4cb7f6
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/mountain.yml
@@ -0,0 +1,4 @@
+env:
+  id: MountainCar
+runner:
+  max_steps: 200
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/pendulum.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/pendulum.yml
new file mode 100644
index 0000000..8ea91c3
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/pendulum.yml
@@ -0,0 +1,4 @@
+env:
+  id: Pendulum
+runner:
+  max_steps: 200
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/reacher.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/reacher.yml
new file mode 100644
index 0000000..bfaf65b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/reacher.yml
@@ -0,0 +1,4 @@
+env:
+  id: Reacher
+runner:
+  max_steps: 50
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/swimmer.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/swimmer.yml
new file mode 100644
index 0000000..abe2842
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/swimmer.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Swimmer
+runner:
+  max_steps: 1000
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/walker.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/walker.yml
new file mode 100644
index 0000000..e42fa53
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/envs/walker.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Walker2D
+runner:
+  max_steps: 1000
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/multi_step/1.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/multi_step/1.yml
new file mode 100644
index 0000000..e102872
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/multi_step/1.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 1
+  dev_batch_size: 1024
+  train_batch_size: 256
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/multi_step/2.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/multi_step/2.yml
new file mode 100644
index 0000000..c866952
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/multi_step/2.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 2
+  dev_batch_size: 512
+  train_batch_size: 128
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/multi_step/4.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/multi_step/4.yml
new file mode 100644
index 0000000..16a1e5c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/multi_step/4.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 4
+  dev_batch_size: 256
+  train_batch_size: 64
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/multi_step/8.yml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/multi_step/8.yml
new file mode 100644
index 0000000..7262945
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/configs/multi_step/8.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 8
+  dev_batch_size: 128
+  train_batch_size: 32
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/cpu_requirements.txt b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/cpu_requirements.txt
new file mode 100644
index 0000000..141cfbb
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/cpu_requirements.txt
@@ -0,0 +1,8 @@
+tensorflow==1.13.1
+pyyaml==5.1
+termcolor==1.1.0
+gym
+mujoco-py
+json_tricks==3.13.1
+baselines==0.1.5
+
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/Logger.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/Logger.py
new file mode 100644
index 0000000..9f41014
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/Logger.py
@@ -0,0 +1,133 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from termcolor import colored
+import datetime
+import sys
+import os
+from collections import Counter, defaultdict
+import json_tricks
+
+
+def a():
+    pass
+
+
+_srcfile = os.path.normcase(a.__code__.co_filename)
+
+
+class BaseSink(object):
+    @staticmethod
+    def _time():
+        return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S,%f')
+
+    def info(self, fmt, *args, **kwargs):
+        raise NotImplementedError
+
+    def warning(self, fmt, *args, **kwargs):
+        self.info(fmt, *args, **kwargs)
+
+    def verbose(self, fmt, *args, **kwargs):
+        pass
+
+
+class StdoutSink(BaseSink):
+    def __init__(self):
+        self.freq_count = Counter()
+
+    def info(self, fmt, *args, freq=1, caller=None):
+        if args:
+            fmt = fmt % args
+        self.freq_count[caller] += 1
+        if self.freq_count[caller] % freq == 0:
+            print("%s - %s - %s" % (colored(self._time(), 'green'),
+                                    colored(caller, 'cyan'), fmt), flush=True)
+
+    def warning(self, fmt, *args, **kwargs):
+        if args:
+            fmt = fmt % args
+        self.info(colored(fmt, 'yellow'), **kwargs)
+
+
+class FileSink(BaseSink):
+    def __init__(self, fn):
+        self.log_file = open(fn, 'w')
+        self.callers = {}
+
+    def info(self, fmt, *args, **kwargs):
+        self._kv(level='info', fmt=fmt, args=args, **kwargs)
+
+    def warning(self, fmt, *args, **kwargs):
+        self._kv(level='warning', fmt=fmt, args=args, **kwargs)
+
+    def _kv(self, **kwargs):
+        kwargs.update(time=datetime.datetime.now())
+        self.log_file.write(json_tricks.dumps(kwargs, primitives=True) + '\n')
+        self.log_file.flush()
+
+    def verbose(self, fmt, *args, **kwargs):
+        self._kv(level='verbose', fmt=fmt, args=args, **kwargs)
+
+
+class LibLogger(object):
+    logfile = ""
+
+    def __init__(self, name='logger', is_root=True):
+        self.name = name
+        self.is_root = is_root
+        self.tab_keys = None
+        self.sinks = []
+        self.key_prior = defaultdict(np.random.randn)
+
+    def add_sink(self, sink):
+        self.sinks.append(sink)
+
+    def info(self, fmt, *args, **kwargs):
+        caller = self.find_caller()
+        for sink in self.sinks:
+            sink.info(fmt, *args, caller=caller, **kwargs)
+
+    def warning(self, fmt, *args, **kwargs):
+        caller = self.find_caller()
+        for sink in self.sinks:
+            sink.warning(fmt, *args, caller=caller, **kwargs)
+
+    def verbose(self, fmt, *args, **kwargs):
+        caller = self.find_caller()
+        for sink in self.sinks:
+            sink.verbose(fmt, *args, caller=caller, **kwargs)
+
+    def find_caller(self):
+        """
+        Copy from `python.logging` module
+
+        Find the stack frame of the caller so that we can note the source
+        file name, line number and function name.
+        """
+        f = sys._getframe(1)
+        if f is not None:
+            f = f.f_back
+        caller = ''
+        while hasattr(f, "f_code"):
+            co = f.f_code
+            filename = os.path.normcase(co.co_filename)
+            if filename == _srcfile:
+                f = f.f_back
+                continue
+            # if stack_info:
+            #     sio = io.StringIO()
+            #     sio.write('Stack (most recent call last):\n')
+            #     traceback.print_stack(f, file=sio)
+            #     sio.close()
+            # rv = (co.co_filename, f.f_lineno, co.co_name, sinfo)
+            rel_path = os.path.relpath(co.co_filename, '')
+            caller = f'{rel_path}:{f.f_lineno}'
+            break
+        return caller
+
+
+def get_logger(name):
+    return LibLogger(name)
+
+
+logger = get_logger('Logger')
+logger.add_sink(StdoutSink())
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/__init__.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/__init__.py
new file mode 100644
index 0000000..0c5417b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/__init__.py
@@ -0,0 +1,4 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from .stubs import Tensor
+import lunzi.nn
+import lunzi.Logger
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/config.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/config.py
new file mode 100644
index 0000000..5c39b1e
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/config.py
@@ -0,0 +1,97 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import argparse
+import os
+import yaml
+from lunzi.Logger import logger
+
+
+_frozen = False
+_initialized = False
+
+
+def expand(path):
+    return os.path.abspath(os.path.expanduser(path))
+
+
+class MetaFLAGS(type):
+    _initialized = False
+
+    def __setattr__(self, key, value):
+        assert not _frozen, 'Modifying FLAGS after dumping is not allowed!'
+        super().__setattr__(key, value)
+
+    def __getitem__(self, item):
+        return self.__dict__[item]
+
+    def __iter__(self):
+        for key, value in self.__dict__.items():
+            if not key.startswith('_') and not isinstance(value, classmethod):
+                if isinstance(value, MetaFLAGS):
+                    value = dict(value)
+                yield key, value
+
+    def as_dict(self):
+        return dict(self)
+
+    def merge(self, other: dict):
+        for key in other:
+            assert key in self.__dict__, f"Can't find key `{key}`"
+            if isinstance(self[key], MetaFLAGS) and isinstance(other[key], dict):
+                self[key].merge(other[key])
+            else:
+                setattr(self, key, other[key])
+
+    def set_value(self, path, value):
+        key, *rest = path
+        assert key in self.__dict__, f"Can't find key `{key}`"
+        if not rest:
+            setattr(self, key, value)
+        else:
+            self[key]: MetaFLAGS
+            self[key].set_value(rest, value)
+
+    @staticmethod
+    def set_frozen():
+        global _frozen
+        _frozen = True
+
+    def freeze(self):
+        for key, value in self.__dict__.items():
+            if not key.startswith('_'):
+                if isinstance(value, MetaFLAGS):
+                    value.freeze()
+        self.finalize()
+
+    def finalize(self):
+        pass
+
+
+class BaseFLAGS(metaclass=MetaFLAGS):
+    pass
+
+
+def parse(cls):
+    global _initialized
+
+    if _initialized:
+        return
+    parser = argparse.ArgumentParser(description='Stochastic Lower Bound Optimization')
+    parser.add_argument('-c', '--config', type=str, help='configuration file (YAML)', nargs='+', action='append')
+    parser.add_argument('-s', '--set', type=str, help='additional options', nargs='*', action='append')
+
+    args, unknown = parser.parse_known_args()
+    for a in unknown:
+        logger.info('unknown arguments: %s', a)
+    # logger.info('parsed arguments = %s, unknown arguments: %s', args, unknown)
+    if args.config:
+        for config in sum(args.config, []):
+            cls.merge(yaml.load(open(expand(config))))
+    else:
+        logger.info('no config file specified.')
+    if args.set:
+        for instruction in sum(args.set, []):
+            path, *value = instruction.split('=')
+            cls.set_value(path.split('.'), yaml.load('='.join(value)))
+
+    _initialized = True
+
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/dataset.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/dataset.py
new file mode 100644
index 0000000..7cb5d5f
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/dataset.py
@@ -0,0 +1,82 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+
+
+class Dataset(np.recarray):
+    """
+        Overallocation can be supported, by making examinations before
+        each `append` and `extend`.
+    """
+
+    @staticmethod
+    def fromarrays(array_lists, dtype):
+        array = np.rec.fromarrays(array_lists, dtype=dtype)
+        ret = Dataset(dtype, len(array))
+        ret.extend(array)
+        return ret
+
+    def __init__(self, dtype, max_size, verbose=False):
+        super().__init__()
+        self.max_size = max_size
+        self._index = 0
+        self._buf_size = 0
+        self._len = 0
+
+        self.resize(max_size)
+        self._buf_size = max_size
+
+    def __new__(cls, dtype, max_size):
+        return np.recarray.__new__(cls, max_size, dtype=dtype)
+
+    def size(self):
+        return self._len
+
+    def reserve(self, size):
+        cur_size = max(self._buf_size, 1)
+        while cur_size < size:
+            cur_size *= 2
+        if cur_size != self._buf_size:
+            self.resize(cur_size)
+
+    def clear(self):
+        self._index = 0
+        self._len = 0
+        return self
+
+    def append(self, item):
+        self[self._index] = item
+        self._index = (self._index + 1) % self.max_size
+        self._len = min(self._len + 1, self.max_size)
+        return self
+
+    def extend(self, items):
+        n_new = len(items)
+        if n_new > self.max_size:
+            items = items[-self.max_size:]
+            n_new = self.max_size
+
+        n_tail = self.max_size - self._index
+        if n_new <= n_tail:
+            self[self._index:self._index + n_new] = items
+        else:
+            n_head = n_new - n_tail
+            self[self._index:] = items[:n_tail]
+            self[:n_head] = items[n_tail:]
+
+        self._index = (self._index + n_new) % self.max_size
+        self._len = min(self._len + n_new, self.max_size)
+        return self
+
+    def sample(self, size, indices=None):
+        if indices is None:
+            indices = np.random.randint(0, self._len, size=size)
+        return self[indices]
+
+    def iterator(self, batch_size):
+        indices = np.arange(self._len, dtype=np.int32)
+        np.random.shuffle(indices)
+        index = 0
+        while index + batch_size <= self._len:
+            end = index + batch_size
+            yield self[indices[index:end]]
+            index = end
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/__init__.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/__init__.py
new file mode 100644
index 0000000..2315809
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/__init__.py
@@ -0,0 +1,10 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from . import patch
+from .parameter import Parameter
+from .module import Module
+from .container import *
+from . import utils
+from .utils import make_method
+from .layers import *
+from .loss import *
+from .flat_param import FlatParam
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/container.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/container.py
new file mode 100644
index 0000000..6dbafdc
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/container.py
@@ -0,0 +1,35 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import Dict, Any
+from .module import Module
+from .parameter import Parameter
+
+_dict_methods = ['__setitem__', '__getitem__', '__delitem__', '__len__', '__iter__', '__contains__',
+                 'update', 'keys', 'values', 'items', 'clear', 'pop']
+
+
+class ModuleDict(Module, dict):  # use dict for auto-complete
+    """
+        Essentially this exposes some methods of `Module._modules`.
+    """
+    def __init__(self, modules: Dict[Any, Module] = None):
+        super().__init__()
+        for method in _dict_methods:
+            setattr(self, method, getattr(self._modules, method))
+        if modules is not None:
+            self.update(modules)
+
+    def forward(self):
+        raise RuntimeError("ModuleDict is not callable")
+
+
+# Do we need a factory for it?
+class ParameterDict(Module, dict):
+    def __init__(self, parameters: Dict[Any, Parameter] = None):
+        super().__init__()
+        for method in _dict_methods:
+            setattr(self, method, getattr(self._modules, method))
+        if parameters is not None:
+            self.update(parameters)
+
+    def forward(self):
+        raise RuntimeError("ParameterDict is not callable")
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/flat_param.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/flat_param.py
new file mode 100644
index 0000000..5c4bb52
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/flat_param.py
@@ -0,0 +1,34 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+from lunzi.Logger import logger
+from .module import Module
+from .utils import make_method, n_parameters, parameters_to_vector, vector_to_parameters
+
+
+class FlatParam(Module):
+    def __init__(self, parameters):
+        super().__init__()
+        self.params = parameters
+        self.op_feed_flat, self.op_set_flat, self.op_get_flat = \
+            self.enable_flat()
+
+    def enable_flat(self):
+        params = self.params
+        logger.info('Enabling flattening... %s', [p.name for p in params])
+        n_params = n_parameters(params)
+        feed_flat = tf.placeholder(tf.float32, [n_params])
+        get_flat = parameters_to_vector(params)
+        set_flat = tf.group(*[tf.assign(param, value) for param, value in
+                            zip(params, vector_to_parameters(feed_flat, params))])
+        return feed_flat, set_flat, get_flat
+
+    def forward(self):
+        return self.op_get_flat
+
+    @make_method(feed='feed_flat', fetch='set_flat')
+    def set_flat(self, feed_flat):
+        pass
+
+    @make_method(fetch='get_flat')
+    def get_flat(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/layers.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/layers.py
new file mode 100644
index 0000000..a0cd9c2
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/layers.py
@@ -0,0 +1,88 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+import numpy as np
+from .module import Module
+from .parameter import Parameter
+
+
+class Linear(Module):
+    def __init__(self, in_features: int, out_features: int, bias=True, weight_initializer=None):
+        super().__init__()
+        self.in_features = in_features
+        self.out_features = out_features
+        if weight_initializer is None:
+            init_range = tf.sqrt(6.0 / (in_features + out_features))
+            weight_initializer = tf.random_uniform_initializer(-init_range, init_range, dtype=tf.float32)
+
+        self.use_bias = bias
+        with self.scope:
+            self.op_input = tf.placeholder(dtype=tf.float32, shape=[None, in_features], name='input')
+            self.weight = Parameter(weight_initializer([in_features, out_features],dtype=tf.float32), name='weight')
+            if bias:
+                self.bias = Parameter(tf.zeros([out_features], dtype=tf.float32), name='bias')
+
+        self.op_output = self(self.op_input)
+
+    def forward(self, x):
+        shape = x.get_shape().as_list()
+        if len(shape) > 2:
+            y = tf.tensordot(x, self.weight, [[len(shape) - 1], [0]])
+        else:
+            y = x.matmul(self.weight)
+        if self.use_bias:
+            y = y + self.bias
+        return y
+
+    def fast(self, x):
+        x = x.dot(self.weight.numpy())
+        if self.use_bias:
+            x = x + self.bias.numpy()
+        return x
+
+    def extra_repr(self):
+        return f'in_features={self.in_features}, out_features={self.out_features}, bias={self.use_bias}'
+
+
+class Sequential(Module):
+    def __init__(self, *modules):
+        super().__init__()
+        for i, module in enumerate(modules):
+            self._modules[i] = module
+
+    def forward(self, x):
+        for module in self._modules.values():
+            x = module(x)
+        return x
+
+    def fast(self, x):
+        for module in self._modules.values():
+            x = module.fast(x)
+        return x
+
+
+class ReLU(Module):
+    def forward(self, x):
+        return tf.nn.relu(x)
+
+    def fast(self, x: np.ndarray):
+        return np.maximum(x, 0)
+
+
+class Tanh(Module):
+    def forward(self, x):
+        return tf.nn.tanh(x)
+
+    def fast(self, x: np.ndarray):
+        return np.tanh(x)
+
+
+class Squeeze(Module):
+    def __init__(self, axis=None):
+        super().__init__()
+        self._axis = axis
+
+    def forward(self, x):
+        return x.squeeze(axis=self._axis)
+
+    def fast(self, x):
+        return x.squeeze(axis=self._axis)
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/loss.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/loss.py
new file mode 100644
index 0000000..26e662b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/loss.py
@@ -0,0 +1,40 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from lunzi import Tensor
+from .module import Module
+
+
+class PointwiseLoss(Module):
+    def __init__(self, size_average=True, reduce=True):
+        super().__init__()
+        self.size_average = size_average
+        self.reduce = reduce
+
+    def pointwise(self, output: Tensor, target: Tensor):
+        raise NotImplementedError
+
+    def forward(self, output: Tensor, target: Tensor, input: Tensor = None):
+        loss = self.pointwise(output, target)
+        if self.reduce and len(loss.shape) > 1:
+            if self.size_average:
+                loss = loss.reduce_mean(axis=1)
+            else:
+                loss = loss.reduce_sum(axis=1)
+        return loss
+
+
+class L1Loss(PointwiseLoss):
+    def pointwise(self, output: Tensor, target: Tensor):
+        return output.sub(target).abs()
+
+
+class L2Loss(PointwiseLoss):
+    def pointwise(self, output: Tensor, target: Tensor):
+        return output.sub(target).pow(2)
+
+    def forward(self, output: Tensor, target: Tensor, input: Tensor = None):
+        return super().forward(output, target).sqrt()
+
+
+class MSELoss(PointwiseLoss):
+    def pointwise(self, output: Tensor, target: Tensor):
+        return output.sub(target).pow(2)
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/module.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/module.py
new file mode 100644
index 0000000..8eacf3b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/module.py
@@ -0,0 +1,158 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import Dict, Any, Callable, List
+from collections import Counter
+import tensorflow as tf
+import numpy as np
+
+from lunzi import Tensor
+from lunzi.Logger import logger
+from .parameter import Parameter
+
+
+class Module(object):
+    """
+        A front-end for TensorFlow, heavily inspired by PyTorch's design and implementation.
+
+        Deepcopy is not supported since I didn't find a good way to duplicate `tf.Variables` and `tf.variable_scope`.
+    """
+
+    # To generate unique name scope
+    # The only reason we keep variable scope here is that we want the variables have meaning names,
+    # since the internal operations always look messy, I put no hope maintaining their names,
+    # So let's just do it for variables.
+    prefix_count = Counter()
+
+    @staticmethod
+    def _create_uid(prefix: str) -> str:
+        scope = tf.get_variable_scope().name + '/'
+        uid = Module.prefix_count[scope + prefix]
+        Module.prefix_count[scope + prefix] += 1
+        if uid == 0:
+            return prefix
+        return f'{prefix}_{uid}'
+
+    def __init__(self):
+        scope = Module._create_uid(self.__class__.__name__)
+        with tf.variable_scope(scope, reuse=False) as self._scope:
+            pass
+        # Since we only plan to support Python 3.6+, in which dict is already ordered, we don't use OrderedDict here.
+        self._parameters: Dict[Any, Parameter] = {}
+        self._modules: Dict[Any, Module] = {}
+        self._callables: Dict[Any, Callable] = {}
+
+    def forward(self, *args: List[Any], **kwargs: Dict[str, Any]) -> Tensor:
+        raise NotImplementedError
+
+    def fast(self, *args, **kwargs):
+        pass
+
+    def __setattr__(self, key, value):
+        # dynamically maintain sub modules.
+        modules = self.__dict__.get('_modules')
+        if isinstance(value, Parameter):
+            self._parameters[key] = value
+        if isinstance(value, Module):
+            assert modules is not None, 'Call `super().__init__` before assigning modules'
+            modules[key] = value
+        else:
+            if modules and key in modules:
+                del modules[key]
+        object.__setattr__(self, key, value)
+
+    def __call__(self, *args, **kwargs):
+        return self.forward(*args, **kwargs)
+
+    def register_callable(self, key, callable):
+        self._callables[key] = callable
+
+    def eval(self, fetch: str, **feed: Dict[str, np.ndarray]):
+        cache_key = f'[{" ".join(feed.keys())}] => [{fetch}]'
+        if cache_key not in self._callables:
+            logger.info('[%s] is making TensorFlow callables, key = %s', self.__class__.__name__, cache_key)
+            feed_ops = []
+            for key in feed.keys():
+                feed_ops.append(self.__dict__['op_' + key])
+            if isinstance(fetch, str):
+                fetch_ops = [self.__dict__['op_' + key] for key in fetch.split(' ')]
+                if len(fetch_ops) == 1:
+                    fetch_ops = fetch_ops[0]
+            else:
+                fetch_ops = fetch
+            self.register_callable(cache_key, tf.get_default_session().make_callable(fetch_ops, feed_ops))
+        return self._callables[cache_key](*feed.values())
+
+    def parameters(self, trainable=True, non_trainable=False, recursive=True, out=None) -> List[Parameter]:
+        """
+            We don't introduce `buffers` here. PyTorch has it since it doesn't have non-trainable Parameter.
+            A tensor in `buffers` is essentially a non-trainable Parameter (part of state_dict but isn't
+            optimized over).
+        """
+        if out is None:
+            out = []
+        for param in self._parameters.values():
+            if param.trainable and trainable or not param.trainable and non_trainable:
+                out.append(param)
+        if recursive:
+            for module in self._modules.values():
+                module.parameters(trainable=trainable, non_trainable=non_trainable, recursive=True, out=out)
+        # probably we don't need to sort since we're using `OrderedDict`
+        return out
+
+    @property
+    def scope(self) -> tf.variable_scope:
+        return tf.variable_scope(self._scope, reuse=tf.AUTO_REUSE)
+
+    def extra_repr(self) -> str:
+        return ''
+
+    def named_modules(self) -> dict:
+        return self._modules
+
+    def __repr__(self):
+        def dfs(node, prefix):
+            root_info = node.__class__.__name__
+            modules = node.named_modules()
+            if not modules:
+                return root_info + f'({node.extra_repr()})'
+
+            root_info += '(\n'
+            for key, module in modules.items():
+                module_repr = dfs(module, prefix + '    ')
+                root_info += f'{prefix}    ({key}): {module_repr}\n'
+            root_info += prefix + ')'
+            return root_info
+        return dfs(self, '')
+
+    def state_dict(self, recursive=True):
+        """
+            A better option is to find all parameters and then sess.run(state) but I assume this can't be the
+            bottleneck.
+        """
+        state = {}
+        for key, parameter in self._parameters.items():
+            # although we can use `.numpy()` here, for safety I'd use `.eval()`
+            state[key] = parameter.eval()
+        if recursive:
+            for key, module in self._modules.items():
+                state[key] = module.state_dict()
+        return state
+
+    def load_state_dict(self, state_dict: Dict[Any, Any], recursive=True, strict=True):
+        for key, parameter in self._parameters.items():
+            if key in state_dict:
+                parameter.load(state_dict[key])
+                parameter.invalidate()
+            else:
+                assert not strict, f'Missing Parameter {key} in state_dict'
+        if recursive:
+            for key, module in self._modules.items():
+                if key in state_dict:
+                    module.load_state_dict(state_dict[key], recursive=recursive, strict=strict)
+                else:
+                    assert not strict, f'Missing Module {key} in state_dict.'
+
+    def apply(self, fn):
+        for module in self._modules.values():
+            module.apply(fn)
+        fn(self)
+        return self
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/parameter.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/parameter.py
new file mode 100644
index 0000000..969d79f
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/parameter.py
@@ -0,0 +1,24 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+from lunzi import Tensor
+
+
+def numpy(self):
+    if self.__dict__.get('_numpy_cache', None) is None:
+        self._numpy_cache: Tensor = self.eval()
+    return self._numpy_cache
+
+
+def invalidate(self):
+    self._numpy_cache = None
+
+
+# Q: Why not inherit from `tf.Variable`?
+# A: Since TensorFlow 1.11, `tf.Variable` has a meta class VariableMetaClass, which overrides `__call__`.
+#    And it's `_variable_call` function doesn't explicitly call `tf.Variable` so the return value must
+#    be a `tf.Variable`, which makes inheritance impossible.
+Parameter = tf.Variable
+
+Parameter.numpy = numpy
+Parameter.invalidate = invalidate
+
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/patch.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/patch.py
new file mode 100644
index 0000000..5db96d8
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/patch.py
@@ -0,0 +1,60 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+
+from lunzi.Logger import logger
+
+
+def find_monkey_patch_keys(avoid_set=None):
+    if avoid_set is None:
+        avoid_set = {"shape"}  # tf.shape conflicts with Tensor.shape
+    patched = []
+    for key, value in tf.__dict__.items():
+        if not callable(value) or key in avoid_set:
+            continue
+        doc = value.__doc__
+        if doc is None:
+            continue
+        loc = doc.find('Args:\n')
+        if loc == -1:
+            continue
+
+        # Am I doing NLP?
+        # It seems that PyTorch has better doc. They always write `x (Tensor): ...` which is much easier to parse.
+        first_arg_doc = doc[loc + 6:].split('\n')[0].split(': ')[1]
+        if first_arg_doc.startswith('A `Tensor`') or first_arg_doc.startswith('`Tensor`') or key.startswith('reduce_'):
+            patched.append(key)
+    logger.warning(f'Monkey patched TensorFlow: {patched}')
+    return patched
+
+
+def monkey_patch(avoid_set=None):
+    logger.warning('Monkey patching TensorFlow...')
+
+    patched = ['abs', 'acos', 'acosh', 'add', 'angle', 'argmax', 'argmin', 'asin', 'asinh', 'atan', 'atan2', 'atanh',
+            'betainc', 'cast', 'ceil', 'check_numerics', 'clip_by_average_norm', 'clip_by_norm', 'clip_by_value',
+            'complex', 'conj', 'cos', 'cosh', 'cross', 'cumprod', 'cumsum', 'dequantize', 'diag', 'digamma', 'div',
+            'equal', 'erf', 'erfc', 'exp', 'expand_dims', 'expm1', 'fill', 'floor', 'floor_div', 'floordiv', 'floormod',
+            'gather', 'gather_nd', 'greater', 'greater_equal', 'hessians', 'identity', 'igamma', 'igammac', 'imag',
+            'is_finite', 'is_inf', 'is_nan', 'less', 'less_equal', 'lgamma', 'log', 'log1p', 'logical_and',
+            'logical_not', 'logical_or', 'matmul', 'maximum', 'meshgrid', 'minimum', 'mod', 'multiply', 'negative',
+            'norm', 'not_equal', 'one_hot', 'ones_like', 'pad', 'polygamma', 'pow', 'quantize', 'real', 'realdiv',
+            'reciprocal', 'reduce_all', 'reduce_any', 'reduce_logsumexp', 'reduce_max', 'reduce_mean', 'reduce_min',
+            'reduce_prod', 'reduce_sum', 'reshape', 'reverse', 'rint', 'round', 'rsqrt', 'scatter_nd', 'sign', 'sin',
+            'sinh', 'size', 'slice', 'sqrt', 'square', 'squeeze', 'stop_gradient', 'subtract', 'tan', 'tensordot',
+            'tile', 'to_bfloat16', 'to_complex128', 'to_complex64', 'to_double', 'to_float', 'to_int32', 'to_int64',
+            'transpose', 'truediv', 'truncatediv', 'truncatemod', 'unique', 'where', 'zeros_like', 'zeta']
+    alias = {
+        'mul': 'multiply',
+        'sub': 'subtract',
+    }
+
+    # use the code below for more ops
+    # patched = find_monkey_patch_keys(avoid_set)
+
+    for key, method in list(zip(patched, patched)) + list(alias.items()):
+        value = tf.__dict__[method]
+        setattr(tf.Tensor, key, value)
+        setattr(tf.Variable, key, value)
+
+
+monkey_patch()
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/utils.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/utils.py
new file mode 100644
index 0000000..9fa5707
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/nn/utils.py
@@ -0,0 +1,85 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import Callable, List, Union
+import inspect
+from functools import wraps
+import tensorflow as tf
+import numpy as np
+from lunzi import Tensor
+
+from .parameter import Parameter
+
+
+def make_method(feed: str = None, fetch: str = ''):
+    """
+        The following code:
+
+            @make_method('. w', fetch='d)
+            def func(a, c):
+                pass
+
+        will be converted to
+
+            def func(a, c, fetch='d'):
+                return self.eval(fetch, a=a, w=c)
+
+        Note that `func(1, c=2, b=1)` is also supported. This is
+        useful when writing PyTorch-like object method.
+
+    """
+
+    def decorator(func: Callable):
+        arg_names = inspect.signature(func).parameters.keys()
+        arg_map = {}
+        if feed is None:
+            arg_map = {op_name: op_name for op_name in arg_names if op_name != 'self'}
+        else:
+            feeds = ['-'] + feed.split(' ')  # ignore first `self`
+            for op_name, arg_name in zip(feeds, arg_names):
+                if op_name == '.':
+                    arg_map[op_name] = op_name
+                elif op_name != '-':  # deprecated
+                    arg_map[op_name] = arg_name
+
+        @wraps(func)
+        def wrapper(self, *args, **kwargs):
+            cur_fetch = kwargs.pop('fetch', fetch)
+            call_args = inspect.getcallargs(func, self, *args, **kwargs)
+            feed_dict = {op_name: call_args[arg_name] for op_name, arg_name in arg_map.items()}
+            return self.eval(cur_fetch, **feed_dict)
+
+        return wrapper
+
+    return decorator
+
+
+def n_parameters(params: List[Parameter]) -> int:
+    return sum([np.prod(p.shape) for p in params])
+
+
+def parameters_to_vector(parameters: List[Union[Parameter, Tensor]]) -> Tensor:
+    return tf.concat([param.reshape([-1]) for param in parameters], axis=0)
+
+
+def vector_to_parameters(vec: Tensor, parameters: List[Parameter]) -> List[Tensor]:
+    params: List[Tensor] = []
+    start = 0
+    for p in parameters:
+        end = start + np.prod(p.shape)
+        params.append(vec[start:end].reshape(p.shape))
+        start = end
+    return params
+
+
+def hessian_vec_prod(ys: Tensor, xs: List[Parameter], vs: Tensor) -> Tensor:
+    grad = parameters_to_vector(tf.gradients(ys, xs))
+    aux = (grad * vs).reduce_sum()
+    return parameters_to_vector(tf.gradients(aux, xs))
+
+
+# credit to https://github.com/renmengye/tensorflow-forward-ad/issues/2#issue-234418055
+def jacobian_vec_prod(ys: Tensor, xs: List[Parameter], vs: Tensor) -> Tensor:
+    u = tf.zeros_like(ys)  # dummy variable
+    grad = tf.gradients(ys, xs, grad_ys=u)
+    return tf.gradients(grad, u, grad_ys=vs)
+
+
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/stubs.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/stubs.py
new file mode 100644
index 0000000..644c477
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/stubs.py
@@ -0,0 +1,6 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+
+
+class Tensor(tf.Tensor):
+    pass
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/stubs.pyi b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/stubs.pyi
new file mode 100644
index 0000000..7bab814
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/lunzi/stubs.pyi
@@ -0,0 +1,138 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+
+class Tensor(tf.Tensor):
+    def abs(x, name=None) -> Tensor: ...
+    def acos(x, name=None) -> Tensor: ...
+    def acosh(x, name=None) -> Tensor: ...
+    def add(x, y, name=None) -> Tensor: ...
+    def angle(input, name=None) -> Tensor: ...
+    def argmax(input, axis=None, name=None, dimension=None, output_type=tf.int64) -> Tensor: ...
+    def argmin(input, axis=None, name=None, dimension=None, output_type=tf.int64) -> Tensor: ...
+    def asin(x, name=None) -> Tensor: ...
+    def asinh(x, name=None) -> Tensor: ...
+    def atan(x, name=None) -> Tensor: ...
+    def atan2(y, x, name=None) -> Tensor: ...
+    def atanh(x, name=None) -> Tensor: ...
+    def betainc(a, b, x, name=None) -> Tensor: ...
+    def cast(x, dtype, name=None) -> Tensor: ...
+    def ceil(x, name=None) -> Tensor: ...
+    def check_numerics(tensor, message, name=None) -> Tensor: ...
+    def clip_by_average_norm(t, clip_norm, name=None) -> Tensor: ...
+    def clip_by_norm(t, clip_norm, axes=None, name=None) -> Tensor: ...
+    def clip_by_value(t, clip_value_min, clip_value_max, name=None) -> Tensor: ...
+    def complex(real, imag, name=None) -> Tensor: ...
+    def conj(x, name=None) -> Tensor: ...
+    def cos(x, name=None) -> Tensor: ...
+    def cosh(x, name=None) -> Tensor: ...
+    def cross(a, b, name=None) -> Tensor: ...
+    def cumprod(x, axis=0, exclusive=False, reverse=False, name=None) -> Tensor: ...
+    def cumsum(x, axis=0, exclusive=False, reverse=False, name=None) -> Tensor: ...
+    def dequantize(input, min_range, max_range, mode='MIN_COMBINED', name=None) -> Tensor: ...
+    def diag(diagonal, name=None) -> Tensor: ...
+    def digamma(x, name=None) -> Tensor: ...
+    def div(x, y, name=None) -> Tensor: ...
+    def equal(x, y, name=None) -> Tensor: ...
+    def erf(x, name=None) -> Tensor: ...
+    def erfc(x, name=None) -> Tensor: ...
+    def exp(x, name=None) -> Tensor: ...
+    def expand_dims(input, axis=None, name=None, dim=None) -> Tensor: ...
+    def expm1(x, name=None) -> Tensor: ...
+    def fill(dims, value, name=None) -> Tensor: ...
+    def floor(x, name=None) -> Tensor: ...
+    def floor_div(x, y, name=None) -> Tensor: ...
+    def floordiv(x, y, name=None) -> Tensor: ...
+    def floormod(x, y, name=None) -> Tensor: ...
+    def gather(params, indices, validate_indices=None, name=None, axis=0) -> Tensor: ...
+    def gather_nd(params, indices, name=None) -> Tensor: ...
+    def greater(x, y, name=None) -> Tensor: ...
+    def greater_equal(x, y, name=None) -> Tensor: ...
+    def hessians(ys, xs, name='hessians', colocate_gradients_with_ops=False, gate_gradients=False, aggregation_method=None) -> Tensor: ...
+    def identity(input, name=None) -> Tensor: ...
+    def igamma(a, x, name=None) -> Tensor: ...
+    def igammac(a, x, name=None) -> Tensor: ...
+    def imag(input, name=None) -> Tensor: ...
+    def is_finite(x, name=None) -> Tensor: ...
+    def is_inf(x, name=None) -> Tensor: ...
+    def is_nan(x, name=None) -> Tensor: ...
+    def less(x, y, name=None) -> Tensor: ...
+    def less_equal(x, y, name=None) -> Tensor: ...
+    def lgamma(x, name=None) -> Tensor: ...
+    def log(x, name=None) -> Tensor: ...
+    def log1p(x, name=None) -> Tensor: ...
+    def logical_and(x, y, name=None) -> Tensor: ...
+    def logical_not(x, name=None) -> Tensor: ...
+    def logical_or(x, y, name=None) -> Tensor: ...
+    def matmul(a, b, transpose_a=False, transpose_b=False, adjoint_a=False, adjoint_b=False, a_is_sparse=False, b_is_sparse=False, name=None) -> Tensor: ...
+    def maximum(x, y, name=None) -> Tensor: ...
+    def meshgrid(*args, **kwargs) -> Tensor: ...
+    def minimum(x, y, name=None) -> Tensor: ...
+    def mod(x, y, name=None) -> Tensor: ...
+    def mul(x, y, name=None) -> Tensor: ...
+    def multiply(x, y, name=None) -> Tensor: ...
+    def negative(x, name=None) -> Tensor: ...
+    def norm(tensor, ord='euclidean', axis=None, keepdims=None, name=None) -> Tensor: ...
+    def not_equal(x, y, name=None) -> Tensor: ...
+    def one_hot(indices, depth, on_value=None, off_value=None, axis=None, dtype=None, name=None) -> Tensor: ...
+    def ones_like(tensor, dtype=None, name=None, optimize=True) -> Tensor: ...
+    def pad(tensor, paddings, mode='CONSTANT', name=None, constant_values=0) -> Tensor: ...
+    def polygamma(a, x, name=None) -> Tensor: ...
+    def pow(x, y, name=None) -> Tensor: ...
+    def quantize(input, min_range, max_range, T, mode='MIN_COMBINED', round_mode='HALF_AWAY_FROM_ZERO', name=None) -> Tensor: ...
+    def real(input, name=None) -> Tensor: ...
+    def realdiv(x, y, name=None) -> Tensor: ...
+    def reciprocal(x, name=None) -> Tensor: ...
+    def reduce_all(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_any(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_logsumexp(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_max(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_mean(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_min(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_prod(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_sum(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reshape(tensor, shape, name=None) -> Tensor: ...
+    def reverse(tensor, axis, name=None) -> Tensor: ...
+    def rint(x, name=None) -> Tensor: ...
+    def round(x, name=None) -> Tensor: ...
+    def rsqrt(x, name=None) -> Tensor: ...
+    def scatter_nd(indices, updates, shape, name=None) -> Tensor: ...
+    def sign(x, name=None) -> Tensor: ...
+    def sin(x, name=None) -> Tensor: ...
+    def sinh(x, name=None) -> Tensor: ...
+    def size(input, name=None, out_type=tf.int32) -> Tensor: ...
+    def slice(input_, begin, size, name=None) -> Tensor: ...
+    def sqrt(x, name=None) -> Tensor: ...
+    def square(x, name=None) -> Tensor: ...
+    def squeeze(input, axis=None, name=None, squeeze_dims=None) -> Tensor: ...
+    def stop_gradient(input, name=None) -> Tensor: ...
+    def sub(x, y, name=None) -> Tensor: ...
+    def subtract(x, y, name=None) -> Tensor: ...
+    def tan(x, name=None) -> Tensor: ...
+    def tensordot(a, b, axes, name=None) -> Tensor: ...
+    def tile(input, multiples, name=None) -> Tensor: ...
+    def to_bfloat16(x, name='ToBFloat16') -> Tensor: ...
+    def to_complex128(x, name='ToComplex128') -> Tensor: ...
+    def to_complex64(x, name='ToComplex64') -> Tensor: ...
+    def to_double(x, name='ToDouble') -> Tensor: ...
+    def to_float(x, name='ToFloat') -> Tensor: ...
+    def to_int32(x, name='ToInt32') -> Tensor: ...
+    def to_int64(x, name='ToInt64') -> Tensor: ...
+    def transpose(a, perm=None, name='transpose', conjugate=False) -> Tensor: ...
+    def truediv(x, y, name=None) -> Tensor: ...
+    def truncatediv(x, y, name=None) -> Tensor: ...
+    def truncatemod(x, y, name=None) -> Tensor: ...
+    def unique(x, out_idx=tf.int32, name=None) -> Tensor: ...
+    def where(condition, x=None, y=None, name=None) -> Tensor: ...
+    def zeros_like(tensor, dtype=None, name=None, optimize=True) -> Tensor: ...
+    def zeta(x, q, name=None) -> Tensor: ...
+
+    def __add__(self, other) -> Tensor: ...
+    def __sub__(self, other) -> Tensor: ...
+    def __mul__(self, other) -> Tensor: ...
+    def __rdiv__(self, other) -> Tensor: ...
+    def __itruediv__(self, other) -> Tensor: ...
+    def __rsub__(self, other) -> Tensor: ...
+    def __isub__(self, other) -> Tensor: ...
+    def __imul__(self, other) -> Tensor: ...
+    def __rmul__(self, other) -> Tensor: ...
+    def __radd__(self, other) -> Tensor: ...
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/main.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/main.py
new file mode 100644
index 0000000..38aafb2
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/main.py
@@ -0,0 +1,232 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import pickle
+from collections import deque
+import tensorflow as tf
+import numpy as np
+from tqdm import tqdm
+
+import lunzi.nn as nn
+from lunzi.Logger import logger
+from slbo.utils.average_meter import AverageMeter
+from slbo.utils.flags import FLAGS
+from slbo.utils.dataset import Dataset, gen_dtype
+from slbo.utils.OU_noise import OUNoise
+from slbo.utils.normalizer import Normalizers
+from slbo.utils.tf_utils import get_tf_config
+from slbo.utils.runner import Runner
+from slbo.policies.gaussian_mlp_policy import GaussianMLPPolicy
+from slbo.envs.virtual_env import VirtualEnv
+from slbo.dynamics_model import DynamicsModel
+from slbo.v_function.mlp_v_function import MLPVFunction
+from slbo.partial_envs import make_env
+from slbo.loss.multi_step_loss import MultiStepLoss
+from slbo.algos.TRPO import TRPO
+from slbo.random_net import RandomNet
+
+
+def evaluate(settings, tag):
+    return_means = []
+    for runner, policy, name in settings:
+        runner.reset()
+        _, ep_infos = runner.run(policy, FLAGS.rollout.n_test_samples)
+        if name == 'Real Env':
+            returns = np.array([ep_info['success'] for ep_info in ep_infos])
+        else:
+            returns = np.array([ep_info['return'] for ep_info in ep_infos])
+        logger.info('Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f', tag, name,
+                    len(returns), np.mean(returns), np.std(returns))
+
+        return_means.append(np.mean(returns))
+
+    return return_means
+
+
+
+def add_multi_step(src: Dataset, dst: Dataset):
+    n_envs = 1
+    dst.extend(src[:-n_envs])
+
+    ending = src[-n_envs:].copy()
+    ending.timeout = True
+    dst.extend(ending)
+
+
+def make_real_runner(n_envs):
+    from slbo.envs.batched_env import BatchedEnv
+    batched_env = BatchedEnv([make_env(FLAGS.env.id) for _ in range(n_envs)])
+    return Runner(batched_env, rescale_action=True, **FLAGS.runner.as_dict())
+
+
+def main():
+    FLAGS.set_seed()
+    FLAGS.freeze()
+
+    env = make_env(FLAGS.env.id)
+    dim_state = int(np.prod(env.observation_space.shape))
+    dim_action = int(np.prod(env.action_space.shape))
+
+    env.verify()
+
+    normalizers = Normalizers(dim_action=dim_action, dim_state=dim_state)
+
+    dtype = gen_dtype(env, 'state action next_state reward done timeout')
+    train_set = Dataset(dtype, FLAGS.rollout.max_buf_size)
+    dev_set = Dataset(dtype, FLAGS.rollout.max_buf_size)
+
+    policy = GaussianMLPPolicy(dim_state, dim_action, normalizer=normalizers.state, **FLAGS.policy.as_dict())
+    # batched noises
+    #noise = OUNoise(env.action_space, theta=FLAGS.OUNoise.theta, sigma=FLAGS.OUNoise.sigma, shape=(1, dim_action))
+    vfn = MLPVFunction(dim_state, [64, 64], normalizers.state)
+    model = DynamicsModel(dim_state, dim_action, normalizers, FLAGS.model.hidden_sizes)
+    random_net = RandomNet(dim_state, dim_action, normalizers, FLAGS.model.hidden_sizes)
+
+    virt_env = VirtualEnv(model, make_env(FLAGS.env.id), random_net, FLAGS.plan.n_envs,FLAGS.model.hidden_sizes[-1], 
+                            FLAGS.pc.bonus_scale,FLAGS.pc.lamb, opt_model=FLAGS.slbo.opt_model)
+    virt_runner = Runner(virt_env, **{**FLAGS.runner.as_dict(), 'max_steps': FLAGS.plan.max_steps})
+
+    criterion_map = {
+        'L1': nn.L1Loss(),
+        'L2': nn.L2Loss(),
+        'MSE': nn.MSELoss(),
+    }
+    criterion = criterion_map[FLAGS.model.loss]
+    loss_mod = MultiStepLoss(model, normalizers, dim_state, dim_action, criterion, FLAGS.model.multi_step)
+    loss_mod.build_backward(FLAGS.model.lr, FLAGS.model.weight_decay)
+    algo = TRPO(vfn=vfn, policy=policy, dim_state=dim_state, dim_action=dim_action, **FLAGS.TRPO.as_dict())
+
+    tf.get_default_session().run(tf.global_variables_initializer())
+
+    runners = {
+        'test': make_real_runner(4),
+        'collect': make_real_runner(1),
+        'dev': make_real_runner(1),
+        'train': make_real_runner(FLAGS.plan.n_envs) if FLAGS.algorithm == 'MF' else virt_runner,
+    }
+    settings = [(runners['test'], policy, 'Real Env'), (runners['train'], policy, 'Virt Env')]
+
+    saver = nn.ModuleDict({'policy': policy, 'model': model, 'vfn': vfn})
+    print(saver)
+
+    eval_real_returns = []
+    timesteps = []
+
+    if FLAGS.ckpt.model_load:
+        saver.load_state_dict(np.load(FLAGS.ckpt.model_load)[()])
+        logger.warning('Load model from %s', FLAGS.ckpt.model_load)
+
+    if FLAGS.ckpt.buf_load:
+        n_samples = 0
+        for i in range(FLAGS.ckpt.buf_load_index):
+            data = pickle.load(open(f'{FLAGS.ckpt.buf_load}/stage-{i}.inc-buf.pkl', 'rb'))
+            add_multi_step(data, train_set)
+            n_samples += len(data)
+        logger.warning('Loading %d samples from %s', n_samples, FLAGS.ckpt.buf_load)
+
+    max_ent_coef = 0
+    print("!!!!!!!!")
+    print(virt_env.bonus_scale)
+    for T in range(FLAGS.slbo.n_stages):
+        logger.info('------ Starting Stage %d --------', T)
+        eval_returns = evaluate(settings, 'episode')
+        eval_real_returns.append(eval_returns[0])
+        timesteps.append(T*FLAGS.rollout.n_train_samples)
+
+        if not FLAGS.use_prev:
+            train_set.clear()
+            dev_set.clear()
+
+        # collect data
+        recent_train_set, ep_infos = runners['collect'].run(policy, FLAGS.rollout.n_train_samples, render=False)
+        add_multi_step(recent_train_set, train_set)
+        add_multi_step(
+            runners['dev'].run(policy, FLAGS.rollout.n_dev_samples)[0],
+            dev_set,
+        )
+
+        returns = np.array([ep_info['return'] for ep_info in ep_infos])
+
+        if len(returns) > 0:
+            logger.info("episode: %s", np.mean(returns))
+
+        if T == 0:  # check
+            samples = train_set.sample_multi_step(100, 1, FLAGS.model.multi_step)
+            for i in range(FLAGS.model.multi_step - 1):
+                masks = 1 - (samples.done[i] | samples.timeout[i])[..., np.newaxis]
+                assert np.allclose(samples.state[i + 1] * masks, samples.next_state[i] * masks)
+
+        # recent_states = obsvs
+        # ref_actions = policy.eval('actions_mean actions_std', states=recent_states)
+        if FLAGS.rollout.normalizer == 'policy' or FLAGS.rollout.normalizer == 'uniform' and T == 0:
+            normalizers.state.update(recent_train_set.state)
+            normalizers.action.update(recent_train_set.action)
+            normalizers.diff.update(recent_train_set.next_state - recent_train_set.state)
+        #print(recent_train_set.state.shape)
+        virt_env.update_cov(recent_train_set.state,recent_train_set.action)
+
+        #if T == FLAGS.pc.bonus_stop_time:
+        #    virt_env.bonus_scale = 0.
+
+        for i in range(FLAGS.slbo.n_iters):
+            #if i % FLAGS.slbo.n_evaluate_iters == 0 and i != 0:
+                # cur_actions = policy.eval('actions_mean actions_std', states=recent_states)
+                # kl_old_new = gaussian_kl(*ref_actions, *cur_actions).sum(axis=1).mean()
+                # logger.info('KL(old || cur) = %.6f', kl_old_new)
+            #    evaluate(settings, 'iteration')
+
+            losses = deque(maxlen=FLAGS.slbo.n_model_iters)
+            grad_norm_meter = AverageMeter()
+            n_model_iters = FLAGS.slbo.n_model_iters
+            for _ in range(n_model_iters):
+                samples = train_set.sample_multi_step(FLAGS.model.train_batch_size, 1, FLAGS.model.multi_step)
+                _, train_loss, grad_norm = loss_mod.get_loss(
+                    samples.state, samples.next_state, samples.action, ~samples.done & ~samples.timeout,
+                    fetch='train loss grad_norm')
+                losses.append(train_loss.mean())
+                grad_norm_meter.update(grad_norm)
+                # ideally, we should define an Optimizer class, which takes parameters as inputs.
+                # The `update` method of `Optimizer` will invalidate all parameters during updates.
+                for param in model.parameters():
+                    param.invalidate()
+
+            if i % FLAGS.model.validation_freq == 0:
+                samples = train_set.sample_multi_step(
+                    FLAGS.model.train_batch_size, 1, FLAGS.model.multi_step)
+                loss = loss_mod.get_loss(
+                    samples.state, samples.next_state, samples.action, ~samples.done & ~samples.timeout)
+                loss = loss.mean()
+                if np.isnan(loss) or np.isnan(np.mean(losses)):
+                    logger.info('nan! %s %s', np.isnan(loss), np.isnan(np.mean(losses)))
+                logger.info('# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f',
+                            i, np.mean(losses), loss, n_model_iters, grad_norm_meter.get())
+
+            for n_updates in tqdm(range(FLAGS.slbo.n_policy_iters)):
+                if FLAGS.algorithm != 'MF' and FLAGS.slbo.start == 'buffer':
+                    runners['train'].set_state(train_set.sample(FLAGS.plan.n_envs).state)
+                else:
+                    runners['train'].reset()
+
+                data, ep_infos = runners['train'].run(policy, FLAGS.plan.n_trpo_samples)
+                advantages, values = runners['train'].compute_advantage(vfn, data)
+                dist_mean, dist_std, vf_loss = algo.train(max_ent_coef, data, advantages, values)
+                returns = [info['return'] for info in ep_infos]
+                #logger.info('[TRPO] # %d: n_episodes = %d, returns: {mean = %.0f, std = %.0f}, '
+                #            'dist std = %.10f, dist mean = %.10f, vf_loss = %.3f',
+                #            n_updates, len(returns), np.mean(returns), np.std(returns) / np.sqrt(len(returns)),
+                #            dist_std, dist_mean, vf_loss)
+
+        if T % FLAGS.ckpt.n_save_stages == 0:
+            np.save(f'{FLAGS.log_dir}/stage-{T}', saver.state_dict())
+            np.save(f'{FLAGS.log_dir}/final', saver.state_dict())
+        if FLAGS.ckpt.n_save_stages == 1:
+            pickle.dump(recent_train_set, open(f'{FLAGS.log_dir}/stage-{T}.inc-buf.pkl', 'wb'))
+
+    eval_returns = evaluate(settings, 'episode')
+    eval_real_returns.append(eval_returns[0])
+    timesteps.append(T*FLAGS.rollout.n_train_samples)
+    np.save(f'{FLAGS.log_dir}/eval_real_returns', eval_real_returns)
+    np.save(f'{FLAGS.log_dir}/timesteps', timesteps)
+
+
+if __name__ == '__main__':
+    with tf.Session(config=get_tf_config()):
+        main()
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/requirements.txt b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/requirements.txt
new file mode 100644
index 0000000..8991cc0
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/requirements.txt
@@ -0,0 +1,6 @@
+tensorflow
+numpy
+pyyaml
+termcolor
+gym
+json_tricks
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/rllab_requirements.txt b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/rllab_requirements.txt
new file mode 100644
index 0000000..9fb2d37
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/rllab_requirements.txt
@@ -0,0 +1,7 @@
+theano
+cached_property
+pyopengl
+joblib
+mako
+mujoco_py
+
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/run2.sh b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/run2.sh
new file mode 100644
index 0000000..7440dc5
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/run2.sh
@@ -0,0 +1,10 @@
+#!/usr/bin/env bash
+
+for env_name in $1; do
+    echo "=> Running environment ${env_name}"
+    #for random_seed in 1234 2314 2345 1235; do
+    for random_seed in 1234; do
+        python main.py -c configs/algos/slbo_bm_200k.yml configs/env_tingwu/${env_name}.yml \
+	    -s pc.bonus_scale=0.2 log_dir=./experiments02/${env_name}_${random_seed} seed=${random_seed}
+    done
+done
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/run_experiments.sh b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/run_experiments.sh
new file mode 100644
index 0000000..e3f2ad7
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/run_experiments.sh
@@ -0,0 +1,10 @@
+#!/usr/bin/env bash
+
+for env_name in $1; do
+    echo "=> Running environment ${env_name}"
+    #for random_seed in 1234 2314 2345 1235; do
+    for random_seed in 1234; do
+        python main.py -c configs/algos/slbo_bm_200k.yml configs/env_tingwu/${env_name}.yml \
+	    -s pc.bonus_scale=0.5 log_dir=./experiments05/${env_name}_${random_seed} seed=${random_seed}
+    done
+done
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/__init__.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/__init__.py
new file mode 100644
index 0000000..5c7f19c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/__init__.py
@@ -0,0 +1 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/algos/TRPO.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/algos/TRPO.py
new file mode 100644
index 0000000..e8469e3
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/algos/TRPO.py
@@ -0,0 +1,183 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List, Callable
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi import Tensor
+from lunzi.Logger import logger
+from slbo.utils.dataset import Dataset
+from slbo.policies import BaseNNPolicy
+from slbo.v_function import BaseVFunction
+
+
+def average_l2_norm(x):
+    return np.sqrt((x**2).mean())
+
+
+# for damping, modify func_Ax
+def conj_grad(mat_mul_vec: Callable[[np.ndarray], np.ndarray], b, n_iters=10, residual_tol=1e-10, verbose=False):
+    p = b.copy()
+    r = b.copy()
+    x = np.zeros_like(b)
+    r_dot_r = r.dot(r)
+
+    for i in range(n_iters):
+        if verbose:
+            logger.info('[CG] iters = %d, |Res| = %.6f, |x| = %.6f', i, r_dot_r, np.linalg.norm(x))
+        z = mat_mul_vec(p)
+        v = r_dot_r / p.dot(z)
+        x += v * p
+        r -= v * z
+        new_r_dot_r = r.dot(r)
+        if new_r_dot_r < residual_tol:
+            break
+        mu = new_r_dot_r / r_dot_r
+        p = r + mu * p
+        r_dot_r = new_r_dot_r
+    return x
+
+
+class TRPO(nn.Module):
+    def __init__(self, dim_state: int, dim_action: int, policy: BaseNNPolicy, vfn: BaseVFunction, max_kl: float,
+                 n_cg_iters: int, ent_coef=0.0, cg_damping=0.01, vf_lr=3e-4, n_vf_iters=3):
+        super().__init__()
+        self.dim_state = dim_state
+        self.policy = policy
+        self.ent_coef = ent_coef
+        self.vf = vfn
+        self.n_cg_iters = n_cg_iters
+        self.max_kl = max_kl
+        self.cg_damping = cg_damping
+        self.n_vf_iters = n_vf_iters
+        self.vf_lr = vf_lr
+
+        # doing backtrace, so don't need to separate.
+        self.flatten = nn.FlatParam(self.policy.parameters())
+        self.old_policy: nn.Module = policy.clone()
+
+        with self.scope:
+            self.op_returns = tf.placeholder(dtype=tf.float32, shape=[None], name='returns')
+            self.op_advantages = tf.placeholder(dtype=tf.float32, shape=[None], name='advantages')
+            self.op_states = tf.placeholder(dtype=tf.float32, shape=[None, dim_state], name='states')
+            self.op_actions = tf.placeholder(dtype=tf.float32, shape=[None, dim_action], name='actions')
+            self.op_feed_params = tf.placeholder(dtype=tf.float32, shape=[None], name='feed_params')
+
+            self.op_tangents = tf.placeholder(
+                dtype=tf.float32, shape=[nn.utils.n_parameters(self.policy.parameters())])
+            self.op_ent_coef = tf.placeholder(dtype=tf.float32, shape=[], name='ent_coef')
+
+        self.op_mean_kl, self.op_loss, self.op_dist_std, self.op_dist_mean, self.op_policy_loss = \
+            self(self.op_states, self.op_actions, self.op_advantages, self.op_ent_coef)
+
+        self.op_sync_old, self.op_hessian_vec_prod, self.op_flat_grad = \
+            self.compute_natural_grad(self.op_loss, self.op_mean_kl, self.op_tangents)
+
+        self.op_vf_loss, self.op_train_vf = self.compute_vf(self.op_states, self.op_returns)
+
+    def forward(self, states, actions, advantages, ent_coef):
+        old_distribution: tf.distributions.Normal = self.old_policy(states)
+        distribution: tf.distributions.Normal = self.policy(states)
+        mean_kl = old_distribution.kl_divergence(distribution).reduce_sum(axis=1).reduce_mean()
+        entropy = distribution.entropy().reduce_sum(axis=1).reduce_mean()
+        entropy_bonus = ent_coef * entropy
+
+        ratios: Tensor = (distribution.log_prob(actions) - old_distribution.log_prob(actions)) \
+            .reduce_sum(axis=1).exp()
+        # didn't output op_policy_loss since in principle it should be 0.
+        policy_loss = ratios.mul(advantages).reduce_mean()
+
+        # We're doing Gradient Ascent so this is, in fact, gain.
+        loss = policy_loss + entropy_bonus
+
+        return mean_kl, loss, distribution.stddev().log().reduce_mean().exp(), \
+            distribution.mean().norm(axis=1).reduce_mean() / np.sqrt(10), policy_loss
+
+    def compute_natural_grad(self, loss, mean_kl, tangents):
+        params = self.policy.parameters()
+        old_params = self.old_policy.parameters()
+        hessian_vec_prod = nn.utils.hessian_vec_prod(mean_kl, params, tangents)
+        flat_grad = nn.utils.parameters_to_vector(tf.gradients(loss, params))
+        sync_old = tf.group(*[tf.assign(old_v, new_v) for old_v, new_v in zip(old_params, params)])
+
+        return sync_old, hessian_vec_prod, flat_grad
+
+    def compute_vf(self, states, returns):
+        vf_loss = nn.MSELoss()(self.vf(states), returns).reduce_mean()
+        optimizer = tf.train.AdamOptimizer(self.vf_lr)
+        train_vf = optimizer.minimize(vf_loss)
+
+        return vf_loss, train_vf
+
+    @nn.make_method()
+    def get_vf_loss(self, states, returns) -> List[np.ndarray]: pass
+
+    @nn.make_method(fetch='sync_old')
+    def sync_old(self) -> List[np.ndarray]: pass
+
+    @nn.make_method(fetch='hessian_vec_prod')
+    def get_hessian_vec_prod(self, states, tangents, actions) -> List[np.ndarray]: pass
+
+    @nn.make_method(fetch='loss')
+    def get_loss(self, states, actions, advantages, ent_coef) -> List[np.ndarray]: pass
+
+    def train(self, ent_coef, samples, advantages, values):
+        returns = advantages + values
+        advantages = (advantages - advantages.mean()) / np.maximum(advantages.std(), 1e-8)
+        assert np.isfinite(advantages).all()
+        self.sync_old()
+        old_loss, grad, dist_std, mean_kl, dist_mean = self.get_loss(
+            samples.state, samples.action, advantages, ent_coef, fetch='loss flat_grad dist_std mean_kl dist_mean')
+
+        if np.allclose(grad, 0):
+            logger.info('Zero gradient, not updating...')
+            return
+
+        def fisher_vec_prod(x):
+            return self.get_hessian_vec_prod(samples.state, x, samples.action) + self.cg_damping * x
+
+        assert np.isfinite(grad).all()
+        nat_grad = conj_grad(fisher_vec_prod, grad, n_iters=self.n_cg_iters, verbose=False)
+
+        assert np.isfinite(nat_grad).all()
+
+        old_params = self.flatten.get_flat()
+        step_size = np.sqrt(2 * self.max_kl / nat_grad.dot(fisher_vec_prod(nat_grad)))
+
+        for _ in range(10):
+            new_params = old_params + nat_grad * step_size
+            self.flatten.set_flat(new_params)
+            loss, mean_kl = self.get_loss(samples.state, samples.action, advantages, ent_coef, fetch='loss mean_kl')
+            improve = loss - old_loss
+            if not np.isfinite([loss, mean_kl]).all():
+                logger.info('Got non-finite loss.')
+            elif mean_kl > self.max_kl * 1.5:
+                logger.info('Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f',
+                            mean_kl, self.max_kl)
+            elif improve < 0:
+                logger.info("Surrogate didn't improve, shrinking step... %.6f => %.6f", old_loss, loss)
+            else:
+                break
+            step_size *= 0.5
+        else:
+            logger.info("Couldn't find a good step.")
+            self.flatten.set_flat(old_params)
+        for param in self.policy.parameters():
+            param.invalidate()
+
+        # optimize value function
+        vf_dataset = Dataset.fromarrays([samples.state, returns],
+                                        dtype=[('state', ('f8', self.dim_state)), ('return_', 'f8')])
+        vf_loss = self.train_vf(vf_dataset)
+
+        return dist_mean, dist_std, vf_loss
+
+    def train_vf(self, dataset: Dataset):
+        for _ in range(self.n_vf_iters):
+            for subset in dataset.iterator(64):
+                self.get_vf_loss(subset.state, subset.return_, fetch='train_vf vf_loss')
+        for param in self.parameters():
+            param.invalidate()
+        vf_loss = self.get_vf_loss(dataset.state, dataset.return_, fetch='vf_loss')
+        return vf_loss
+
+
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/algos/__init__.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/algos/__init__.py
new file mode 100644
index 0000000..5c7f19c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/algos/__init__.py
@@ -0,0 +1 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/dynamics_model.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/dynamics_model.py
new file mode 100644
index 0000000..19662ec
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/dynamics_model.py
@@ -0,0 +1,43 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List
+import tensorflow as tf
+from lunzi import Tensor
+import lunzi.nn as nn
+from slbo.utils.normalizer import Normalizers
+from slbo.utils.multi_layer_perceptron import MultiLayerPerceptron
+
+
+class DynamicsModel(MultiLayerPerceptron):
+    op_loss: Tensor
+    op_train: Tensor
+    op_grad_norm: Tensor
+
+    def __init__(self, dim_state: int, dim_action: int, normalizers: Normalizers, hidden_sizes: List[int]):
+        initializer = tf.truncated_normal_initializer(mean=0.0, stddev=1e-5)
+
+        self.dim_state = dim_state
+        self.dim_action = dim_action
+        self.hidden_sizes = hidden_sizes
+        self.op_states = tf.placeholder(tf.float32, shape=[None, self.dim_state], name='states')
+        self.op_actions = tf.placeholder(tf.float32, shape=[None, self.dim_action], name='actions')
+        super().__init__([dim_state + dim_action, *hidden_sizes, dim_state],
+                         activation=nn.ReLU,
+                         weight_initializer=initializer, build=False)
+
+        self.normalizers = normalizers
+        self.build()
+
+    def build(self):
+        self.op_next_states = self.forward(self.op_states, self.op_actions)
+
+    def forward(self, states, actions):
+        assert actions.shape[-1] == self.dim_action
+        inputs = tf.concat([self.normalizers.state(states), actions.clip_by_value(-1., 1.)], axis=1)
+
+        normalized_diffs = super().forward(inputs)
+        next_states = states + self.normalizers.diff(normalized_diffs, inverse=True)
+        next_states = self.normalizers.state(self.normalizers.state(next_states).clip_by_value(-100, 100), inverse=True)
+        return next_states
+
+    def clone(self):
+        return DynamicsModel(self.dim_state, self.dim_action, self.normalizers, self.hidden_sizes)
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/__init__.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/__init__.py
new file mode 100644
index 0000000..7efdf01
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/__init__.py
@@ -0,0 +1,55 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+import abc
+import gym
+from slbo.utils.dataset import Dataset, gen_dtype
+from lunzi.Logger import logger
+
+
+class BaseBatchedEnv(gym.Env, abc.ABC):
+    # thought about using `@property @abc.abstractmethod` here but we don't need explicit `@property` function here.
+    n_envs: int
+
+    @abc.abstractmethod
+    def step(self, actions):
+        pass
+
+    def reset(self):
+        return self.partial_reset(range(self.n_envs))
+
+    @abc.abstractmethod
+    def partial_reset(self, indices):
+        pass
+
+    def set_state(self, state):
+        logger.warning('`set_state` is not implemented')
+
+
+class BaseModelBasedEnv(gym.Env, abc.ABC):
+    @abc.abstractmethod
+    def mb_step(self, states: np.ndarray, actions: np.ndarray, next_states: np.ndarray):
+        raise NotImplementedError
+
+    def verify(self, n=2000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
+
+    def seed(self, seed: int = None):
+        pass
+
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/batched_env.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/batched_env.py
new file mode 100644
index 0000000..cbe4bac
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/batched_env.py
@@ -0,0 +1,37 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from gym import Wrapper
+from . import BaseBatchedEnv
+
+
+class BatchedEnv(BaseBatchedEnv, Wrapper):
+    def __init__(self, envs):
+        super().__init__(envs[0])
+        self.envs = envs
+        self.n_envs = len(envs)
+
+    def step(self, actions):
+
+        buf, infos = [], []
+        for env, action in zip(self.envs, actions):
+            next_state, reward, done, info = env.step(action)
+            buf.append((next_state, reward, done))
+            infos.append(info)
+
+        return [*(np.array(x) for x in zip(*buf)), infos]
+
+    def reset(self):
+        return self.partial_reset(range(self.n_envs))
+
+    def partial_reset(self, indices):
+        states = []
+        for index in indices:
+            states.append(self.envs[index].reset())
+        return np.array(states)
+
+    def __repr__(self):
+        return f'Batch<{self.n_envs}x {self.env}>'
+
+    def set_state(self, state):
+        pass
+
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/__init__.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/ant.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/ant.xml
new file mode 100644
index 0000000..18ad38b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/ant.xml
@@ -0,0 +1,80 @@
+<mujoco model="ant">
+  <compiler angle="degree" coordinate="local" inertiafromgeom="true"/>
+  <option integrator="RK4" timestep="0.01"/>
+  <custom>
+    <numeric data="0.0 0.0 0.55 1.0 0.0 0.0 0.0 0.0 1.0 0.0 -1.0 0.0 -1.0 0.0 1.0" name="init_qpos"/>
+  </custom>
+  <default>
+    <joint armature="1" damping="1" limited="true"/>
+    <geom conaffinity="0" condim="3" density="5.0" friction="1 0.5 0.5" margin="0.01" rgba="0.8 0.6 0.4 1"/>
+  </default>
+  <asset>
+    <texture builtin="gradient" height="100" rgb1="1 1 1" rgb2="0 0 0" type="skybox" width="100"/>
+    <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+    <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+    <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="60 60" texture="texplane"/>
+    <material name="geom" texture="texgeom" texuniform="true"/>
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="40 40 40" type="plane"/>
+    <body name="torso" pos="0 0 0.75">
+      <geom name="torso_geom" pos="0 0 0" size="0.25" type="sphere"/>
+      <joint armature="0" damping="0" limited="false" margin="0.01" name="root" pos="0 0 0" type="free"/>
+      <body name="front_left_leg" pos="0 0 0">
+        <geom fromto="0.0 0.0 0.0 0.2 0.2 0.0" name="aux_1_geom" size="0.08" type="capsule"/>
+        <body name="aux_1" pos="0.2 0.2 0">
+          <joint axis="0 0 1" name="hip_1" pos="0.0 0.0 0.0" range="-30 30" type="hinge"/>
+          <geom fromto="0.0 0.0 0.0 0.2 0.2 0.0" name="left_leg_geom" size="0.08" type="capsule"/>
+          <body pos="0.2 0.2 0">
+            <joint axis="-1 1 0" name="ankle_1" pos="0.0 0.0 0.0" range="30 70" type="hinge"/>
+            <geom fromto="0.0 0.0 0.0 0.4 0.4 0.0" name="left_ankle_geom" size="0.08" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <body name="front_right_leg" pos="0 0 0">
+        <geom fromto="0.0 0.0 0.0 -0.2 0.2 0.0" name="aux_2_geom" size="0.08" type="capsule"/>
+        <body name="aux_2" pos="-0.2 0.2 0">
+          <joint axis="0 0 1" name="hip_2" pos="0.0 0.0 0.0" range="-30 30" type="hinge"/>
+          <geom fromto="0.0 0.0 0.0 -0.2 0.2 0.0" name="right_leg_geom" size="0.08" type="capsule"/>
+          <body pos="-0.2 0.2 0">
+            <joint axis="1 1 0" name="ankle_2" pos="0.0 0.0 0.0" range="-70 -30" type="hinge"/>
+            <geom fromto="0.0 0.0 0.0 -0.4 0.4 0.0" name="right_ankle_geom" size="0.08" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <body name="back_leg" pos="0 0 0">
+        <geom fromto="0.0 0.0 0.0 -0.2 -0.2 0.0" name="aux_3_geom" size="0.08" type="capsule"/>
+        <body name="aux_3" pos="-0.2 -0.2 0">
+          <joint axis="0 0 1" name="hip_3" pos="0.0 0.0 0.0" range="-30 30" type="hinge"/>
+          <geom fromto="0.0 0.0 0.0 -0.2 -0.2 0.0" name="back_leg_geom" size="0.08" type="capsule"/>
+          <body pos="-0.2 -0.2 0">
+            <joint axis="-1 1 0" name="ankle_3" pos="0.0 0.0 0.0" range="-70 -30" type="hinge"/>
+            <geom fromto="0.0 0.0 0.0 -0.4 -0.4 0.0" name="third_ankle_geom" size="0.08" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <body name="right_back_leg" pos="0 0 0">
+        <geom fromto="0.0 0.0 0.0 0.2 -0.2 0.0" name="aux_4_geom" size="0.08" type="capsule"/>
+        <body name="aux_4" pos="0.2 -0.2 0">
+          <joint axis="0 0 1" name="hip_4" pos="0.0 0.0 0.0" range="-30 30" type="hinge"/>
+          <geom fromto="0.0 0.0 0.0 0.2 -0.2 0.0" name="rightback_leg_geom" size="0.08" type="capsule"/>
+          <body pos="0.2 -0.2 0">
+            <joint axis="1 1 0" name="ankle_4" pos="0.0 0.0 0.0" range="30 70" type="hinge"/>
+            <geom fromto="0.0 0.0 0.0 0.4 -0.4 0.0" name="fourth_ankle_geom" size="0.08" type="capsule"/>
+          </body>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="hip_4" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="ankle_4" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="hip_1" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="ankle_1" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="hip_2" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="ankle_2" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="hip_3" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="ankle_3" gear="150"/>
+  </actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/half_cheetah.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/half_cheetah.xml
new file mode 100644
index 0000000..b07aada
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/half_cheetah.xml
@@ -0,0 +1,95 @@
+<!-- Cheetah Model
+
+    The state space is populated with joints in the order that they are
+    defined in this file. The actuators also operate on joints.
+
+    State-Space (name/joint/parameter):
+        - rootx     slider      position (m)
+        - rootz     slider      position (m)
+        - rooty     hinge       angle (rad)
+        - bthigh    hinge       angle (rad)
+        - bshin     hinge       angle (rad)
+        - bfoot     hinge       angle (rad)
+        - fthigh    hinge       angle (rad)
+        - fshin     hinge       angle (rad)
+        - ffoot     hinge       angle (rad)
+        - rootx     slider      velocity (m/s)
+        - rootz     slider      velocity (m/s)
+        - rooty     hinge       angular velocity (rad/s)
+        - bthigh    hinge       angular velocity (rad/s)
+        - bshin     hinge       angular velocity (rad/s)
+        - bfoot     hinge       angular velocity (rad/s)
+        - fthigh    hinge       angular velocity (rad/s)
+        - fshin     hinge       angular velocity (rad/s)
+        - ffoot     hinge       angular velocity (rad/s)
+
+    Actuators (name/actuator/parameter):
+        - bthigh    hinge       torque (N m)
+        - bshin     hinge       torque (N m)
+        - bfoot     hinge       torque (N m)
+        - fthigh    hinge       torque (N m)
+        - fshin     hinge       torque (N m)
+        - ffoot     hinge       torque (N m)
+
+-->
+<mujoco model="cheetah">
+  <compiler angle="radian" coordinate="local" inertiafromgeom="true" settotalmass="14"/>
+  <default>
+    <joint armature=".1" damping=".01" limited="true" solimplimit="0 .8 .03" solreflimit=".02 1" stiffness="8"/>
+    <geom conaffinity="0" condim="3" contype="1" friction=".4 .1 .1" rgba="0.8 0.6 .4 1" solimp="0.0 0.8 0.01" solref="0.02 1"/>
+    <motor ctrllimited="true" ctrlrange="-1 1"/>
+  </default>
+  <size nstack="300000" nuser_geom="1"/>
+  <option gravity="0 0 -9.81" timestep="0.01"/>
+  <asset>
+    <texture builtin="gradient" height="100" rgb1="1 1 1" rgb2="0 0 0" type="skybox" width="100"/>
+    <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+    <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+    <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="60 60" texture="texplane"/>
+    <material name="geom" texture="texgeom" texuniform="true"/>
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="40 40 40" type="plane"/>
+    <body name="torso" pos="0 0 .7">
+      <joint armature="0" axis="1 0 0" damping="0" limited="false" name="rootx" pos="0 0 0" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 0 1" damping="0" limited="false" name="rootz" pos="0 0 0" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 1 0" damping="0" limited="false" name="rooty" pos="0 0 0" stiffness="0" type="hinge"/>
+      <geom fromto="-.5 0 0 .5 0 0" name="torso" size="0.046" type="capsule"/>
+      <geom axisangle="0 1 0 .87" name="head" pos=".6 0 .1" size="0.046 .15" type="capsule"/>
+      <!-- <site name='tip'  pos='.15 0 .11'/>-->
+      <body name="bthigh" pos="-.5 0 0">
+        <joint axis="0 1 0" damping="6" name="bthigh" pos="0 0 0" range="-.52 1.05" stiffness="240" type="hinge"/>
+        <geom axisangle="0 1 0 -3.8" name="bthigh" pos=".1 0 -.13" size="0.046 .145" type="capsule"/>
+        <body name="bshin" pos=".16 0 -.25">
+          <joint axis="0 1 0" damping="4.5" name="bshin" pos="0 0 0" range="-.785 .785" stiffness="180" type="hinge"/>
+          <geom axisangle="0 1 0 -2.03" name="bshin" pos="-.14 0 -.07" rgba="0.9 0.6 0.6 1" size="0.046 .15" type="capsule"/>
+          <body name="bfoot" pos="-.28 0 -.14">
+            <joint axis="0 1 0" damping="3" name="bfoot" pos="0 0 0" range="-.4 .785" stiffness="120" type="hinge"/>
+            <geom axisangle="0 1 0 -.27" name="bfoot" pos=".03 0 -.097" rgba="0.9 0.6 0.6 1" size="0.046 .094" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <body name="fthigh" pos=".5 0 0">
+        <joint axis="0 1 0" damping="4.5" name="fthigh" pos="0 0 0" range="-1 .7" stiffness="180" type="hinge"/>
+        <geom axisangle="0 1 0 .52" name="fthigh" pos="-.07 0 -.12" size="0.046 .133" type="capsule"/>
+        <body name="fshin" pos="-.14 0 -.24">
+          <joint axis="0 1 0" damping="3" name="fshin" pos="0 0 0" range="-1.2 .87" stiffness="120" type="hinge"/>
+          <geom axisangle="0 1 0 -.6" name="fshin" pos=".065 0 -.09" rgba="0.9 0.6 0.6 1" size="0.046 .106" type="capsule"/>
+          <body name="ffoot" pos=".13 0 -.18">
+            <joint axis="0 1 0" damping="1.5" name="ffoot" pos="0 0 0" range="-.5 .5" stiffness="60" type="hinge"/>
+            <geom axisangle="0 1 0 -.6" name="ffoot" pos=".045 0 -.07" rgba="0.9 0.6 0.6 1" size="0.046 .07" type="capsule"/>
+          </body>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor gear="120" joint="bthigh" name="bthigh"/>
+    <motor gear="90" joint="bshin" name="bshin"/>
+    <motor gear="60" joint="bfoot" name="bfoot"/>
+    <motor gear="120" joint="fthigh" name="fthigh"/>
+    <motor gear="60" joint="fshin" name="fshin"/>
+    <motor gear="30" joint="ffoot" name="ffoot"/>
+  </actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/hopper.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/hopper.xml
new file mode 100644
index 0000000..b0ebc0e
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/hopper.xml
@@ -0,0 +1,44 @@
+<mujoco model="hopper">
+  <compiler angle="degree" coordinate="global" inertiafromgeom="true"/>
+  <default>
+    <joint armature="1" damping="1" limited="true"/>
+    <geom conaffinity="1" condim="1" contype="1" margin="0.001" material="geom" rgba="0.8 0.6 .4 1" solimp=".8 .8 .01" solref=".02 1"/>
+    <motor ctrllimited="true" ctrlrange="-.4 .4"/>
+  </default>
+  <option integrator="RK4" timestep="0.002"/>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="20 20 .125" type="plane" material="MatPlane"/>
+    <body name="torso" pos="0 0 1.25">
+      <joint armature="0" axis="1 0 0" damping="0" limited="false" name="rootx" pos="0 0 0" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 0 1" damping="0" limited="false" name="rootz" pos="0 0 0" ref="1.25" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 1 0" damping="0" limited="false" name="rooty" pos="0 0 1.25" stiffness="0" type="hinge"/>
+      <geom friction="0.9" fromto="0 0 1.45 0 0 1.05" name="torso_geom" size="0.05" type="capsule"/>
+      <body name="thigh" pos="0 0 1.05">
+        <joint axis="0 -1 0" name="thigh_joint" pos="0 0 1.05" range="-150 0" type="hinge"/>
+        <geom friction="0.9" fromto="0 0 1.05 0 0 0.6" name="thigh_geom" size="0.05" type="capsule"/>
+        <body name="leg" pos="0 0 0.35">
+          <joint axis="0 -1 0" name="leg_joint" pos="0 0 0.6" range="-150 0" type="hinge"/>
+          <geom friction="0.9" fromto="0 0 0.6 0 0 0.1" name="leg_geom" size="0.04" type="capsule"/>
+          <body name="foot" pos="0.13/2 0 0.1">
+            <joint axis="0 -1 0" name="foot_joint" pos="0 0 0.1" range="-45 45" type="hinge"/>
+            <geom friction="2.0" fromto="-0.13 0 0.1 0.26 0 0.1" name="foot_geom" size="0.06" type="capsule"/>
+          </body>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="thigh_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="leg_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="foot_joint"/>
+  </actuator>
+    <asset>
+        <texture type="skybox" builtin="gradient" rgb1=".4 .5 .6" rgb2="0 0 0"
+            width="100" height="100"/>        
+        <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+        <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+        <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="60 60" texture="texplane"/>
+        <material name="geom" texture="texgeom" texuniform="true"/>
+    </asset>
+</mujoco>
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/point.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/point.xml
new file mode 100644
index 0000000..e35ef3d
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/point.xml
@@ -0,0 +1,31 @@
+<mujoco>
+  <compiler angle="degree" coordinate="local" inertiafromgeom="true"/>
+  <option integrator="RK4" timestep="0.02"/>
+  <default>
+    <joint armature="0" damping="0" limited="false"/>
+    <geom conaffinity="0" condim="3" density="100" friction="1 0.5 0.5" margin="0" rgba="0.8 0.6 0.4 1"/>
+  </default>
+  <asset>
+    <texture builtin="gradient" height="100" rgb1="1 1 1" rgb2="0 0 0" type="skybox" width="100"/>
+    <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+    <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+    <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="30 30" texture="texplane"/>
+    <material name="geom" texture="texgeom" texuniform="true"/>
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="40 40 40" type="plane"/>
+    <body name="torso" pos="0 0 0">
+      <geom name="pointbody" pos="0 0 0.5" size="0.5" type="sphere"/>
+      <geom name="pointarrow" pos="0.6 0 0.5" size="0.5 0.1 0.1" type="box"/>
+      <joint axis="1 0 0" name="ballx" pos="0 0 0" type="slide"/>
+      <joint axis="0 1 0" name="bally" pos="0 0 0" type="slide"/>
+      <joint axis="0 0 1" limited="false" name="rot" pos="0 0 0" type="hinge"/>
+    </body>
+  </worldbody>
+  <actuator>
+    <!-- Those are just dummy actuators for providing ranges -->
+    <motor ctrllimited="true" ctrlrange="-1 1" joint="ballx"/>
+    <motor ctrllimited="true" ctrlrange="-0.25 0.25" joint="rot"/>
+  </actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/pusher.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/pusher.xml
new file mode 100644
index 0000000..31a5ef7
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/pusher.xml
@@ -0,0 +1,91 @@
+<mujoco model="arm3d">
+  <compiler inertiafromgeom="true" angle="radian" coordinate="local"/>
+  <option timestep="0.01" gravity="0 0 0" iterations="20" integrator="Euler" />
+  
+  <default>
+    <joint armature='0.04' damping="1" limited="true"/>
+    <geom friction=".8 .1 .1" density="300" margin="0.002" condim="1" contype="0" conaffinity="0"/>
+  </default>
+  
+  <worldbody>
+    <light diffuse=".5 .5 .5" pos="0 0 3" dir="0 0 -1"/>
+    <geom name="table" type="plane" pos="0 0.5 -0.325" size="1 1 0.1" contype="1" conaffinity="1"/>
+
+    <body name="r_shoulder_pan_link" pos="0 -0.6 0">
+      <geom name="e1" type="sphere" rgba="0.6 0.6 0.6 1" pos="-0.06 0.05 0.2" size="0.05" />
+      <geom name="e2" type="sphere" rgba="0.6 0.6 0.6 1" pos=" 0.06 0.05 0.2" size="0.05" />
+      <geom name="e1p" type="sphere" rgba="0.1 0.1 0.1 1" pos="-0.06 0.09 0.2" size="0.03" />
+      <geom name="e2p" type="sphere" rgba="0.1 0.1 0.1 1" pos=" 0.06 0.09 0.2" size="0.03" />
+      <geom name="sp" type="capsule" fromto="0 0 -0.4 0 0 0.2" size="0.1" />
+      <joint name="r_shoulder_pan_joint" type="hinge" pos="0 0 0" axis="0 0 1" range="-2.2854 1.714602" damping="1.0" />
+
+      <body name="r_shoulder_lift_link" pos="0.1 0 0">
+        <geom name="sl" type="capsule" fromto="0 -0.1 0 0 0.1 0" size="0.1" />
+        <joint name="r_shoulder_lift_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.5236 1.3963" damping="1.0" />
+
+        <body name="r_upper_arm_roll_link" pos="0 0 0">
+          <geom name="uar" type="capsule" fromto="-0.1 0 0 0.1 0 0" size="0.02" />
+          <joint name="r_upper_arm_roll_joint" type="hinge" pos="0 0 0" axis="1 0 0" range="-1.5 1.7" damping="0.1" />
+
+          <body name="r_upper_arm_link" pos="0 0 0">
+            <geom name="ua" type="capsule" fromto="0 0 0 0.4 0 0" size="0.06" />
+
+            <body name="r_elbow_flex_link" pos="0.4 0 0">
+              <geom name="ef" type="capsule" fromto="0 -0.02 0 0.0 0.02 0" size="0.06" />
+              <joint name="r_elbow_flex_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-2.3213 0" damping="0.1" />
+
+              <body name="r_forearm_roll_link" pos="0 0 0">
+                <geom name="fr" type="capsule" fromto="-0.1 0 0 0.1 0 0" size="0.02" />
+                <joint name="r_forearm_roll_joint" type="hinge" limited="true" pos="0 0 0" axis="1 0 0" damping=".1" range="-1.5 1.5"/>
+
+                <body name="r_forearm_link" pos="0 0 0">
+                  <geom name="fa" type="capsule" fromto="0 0 0 0.291 0 0" size="0.05" />
+
+                  <body name="r_wrist_flex_link" pos="0.321 0 0">
+                    <geom name="wf" type="capsule" fromto="0 -0.02 0 0 0.02 0" size="0.01" />
+                    <joint name="r_wrist_flex_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-1.094 0" damping=".1" />
+
+                    <body name="r_wrist_roll_link" pos="0 0 0">
+                      <joint name="r_wrist_roll_joint" type="hinge" pos="0 0 0" limited="true" axis="1 0 0" damping="0.1" range="-1.5 1.5"/>
+                      <body name="tips_arm" pos="0 0 0">
+                        <geom name="tip_arml" type="sphere" pos="0.1 -0.1 0." size="0.01" />
+                        <geom name="tip_armr" type="sphere" pos="0.1 0.1 0." size="0.01" />
+                      </body>
+                      <geom type="capsule" fromto="0 -0.1 0. 0.0 +0.1 0" size="0.02" contype="1" conaffinity="1" />
+                      <geom type="capsule" fromto="0 -0.1 0. 0.1 -0.1 0" size="0.02" contype="1" conaffinity="1" />
+                      <geom type="capsule" fromto="0 +0.1 0. 0.1 +0.1 0." size="0.02" contype="1" conaffinity="1" />
+                    </body>
+                  </body>
+                </body>
+              </body>
+            </body>
+          </body>
+        </body>
+      </body>
+    </body>
+
+    <!--<body name="object" pos="0.55 -0.3 -0.275" >-->
+    <body name="object" pos="0.45 -0.05 -0.275" >
+      <geom rgba="1 1 1 0" type="sphere" size="0.05 0.05 0.05" density="0.00001" conaffinity="0"/>
+      <geom rgba="1 1 1 1" type="cylinder" size="0.05 0.05 0.05" density="0.00001" contype="1" conaffinity="0"/>
+      <joint name="obj_slidey" type="slide" pos="0 0 0" axis="0 1 0" range="-10.3213 10.3" damping="0.5"/>
+      <joint name="obj_slidex" type="slide" pos="0 0 0" axis="1 0 0" range="-10.3213 10.3" damping="0.5"/>
+    </body>
+
+    <body name="goal" pos="0.45 -0.05 -0.3230">
+      <geom rgba="1 0 0 1" type="cylinder" size="0.08 0.001 0.1" density='0.00001' contype="0" conaffinity="0"/>
+      <joint name="goal_slidey" type="slide" pos="0 0 0" axis="0 1 0" range="-10.3213 10.3" damping="0.5"/>
+      <joint name="goal_slidex" type="slide" pos="0 0 0" axis="1 0 0" range="-10.3213 10.3" damping="0.5"/> 
+    </body>
+  </worldbody>
+
+  <actuator>
+    <motor joint="r_shoulder_pan_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_shoulder_lift_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_upper_arm_roll_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_elbow_flex_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_forearm_roll_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_wrist_flex_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_wrist_roll_joint" ctrlrange="-2.0 2.0" ctrllimited="true"/>
+  </actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/reacher.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/reacher.xml
new file mode 100644
index 0000000..64a67b9
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/reacher.xml
@@ -0,0 +1,39 @@
+<mujoco model="reacher">
+	<compiler angle="radian" inertiafromgeom="true"/>
+	<default>
+		<joint armature="1" damping="1" limited="true"/>
+		<geom contype="0" friction="1 0.1 0.1" rgba="0.7 0.7 0 1"/>
+	</default>
+	<option gravity="0 0 -9.81" integrator="RK4" timestep="0.01"/>
+	<worldbody>
+		<!-- Arena -->
+		<geom conaffinity="0" contype="0" name="ground" pos="0 0 0" rgba="0.9 0.9 0.9 1" size="1 1 10" type="plane"/>
+		<geom conaffinity="0" fromto="-.3 -.3 .01 .3 -.3 .01" name="sideS" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
+		<geom conaffinity="0" fromto=" .3 -.3 .01 .3  .3 .01" name="sideE" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
+		<geom conaffinity="0" fromto="-.3  .3 .01 .3  .3 .01" name="sideN" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
+		<geom conaffinity="0" fromto="-.3 -.3 .01 -.3 .3 .01" name="sideW" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
+		<!-- Arm -->
+		<geom conaffinity="0" contype="0" fromto="0 0 0 0 0 0.02" name="root" rgba="0.9 0.4 0.6 1" size=".011" type="cylinder"/>
+		<body name="body0" pos="0 0 .01">
+			<geom fromto="0 0 0 0.1 0 0" name="link0" rgba="0.0 0.4 0.6 1" size=".01" type="capsule"/>
+			<joint axis="0 0 1" limited="false" name="joint0" pos="0 0 0" type="hinge"/>
+			<body name="body1" pos="0.1 0 0">
+				<joint axis="0 0 1" limited="true" name="joint1" pos="0 0 0" range="-3.0 3.0" type="hinge"/>
+				<geom fromto="0 0 0 0.1 0 0" name="link1" rgba="0.0 0.4 0.6 1" size=".01" type="capsule"/>
+				<body name="fingertip" pos="0.11 0 0">
+					<geom contype="0" name="fingertip" pos="0 0 0" rgba="0.0 0.8 0.6 1" size=".01" type="sphere"/>
+				</body>
+			</body>
+		</body>
+		<!-- Target -->
+		<body name="target" pos=".1 -.1 .01">
+			<joint armature="0" axis="1 0 0" damping="0" limited="true" name="target_x" pos="0 0 0" range="-.27 .27" ref=".1" stiffness="0" type="slide"/>
+			<joint armature="0" axis="0 1 0" damping="0" limited="true" name="target_y" pos="0 0 0" range="-.27 .27" ref="-.1" stiffness="0" type="slide"/>
+			<geom conaffinity="0" contype="0" name="target" pos="0 0 0" rgba="0.9 0.2 0.2 1" size=".009" type="sphere"/>
+		</body>
+	</worldbody>
+	<actuator>
+		<motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="joint0"/>
+		<motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="joint1"/>
+	</actuator>
+</mujoco>
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/swimmer.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/swimmer.xml
new file mode 100644
index 0000000..cda25da
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/swimmer.xml
@@ -0,0 +1,38 @@
+<mujoco model="swimmer">
+  <compiler angle="degree" coordinate="local" inertiafromgeom="true"/>
+  <option collision="predefined" density="4000" integrator="RK4" timestep="0.01" viscosity="0.1"/>
+  <default>
+    <geom conaffinity="1" condim="1" contype="1" material="geom" rgba="0.8 0.6 .4 1"/>
+    <joint armature='0.1'  />
+  </default>
+  <asset>
+    <texture builtin="gradient" height="100" rgb1="1 1 1" rgb2="0 0 0" type="skybox" width="100"/>
+    <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+    <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+    <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="30 30" texture="texplane"/>
+    <material name="geom" texture="texgeom" texuniform="true"/>
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 -0.1" rgba="0.8 0.9 0.8 1" size="40 40 0.1" type="plane"/>
+    <!--  ================= SWIMMER ================= /-->
+    <body name="torso" pos="0 0 0">
+      <geom density="1000" fromto="1.5 0 0 0.5 0 0" size="0.1" type="capsule"/>
+      <joint axis="1 0 0" name="slider1" pos="0 0 0" type="slide"/>
+      <joint axis="0 1 0" name="slider2" pos="0 0 0" type="slide"/>
+      <joint axis="0 0 1" name="rot" pos="0 0 0" type="hinge"/>
+      <body name="mid" pos="0.5 0 0">
+        <geom density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule"/>
+        <joint axis="0 0 1" limited="true" name="rot2" pos="0 0 0" range="-100 100" type="hinge"/>
+        <body name="back" pos="-1 0 0">
+          <geom density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule"/>
+          <joint axis="0 0 1" limited="true" name="rot3" pos="0 0 0" range="-100 100" type="hinge"/>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot2"/>
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot3"/>
+  </actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/walker2d.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/walker2d.xml
new file mode 100644
index 0000000..cbc074d
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/walker2d.xml
@@ -0,0 +1,61 @@
+<mujoco model="walker2d">
+  <compiler angle="degree" coordinate="global" inertiafromgeom="true"/>
+  <default>
+    <joint armature="0.01" damping=".1" limited="true"/>
+    <geom conaffinity="0" condim="3" contype="1" density="1000" friction=".7 .1 .1" rgba="0.8 0.6 .4 1"/>
+  </default>
+  <option integrator="RK4" timestep="0.002"/>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="40 40 40" type="plane" material="MatPlane"/>
+    <body name="torso" pos="0 0 1.25">
+      <joint armature="0" axis="1 0 0" damping="0" limited="false" name="rootx" pos="0 0 0" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 0 1" damping="0" limited="false" name="rootz" pos="0 0 0" ref="1.25" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 1 0" damping="0" limited="false" name="rooty" pos="0 0 1.25" stiffness="0" type="hinge"/>
+      <geom friction="0.9" fromto="0 0 1.45 0 0 1.05" name="torso_geom" size="0.05" type="capsule"/>
+      <body name="thigh" pos="0 0 1.05">
+        <joint axis="0 -1 0" name="thigh_joint" pos="0 0 1.05" range="-150 0" type="hinge"/>
+        <geom friction="0.9" fromto="0 0 1.05 0 0 0.6" name="thigh_geom" size="0.05" type="capsule"/>
+        <body name="leg" pos="0 0 0.35">
+          <joint axis="0 -1 0" name="leg_joint" pos="0 0 0.6" range="-150 0" type="hinge"/>
+          <geom friction="0.9" fromto="0 0 0.6 0 0 0.1" name="leg_geom" size="0.04" type="capsule"/>
+          <body name="foot" pos="0.2/2 0 0.1">
+            <joint axis="0 -1 0" name="foot_joint" pos="0 0 0.1" range="-45 45" type="hinge"/>
+            <geom friction="0.9" fromto="-0.0 0 0.1 0.2 0 0.1" name="foot_geom" size="0.06" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <!-- copied and then replace thigh->thigh_left, leg->leg_left, foot->foot_right -->
+      <body name="thigh_left" pos="0 0 1.05">
+        <joint axis="0 -1 0" name="thigh_left_joint" pos="0 0 1.05" range="-150 0" type="hinge"/>
+        <geom friction="0.9" fromto="0 0 1.05 0 0 0.6" name="thigh_left_geom" rgba=".7 .3 .6 1" size="0.05" type="capsule"/>
+        <body name="leg_left" pos="0 0 0.35">
+          <joint axis="0 -1 0" name="leg_left_joint" pos="0 0 0.6" range="-150 0" type="hinge"/>
+          <geom friction="0.9" fromto="0 0 0.6 0 0 0.1" name="leg_left_geom" rgba=".7 .3 .6 1" size="0.04" type="capsule"/>
+          <body name="foot_left" pos="0.2/2 0 0.1">
+            <joint axis="0 -1 0" name="foot_left_joint" pos="0 0 0.1" range="-45 45" type="hinge"/>
+            <geom friction="1.9" fromto="-0.0 0 0.1 0.2 0 0.1" name="foot_left_geom" rgba=".7 .3 .6 1" size="0.06" type="capsule"/>
+          </body>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <!-- <motor joint="torso_joint" ctrlrange="-100.0 100.0" isctrllimited="true"/>-->
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="100" joint="thigh_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="100" joint="leg_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="100" joint="foot_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="100" joint="thigh_left_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="100" joint="leg_left_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="100" joint="foot_left_joint"/>
+    <!-- <motor joint="finger2_rot" ctrlrange="-20.0 20.0" isctrllimited="true"/>-->
+  </actuator>
+    <asset>
+        <texture type="skybox" builtin="gradient" rgb1=".4 .5 .6" rgb2="0 0 0"
+            width="100" height="100"/>        
+        <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+        <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+        <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="60 60" texture="texplane"/>
+        <material name="geom" texture="texgeom" texuniform="true"/>
+    </asset>
+</mujoco>
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/__init__.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/__init__.py
new file mode 100644
index 0000000..eff23c0
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/__init__.py
@@ -0,0 +1,210 @@
+from gym.envs.registration import register
+
+register(
+    id='MBRLHalfCheetah-v0',
+    entry_point='envs.gym.half_cheetah:HalfCheetahEnv',
+    kwargs={'frame_skip': 5},
+    max_episode_steps=1000,
+)
+
+register(
+    id='MBRLWalker2d-v0',
+    entry_point='envs.gym.walker2d:Walker2dEnv',
+    kwargs={'frame_skip': 4},
+    max_episode_steps=1000,
+)
+
+register(
+    id='MBRLSwimmer-v0',
+    entry_point='envs.gym.swimmer:SwimmerEnv',
+    kwargs={'frame_skip': 4},
+    max_episode_steps=1000,
+)
+
+register(
+    id='MBRLAnt-v0',
+    entry_point='envs.gym.ant:AntEnv',
+    kwargs={'frame_skip': 5},
+    max_episode_steps=1000,
+)
+
+register(
+    id='MBRLHopper-v0',
+    entry_point='envs.gym.hopper:HopperEnv',
+    kwargs={'frame_skip': 4},
+    max_episode_steps=1000,
+)
+
+register(
+    id='MBRLReacher-v0',
+    entry_point='envs.gym.reacher:ReacherEnv',
+    max_episode_steps=50,
+)
+
+
+# second batch of environments
+
+register(
+    id='MBRLInvertedPendulum-v0',
+    entry_point='envs.gym.inverted_pendulum:InvertedPendulumEnv',
+    max_episode_steps=100,
+)
+register(
+    id='MBRLAcrobot-v0',
+    entry_point='envs.gym.acrobot:AcrobotEnv',
+    max_episode_steps=200,
+)
+register(
+    id='MBRLCartpole-v0',
+    entry_point='envs.gym.cartpole:CartPoleEnv',
+    max_episode_steps=200,
+
+)
+register(
+    id='MBRLMountain-v0',
+    entry_point='envs.gym.mountain_car:Continuous_MountainCarEnv',
+    max_episode_steps=200,
+
+)
+register(
+    id='MBRLPendulum-v0',
+    entry_point='envs.gym.pendulum:PendulumEnv',
+    max_episode_steps=200,
+)
+
+register(
+    id='gym_petsPusher-v0',
+    entry_point='envs.gym.pets_pusher:PusherEnv',
+    max_episode_steps=150,
+)
+register(
+    id='gym_petsReacher-v0',
+    entry_point='envs.gym.pets_reacher:Reacher3DEnv',
+    max_episode_steps=150,
+)
+register(
+    id='gym_petsCheetah-v0',
+    entry_point='envs.gym.pets_cheetah:HalfCheetahEnv',
+    max_episode_steps=1000,
+)
+
+# noisy env
+register(
+    id='gym_cheetahO01-v0',
+    entry_point='envs.gym.gym_cheetahO01:HalfCheetahEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_cheetahO001-v0',
+    entry_point='envs.gym.gym_cheetahO001:HalfCheetahEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_cheetahA01-v0',
+    entry_point='envs.gym.gym_cheetahA01:HalfCheetahEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_cheetahA003-v0',
+    entry_point='envs.gym.gym_cheetahA003:HalfCheetahEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_pendulumO01-v0',
+    entry_point='envs.gym.gym_pendulumO01:PendulumEnv',
+    max_episode_steps=200,
+)
+register(
+    id='gym_pendulumO001-v0',
+    entry_point='envs.gym.gym_pendulumO001:PendulumEnv',
+    max_episode_steps=200,
+)
+register(
+    id='gym_cartpoleO01-v0',
+    entry_point='envs.gym.gym_cartpoleO01:CartPoleEnv',
+    max_episode_steps=200,
+)
+register(
+    id='gym_cartpoleO001-v0',
+    entry_point='envs.gym.gym_cartpoleO001:CartPoleEnv',
+    max_episode_steps=200,
+)
+
+register(
+    id='gym_fant-v0',
+    entry_point='envs.gym.gym_fant:AntEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_fhopper-v0',
+    entry_point='envs.gym.gym_fhopper:HopperEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_fwalker2d-v0',
+    entry_point='envs.gym.gym_fwalker2d:Walker2dEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_fswimmer-v0',
+    entry_point='envs.gym.gym_fswimmer:fixedSwimmerEnv',
+    max_episode_steps=1000,
+)
+register(
+    id="gym_humanoid-v0",
+    entry_point='envs.gym.gym_humanoid:HumanoidEnv',
+    max_episode_steps=1000,
+)
+register(
+    id="gym_slimhumanoid-v0",
+    entry_point='envs.gym.gym_slimhumanoid:HumanoidEnv',
+    max_episode_steps=1000,
+)
+register(
+    id="gym_nostopslimhumanoid-v0",
+    entry_point='envs.gym.gym_nostopslimhumanoid:HumanoidEnv',
+    max_episode_steps=1000,
+)
+
+env_name_to_gym_registry = {
+    # first batch
+    "half_cheetah": "MBRLHalfCheetah-v0",
+    "swimmer": "MBRLSwimmer-v0",
+    "ant": "MBRLAnt-v0",
+    "hopper": "MBRLHopper-v0",
+    "reacher": "MBRLReacher-v0",
+    "walker2d": "MBRLWalker2d-v0",
+
+    # second batch
+    "invertedPendulum": "MBRLInvertedPendulum-v0",
+    "acrobot": 'MBRLAcrobot-v0',
+    "cartpole": 'MBRLCartpole-v0',
+    "mountain": 'MBRLMountain-v0',
+    "pendulum": 'MBRLPendulum-v0',
+
+    # the pets env
+    "gym_petsPusher": "gym_petsPusher-v0",
+    "gym_petsReacher": "gym_petsReacher-v0",
+    "gym_petsCheetah": "gym_petsCheetah-v0",
+
+    # the noise env
+    "gym_cheetahO01": "gym_cheetahO01-v0",
+    "gym_cheetahO001": "gym_cheetahO001-v0",
+    "gym_cheetahA01": "gym_cheetahA01-v0",
+    "gym_cheetahA003": "gym_cheetahA003-v0",
+
+    "gym_pendulumO01": "gym_pendulumO01-v0",
+    "gym_pendulumO001": "gym_pendulumO001-v0",
+
+    "gym_cartpoleO01": "gym_cartpoleO01-v0",
+    "gym_cartpoleO001": "gym_cartpoleO001-v0",
+
+    "gym_fant": "gym_fant-v0",
+    "gym_fswimmer": "gym_fswimmer-v0",
+    "gym_fhopper": "gym_fhopper-v0",
+    "gym_fwalker2d": "gym_fwalker2d-v0",
+
+    "gym_humanoid": "gym_humanoid-v0",
+    "gym_slimhumanoid": "gym_slimhumanoid-v0",
+    "gym_nostopslimhumanoid": "gym_nostopslimhumanoid-v0",
+}
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/acrobot.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/acrobot.py
new file mode 100644
index 0000000..1af57a6
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/acrobot.py
@@ -0,0 +1,357 @@
+"""classic Acrobot task"""
+from gym import core, spaces
+from gym.utils import seeding
+import numpy as np
+from numpy import sin, cos, pi
+from slbo.utils.dataset import Dataset, gen_dtype
+from lunzi.Logger import logger
+
+
+__copyright__ = "Copyright 2013, RLPy http://acl.mit.edu/RLPy"
+__credits__ = ["Alborz Geramifard", "Robert H. Klein", "Christoph Dann",
+               "William Dabney", "Jonathan P. How"]
+__license__ = "BSD 3-Clause"
+__author__ = "Christoph Dann <cdann@cdann.de>"
+
+# SOURCE:
+# https://github.com/rlpy/rlpy/blob/master/rlpy/Domains/Acrobot.py
+
+
+class AcrobotEnv(core.Env):
+
+    """
+    Acrobot is a 2-link pendulum with only the second joint actuated
+    Intitially, both links point downwards. The goal is to swing the
+    end-effector at a height at least the length of one link above the base.
+    Both links can swing freely and can pass by each other, i.e., they don't
+    collide when they have the same angle.
+    **STATE:**
+    The state consists of the two rotational joint angles and their velocities
+    [theta1 theta2 thetaDot1 thetaDot2]. An angle of 0 corresponds to corresponds
+    to the respective link pointing downwards (angles are in world coordinates).
+    **ACTIONS:**
+    The action is either applying +1, 0 or -1 torque on the joint between
+    the two pendulum links.
+    .. note::
+        The dynamics equations were missing some terms in the NIPS paper which
+        are present in the book. R. Sutton confirmed in personal correspondance
+        that the experimental results shown in the paper and the book were
+        generated with the equations shown in the book.
+        However, there is the option to run the domain with the paper equations
+        by setting book_or_nips = 'nips'
+    **REFERENCE:**
+    .. seealso::
+        R. Sutton: Generalization in Reinforcement Learning:
+        Successful Examples Using Sparse Coarse Coding (NIPS 1996)
+    .. seealso::
+        R. Sutton and A. G. Barto:
+        Reinforcement learning: An introduction.
+        Cambridge: MIT press, 1998.
+    .. warning::
+        This version of the domain uses the Runge-Kutta method for integrating
+        the system dynamics and is more realistic, but also considerably harder
+        than the original version which employs Euler integration,
+        see the AcrobotLegacy class.
+    """
+
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 15
+    }
+
+    dt = .2
+
+    LINK_LENGTH_1 = 1.  # [m]
+    LINK_LENGTH_2 = 1.  # [m]
+    LINK_MASS_1 = 1.  #: [kg] mass of link 1
+    LINK_MASS_2 = 1.  #: [kg] mass of link 2
+    LINK_COM_POS_1 = 0.5  #: [m] position of the center of mass of link 1
+    LINK_COM_POS_2 = 0.5  #: [m] position of the center of mass of link 2
+    LINK_MOI = 1.  #: moments of inertia for both links
+
+    MAX_VEL_1 = 4 * np.pi
+    MAX_VEL_2 = 9 * np.pi
+
+    AVAIL_TORQUE = [-1., 0., +1]
+
+    torque_noise_max = 0.
+
+    #: use dynamics equations from the nips paper or the book
+    book_or_nips = "book"
+    action_arrow = None
+    domain_fig = None
+    actions_num = 3
+
+    def __init__(self):
+        self.viewer = None
+        high = np.array([1.0, 1.0, 1.0, 1.0, self.MAX_VEL_1, self.MAX_VEL_2])
+        low = -high
+        self.observation_space = spaces.Box(low, high)
+        self.action_space = spaces.Box(np.array([-1.0]), np.array([1.0]))
+        self.state = None
+        self._seed()
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _reset(self):
+        self.state = self.np_random.uniform(low=-0.1, high=0.1, size=(4,))
+        return self._get_ob()
+
+    def _step(self, a):
+        # Discretize
+        if a[0] < -.33:
+            action = 0
+        elif a[0] < .33:
+            action = 1
+        else:
+            action = 2
+
+        s = self.state
+        reward = -np.cos(s[0]) - np.cos(s[1] + s[0])
+        torque = self.AVAIL_TORQUE[action]
+
+        # Add noise to the force action
+        if self.torque_noise_max > 0:
+            torque += self.np_random.uniform(-self.torque_noise_max, self.torque_noise_max)
+
+        # Now, augment the state with our force action so it can be passed to
+        # _dsdt
+        s_augmented = np.append(s, torque)
+
+        ns = rk4(self._dsdt, s_augmented, [0, self.dt])
+        # only care about final timestep of integration returned by integrator
+        ns = ns[-1]
+        ns = ns[:4]  # omit action
+        # ODEINT IS TOO SLOW!
+        # ns_continuous = integrate.odeint(self._dsdt, self.s_continuous, [0, self.dt])
+        # self.s_continuous = ns_continuous[-1] # We only care about the state
+        # at the ''final timestep'', self.dt
+
+        ns[0] = wrap(ns[0], -pi, pi)
+        ns[1] = wrap(ns[1], -pi, pi)
+        ns[2] = bound(ns[2], -self.MAX_VEL_1, self.MAX_VEL_1)
+        ns[3] = bound(ns[3], -self.MAX_VEL_2, self.MAX_VEL_2)
+        self.state = ns
+        # terminal = self._terminal()
+        terminal = False
+        # reward = -1. if not terminal else 0.
+        return (self._get_ob(), reward, terminal, {})
+
+    def _get_ob(self):
+        s = self.state
+        return np.array([cos(s[0]), np.sin(s[0]), cos(s[1]), sin(s[1]), s[2], s[3]])
+
+    def _terminal(self):
+        s = self.state
+        return bool(-np.cos(s[0]) - np.cos(s[1] + s[0]) > 1.)
+
+    def _dsdt(self, s_augmented, t):
+        m1 = self.LINK_MASS_1
+        m2 = self.LINK_MASS_2
+        l1 = self.LINK_LENGTH_1
+        lc1 = self.LINK_COM_POS_1
+        lc2 = self.LINK_COM_POS_2
+        I1 = self.LINK_MOI
+        I2 = self.LINK_MOI
+        g = 9.8
+        a = s_augmented[-1]
+        s = s_augmented[:-1]
+        theta1 = s[0]
+        theta2 = s[1]
+        dtheta1 = s[2]
+        dtheta2 = s[3]
+        d1 = m1 * lc1 ** 2 + m2 * \
+            (l1 ** 2 + lc2 ** 2 + 2 * l1 * lc2 * np.cos(theta2)) + I1 + I2
+        d2 = m2 * (lc2 ** 2 + l1 * lc2 * np.cos(theta2)) + I2
+        phi2 = m2 * lc2 * g * np.cos(theta1 + theta2 - np.pi / 2.)
+        phi1 = - m2 * l1 * lc2 * dtheta2 ** 2 * np.sin(theta2) \
+               - 2 * m2 * l1 * lc2 * dtheta2 * dtheta1 * np.sin(theta2)  \
+            + (m1 * lc1 + m2 * l1) * g * np.cos(theta1 - np.pi / 2) + phi2
+        if self.book_or_nips == "nips":
+            # the following line is consistent with the description in the
+            # paper
+            ddtheta2 = (a + d2 / d1 * phi1 - phi2) / \
+                (m2 * lc2 ** 2 + I2 - d2 ** 2 / d1)
+        else:
+            # the following line is consistent with the java implementation and the
+            # book
+            ddtheta2 = (a + d2 / d1 * phi1 - m2 * l1 * lc2 * dtheta1 ** 2 * np.sin(theta2) - phi2) \
+                / (m2 * lc2 ** 2 + I2 - d2 ** 2 / d1)
+        ddtheta1 = -(d2 * ddtheta2 + phi1) / d1
+        return (dtheta1, dtheta2, ddtheta1, ddtheta2, 0.)
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+        from gym.envs.classic_control import rendering
+
+        s = self.state
+
+        if self.viewer is None:
+            self.viewer = rendering.Viewer(500, 500)
+            self.viewer.set_bounds(-2.2, 2.2, -2.2, 2.2)
+
+        if s is None:
+            return None
+
+        p1 = [-self.LINK_LENGTH_1 *
+              np.cos(s[0]), self.LINK_LENGTH_1 * np.sin(s[0])]
+
+        p2 = [p1[0] - self.LINK_LENGTH_2 * np.cos(s[0] + s[1]),
+              p1[1] + self.LINK_LENGTH_2 * np.sin(s[0] + s[1])]
+
+        xys = np.array([[0, 0], p1, p2])[:, ::-1]
+        thetas = [s[0] - np.pi / 2, s[0] + s[1] - np.pi / 2]
+
+        self.viewer.draw_line((-2.2, 1), (2.2, 1))
+        for ((x, y), th) in zip(xys, thetas):
+            l, r, t, b = 0, 1, .1, -.1
+            jtransform = rendering.Transform(rotation=th, translation=(x, y))
+            link = self.viewer.draw_polygon([(l, b), (l, t), (r, t), (r, b)])
+            link.add_attr(jtransform)
+            link.set_color(0, .8, .8)
+            circ = self.viewer.draw_circle(.1)
+            circ.set_color(.8, .8, 0)
+            circ.add_attr(jtransform)
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+            def height(obs):
+                h1 = obs[0]  # Height of first arm
+                h2 = obs[0] * obs[2] - obs[1] * obs[3]  # Height of second arm
+                return -(h1 + h2)  # total height
+
+            start_height = height(data_dict['start_state'])
+        """
+        h1 = obs[:, 0]  # Height of first arm
+        h2 = obs[:, 0] * obs[:, 2] - obs[:, 1] * obs[:, 3]  # Height of second arm
+        return (h1 + h2)
+
+    def verify(self, n=2000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
+
+
+def wrap(x, m, M):
+    """
+    :param x: a scalar
+    :param m: minimum possible value in range
+    :param M: maximum possible value in range
+    Wraps ``x`` so m <= x <= M; but unlike ``bound()`` which
+    truncates, ``wrap()`` wraps x around the coordinate system defined by m,M.\n
+    For example, m = -180, M = 180 (degrees), x = 360 --> returns 0.
+    """
+    diff = M - m
+    while x > M:
+        x = x - diff
+    while x < m:
+        x = x + diff
+    return x
+
+
+def bound(x, m, M=None):
+    """
+    :param x: scalar
+    Either have m as scalar, so bound(x,m,M) which returns m <= x <= M *OR*
+    have m as length 2 vector, bound(x,m, <IGNORED>) returns m[0] <= x <= m[1].
+    """
+    if M is None:
+        M = m[1]
+        m = m[0]
+    # bound x between min (m) and Max (M)
+    return min(max(x, m), M)
+
+
+def rk4(derivs, y0, t, *args, **kwargs):
+    """
+    Integrate 1D or ND system of ODEs using 4-th order Runge-Kutta.
+    This is a toy implementation which may be useful if you find
+    yourself stranded on a system w/o scipy.  Otherwise use
+    :func:`scipy.integrate`.
+    *y0*
+        initial state vector
+    *t*
+        sample times
+    *derivs*
+        returns the derivative of the system and has the
+        signature ``dy = derivs(yi, ti)``
+    *args*
+        additional arguments passed to the derivative function
+    *kwargs*
+        additional keyword arguments passed to the derivative function
+    Example 1 ::
+        ## 2D system
+        def derivs6(x,t):
+            d1 =  x[0] + 2*x[1]
+            d2 =  -3*x[0] + 4*x[1]
+            return (d1, d2)
+        dt = 0.0005
+        t = arange(0.0, 2.0, dt)
+        y0 = (1,2)
+        yout = rk4(derivs6, y0, t)
+    Example 2::
+        ## 1D system
+        alpha = 2
+        def derivs(x,t):
+            return -alpha*x + exp(-t)
+        y0 = 1
+        yout = rk4(derivs, y0, t)
+    If you have access to scipy, you should probably be using the
+    scipy.integrate tools rather than this function.
+    """
+
+    try:
+        Ny = len(y0)
+    except TypeError:
+        yout = np.zeros((len(t),), np.float_)
+    else:
+        yout = np.zeros((len(t), Ny), np.float_)
+
+    yout[0] = y0
+    i = 0
+
+    for i in np.arange(len(t) - 1):
+
+        thist = t[i]
+        dt = t[i + 1] - thist
+        dt2 = dt / 2.0
+        y0 = yout[i]
+
+        k1 = np.asarray(derivs(y0, thist, *args, **kwargs))
+        k2 = np.asarray(derivs(y0 + dt2 * k1, thist + dt2, *args, **kwargs))
+        k3 = np.asarray(derivs(y0 + dt2 * k2, thist + dt2, *args, **kwargs))
+        k4 = np.asarray(derivs(y0 + dt * k3, thist + dt, *args, **kwargs))
+        yout[i + 1] = y0 + dt / 6.0 * (k1 + 2 * k2 + 2 * k3 + k4)
+    return yout
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/ant.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/ant.py
new file mode 100644
index 0000000..a2699aa
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/ant.py
@@ -0,0 +1,81 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class AntEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/ant.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        ob = self._get_obs()
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = old_ob[13]
+        reward_height = -3.0 * np.square(old_ob[0] - 0.57)
+        reward = reward_run + reward_ctrl + reward_height + 1.0
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            # (self.model.data.qpos.flat[:1] - self.prev_qpos[:1]) / self.dt,
+            # self.get_body_comvel("torso")[:1],
+            self.model.data.qpos.flat[2:],
+            self.model.data.qvel.flat,
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        qpos = self.init_qpos + self.np_random.uniform(size=self.model.nq, low=-.1, high=.1)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        # self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 13]
+        reward_height = -3.0 * np.square(obs[:, 0] - 0.57)
+        reward = reward_run + reward_ctrl + reward_height + 1.0
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        # reward_height = -3.0 * tf.square(next_obs[:, 0] - 0.57)
+        reward = reward_run + reward_ctrl # + reward_height
+        return -reward
+        """
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/cartpole.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/cartpole.xml
new file mode 100644
index 0000000..284a58c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/cartpole.xml
@@ -0,0 +1,35 @@
+<mujoco model="cartpole">
+	<compiler inertiafromgeom="true"/>
+	<default>
+		<joint armature="0" damping="1" limited="true"/>
+		<geom contype="0" friction="1 0.1 0.1" rgba="0.7 0.7 0 1"/>
+		<tendon/>
+		<motor ctrlrange="-3 3" ctrllimited='true'/>
+	</default>
+    <asset>
+		<texture type="skybox" builtin="checker" rgb1="1 1 1" rgb2="1 1 1"
+                    width="256" height="256"/>
+        <texture name="texgeom" type="cube" builtin="flat" mark="cross" width="127" height="1278" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" markrgb="1 1 1" random="0.01" />
+		<texture name="texplane" type="2d" builtin="checker" rgb1=".5 .5 .5" rgb2=".5 .5 .5" width="100" height="100" />
+        <texture name="texplane_show" type="2d" builtin="checker" rgb1="0 0 0" rgb2="0.9 0.9 0.9" width="100" height="100" />
+        <material name='MatPlane' texture="texplane" shininess="1" texrepeat="30 30" specular="1"  reflectance="0.5" />
+        <material name='geom' texture="texgeom" texuniform="true" />
+ 	</asset>
+	<option gravity="0 0 -9.81" integrator="RK4" timestep="0.02"/>
+	<size nstack="3000"/>
+	<worldbody>
+		<geom name="rail" pos="0 0 0" quat="0.707 0 0.707 0" rgba="0.3 0.3 0.7 1" size="0.02 3" type="capsule"/>
+		<body name="cart" pos="0 0 0">
+			<joint axis="1 0 0" limited="true" name="slider" pos="0 0 0" range="-2.5 2.5" type="slide"/>
+			<geom name="cart" pos="0 0 0" quat="0.707 0 0.707 0" size="0.1 0.1" type="capsule"/>
+			<body name="pole" pos="0 0 0">
+				<joint axis="0 1 0" limited="false" name="hinge" pos="0 0 0" range="-180 180" type="hinge"/>
+				<geom fromto="0 0 0 0.001 0 -0.6" name="cpole" rgba="0 0.7 0.7 1" size="0.049 0.3" type="capsule"/>
+			</body>
+		</body>
+	</worldbody>
+	<actuator>
+		<motor gear="100" joint="slider" name="slide"/>
+	</actuator>
+</mujoco>
+
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/fixed_swimmer.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/fixed_swimmer.xml
new file mode 100644
index 0000000..142f344
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/fixed_swimmer.xml
@@ -0,0 +1,43 @@
+<mujoco model="swimmer">
+  <compiler angle="degree" coordinate="local" inertiafromgeom="true"/>
+  <option collision="predefined" density="4000" integrator="RK4" timestep="0.01" viscosity="0.1"/>
+  <default>
+    <geom conaffinity="1" condim="1" contype="1" material="geom" rgba="0.8 0.6 .4 1"/>
+    <joint armature='0.1'  />
+  </default>
+  <asset>
+    <texture builtin="gradient" height="100" rgb1="1 1 1" rgb2="0 0 0" type="skybox" width="100"/>
+    <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+    <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+    <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="30 30" texture="texplane"/>
+    <material name="geom" texture="texgeom" texuniform="true"/>
+  </asset>
+
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 -0.1" rgba="0.8 0.9 0.8 1" size="40 40 0.1" type="plane"/>
+    <body name="podBody_1" pos="0 0 0">
+      <geom name='pod_1' density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule"/>
+      <joint axis="1 0 0" name="slider1" pos="0 0 0" type="slide"/>
+      <joint axis="0 1 0" name="slider2" pos="0 0 0" type="slide"/>
+      <joint axis="0 0 1" name="rot_1" pos="-1.5 0 0" type="hinge"/>
+      <site name="tip" pos="0 0 0" size="0.02 0.02"/>
+      <body name="podBody_2" pos="-1 0 0">
+        <geom name='pod_2' density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule"/>
+        <joint axis="0 0 1" limited="true" name="rot_2" pos="0 0 0" range="-100 100" type="hinge"/>
+        
+        <body name="podBody_3" pos="-1 0 0">
+          <geom name='pod_3' density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule"/>
+          <joint axis="0 0 1" limited="true" name="rot_3" pos="0 0 0" range="-100 100" type="hinge"/>
+        
+        </body>
+
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot_2"/>
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot_3"/>
+  </actuator>
+
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/half_cheetah.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/half_cheetah.xml
new file mode 100644
index 0000000..40a1cb6
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/half_cheetah.xml
@@ -0,0 +1,95 @@
+<!-- Cheetah Model
+
+    The state space is populated with joints in the order that they are
+    defined in this file. The actuators also operate on joints.
+
+    State-Space (name/joint/parameter):
+        - rootx     slider      position (m)
+        - rootz     slider      position (m)
+        - rooty     hinge       angle (rad)
+        - bthigh    hinge       angle (rad)
+        - bshin     hinge       angle (rad)
+        - bfoot     hinge       angle (rad)
+        - fthigh    hinge       angle (rad)
+        - fshin     hinge       angle (rad)
+        - ffoot     hinge       angle (rad)
+        - rootx     slider      velocity (m/s)
+        - rootz     slider      velocity (m/s)
+        - rooty     hinge       angular velocity (rad/s)
+        - bthigh    hinge       angular velocity (rad/s)
+        - bshin     hinge       angular velocity (rad/s)
+        - bfoot     hinge       angular velocity (rad/s)
+        - fthigh    hinge       angular velocity (rad/s)
+        - fshin     hinge       angular velocity (rad/s)
+        - ffoot     hinge       angular velocity (rad/s)
+
+    Actuators (name/actuator/parameter):
+        - bthigh    hinge       torque (N m)
+        - bshin     hinge       torque (N m)
+        - bfoot     hinge       torque (N m)
+        - fthigh    hinge       torque (N m)
+        - fshin     hinge       torque (N m)
+        - ffoot     hinge       torque (N m)
+
+-->
+<mujoco model="cheetah">
+  <compiler angle="radian" coordinate="local" inertiafromgeom="true" settotalmass="14"/>
+  <default>
+    <joint armature=".1" damping=".01" limited="true" solimplimit="0 .8 .03" solreflimit=".02 1" stiffness="8"/>
+    <geom conaffinity="0" condim="3" contype="1" friction=".4 .1 .1" rgba="0.8 0.6 .4 1" solimp="0.0 0.8 0.01" solref="0.02 1"/>
+    <motor ctrllimited="true" ctrlrange="-1 1"/>
+  </default>
+  <size nstack="300000" nuser_geom="1"/>
+  <option gravity="0 0 -9.81" timestep="0.01"/>
+  <asset>
+    <texture builtin="gradient" height="100" rgb1="1 1 1" rgb2="0 0 0" type="skybox" width="100"/>
+    <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+    <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+    <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="150 150" texture="texplane"/>
+    <material name="geom" texture="texgeom" texuniform="true"/>
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="200 200 200" type="plane"/>
+    <body name="torso" pos="0 0 .7">
+      <joint armature="0" axis="1 0 0" damping="0" limited="false" name="rootx" pos="0 0 0" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 0 1" damping="0" limited="false" name="rootz" pos="0 0 0" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 1 0" damping="0" limited="false" name="rooty" pos="0 0 0" stiffness="0" type="hinge"/>
+      <geom fromto="-.5 0 0 .5 0 0" name="torso" size="0.046" type="capsule"/>
+      <geom axisangle="0 1 0 .87" name="head" pos=".6 0 .1" size="0.046 .15" type="capsule"/>
+      <!-- <site name='tip'  pos='.15 0 .11'/>-->
+      <body name="bthigh" pos="-.5 0 0">
+        <joint axis="0 1 0" damping="6" name="bthigh" pos="0 0 0" range="-.52 1.05" stiffness="240" type="hinge"/>
+        <geom axisangle="0 1 0 -3.8" name="bthigh" pos=".1 0 -.13" size="0.046 .145" type="capsule"/>
+        <body name="bshin" pos=".16 0 -.25">
+          <joint axis="0 1 0" damping="4.5" name="bshin" pos="0 0 0" range="-.785 .785" stiffness="180" type="hinge"/>
+          <geom axisangle="0 1 0 -2.03" name="bshin" pos="-.14 0 -.07" rgba="0.9 0.6 0.6 1" size="0.046 .15" type="capsule"/>
+          <body name="bfoot" pos="-.28 0 -.14">
+            <joint axis="0 1 0" damping="3" name="bfoot" pos="0 0 0" range="-.4 .785" stiffness="120" type="hinge"/>
+            <geom axisangle="0 1 0 -.27" name="bfoot" pos=".03 0 -.097" rgba="0.9 0.6 0.6 1" size="0.046 .094" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <body name="fthigh" pos=".5 0 0">
+        <joint axis="0 1 0" damping="4.5" name="fthigh" pos="0 0 0" range="-1 .7" stiffness="180" type="hinge"/>
+        <geom axisangle="0 1 0 .52" name="fthigh" pos="-.07 0 -.12" size="0.046 .133" type="capsule"/>
+        <body name="fshin" pos="-.14 0 -.24">
+          <joint axis="0 1 0" damping="3" name="fshin" pos="0 0 0" range="-1.2 .87" stiffness="120" type="hinge"/>
+          <geom axisangle="0 1 0 -.6" name="fshin" pos=".065 0 -.09" rgba="0.9 0.6 0.6 1" size="0.046 .106" type="capsule"/>
+          <body name="ffoot" pos=".13 0 -.18">
+            <joint axis="0 1 0" damping="1.5" name="ffoot" pos="0 0 0" range="-.5 .5" stiffness="60" type="hinge"/>
+            <geom axisangle="0 1 0 -.6" name="ffoot" pos=".045 0 -.07" rgba="0.9 0.6 0.6 1" size="0.046 .07" type="capsule"/>
+          </body>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor gear="120" joint="bthigh" name="bthigh"/>
+    <motor gear="90" joint="bshin" name="bshin"/>
+    <motor gear="60" joint="bfoot" name="bfoot"/>
+    <motor gear="120" joint="fthigh" name="fthigh"/>
+    <motor gear="60" joint="fshin" name="fshin"/>
+    <motor gear="30" joint="ffoot" name="ffoot"/>
+  </actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/pusher.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/pusher.xml
new file mode 100644
index 0000000..9e81b01
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/pusher.xml
@@ -0,0 +1,101 @@
+<mujoco model="arm3d">
+  <compiler inertiafromgeom="true" angle="radian" coordinate="local"/>
+  <option timestep="0.01" gravity="0 0 0" iterations="20" integrator="Euler" />
+  
+  <default>
+    <joint armature='0.04' damping="1" limited="true"/>
+    <geom friction=".8 .1 .1" density="300" margin="0.002" condim="1" contype="0" conaffinity="0"/>
+  </default>
+  <asset>
+    <texture type="skybox" builtin="checker" rgb1="1 1 1" rgb2="1 1 1"
+             width="256" height="256"/>
+    <texture name="texgeom" type="cube" builtin="flat" mark="cross" width="127" height="1278" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" markrgb="1 1 1" random="0.01" />
+    <texture name="texplane" type="2d" builtin="checker" rgb1=".5 .5 .5" rgb2=".5 .5 .5" width="100" height="100" />
+    <texture name="texplane_show" type="2d" builtin="checker" rgb1="0 0 0" rgb2="0.9 0.9 0.9" width="100" height="100" />
+    <material name='MatPlane' texture="texplane" shininess="1" texrepeat="30 30" specular="1"  reflectance="0.5" />
+    <material name='geom' texture="texgeom" texuniform="true" />
+  </asset>
+
+  <worldbody>
+    <light diffuse=".5 .5 .5" pos="0 0 3" dir="0 0 -1"/>
+    <geom name="table" type="plane" pos="0 0.5 -0.325" size="1 1 0.1" contype="1" conaffinity="1"/>
+
+    <body name="r_shoulder_pan_link" pos="0 -0.6 0">
+      <geom name="e1" type="sphere" rgba="0.6 0.6 0.6 1" pos="-0.06 0.05 0.2" size="0.05" />
+      <geom name="e2" type="sphere" rgba="0.6 0.6 0.6 1" pos=" 0.06 0.05 0.2" size="0.05" />
+      <geom name="e1p" type="sphere" rgba="0.1 0.1 0.1 1" pos="-0.06 0.09 0.2" size="0.03" />
+      <geom name="e2p" type="sphere" rgba="0.1 0.1 0.1 1" pos=" 0.06 0.09 0.2" size="0.03" />
+      <geom name="sp" type="capsule" fromto="0 0 -0.4 0 0 0.2" size="0.1" />
+      <!--<joint name="r_shoulder_pan_joint" type="hinge" pos="0 0 0" axis="0 0 1" range="-2.2854 1.714602" damping="1.0" />-->
+      <joint name="r_shoulder_pan_joint" type="hinge" pos="0 0 0" axis="0 0 1" range="-2.2854 2.0" damping="1.0" />
+
+      <body name="r_shoulder_lift_link" pos="0.1 0 0">
+        <geom name="sl" type="capsule" fromto="0 -0.1 0 0 0.1 0" size="0.1" />
+        <joint name="r_shoulder_lift_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.5236 1.3963" damping="1.0" />
+
+        <body name="r_upper_arm_roll_link" pos="0 0 0">
+          <geom name="uar" type="capsule" fromto="-0.1 0 0 0.1 0 0" size="0.02" />
+          <joint name="r_upper_arm_roll_joint" type="hinge" pos="0 0 0" axis="1 0 0" range="-1.5 1.7" damping="0.1" />
+
+          <body name="r_upper_arm_link" pos="0 0 0">
+            <geom name="ua" type="capsule" fromto="0 0 0 0.4 0 0" size="0.06" />
+
+            <body name="r_elbow_flex_link" pos="0.4 0 0">
+              <geom name="ef" type="capsule" fromto="0 -0.02 0 0.0 0.02 0" size="0.06" />
+              <joint name="r_elbow_flex_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-2.3213 0" damping="0.1" />
+
+              <body name="r_forearm_roll_link" pos="0 0 0">
+                <geom name="fr" type="capsule" fromto="-0.1 0 0 0.1 0 0" size="0.02" />
+                <joint name="r_forearm_roll_joint" type="hinge" limited="true" pos="0 0 0" axis="1 0 0" damping=".1" range="-1.5 1.5"/>
+
+                <body name="r_forearm_link" pos="0 0 0">
+                  <geom name="fa" type="capsule" fromto="0 0 0 0.291 0 0" size="0.05" />
+
+                  <body name="r_wrist_flex_link" pos="0.321 0 0">
+                    <geom name="wf" type="capsule" fromto="0 -0.02 0 0 0.02 0" size="0.01" />
+                    <joint name="r_wrist_flex_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-1.094 0" damping=".1" />
+
+                    <body name="r_wrist_roll_link" pos="0 0 0">
+                      <joint name="r_wrist_roll_joint" type="hinge" pos="0 0 0" limited="true" axis="1 0 0" damping="0.1" range="-1.5 1.5"/>
+                      <body name="tips_arm" pos="0 0 0">
+                        <geom name="tip_arml" type="sphere" pos="0.1 -0.1 0." size="0.01" />
+                        <geom name="tip_armr" type="sphere" pos="0.1 0.1 0." size="0.01" />
+                      </body>
+                      <geom type="capsule" fromto="0 -0.1 0. 0.0 +0.1 0" size="0.02" contype="1" conaffinity="1" />
+                      <geom type="capsule" fromto="0 -0.1 0. 0.1 -0.1 0" size="0.02" contype="1" conaffinity="1" />
+                      <geom type="capsule" fromto="0 +0.1 0. 0.1 +0.1 0." size="0.02" contype="1" conaffinity="1" />
+                    </body>
+                  </body>
+                </body>
+              </body>
+            </body>
+          </body>
+        </body>
+      </body>
+    </body>
+
+    <!--<body name="object" pos="0.55 -0.3 -0.275" >-->
+    <body name="object" pos="0.45 -0.05 -0.275" >
+      <geom rgba="1 1 1 0" type="sphere" size="0.05 0.05 0.05" density="0.00001" conaffinity="0"/>
+      <geom rgba="1 1 1 1" type="cylinder" size="0.05 0.05 0.05" density="0.00001" contype="1" conaffinity="0"/>
+      <joint name="obj_slidey" type="slide" pos="0 0 0" axis="0 1 0" range="-10.3213 10.3" damping="0.5"/>
+      <joint name="obj_slidex" type="slide" pos="0 0 0" axis="1 0 0" range="-10.3213 10.3" damping="0.5"/>
+    </body>
+
+    <body name="goal" pos="0.45 -0.05 -0.3230">
+      <geom rgba="1 0 0 1" type="cylinder" size="0.08 0.001 0.1" density='0.00001' contype="0" conaffinity="0"/>
+      <joint name="goal_slidey" type="slide" pos="0 0 0" axis="0 1 0" range="-10.3213 10.3" damping="0.5"/>
+      <joint name="goal_slidex" type="slide" pos="0 0 0" axis="1 0 0" range="-10.3213 10.3" damping="0.5"/> 
+    </body>
+  </worldbody>
+
+  <actuator>
+    <motor joint="r_shoulder_pan_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_shoulder_lift_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_upper_arm_roll_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_elbow_flex_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_forearm_roll_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_wrist_flex_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_wrist_roll_joint" ctrlrange="-2.0 2.0" ctrllimited="true"/>
+  </actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/reacher3d.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/reacher3d.xml
new file mode 100644
index 0000000..a51c71b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/reacher3d.xml
@@ -0,0 +1,154 @@
+<mujoco model="arm3d">
+
+    <compiler inertiafromgeom="true" angle="radian" coordinate="local" />
+    <option timestep="0.01" gravity="0 0 0" iterations="20" integrator="RK4" />
+    <default>
+        <joint armature="0.04" damping="1" limited="true" />
+        <geom friction=".5 .1 .1" margin="0.002" condim="1" contype="0" conaffinity="0" />
+    </default>
+    <asset>
+        <texture type="skybox" builtin="checker" rgb1="1 1 1" rgb2="1 1 1"
+                 width="256" height="256"/>
+        <texture name="texgeom" type="cube" builtin="flat" mark="cross" width="127" height="1278" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" markrgb="1 1 1" random="0.01" />
+        <texture name="texplane" type="2d" builtin="checker" rgb1=".5 .5 .5" rgb2=".5 .5 .5" width="100" height="100" />
+        <texture name="texplane_show" type="2d" builtin="checker" rgb1="0 0 0" rgb2="0.9 0.9 0.9" width="100" height="100" />
+        <material name='MatPlane' texture="texplane" shininess="1" texrepeat="30 30" specular="1"  reflectance="0.5" />
+        <material name='geom' texture="texgeom" texuniform="true" />
+    </asset>
+    <worldbody>
+        <light pos="0 0 5" />
+        <body name="r_shoulder_pan_link" pos="0 -0.188 0">
+            <geom name="e1" type="sphere" rgba="0.6 0.6 0.6 1" pos="-0.06 0.05 0.2" size="0.05" />
+            <geom name="e2" type="sphere" rgba="0.6 0.6 0.6 1" pos=" 0.06 0.05 0.2" size="0.05" />
+            <geom name="e1p" type="sphere" rgba="0.1 0.1 0.1 1" pos="-0.06 0.09 0.2" size="0.03" />
+            <geom name="e2p" type="sphere" rgba="0.1 0.1 0.1 1" pos=" 0.06 0.09 0.2" size="0.03" />
+            <geom name="sp" type="capsule" fromto="0 0 -0.4 0 0 0.2" size="0.1" />
+            <joint name="r_shoulder_pan_joint" type="hinge" pos="0 0 0" axis="0 0 1" range="-2.2854 1.714602" damping="10.0" />
+
+            <body name="r_shoulder_lift_link" pos="0.1 0 0">
+                <geom name="sl" type="capsule" fromto="0 -0.1 0 0 0.1 0" size="0.1" />
+                <joint name="r_shoulder_lift_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.5236 1.3963" damping="10.0" />
+
+                <body name="r_upper_arm_roll_link" pos="0 0 0">
+                    <geom name="uar" type="capsule" fromto="-0.1 0 0 0.1 0 0" size="0.02" />
+                    <joint name="r_upper_arm_roll_joint" type="hinge" pos="0 0 0" axis="1 0 0" range="-3.9 0.8" damping="0.1" />
+
+                    <body name="r_upper_arm_link" pos="0 0 0">
+                        <geom name="ua" type="capsule" fromto="0 0 0 0.4 0 0" size="0.06" />
+
+                        <body name="r_elbow_flex_link" pos="0.4 0 0">
+                            <geom name="ef" type="capsule" fromto="0 -0.02 0 0.0 0.02 0" size="0.06" />
+                            <joint name="r_elbow_flex_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-2.3213 0" damping="1.0" />
+
+                            <body name="r_forearm_roll_link" pos="0 0 0">
+                                <geom name="fr" type="capsule" fromto="-0.1 0 0 0.1 0 0" size="0.02" />
+                                <joint name="r_forearm_roll_joint" type="hinge" limited="false" pos="0 0 0" axis="1 0 0" damping=".1" />
+
+                                <body name="r_forearm_link" pos="0 0 0">
+                                    <geom name="fa" type="capsule" fromto="0 0 0 0.321 0 0" size="0.05" />
+
+                                    <body name="r_wrist_flex_link" pos="0.321 0 0">
+                                        <geom name="wf" type="capsule" fromto="0 -0.02 0 0 0.02 0" size="0.01" />
+                                        <joint name="r_wrist_flex_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-2.094 0" damping=".1" />
+
+                                        <body name="r_wrist_roll_link" pos="0 0 0">
+                                            <geom name="wr" type="capsule" fromto="-0.02 0 0 0.02 0 0" size="0.01" />
+                                            <joint name="r_wrist_roll_joint" type="hinge" pos="0 0 0" limited="false" axis="1 0 0" damping="0.1" />
+
+                                            <body name="r_gripper_palm_link" pos="0 0 0">
+                                                <geom name="pl" type="capsule" fromto="0.05 0 -0.02 0.05 0 0.02" size="0.05" />
+
+                                                <!--
+                                                <body name="r_gripper_tool_frame" pos="0.18 0 0">
+                                                    <site name="leg_bottom" pos="0 0 -0.15" size="0.01" />
+                                                    <site name="leg_top" pos="0 0 0.15" size="0.01" />
+
+                                                    <body name="ball" pos="0 0 0">
+                                                        <geom name="ball_geom" rgba="0.8 0.6 0.6 1" type="cylinder" fromto="0 0 -0.15 0 0 0.15" size="0.028" density="2000" contype="2" conaffinity="1" />
+                                                    </body>
+                                                </body>
+                                                -->
+
+                                                <body name="r_gripper_l_finger_link" pos="0.07691 0.03 0">
+                                                    <geom name="gf3" type="capsule" fromto="0 0 0 0.09137 0.00495 0" size="0.01" />
+
+                                                    <body name="r_gripper_l_finger_tip_link" pos="0.09137 0.00495 0">
+                                                        <geom name="gf4" type="capsule" fromto="0 0 0 0.09137 0.0 0" size="0.01" />
+                                                    </body>
+                                                </body>
+
+                                                <body name="r_gripper_r_finger_link" pos="0.07691 -0.03 0">
+                                                    <geom name="gf1" type="capsule" fromto="0 0 0 0.09137 -0.00495 0" size="0.01" />
+
+                                                    <body name="r_gripper_r_finger_tip_link" pos="0.09137 -0.00495 0">
+                                                        <geom name="gf2" type="capsule" fromto="0 0 0 0.09137 0.0 0" size="0.01" />
+                                                    </body>
+                                                </body>
+                                            </body>
+                                        </body>
+                                    </body>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                </body>
+            </body>
+        </body>
+
+        <!--
+        <body name="g1" pos="0.034 0.3 -0.47" axisangle="0 1 0 0.05">
+            <geom name="g1" rgba="0.2 0.2 0.2 1" type="box" size="0.003 0.01 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="g2" pos="-0.034 0.3 -0.47" axisangle="0 1 0 -0.05">
+            <geom name="g2" rgba="0.2 0.2 0.2 1" type="box" size="0.003 0.01 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="g3" pos="0.0 0.334 -0.47" axisangle="1 0 0 -0.05">
+            <geom name="g3" rgba="0.2 0.2 0.2 1" type="box" size="0.01 0.003 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="g4" pos="0.0 0.266 -0.47" axisangle="1 0 0 0.05">
+            <geom name="g4" rgba="0.2 0.2 0.2 1" type="box" size="0.01 0.003 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="fl" pos="0.0 0.3 -0.55">
+            <geom name="fl" rgba="0.2 0.2 0.2 1" type="box" size="0.2 0.2 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="w1" pos="0.216 0.3 -0.45">
+            <geom name="w1" rgba="0.2 0.2 0.2 1" type="box" size="0.183 0.3 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="w2" pos="-0.216 0.3 -0.45">
+            <geom name="w2" rgba="0.2 0.2 0.2 1" type="box" size="0.183 0.3 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="w3" pos="0.0 0.516 -0.45">
+            <geom name="w3" rgba="0.2 0.2 0.2 1" type="box" size="0.032 0.183 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="w4" pos="0.0 0.084 -0.45">
+            <geom name="w4" rgba="0.2 0.2 0.2 1" type="box" size="0.032 0.183 0.05" contype="1" conaffinity="1" />
+        </body>
+        -->
+
+        <body name="target" pos="0 0.25 0">
+            <joint armature="0" axis="1 0 0" damping="0" limited="false" name="target_x" pos="0 0 0" ref="0" stiffness="0" type="slide"/>
+            <joint armature="0" axis="0 1 0" damping="0" limited="false" name="target_y" pos="0 0 0" ref="0.25" stiffness="0" type="slide"/>
+            <joint armature="0" axis="0 0 1" damping="0" limited="false" name="target_z" pos="0 0 0" ref="0" stiffness="0" type="slide"/>
+            <geom conaffinity="0" contype="0" name="target" pos="0 0 0" rgba="0.9 0.2 0.2 1" size=".035" type="sphere"/>
+        </body>
+    </worldbody>
+
+    <actuator>
+        <motor joint="r_shoulder_pan_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+        <motor joint="r_shoulder_lift_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+        <motor joint="r_upper_arm_roll_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+        <motor joint="r_elbow_flex_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+        <motor joint="r_forearm_roll_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+        <motor joint="r_wrist_flex_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+        <motor joint="r_wrist_roll_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+    </actuator>
+
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/cartpole.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/cartpole.py
new file mode 100644
index 0000000..ddc3d41
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/cartpole.py
@@ -0,0 +1,193 @@
+"""
+Classic cart-pole system implemented by Rich Sutton et al.
+Copied from https://webdocs.cs.ualberta.ca/~sutton/book/code/pole.c
+"""
+
+import logging
+import math
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+from slbo.utils.dataset import Dataset, gen_dtype
+
+
+logger = logging.getLogger(__name__)
+
+
+class CartPoleEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 50
+    }
+
+    def __init__(self):
+        self.gravity = 9.8
+        self.masscart = 1.0
+        self.masspole = 0.1
+        self.total_mass = (self.masspole + self.masscart)
+        self.length = 0.5  # actually half the pole's length
+        self.polemass_length = (self.masspole * self.length)
+        self.force_mag = 10.0
+        self.tau = 0.02  # seconds between state updates
+
+        # Angle at which to fail the episode
+        self.theta_threshold_radians = 12 * 2 * math.pi / 360
+        self.x_threshold = 2.4
+
+        # Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds
+        high = np.array([
+            self.x_threshold * 2,
+            np.finfo(np.float32).max,
+            self.theta_threshold_radians * 2,
+            np.finfo(np.float32).max])
+
+        # self.action_space = spaces.Discrete(2)
+        self.action_space = \
+            spaces.Box(low=np.array([-1.0]), high=np.array([1.0]))
+        self.observation_space = spaces.Box(-high, high)
+
+        self._seed()
+        self.viewer = None
+        self.state = None
+
+        self.steps_beyond_done = None
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, action):
+        action = 1 if action[0] > .0 else 0
+        # assert self.action_space.contains(action), "%r (%s) invalid" % (action, type(action))
+        state = self.state
+        obs = self.state
+        reward = np.cos(obs[2]) - 0.01 * (obs[0] ** 2)
+
+        x, x_dot, theta, theta_dot = state
+        force = self.force_mag if action == 1 else -self.force_mag
+        costheta = math.cos(theta)
+        sintheta = math.sin(theta)
+        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass
+        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta * costheta / self.total_mass))
+        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass
+        x = x + self.tau * x_dot
+        x_dot = x_dot + self.tau * xacc
+        theta = theta + self.tau * theta_dot
+        theta_dot = theta_dot + self.tau * thetaacc
+        self.state = (x, x_dot, theta, theta_dot)
+        '''
+        done = x < -self.x_threshold \
+            or x > self.x_threshold \
+            or theta < -self.theta_threshold_radians \
+            or theta > self.theta_threshold_radians
+        done = bool(done)
+
+        if not done:
+            reward = 1.0
+        elif self.steps_beyond_done is None:
+            # Pole just fell!
+            self.steps_beyond_done = 0
+            reward = 1.0
+        else:
+            if self.steps_beyond_done == 0:
+                logger.warning("You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.")
+            self.steps_beyond_done += 1
+            reward = 0.0
+        '''
+        done = False
+        self.steps_beyond_done = None
+
+        return np.array(self.state), reward, done, {}
+
+    def _reset(self):
+        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))
+        self.steps_beyond_done = None
+        return np.array(self.state)
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        screen_width = 600
+        screen_height = 400
+
+        world_width = self.x_threshold * 2
+        scale = screen_width / world_width
+        carty = 100  # TOP OF CART
+        polewidth = 10.0
+        polelen = scale * 1.0
+        cartwidth = 50.0
+        cartheight = 30.0
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(screen_width, screen_height)
+            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2
+            axleoffset = cartheight / 4.0
+            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            self.carttrans = rendering.Transform()
+            cart.add_attr(self.carttrans)
+            self.viewer.add_geom(cart)
+            l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2
+            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            pole.set_color(.8, .6, .4)
+            self.poletrans = rendering.Transform(translation=(0, axleoffset))
+            pole.add_attr(self.poletrans)
+            pole.add_attr(self.carttrans)
+            self.viewer.add_geom(pole)
+            self.axle = rendering.make_circle(polewidth / 2)
+            self.axle.add_attr(self.poletrans)
+            self.axle.add_attr(self.carttrans)
+            self.axle.set_color(.5, .5, .8)
+            self.viewer.add_geom(self.axle)
+            self.track = rendering.Line((0, carty), (screen_width, carty))
+            self.track.set_color(0, 0, 0)
+            self.viewer.add_geom(self.track)
+
+        if self.state is None:
+            return None
+
+        x = self.state
+        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART
+        self.carttrans.set_translation(cartx, carty)
+        self.poletrans.set_rotation(-x[2])
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        x = obs[:, 0]
+        theta = obs[:, 2]
+        return -(np.cos(theta) - 0.01 * (x ** 2))
+
+    def verify(self, n=2000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cartpoleO001.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cartpoleO001.py
new file mode 100644
index 0000000..2407b55
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cartpoleO001.py
@@ -0,0 +1,178 @@
+"""
+Classic cart-pole system implemented by Rich Sutton et al.
+Copied from https://webdocs.cs.ualberta.ca/~sutton/book/code/pole.c
+"""
+
+import logging
+import math
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+
+
+logger = logging.getLogger(__name__)
+
+
+class CartPoleEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 50
+    }
+
+    def __init__(self):
+        self.gravity = 9.8
+        self.masscart = 1.0
+        self.masspole = 0.1
+        self.total_mass = (self.masspole + self.masscart)
+        self.length = 0.5  # actually half the pole's length
+        self.polemass_length = (self.masspole * self.length)
+        self.force_mag = 10.0
+        self.tau = 0.02  # seconds between state updates
+
+        # Angle at which to fail the episode
+        self.theta_threshold_radians = 12 * 2 * math.pi / 360
+        self.x_threshold = 2.4
+
+        # Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds
+        high = np.array([
+            self.x_threshold * 2,
+            np.finfo(np.float32).max,
+            self.theta_threshold_radians * 2,
+            np.finfo(np.float32).max])
+
+        # self.action_space = spaces.Discrete(2)
+        self.action_space = \
+            spaces.Box(low=np.array([-1.0]), high=np.array([1.0]))
+        self.observation_space = spaces.Box(-high, high)
+
+        self._seed()
+        self.viewer = None
+        self.state = None
+
+        self.steps_beyond_done = None
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, action):
+        action = 1 if action[0] > .0 else 0
+        # assert self.action_space.contains(action), "%r (%s) invalid" % (action, type(action))
+        state = self.state
+        obs = self.state
+        reward = np.cos(obs[2]) - 0.01 * (obs[0] ** 2)
+
+        x, x_dot, theta, theta_dot = state
+        force = self.force_mag if action == 1 else -self.force_mag
+        costheta = math.cos(theta)
+        sintheta = math.sin(theta)
+        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass
+        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta * costheta / self.total_mass))
+        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass
+        x = x + self.tau * x_dot
+        x_dot = x_dot + self.tau * xacc
+        theta = theta + self.tau * theta_dot
+        theta_dot = theta_dot + self.tau * thetaacc
+        self.state = (x, x_dot, theta, theta_dot)
+        '''
+        done = x < -self.x_threshold \
+            or x > self.x_threshold \
+            or theta < -self.theta_threshold_radians \
+            or theta > self.theta_threshold_radians
+        done = bool(done)
+
+        if not done:
+            reward = 1.0
+        elif self.steps_beyond_done is None:
+            # Pole just fell!
+            self.steps_beyond_done = 0
+            reward = 1.0
+        else:
+            if self.steps_beyond_done == 0:
+                logger.warning("You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.")
+            self.steps_beyond_done += 1
+            reward = 0.0
+        '''
+        done = False
+        self.steps_beyond_done = None
+
+        ob = np.array(self.state)
+        ob += np.random.uniform(low=-0.01, high=0.01, size=ob.shape)
+
+        return ob, reward, done, {}
+
+    def _reset(self):
+        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))
+        self.steps_beyond_done = None
+        return np.array(self.state)
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        screen_width = 600
+        screen_height = 400
+
+        world_width = self.x_threshold * 2
+        scale = screen_width / world_width
+        carty = 100  # TOP OF CART
+        polewidth = 10.0
+        polelen = scale * 1.0
+        cartwidth = 50.0
+        cartheight = 30.0
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(screen_width, screen_height)
+            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2
+            axleoffset = cartheight / 4.0
+            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            self.carttrans = rendering.Transform()
+            cart.add_attr(self.carttrans)
+            self.viewer.add_geom(cart)
+            l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2
+            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            pole.set_color(.8, .6, .4)
+            self.poletrans = rendering.Transform(translation=(0, axleoffset))
+            pole.add_attr(self.poletrans)
+            pole.add_attr(self.carttrans)
+            self.viewer.add_geom(pole)
+            self.axle = rendering.make_circle(polewidth / 2)
+            self.axle.add_attr(self.poletrans)
+            self.axle.add_attr(self.carttrans)
+            self.axle.set_color(.5, .5, .8)
+            self.viewer.add_geom(self.axle)
+            self.track = rendering.Line((0, carty), (screen_width, carty))
+            self.track.set_color(0, 0, 0)
+            self.viewer.add_geom(self.track)
+
+        if self.state is None:
+            return None
+
+        x = self.state
+        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART
+        self.carttrans.set_translation(cartx, carty)
+        self.poletrans.set_rotation(-x[2])
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        x = obs[:, 0]
+        theta = obs[:, 2]
+        return -(np.cos(theta) - 0.01 * (x ** 2))
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cartpoleO01.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cartpoleO01.py
new file mode 100644
index 0000000..62516c3
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cartpoleO01.py
@@ -0,0 +1,177 @@
+"""
+Classic cart-pole system implemented by Rich Sutton et al.
+Copied from https://webdocs.cs.ualberta.ca/~sutton/book/code/pole.c
+"""
+
+import logging
+import math
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+
+logger = logging.getLogger(__name__)
+
+
+class CartPoleEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 50
+    }
+
+    def __init__(self):
+        self.gravity = 9.8
+        self.masscart = 1.0
+        self.masspole = 0.1
+        self.total_mass = (self.masspole + self.masscart)
+        self.length = 0.5  # actually half the pole's length
+        self.polemass_length = (self.masspole * self.length)
+        self.force_mag = 10.0
+        self.tau = 0.02  # seconds between state updates
+
+        # Angle at which to fail the episode
+        self.theta_threshold_radians = 12 * 2 * math.pi / 360
+        self.x_threshold = 2.4
+
+        # Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds
+        high = np.array([
+            self.x_threshold * 2,
+            np.finfo(np.float32).max,
+            self.theta_threshold_radians * 2,
+            np.finfo(np.float32).max])
+
+        # self.action_space = spaces.Discrete(2)
+        self.action_space = \
+            spaces.Box(low=np.array([-1.0]), high=np.array([1.0]))
+        self.observation_space = spaces.Box(-high, high)
+
+        self._seed()
+        self.viewer = None
+        self.state = None
+
+        self.steps_beyond_done = None
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, action):
+        action = 1 if action[0] > .0 else 0
+        # assert self.action_space.contains(action), "%r (%s) invalid" % (action, type(action))
+        state = self.state
+        obs = self.state
+        reward = np.cos(obs[2]) - 0.01 * (obs[0] ** 2)
+
+        x, x_dot, theta, theta_dot = state
+        force = self.force_mag if action == 1 else -self.force_mag
+        costheta = math.cos(theta)
+        sintheta = math.sin(theta)
+        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass
+        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta * costheta / self.total_mass))
+        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass
+        x = x + self.tau * x_dot
+        x_dot = x_dot + self.tau * xacc
+        theta = theta + self.tau * theta_dot
+        theta_dot = theta_dot + self.tau * thetaacc
+        self.state = (x, x_dot, theta, theta_dot)
+        '''
+        done = x < -self.x_threshold \
+            or x > self.x_threshold \
+            or theta < -self.theta_threshold_radians \
+            or theta > self.theta_threshold_radians
+        done = bool(done)
+
+        if not done:
+            reward = 1.0
+        elif self.steps_beyond_done is None:
+            # Pole just fell!
+            self.steps_beyond_done = 0
+            reward = 1.0
+        else:
+            if self.steps_beyond_done == 0:
+                logger.warning("You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.")
+            self.steps_beyond_done += 1
+            reward = 0.0
+        '''
+        done = False
+        self.steps_beyond_done = None
+
+        ob = np.array(self.state)
+        ob += np.random.uniform(low=-0.1, high=0.1, size=ob.shape)
+
+        return ob, reward, done, {}
+
+    def _reset(self):
+        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))
+        self.steps_beyond_done = None
+        return np.array(self.state)
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        screen_width = 600
+        screen_height = 400
+
+        world_width = self.x_threshold * 2
+        scale = screen_width / world_width
+        carty = 100  # TOP OF CART
+        polewidth = 10.0
+        polelen = scale * 1.0
+        cartwidth = 50.0
+        cartheight = 30.0
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(screen_width, screen_height)
+            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2
+            axleoffset = cartheight / 4.0
+            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            self.carttrans = rendering.Transform()
+            cart.add_attr(self.carttrans)
+            self.viewer.add_geom(cart)
+            l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2
+            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            pole.set_color(.8, .6, .4)
+            self.poletrans = rendering.Transform(translation=(0, axleoffset))
+            pole.add_attr(self.poletrans)
+            pole.add_attr(self.carttrans)
+            self.viewer.add_geom(pole)
+            self.axle = rendering.make_circle(polewidth / 2)
+            self.axle.add_attr(self.poletrans)
+            self.axle.add_attr(self.carttrans)
+            self.axle.set_color(.5, .5, .8)
+            self.viewer.add_geom(self.axle)
+            self.track = rendering.Line((0, carty), (screen_width, carty))
+            self.track.set_color(0, 0, 0)
+            self.viewer.add_geom(self.track)
+
+        if self.state is None:
+            return None
+
+        x = self.state
+        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART
+        self.carttrans.set_translation(cartx, carty)
+        self.poletrans.set_rotation(-x[2])
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        x = obs[:, 0]
+        theta = obs[:, 2]
+        return -(np.cos(theta) - 0.01 * (x ** 2))
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahA003.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahA003.py
new file mode 100644
index 0000000..e759126
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahA003.py
@@ -0,0 +1,81 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from slbo.envs import BaseModelBasedEnv
+from gym.envs.mujoco import mujoco_env
+
+
+class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/half_cheetah.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        action = np.array(action)
+        action += np.random.uniform(low=-0.03, high=0.03, size=action.shape)
+        start_ob = self._get_obs()
+        reward_run = start_ob[8]
+
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        reward_ctrl = -0.1 * np.square(action).sum()
+
+        reward = reward_run + reward_ctrl
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        qpos = self.init_qpos + \
+            self.np_random.uniform(low=-.1, high=.1, size=self.model.nq)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahA01.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahA01.py
new file mode 100644
index 0000000..e496056
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahA01.py
@@ -0,0 +1,81 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/half_cheetah.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        action = np.array(action)
+        action += np.random.uniform(low=-0.1, high=0.1, size=action.shape)
+        start_ob = self._get_obs()
+        reward_run = start_ob[8]
+
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        reward_ctrl = -0.1 * np.square(action).sum()
+
+        reward = reward_run + reward_ctrl
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        qpos = self.init_qpos + \
+            self.np_random.uniform(low=-.1, high=.1, size=self.model.nq)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahO001.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahO001.py
new file mode 100644
index 0000000..252505f
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahO001.py
@@ -0,0 +1,80 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/half_cheetah.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        start_ob = self._get_obs()
+        reward_run = start_ob[8]
+
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        reward_ctrl = -0.1 * np.square(action).sum()
+
+        reward = reward_run + reward_ctrl
+        done = False
+        ob += np.random.uniform(low=-0.01, high=0.01, size=ob.shape)
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        qpos = self.init_qpos + \
+            self.np_random.uniform(low=-.1, high=.1, size=self.model.nq)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahO01.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahO01.py
new file mode 100644
index 0000000..4e6fe93
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahO01.py
@@ -0,0 +1,80 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/half_cheetah.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        start_ob = self._get_obs()
+        reward_run = start_ob[8]
+
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        reward_ctrl = -0.1 * np.square(action).sum()
+
+        reward = reward_run + reward_ctrl
+        done = False
+        ob += np.random.uniform(low=-0.1, high=0.1, size=ob.shape)
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        qpos = self.init_qpos + \
+            self.np_random.uniform(low=-.1, high=.1, size=self.model.nq)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fant.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fant.py
new file mode 100644
index 0000000..29633e2
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fant.py
@@ -0,0 +1,84 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+
+from slbo.envs import BaseModelBasedEnv
+
+
+class AntEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/ant.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        ob = self._get_obs()
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = old_ob[13]
+        reward_height = -3.0 * np.square(old_ob[0] - 0.57)
+
+        # the alive bonus
+        height = ob[0]
+        done = (height > 1.0) or (height < 0.2)
+        alive_reward = float(not done)
+
+        reward = reward_run + reward_ctrl + reward_height + alive_reward
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[2:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        qpos = self.init_qpos + \
+            self.np_random.uniform(size=self.model.nq, low=-.1, high=.1)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        # self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 13]
+        reward_height = -3.0 * np.square(obs[:, 0] - 0.57)
+
+        height = next_obs[:, 0]
+        done = np.logical_or((height > 1.0), (height < 0.2))
+        alive_reward = 1.0 - np.array(done, dtype=np.float)
+
+        reward = reward_run + reward_ctrl + reward_height + alive_reward
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+
+    def mb_step(self, states, actions, next_states):
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        height = next_states[:, 0]
+        done = np.logical_or((height > 1.0), (height < 0.2))
+        return rewards, done
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fhopper.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fhopper.py
new file mode 100644
index 0000000..84edf3d
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fhopper.py
@@ -0,0 +1,91 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HopperEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=4):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/hopper.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = old_ob[5]
+        reward_height = -3.0 * np.square(old_ob[0] - 1.3)
+        height, ang = ob[0], ob[1]
+        done = (height <= 0.7) or (abs(ang) >= 0.2)
+        alive_reward = float(not done)
+        reward = reward_run + reward_ctrl + reward_height + alive_reward
+
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-.005, high=.005, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-.005, high=.005, size=self.model.nv)
+        )
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 2
+        self.viewer.cam.distance = self.model.stat.extent * 0.75
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 5]
+        reward_height = -3.0 * np.square(obs[:, 0] - 1.3)
+        height, ang = next_obs[:, 0], next_obs[:, 1]
+        done = np.logical_or(height <= 0.7, abs(ang) >= 0.2)
+        alive_reward = 1.0 - np.array(done, dtype=np.float)
+        reward = reward_run + reward_ctrl + reward_height + alive_reward
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        # reward_height = -3.0 * tf.square(next_obs[:, 1] - 1.3)
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+        raise NotImplementedError
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        height, ang = next_states[:, 0], next_states[:, 1]
+        done = np.logical_or(height <= 0.7, abs(ang) >= 0.2)
+        return rewards, done
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fswimmer.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fswimmer.py
new file mode 100644
index 0000000..86bdc80
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fswimmer.py
@@ -0,0 +1,69 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+import os
+
+
+class fixedSwimmerEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+
+    def __init__(self):
+        dir_path = os.path.dirname(os.path.realpath(__file__))
+        mujoco_env.MujocoEnv.__init__(self, '%s/assets/fixed_swimmer.xml' % dir_path, 4)
+        utils.EzPickle.__init__(self)
+
+    def _step(self, a):
+        ctrl_cost_coeff = 0.0001
+
+        """
+        xposbefore = self.model.data.qpos[0, 0]
+        self.do_simulation(a, self.frame_skip)
+        xposafter = self.model.data.qpos[0, 0]
+        """
+
+        self.xposbefore = self.model.data.site_xpos[0][0] / self.dt
+        self.do_simulation(a, self.frame_skip)
+        self.xposafter = self.model.data.site_xpos[0][0] / self.dt
+        self.pos_diff = self.xposafter - self.xposbefore
+
+        reward_fwd = self.xposafter - self.xposbefore
+        reward_ctrl = - ctrl_cost_coeff * np.square(a).sum()
+        reward = reward_fwd + reward_ctrl
+        ob = self._get_obs()
+        return ob, reward, False, dict(reward_fwd=reward_fwd, reward_ctrl=reward_ctrl)
+
+    def _get_obs(self):
+        qpos = self.model.data.qpos
+        qvel = self.model.data.qvel
+        return np.concatenate([qpos.flat[2:], qvel.flat, self.pos_diff.flat])
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-.1, high=.1, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-.1, high=.1, size=self.model.nv)
+        )
+        return self._get_obs()
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.0001 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, -1]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+
+    def verify(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fwalker2d.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fwalker2d.py
new file mode 100644
index 0000000..25006d3
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fwalker2d.py
@@ -0,0 +1,99 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class Walker2dEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=4):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/walker2d.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = old_ob[8]
+        reward_height = -3.0 * np.square(old_ob[0] - 1.3)
+
+        height, ang = ob[0], ob[1]
+        done = (height >= 2.0) or (height <= 0.8) or (abs(ang) >= 1.0)
+        alive_reward = float(not done)
+
+        reward = reward_run + reward_ctrl + reward_height + alive_reward
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat
+        ])
+
+    def reset_model(self):
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-.005, high=.005, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-.005, high=.005, size=self.model.nv)
+        )
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 2
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward_height = -3.0 * np.square(next_obs[:, 0] - 1.3)
+        height, ang = next_obs[:, 0], next_obs[:, 1]
+        done = np.logical_or(
+            np.logical_or(height >= 2.0, height <= 0.8),
+            np.abs(ang) >= 1.0
+        )
+        alive_reward = 1.0 - np.array(done, dtype=np.float)
+        reward = reward_run + reward_ctrl + reward_height + alive_reward
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        # reward_height = -3.0 * tf.square(next_obs[:, 1] - 1.3)
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+        raise NotImplementedError
+
+    def verify(self):
+        pass
+
+    def mb_step(self, states, actions, next_states):
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        height, ang = next_states[:, 0], next_states[:, 1]
+        done = np.logical_or(
+            np.logical_or(height >= 2.0, height <= 0.8),
+            np.abs(ang) >= 1.0
+        )
+        return rewards, done
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_humanoid.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_humanoid.py
new file mode 100644
index 0000000..7d220ba
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_humanoid.py
@@ -0,0 +1,89 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+import numpy as np
+from gym.envs.mujoco import mujoco_env
+from gym import utils
+from slbo.envs import BaseModelBasedEnv
+
+
+class HumanoidEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self):
+        mujoco_env.MujocoEnv.__init__(self, 'humanoid.xml', 5)
+        utils.EzPickle.__init__(self)
+
+    def _get_obs(self):
+        data = self.model.data
+        return np.concatenate([data.qpos.flat[2:],
+                               data.qvel.flat,
+                               data.cinert.flat,
+                               data.cvel.flat,
+                               data.qfrc_actuator.flat,
+                               data.cfrc_ext.flat])
+
+    def _step(self, a):
+        data = self.model.data
+        action = a
+        if getattr(self, 'action_space', None):
+            action = np.clip(a, self.action_space.low,
+                             self.action_space.high)
+
+        # reward
+        alive_bonus = 5.0
+        lin_vel_cost = 0.25 / 0.015 * data.qvel.flat[0]
+        quad_ctrl_cost = 0.1 * np.square(action).sum()
+        quad_impact_cost = .5e-6 * np.square(data.cfrc_ext).sum()
+        quad_impact_cost = min(quad_impact_cost, 10)
+
+        self.do_simulation(action, self.frame_skip)
+        reward = lin_vel_cost - quad_ctrl_cost - quad_impact_cost + alive_bonus
+        qpos = self.model.data.qpos
+        done = bool((qpos[2] < 1.0) or (qpos[2] > 2.0))
+        return self._get_obs(), reward, done, dict(reward_linvel=lin_vel_cost, reward_quadctrl=-quad_ctrl_cost, reward_alive=alive_bonus, reward_impact=-quad_impact_cost)
+
+    def reset_model(self):
+        c = 0.01
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-c, high=c, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-c, high=c, size=self.model.nv,)
+        )
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 1
+        self.viewer.cam.distance = self.model.stat.extent * 1.0
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = 0.25 / 0.015 * obs[:, 22]
+
+        quad_impact_cost = .5e-6 * np.square(obs[:, -84:]).sum()
+        quad_impact_cost = min(quad_impact_cost, 10)
+
+        height = next_obs[:, 0]
+        done = np.logical_or((height > 2.0), (height < 1.0))
+        alive_reward = 5 * (1.0 - np.array(done, dtype=np.float))
+
+        reward = reward_run + reward_ctrl + (-quad_impact_cost) + alive_reward
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+
+        height = next_states[:, 0]
+        done = np.logical_or((height > 2.0), (height < 1.0))
+        return rewards, done
+
+    def verify(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_nostopslimhumanoid.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_nostopslimhumanoid.py
new file mode 100644
index 0000000..1c87b20
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_nostopslimhumanoid.py
@@ -0,0 +1,81 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+import numpy as np
+from gym.envs.mujoco import mujoco_env
+from gym import utils
+from slbo.envs import BaseModelBasedEnv
+
+
+class HumanoidEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self):
+        mujoco_env.MujocoEnv.__init__(self, 'humanoid.xml', 5)
+        utils.EzPickle.__init__(self)
+
+    def _get_obs(self):
+        data = self.model.data
+        return np.concatenate([data.qpos.flat[2:],
+                               data.qvel.flat])
+
+    def _step(self, a):
+        data = self.model.data
+        action = a
+        if getattr(self, 'action_space', None):
+            action = np.clip(a, self.action_space.low,
+                             self.action_space.high)
+        qpos = self.model.data.qpos
+        done = bool((qpos[2] < 1.0) or (qpos[2] > 2.0))
+
+        # reward
+        alive_bonus = 5 * (1 - float(done))
+        lin_vel_cost = 0.25 / 0.015 * data.qvel.flat[0]
+        quad_ctrl_cost = 0.1 * np.square(action).sum()
+        quad_impact_cost = 0.0
+
+        self.do_simulation(action, self.frame_skip)
+        reward = lin_vel_cost - quad_ctrl_cost - quad_impact_cost + alive_bonus
+        done = False
+        return self._get_obs(), reward, done, dict(reward_linvel=lin_vel_cost, reward_quadctrl=-quad_ctrl_cost, reward_alive=alive_bonus, reward_impact=-quad_impact_cost)
+
+    def reset_model(self):
+        c = 0.01
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-c, high=c, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-c, high=c, size=self.model.nv,)
+        )
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 1
+        self.viewer.cam.distance = self.model.stat.extent * 1.0
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = 0.25 / 0.015 * obs[:, 22]
+
+        quad_impact_cost = 0.0
+
+        height = next_obs[:, 0]
+        done = np.logical_or((height > 2.0), (height < 1.0))
+        alive_reward = 5 * (1.0 - np.array(done, dtype=np.float))
+
+        reward = reward_run + reward_ctrl + (-quad_impact_cost) + alive_reward
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_pendulumO001.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_pendulumO001.py
new file mode 100644
index 0000000..de873df
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_pendulumO001.py
@@ -0,0 +1,138 @@
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+from os import path
+
+
+class PendulumEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 30
+    }
+
+    def __init__(self):
+        self.max_speed = 8
+        self.max_torque = 2.
+        self.dt = .05
+        self.viewer = None
+
+        high = np.array([1., 1., self.max_speed])
+        self.action_space = spaces.Box(low=-self.max_torque, high=self.max_torque, shape=(1,))
+        self.observation_space = spaces.Box(low=-high, high=high)
+
+        self._seed()
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, u):
+        th, thdot = self.state  # th := theta
+        '''
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+        '''
+
+        # for the reward
+        y, x, thetadot = np.cos(th), np.sin(th), thdot
+        u = np.clip(u, -self.max_torque, self.max_torque)[0]
+        costs = y + .1 * np.abs(x) + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        reward = -costs
+
+        g = 10.
+        m = 1.
+        l = 1.
+        dt = self.dt
+
+        self.last_u = u  # for rendering
+        # costs = angle_normalize(th) ** 2 + .1 * thdot ** 2 + .001 * (u ** 2)
+
+        newthdot = thdot + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3. / (m * l ** 2) * u) * dt
+        newth = th + newthdot * dt
+        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)  # pylint: disable=E1111
+
+        self.state = np.array([newth, newthdot])
+        ob = self._get_obs()
+        ob += np.random.uniform(low=-0.01, high=0.01, size=ob.shape)
+        return ob, reward, False, {}
+
+    def _reset(self):
+        high = np.array([np.pi, 1])
+        self.state = self.np_random.uniform(low=-high, high=high)
+        self.last_u = None
+        return self._get_obs()
+
+    def _get_obs(self):
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(500, 500)
+            self.viewer.set_bounds(-2.2, 2.2, -2.2, 2.2)
+            rod = rendering.make_capsule(1, .2)
+            rod.set_color(.8, .3, .3)
+            self.pole_transform = rendering.Transform()
+            rod.add_attr(self.pole_transform)
+            self.viewer.add_geom(rod)
+            axle = rendering.make_circle(.05)
+            axle.set_color(0, 0, 0)
+            self.viewer.add_geom(axle)
+            fname = path.join(path.dirname(__file__), "assets/clockwise.png")
+            self.img = rendering.Image(fname, 1., 1.)
+            self.imgtrans = rendering.Transform()
+            self.img.add_attr(self.imgtrans)
+
+        self.viewer.add_onetime(self.img)
+        self.pole_transform.set_rotation(self.state[0] + np.pi / 2)
+        if self.last_u:
+            self.imgtrans.scale = (-self.last_u / 2, np.abs(self.last_u) / 2)
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+        dist_vec = obs[:, -3:]
+        reward_dist = - np.linalg.norm(dist_vec, axis=1)
+        reward_ctrl = - np.sum(np.square(acts), axis=1)
+        reward = reward_dist + reward_ctrl
+
+        # for the reward
+        y, x, thetadot = np.cos(th), np.sin(th), thdot
+        u = np.clip(u, -self.max_torque, self.max_torque)[0]
+        costs = y + .1 * x + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        reward = -costs
+
+        def _get_obs(self):
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+
+        """
+        y, x, thetadot = obs[:, 0], obs[:, 1], obs[:, 2]
+        u = np.clip(acts[:, 0], -self.max_torque, self.max_torque)
+        costs = y + .1 * np.abs(x) + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        return costs
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
+
+
+def angle_normalize(x):
+    return (((x + np.pi) % (2 * np.pi)) - np.pi)
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_pendulumO01.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_pendulumO01.py
new file mode 100644
index 0000000..dbd460d
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_pendulumO01.py
@@ -0,0 +1,138 @@
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+from os import path
+
+
+class PendulumEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 30
+    }
+
+    def __init__(self):
+        self.max_speed = 8
+        self.max_torque = 2.
+        self.dt = .05
+        self.viewer = None
+
+        high = np.array([1., 1., self.max_speed])
+        self.action_space = spaces.Box(low=-self.max_torque, high=self.max_torque, shape=(1,))
+        self.observation_space = spaces.Box(low=-high, high=high)
+
+        self._seed()
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, u):
+        th, thdot = self.state  # th := theta
+        '''
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+        '''
+
+        # for the reward
+        y, x, thetadot = np.cos(th), np.sin(th), thdot
+        u = np.clip(u, -self.max_torque, self.max_torque)[0]
+        costs = y + .1 * np.abs(x) + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        reward = -costs
+
+        g = 10.
+        m = 1.
+        l = 1.
+        dt = self.dt
+
+        self.last_u = u  # for rendering
+        # costs = angle_normalize(th) ** 2 + .1 * thdot ** 2 + .001 * (u ** 2)
+
+        newthdot = thdot + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3. / (m * l ** 2) * u) * dt
+        newth = th + newthdot * dt
+        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)  # pylint: disable=E1111
+
+        self.state = np.array([newth, newthdot])
+        ob = self._get_obs()
+        ob += np.random.uniform(low=-0.1, high=0.1, size=ob.shape)
+        return ob, reward, False, {}
+
+    def _reset(self):
+        high = np.array([np.pi, 1])
+        self.state = self.np_random.uniform(low=-high, high=high)
+        self.last_u = None
+        return self._get_obs()
+
+    def _get_obs(self):
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(500, 500)
+            self.viewer.set_bounds(-2.2, 2.2, -2.2, 2.2)
+            rod = rendering.make_capsule(1, .2)
+            rod.set_color(.8, .3, .3)
+            self.pole_transform = rendering.Transform()
+            rod.add_attr(self.pole_transform)
+            self.viewer.add_geom(rod)
+            axle = rendering.make_circle(.05)
+            axle.set_color(0, 0, 0)
+            self.viewer.add_geom(axle)
+            fname = path.join(path.dirname(__file__), "assets/clockwise.png")
+            self.img = rendering.Image(fname, 1., 1.)
+            self.imgtrans = rendering.Transform()
+            self.img.add_attr(self.imgtrans)
+
+        self.viewer.add_onetime(self.img)
+        self.pole_transform.set_rotation(self.state[0] + np.pi / 2)
+        if self.last_u:
+            self.imgtrans.scale = (-self.last_u / 2, np.abs(self.last_u) / 2)
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+        dist_vec = obs[:, -3:]
+        reward_dist = - np.linalg.norm(dist_vec, axis=1)
+        reward_ctrl = - np.sum(np.square(acts), axis=1)
+        reward = reward_dist + reward_ctrl
+
+        # for the reward
+        y, x, thetadot = np.cos(th), np.sin(th), thdot
+        u = np.clip(u, -self.max_torque, self.max_torque)[0]
+        costs = y + .1 * x + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        reward = -costs
+
+        def _get_obs(self):
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+
+        """
+        y, x, thetadot = obs[:, 0], obs[:, 1], obs[:, 2]
+        u = np.clip(acts[:, 0], -self.max_torque, self.max_torque)
+        costs = y + .1 * np.abs(x) + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        return costs
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
+
+
+def angle_normalize(x):
+    return (((x + np.pi) % (2 * np.pi)) - np.pi)
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_slimhumanoid.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_slimhumanoid.py
new file mode 100644
index 0000000..cfaf6ad
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_slimhumanoid.py
@@ -0,0 +1,82 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+import numpy as np
+from gym.envs.mujoco import mujoco_env
+from gym import utils
+from slbo.envs import BaseModelBasedEnv
+
+
+class HumanoidEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self):
+        mujoco_env.MujocoEnv.__init__(self, 'humanoid.xml', 5)
+        utils.EzPickle.__init__(self)
+
+    def _get_obs(self):
+        data = self.model.data
+        return np.concatenate([data.qpos.flat[2:],
+                               data.qvel.flat])
+
+    def _step(self, a):
+        data = self.model.data
+        action = a
+        if getattr(self, 'action_space', None):
+            action = np.clip(a, self.action_space.low,
+                             self.action_space.high)
+
+        # reward
+        alive_bonus = 5.0
+        lin_vel_cost = 0.25 / 0.015 * data.qvel.flat[0]
+        quad_ctrl_cost = 0.1 * np.square(action).sum()
+        quad_impact_cost = 0.0
+
+        self.do_simulation(action, self.frame_skip)
+        reward = lin_vel_cost - quad_ctrl_cost - quad_impact_cost + alive_bonus
+        qpos = self.model.data.qpos
+        done = bool((qpos[2] < 1.0) or (qpos[2] > 2.0))
+        return self._get_obs(), reward, done, dict(reward_linvel=lin_vel_cost, reward_quadctrl=-quad_ctrl_cost, reward_alive=alive_bonus, reward_impact=-quad_impact_cost)
+
+    def reset_model(self):
+        c = 0.01
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-c, high=c, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-c, high=c, size=self.model.nv,)
+        )
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 1
+        self.viewer.cam.distance = self.model.stat.extent * 1.0
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = 0.25 / 0.015 * obs[:, 22]
+
+        quad_impact_cost = 0.0
+
+        height = next_obs[:, 0]
+        done = np.logical_or((height > 2.0), (height < 1.0))
+        alive_reward = 5 * (1.0 - np.array(done, dtype=np.float))
+
+        reward = reward_run + reward_ctrl + (-quad_impact_cost) + alive_reward
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        height = next_states[:, 0]
+        done = np.logical_or((height > 2.0), (height < 1.0))
+        return rewards, done
+
+    def verify(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/half_cheetah.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/half_cheetah.py
new file mode 100644
index 0000000..97be1c5
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/half_cheetah.py
@@ -0,0 +1,76 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/half_cheetah.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        start_ob = self._get_obs()
+        reward_run = start_ob[8]
+
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        reward_ctrl = -0.1 * np.square(action).sum()
+
+        reward = reward_run + reward_ctrl
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                             self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        qpos = self.init_qpos + \
+            self.np_random.uniform(low=-.1, high=.1, size=self.model.nq)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/hopper.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/hopper.py
new file mode 100644
index 0000000..bb2f509
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/hopper.py
@@ -0,0 +1,84 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HopperEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=4):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/hopper.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = old_ob[5]
+        reward_height = -3.0 * np.square(old_ob[0] - 1.3)
+        reward = reward_run + reward_ctrl + reward_height + 1.0
+
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-.005, high=.005, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-.005, high=.005, size=self.model.nv)
+        )
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 2
+        self.viewer.cam.distance = self.model.stat.extent * 0.75
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 5]
+        reward_height = -3.0 * np.square(obs[:, 0] - 1.3)
+        reward = reward_run + reward_ctrl + reward_height + 1.0
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        # reward_height = -3.0 * tf.square(next_obs[:, 1] - 1.3)
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+        raise NotImplementedError
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/inverted_pendulum.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/inverted_pendulum.py
new file mode 100644
index 0000000..f05af7b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/inverted_pendulum.py
@@ -0,0 +1,74 @@
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.utils.dataset import Dataset, gen_dtype
+from lunzi.Logger import logger
+
+
+class InvertedPendulumEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+
+    def __init__(self):
+        utils.EzPickle.__init__(self)
+        mujoco_env.MujocoEnv.__init__(self, 'inverted_pendulum.xml', 2)
+
+    def _step(self, a):
+        # reward = 1.0
+        reward = self._get_reward()
+        self.do_simulation(a, self.frame_skip)
+        ob = self._get_obs()
+        # notdone = np.isfinite(ob).all() and (np.abs(ob[1]) <= .2)
+        # done = not notdone
+        done = False
+        return ob, reward, done, {}
+
+    def reset_model(self):
+        qpos = self.init_qpos + self.np_random.uniform(size=self.model.nq, low=-0.01, high=0.01)
+        qvel = self.init_qvel + self.np_random.uniform(size=self.model.nv, low=-0.01, high=0.01)
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def _get_reward(self):
+        old_ob = self._get_obs()
+        reward = -((old_ob[1]) ** 2)
+        return reward
+
+    def _get_obs(self):
+        return np.concatenate([self.model.data.qpos, self.model.data.qvel]).ravel()
+
+    def viewer_setup(self):
+        v = self.viewer
+        v.cam.trackbodyid = 0
+        v.cam.distance = v.model.stat.extent
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        return ((obs[:, 1]) ** 2)
+
+    def verify(self, n=2000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/mountain_car.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/mountain_car.py
new file mode 100644
index 0000000..01ed157
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/mountain_car.py
@@ -0,0 +1,199 @@
+# -*- coding: utf-8 -*-
+"""
+@author: Olivier Sigaud
+A merge between two sources:
+* Adaptation of the MountainCar Environment from the "FAReinforcement" library
+of Jose Antonio Martin H. (version 1.0), adapted by  'Tom Schaul, tom@idsia.ch'
+and then modified by Arnaud de Broissia
+* the OpenAI/gym MountainCar environment
+itself from
+https://webdocs.cs.ualberta.ca/~sutton/MountainCar/MountainCar1.cp
+"""
+
+import math
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+from slbo.utils.dataset import Dataset, gen_dtype
+from lunzi.Logger import logger
+
+
+class Continuous_MountainCarEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 30
+    }
+
+    def __init__(self):
+        self.min_action = -1.0
+        self.max_action = 1.0
+        self.min_position = -1.2
+        self.max_position = 0.6
+        self.max_speed = 0.07
+        self.goal_position = 0.45  # was 0.5 in gym, 0.45 in Arnaud de Broissia's version
+        self.power = 0.0015
+
+        self.low_state = np.array([self.min_position, -self.max_speed])
+        self.high_state = np.array([self.max_position, self.max_speed])
+
+        self.viewer = None
+
+        self.action_space = spaces.Box(self.min_action, self.max_action, shape=(1,))
+        self.observation_space = spaces.Box(self.low_state, self.high_state)
+
+        self._seed()
+        self.reset()
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, action):
+
+        position = self.state[0]
+        velocity = self.state[1]
+        force = min(max(action[0], -1.0), 1.0)
+        #reward = position
+
+        velocity += force * self.power - 0.0025 * math.cos(3 * position)
+        if (velocity > self.max_speed):
+            velocity = self.max_speed
+        if (velocity < -self.max_speed):
+            velocity = -self.max_speed
+        position += velocity
+        if (position > self.max_position):
+            position = self.max_position
+        if (position < self.min_position):
+            position = self.min_position
+        if (position == self.min_position and velocity < 0):
+            velocity = 0
+
+
+        done = bool(position >= self.goal_position)
+
+        reward = 0
+        if done:
+            reward = 100.0
+        reward -= math.pow(action[0], 2) * 0.1
+
+
+        #done = False
+        self.state = np.array([position, velocity])
+        return self.state, reward, done, {}
+
+    def _reset(self):
+        self.state = np.array([self.np_random.uniform(low=-0.6, high=-0.4), 0])
+        return np.array(self.state)
+
+#    def get_state(self):
+#        return self.state
+
+    def _height(self, xs):
+        return np.sin(3 * xs) * .45 + .55
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        screen_width = 600
+        screen_height = 400
+
+        world_width = self.max_position - self.min_position
+        scale = screen_width / world_width
+        carwidth = 40
+        carheight = 20
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(screen_width, screen_height)
+            xs = np.linspace(self.min_position, self.max_position, 100)
+            ys = self._height(xs)
+            xys = list(zip((xs - self.min_position) * scale, ys * scale))
+
+            self.track = rendering.make_polyline(xys)
+            self.track.set_linewidth(4)
+            self.viewer.add_geom(self.track)
+
+            clearance = 10
+
+            l, r, t, b = -carwidth / 2, carwidth / 2, carheight, 0
+            car = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            car.add_attr(rendering.Transform(translation=(0, clearance)))
+            self.cartrans = rendering.Transform()
+            car.add_attr(self.cartrans)
+            self.viewer.add_geom(car)
+            frontwheel = rendering.make_circle(carheight / 2.5)
+            frontwheel.set_color(.5, .5, .5)
+            frontwheel.add_attr(rendering.Transform(translation=(carwidth / 4, clearance)))
+            frontwheel.add_attr(self.cartrans)
+            self.viewer.add_geom(frontwheel)
+            backwheel = rendering.make_circle(carheight / 2.5)
+            backwheel.add_attr(rendering.Transform(translation=(-carwidth / 4, clearance)))
+            backwheel.add_attr(self.cartrans)
+            backwheel.set_color(.5, .5, .5)
+            self.viewer.add_geom(backwheel)
+            flagx = (self.goal_position - self.min_position) * scale
+            flagy1 = self._height(self.goal_position) * scale
+            flagy2 = flagy1 + 50
+            flagpole = rendering.Line((flagx, flagy1), (flagx, flagy2))
+            self.viewer.add_geom(flagpole)
+            flag = rendering.FilledPolygon([(flagx, flagy2), (flagx, flagy2 - 10), (flagx + 25, flagy2 - 5)])
+            flag.set_color(.8, .8, 0)
+            self.viewer.add_geom(flag)
+
+        pos = self.state[0]
+        self.cartrans.set_translation((pos - self.min_position) * scale, self._height(pos) * scale)
+        self.cartrans.set_rotation(math.cos(3 * pos))
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        dones = rewards > 0
+        return rewards, dones
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+        position = self.state[0]
+        velocity = self.state[1]
+        force = min(max(action[0], -1.0), 1.0)
+        reward = position
+        """
+        positions = next_obs[:,0]
+        rewards = np.zeros(len(positions))
+        for i in range(len(rewards)):
+            if positions[i] >= self.goal_position:
+                rewards[i] = 100.0
+
+        rewards = rewards - np.power(acts[:,0], 2) * 0.1
+
+        return -rewards
+
+    def verify(self, n=2000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pendulum.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pendulum.py
new file mode 100644
index 0000000..a64b39c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pendulum.py
@@ -0,0 +1,155 @@
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+from os import path
+from slbo.utils.dataset import Dataset, gen_dtype
+from lunzi.Logger import logger
+
+
+class PendulumEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 30
+    }
+
+    def __init__(self):
+        self.max_speed = 8
+        self.max_torque = 2.
+        self.dt = .05
+        self.viewer = None
+
+        high = np.array([1., 1., self.max_speed])
+        self.action_space = spaces.Box(low=-self.max_torque, high=self.max_torque, shape=(1,))
+        self.observation_space = spaces.Box(low=-high, high=high)
+
+        self._seed()
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, u):
+        th, thdot = self.state  # th := theta
+        '''
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+        '''
+
+        # for the reward
+        y, x, thetadot = np.cos(th), np.sin(th), thdot
+        u = np.clip(u, -self.max_torque, self.max_torque)[0]
+        costs = y + .1 * np.abs(x) + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        reward = -costs
+
+        g = 10.
+        m = 1.
+        l = 1.
+        dt = self.dt
+
+        self.last_u = u  # for rendering
+        # costs = angle_normalize(th) ** 2 + .1 * thdot ** 2 + .001 * (u ** 2)
+
+        newthdot = thdot + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3. / (m * l ** 2) * u) * dt
+        newth = th + newthdot * dt
+        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)  # pylint: disable=E1111
+
+        self.state = np.array([newth, newthdot])
+        return self._get_obs(), reward, False, {}
+
+    def _reset(self):
+        high = np.array([np.pi, 1])
+        self.state = self.np_random.uniform(low=-high, high=high)
+        self.last_u = None
+        return self._get_obs()
+
+    def _get_obs(self):
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(500, 500)
+            self.viewer.set_bounds(-2.2, 2.2, -2.2, 2.2)
+            rod = rendering.make_capsule(1, .2)
+            rod.set_color(.8, .3, .3)
+            self.pole_transform = rendering.Transform()
+            rod.add_attr(self.pole_transform)
+            self.viewer.add_geom(rod)
+            axle = rendering.make_circle(.05)
+            axle.set_color(0, 0, 0)
+            self.viewer.add_geom(axle)
+            fname = path.join(path.dirname(__file__), "assets/clockwise.png")
+            self.img = rendering.Image(fname, 1., 1.)
+            self.imgtrans = rendering.Transform()
+            self.img.add_attr(self.imgtrans)
+
+        self.viewer.add_onetime(self.img)
+        self.pole_transform.set_rotation(self.state[0] + np.pi / 2)
+        if self.last_u:
+            self.imgtrans.scale = (-self.last_u / 2, np.abs(self.last_u) / 2)
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+        dist_vec = obs[:, -3:]
+        reward_dist = - np.linalg.norm(dist_vec, axis=1)
+        reward_ctrl = - np.sum(np.square(acts), axis=1)
+        reward = reward_dist + reward_ctrl
+
+        # for the reward
+        y, x, thetadot = np.cos(th), np.sin(th), thdot
+        u = np.clip(u, -self.max_torque, self.max_torque)[0]
+        costs = y + .1 * x + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        reward = -costs
+
+        def _get_obs(self):
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+
+        """
+        y, x, thetadot = obs[:, 0], obs[:, 1], obs[:, 2]
+        u = np.clip(acts[:, 0], -self.max_torque, self.max_torque)
+        costs = y + .1 * np.abs(x) + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        return costs
+
+    def verify(self, n=2000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
+
+
+def angle_normalize(x):
+    return (((x + np.pi) % (2 * np.pi)) - np.pi)
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_cartpole.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_cartpole.py
new file mode 100644
index 0000000..a048535
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_cartpole.py
@@ -0,0 +1,53 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+
+
+class CartpoleEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+    PENDULUM_LENGTH = 0.6
+
+    def __init__(self):
+        utils.EzPickle.__init__(self)
+        dir_path = os.path.dirname(os.path.realpath(__file__))
+        mujoco_env.MujocoEnv.__init__(self, '%s/assets/cartpole.xml' % dir_path, 2)
+
+    def _step(self, a):
+        self.do_simulation(a, self.frame_skip)
+        ob = self._get_obs()
+
+        cost_lscale = CartpoleEnv.PENDULUM_LENGTH
+        reward = np.exp(
+            -np.sum(np.square(self._get_ee_pos(ob) - np.array([0.0, CartpoleEnv.PENDULUM_LENGTH]))) / (cost_lscale ** 2)
+        )
+        reward -= 0.01 * np.sum(np.square(a))
+
+        done = False
+        return ob, reward, done, {}
+
+    def reset_model(self):
+        qpos = self.init_qpos + np.random.normal(0, 0.1, np.shape(self.init_qpos))
+        qvel = self.init_qvel + np.random.normal(0, 0.1, np.shape(self.init_qvel))
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def _get_obs(self):
+        return np.concatenate([self.model.data.qpos, self.model.data.qvel]).ravel()
+
+    @staticmethod
+    def _get_ee_pos(x):
+        x0, theta = x[0], x[1]
+        return np.array([
+            x0 - CartpoleEnv.PENDULUM_LENGTH * np.sin(theta),
+            -CartpoleEnv.PENDULUM_LENGTH * np.cos(theta)
+        ])
+
+    def viewer_setup(self):
+        v = self.viewer
+        v.cam.trackbodyid = 0
+        v.cam.distance = v.model.stat.extent
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_cheetah.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_cheetah.py
new file mode 100644
index 0000000..1f73b66
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_cheetah.py
@@ -0,0 +1,54 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+
+
+class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+
+    def __init__(self):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.realpath(__file__))
+        mujoco_env.MujocoEnv.__init__(self, '%s/assets/half_cheetah.xml' % dir_path, 5)
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = ob[0] - 0.0 * np.square(ob[2])
+        reward = reward_run + reward_ctrl
+
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            (self.model.data.qpos.flat[:1] - self.prev_qpos[:1]) / self.dt,
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        qpos = self.init_qpos + np.random.normal(loc=0, scale=0.001, size=self.model.nq)
+        qvel = self.init_qvel + np.random.normal(loc=0, scale=0.001, size=self.model.nv)
+        self.set_state(qpos, qvel)
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.25
+        self.viewer.cam.elevation = -55
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_pusher.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_pusher.py
new file mode 100644
index 0000000..854f477
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_pusher.py
@@ -0,0 +1,85 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+
+
+class PusherEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+
+    def __init__(self):
+        dir_path = os.path.dirname(os.path.realpath(__file__))
+        mujoco_env.MujocoEnv.__init__(self, '%s/assets/pusher.xml' % dir_path, 4)
+        utils.EzPickle.__init__(self)
+        self.reset_model()
+
+    def _step(self, a):
+        obj_pos = self.get_body_com("object"),
+        vec_1 = obj_pos - self.get_body_com("tips_arm")
+        vec_2 = obj_pos - self.get_body_com("goal")
+
+        reward_near = -np.sum(np.abs(vec_1))
+        reward_dist = -np.sum(np.abs(vec_2))
+        reward_ctrl = -np.square(a).sum()
+        reward = 1.25 * reward_dist + 0.1 * reward_ctrl + 0.5 * reward_near
+
+        self.do_simulation(a, self.frame_skip)
+        ob = self._get_obs()
+        done = False
+        return ob, reward, done, {}
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = -1
+        self.viewer.cam.distance = 4.0
+
+    def reset_model(self):
+        qpos = self.init_qpos
+
+        self.goal_pos = np.asarray([0, 0])
+        self.cylinder_pos = np.array([-0.25, 0.15]) + np.random.normal(0, 0.025, [2])
+
+        qpos[-4:-2] = self.cylinder_pos
+        qpos[-2:] = self.goal_pos
+        qvel = self.init_qvel + \
+            self.np_random.uniform(low=-0.005, high=0.005, size=self.model.nv)
+        qvel[-4:] = 0
+        self.set_state(qpos, qvel)
+        self.ac_goal_pos = self.get_body_com("goal")
+
+        return self._get_obs()
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[:7],
+            self.model.data.qvel.flat[:7],
+            self.get_body_com("tips_arm"),
+            self.get_body_com("object"),
+            self.get_body_com("goal"),
+        ])
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+        to_w, og_w = 0.5, 1.25
+        tip_pos, obj_pos, goal_pos = obs[:, 14:17], obs[:, 17:20], obs[:, -3:]
+
+        tip_obj_dist = np.sum(np.abs(tip_pos - obj_pos), axis=1)
+        obj_goal_dist = np.sum(np.abs(goal_pos - obj_pos), axis=1)
+        return to_w * tip_obj_dist + og_w * obj_goal_dist
+
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward = reward_run + reward_ctrl
+        """
+        to_w, og_w = 0.5, 1.25
+        tip_pos, obj_pos, goal_pos = obs[:, 14:17], obs[:, 17:20], obs[:, -3:]
+
+        tip_obj_dist = -np.sum(np.abs(tip_pos - obj_pos), axis=1)
+        obj_goal_dist = -np.sum(np.abs(goal_pos - obj_pos), axis=1)
+        ctrl_reward = -0.1 * np.sum(np.square(acts), axis=1)
+
+        reward = to_w * tip_obj_dist + og_w * obj_goal_dist + ctrl_reward
+        return -reward
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_reacher.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_reacher.py
new file mode 100644
index 0000000..ba419ac
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_reacher.py
@@ -0,0 +1,95 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+
+
+class Reacher3DEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+
+    def __init__(self):
+        self.viewer = None
+        utils.EzPickle.__init__(self)
+        dir_path = os.path.dirname(os.path.realpath(__file__))
+        self.goal = np.zeros(3)
+        mujoco_env.MujocoEnv.__init__(self, os.path.join(dir_path, 'assets/reacher3d.xml'), 2)
+
+    def _step(self, a):
+        self.do_simulation(a, self.frame_skip)
+        ob = self._get_obs()
+        reward = -np.sum(np.square(self.get_EE_pos(ob[None]) - self.goal))
+        reward -= 0.01 * np.square(a).sum()
+        done = False
+        return ob, reward, done, dict(reward_dist=0, reward_ctrl=0)
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 1
+        self.viewer.cam.distance = 2.5
+        self.viewer.cam.elevation = -30
+        self.viewer.cam.azimuth = 270
+
+    def reset_model(self):
+        qpos, qvel = np.copy(self.init_qpos), np.copy(self.init_qvel)
+        qpos[-3:] += np.random.normal(loc=0, scale=0.1, size=[3])
+        qvel[-3:] = 0
+        self.goal = qpos[-3:]
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def _get_obs(self):
+        raw_obs = np.concatenate([
+            self.model.data.qpos.flat, self.model.data.qvel.flat[:-3],
+        ])
+
+        EE_pos = np.reshape(self.get_EE_pos(raw_obs[None]), [-1])
+
+        return np.concatenate([raw_obs, EE_pos])
+
+    def get_EE_pos(self, states):
+        theta1, theta2, theta3, theta4, theta5, theta6, theta7 = \
+            states[:, :1], states[:, 1:2], states[:, 2:3], states[:, 3:4], states[:, 4:5], states[:, 5:6], states[:, 6:]
+
+        rot_axis = np.concatenate([np.cos(theta2) * np.cos(theta1), np.cos(theta2) * np.sin(theta1), -np.sin(theta2)],
+                                  axis=1)
+        rot_perp_axis = np.concatenate([-np.sin(theta1), np.cos(theta1), np.zeros(theta1.shape)], axis=1)
+        cur_end = np.concatenate([
+            0.1 * np.cos(theta1) + 0.4 * np.cos(theta1) * np.cos(theta2),
+            0.1 * np.sin(theta1) + 0.4 * np.sin(theta1) * np.cos(theta2) - 0.188,
+            -0.4 * np.sin(theta2)
+        ], axis=1)
+
+        for length, hinge, roll in [(0.321, theta4, theta3), (0.16828, theta6, theta5)]:
+            perp_all_axis = np.cross(rot_axis, rot_perp_axis)
+            x = np.cos(hinge) * rot_axis
+            y = np.sin(hinge) * np.sin(roll) * rot_perp_axis
+            z = -np.sin(hinge) * np.cos(roll) * perp_all_axis
+            new_rot_axis = x + y + z
+            new_rot_perp_axis = np.cross(new_rot_axis, rot_axis)
+            new_rot_perp_axis[np.linalg.norm(new_rot_perp_axis, axis=1) < 1e-30] = \
+                rot_perp_axis[np.linalg.norm(new_rot_perp_axis, axis=1) < 1e-30]
+            new_rot_perp_axis /= np.linalg.norm(new_rot_perp_axis, axis=1, keepdims=True)
+            rot_axis, rot_perp_axis, cur_end = new_rot_axis, new_rot_perp_axis, cur_end + length * new_rot_axis
+
+        return cur_end
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+        def obs_cost_fn(self, obs):
+            self.ENV.goal = obs[:, 7: 10]
+            ee_pos = obs[:, -3:]
+            return np.sum(np.square(ee_pos - self.ENV.goal), axis=1)
+
+        @staticmethod
+        def ac_cost_fn(acs):
+            return 0.01 * np.sum(np.square(acs), axis=1)
+        """
+        reward_ctrl = -0.01 * np.sum(np.square(acts), axis=1)
+        goal = obs[:, 7: 10]
+        ee_pos = obs[:, -3:]
+
+        reward = -np.sum(np.square(ee_pos - goal), axis=1) + reward_ctrl
+        return -reward
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/reacher.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/reacher.py
new file mode 100644
index 0000000..636cdde
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/reacher.py
@@ -0,0 +1,66 @@
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class ReacherEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self):
+        utils.EzPickle.__init__(self)
+        mujoco_env.MujocoEnv.__init__(self, 'reacher.xml', 2)
+
+    def _step(self, a):
+        vec = self.get_body_com("fingertip") - self.get_body_com("target")
+
+        if getattr(self, 'action_space', None):
+            a = np.clip(a, self.action_space.low,
+                        self.action_space.high)
+        reward_dist = - np.linalg.norm(vec)
+        reward_ctrl = - np.square(a).sum()
+        reward = reward_dist + reward_ctrl
+        self.do_simulation(a, self.frame_skip)
+        ob = self._get_obs()
+        done = False
+        return ob, reward, done, dict(reward_dist=reward_dist, reward_ctrl=reward_ctrl)
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 0
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                             self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        qpos = self.np_random.uniform(low=-0.1, high=0.1, size=self.model.nq) + self.init_qpos
+        while True:
+            self.goal = self.np_random.uniform(low=-.2, high=.2, size=2)
+            if np.linalg.norm(self.goal) < 2:
+                break
+        qpos[-2:] = self.goal
+        qvel = self.init_qvel + self.np_random.uniform(low=-.005, high=.005, size=self.model.nv)
+        qvel[-2:] = 0
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def _get_obs(self):
+        theta = self.model.data.qpos.flat[:2]
+        return np.concatenate([
+            np.cos(theta),
+            np.sin(theta),
+            self.model.data.qpos.flat[2:],
+            self.model.data.qvel.flat[:2],
+            self.get_body_com("fingertip") - self.get_body_com("target")
+        ])
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        dist_vec = obs[:, -3:]
+        reward_dist = - np.linalg.norm(dist_vec, axis=1)
+        reward_ctrl = - np.sum(np.square(acts), axis=1)
+        reward = reward_dist + reward_ctrl
+        return -reward
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/readme.md b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/readme.md
new file mode 100644
index 0000000..28f84a0
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/readme.md
@@ -0,0 +1,5 @@
+# reference
+
+1. mbbl/env/gym_env/walker.py or mbbl/env/gym_env/reacher.py 
+
+2. https://github.com/openai/gym/blob/v0.7.4/gym/envs/mujoco/half_cheetah.py
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/swimmer.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/swimmer.py
new file mode 100644
index 0000000..33e8d79
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/swimmer.py
@@ -0,0 +1,77 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class SwimmerEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=4):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/swimmer.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        ob = self._get_obs()
+
+        reward_ctrl = -0.0001 * np.square(action).sum()
+        reward_run = old_ob[3]
+        reward = reward_run + reward_ctrl
+
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            # (self.model.data.qpos.flat[:1] - self.prev_qpos[:1]) / self.dt,
+            # self.get_body_comvel("torso")[:1],
+            self.model.data.qpos.flat[2:],
+            self.model.data.qvel.flat,
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                             self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-.1, high=.1, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-.1, high=.1, size=self.model.nv)
+        )
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.0001 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 3]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        """
+        reward_ctrl = -0.0001 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+        raise NotImplementedError
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/walker2d.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/walker2d.py
new file mode 100644
index 0000000..1031b32
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/walker2d.py
@@ -0,0 +1,84 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class Walker2dEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=4):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/walker2d.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = old_ob[8]
+        reward_height = -3.0 * np.square(old_ob[0] - 1.3)
+        reward = reward_run + reward_ctrl + reward_height + 1.0
+
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-.005, high=.005, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-.005, high=.005, size=self.model.nv)
+        )
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 2
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward_height = -3.0 * np.square(obs[:, 0] - 1.3)
+        reward = reward_run + reward_ctrl + reward_height + 1.0
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        # reward_height = -3.0 * tf.square(next_obs[:, 1] - 1.3)
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+        raise NotImplementedError
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym_env.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym_env.py
new file mode 100644
index 0000000..32b9434
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym_env.py
@@ -0,0 +1,141 @@
+# import gym
+# import gym.wrappers
+# import gym.envs
+# import gym.spaces
+# import traceback
+# import logging
+#
+# try:
+#     from gym.wrappers.monitoring import logger as monitor_logger
+#
+#     monitor_logger.setLevel(logging.WARNING)
+# except Exception as e:
+#     traceback.print_exc()
+#
+# import os
+# import numpy as np
+# from rllab.misc import logger
+#
+#
+# class CappedCubicVideoSchedule(object):
+#     # Copied from gym, since this method is frequently moved around
+#     def __call__(self, count):
+#         if count < 1000:
+#             return int(round(count ** (1. / 3))) ** 3 == count
+#         else:
+#             return count % 1000 == 0
+#
+#
+# class FixedIntervalVideoSchedule(object):
+#     def __init__(self, interval):
+#         self.interval = interval
+#
+#     def __call__(self, count):
+#         return count % self.interval == 0
+#
+#
+# class NoVideoSchedule(object):
+#     def __call__(self, count):
+#         return False
+#
+#
+# class GymEnv(object):
+#     def __init__(self, env_name, record_video=True, video_schedule=None, log_dir=None, record_log=True,
+#                  force_reset=False):
+#         if log_dir is None:
+#             if logger.get_snapshot_dir() is None:
+#                 logger.log("Warning: skipping Gym environment monitoring since snapshot_dir not configured.")
+#             else:
+#                 log_dir = os.path.join(logger.get_snapshot_dir(), "gym_log")
+#
+#         env = gym.make(env_name)
+#         self.env = env
+#         self.env_id = env.spec.id
+#
+#         assert not (not record_log and record_video)
+#
+#         if log_dir is None or record_log is False:
+#             self.monitoring = False
+#         else:
+#             if not record_video:
+#                 video_schedule = NoVideoSchedule()
+#             else:
+#                 if video_schedule is None:
+#                     video_schedule = CappedCubicVideoSchedule()
+#             self.env = gym.wrappers.Monitor(self.env, log_dir, video_callable=video_schedule, force=True)
+#             self.monitoring = True
+#
+#         self._observation_space = env.observation_space
+#         self._action_space = env.action_space
+#         self._horizon = env.spec.tags['wrapper_config.TimeLimit.max_episode_steps']
+#         self._log_dir = log_dir
+#         self._force_reset = force_reset
+#
+#         self.metadata = {'render.modes': ['human', 'rgb_array']}
+#         self.reward_range = (-np.inf, np.inf)
+#         self.unwrapped = self
+#         self._configured = False
+#         self.spec = None
+#
+#     @property
+#     def inner_env(self):
+#         env = self.env
+#         while hasattr(env, "env"):
+#             env = env.env
+#         return env
+#
+#     @property
+#     def observation_space(self):
+#         return self._observation_space
+#
+#     @property
+#     def action_space(self):
+#         return self._action_space
+#
+#     @property
+#     def horizon(self):
+#         return self._horizon
+#
+#     def reset(self):
+#         if self._force_reset and hasattr(self.env, 'stats_recorder'):
+#             recorder = self.env.stats_recorder
+#             if recorder is not None:
+#                 recorder.done = True
+#
+#         return self.env.reset()
+#
+#     def step(self, action_or_predicted_result):
+#         if isinstance(action_or_predicted_result, dict):
+#             return self._step_with_predicted_dynamics(**action_or_predicted_result)
+#         else:
+#             next_obs, reward, done, info = self.env.step(action_or_predicted_result)
+#             return next_obs, reward, done, info
+#
+#     def _step_with_predicted_dynamics(self, next_obs, reward, done):
+#         qpos = self.inner_env.model.data.qpos.flatten()
+#         qvel = self.inner_env.model.data.qvel.flatten()
+#         self.env.env.set_state(qpos, qvel)
+#         return next_obs, reward, done, {}
+#
+#     def cost_np_vec(self, obs, acts, next_obs):
+#         return self.env.env.cost_np_vec(obs, acts, next_obs)
+#
+#     def cost_tf_vec(self, obs, acts, next_obs):
+#         return self.env.env.cost_tf_vec(obs, acts, next_obs)
+#
+#     def render(self, **kwargs):
+#         return self.env.render(**kwargs)
+#
+#     def terminate(self):
+#         if self.monitoring:
+#             self.env._close()
+#             if self._log_dir is not None:
+#                 print("""
+#     ***************************
+#     Training finished! You can upload results to OpenAI Gym by running the following command:
+#     python scripts/submit_gym.py %s
+#     ***************************
+#                 """ % self._log_dir)
+#
+#     def get_geom_xpos(self):
+#         return self.inner_env.data.geom_xpos
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/neural_env.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/neural_env.py
new file mode 100644
index 0000000..243aa73
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/neural_env.py
@@ -0,0 +1,112 @@
+import numpy as np
+
+
+class NeuralNetEnv:
+
+    def __init__(self, env, inner_env, dynamics):
+        self.vectorized = True
+        self.env = env
+        self.inner_env = inner_env
+        self.is_done = getattr(inner_env, 'is_done', lambda x, y: np.asarray([False] * len(x)))
+        self.dynamics = dynamics
+
+    @property
+    def observation_space(self):
+        return self.env.observation_space
+
+    @property
+    def action_space(self):
+        return self.env.action_space
+
+    @property
+    def spec(self):
+        return self.env.spec
+
+    def reset(self):
+        self.state = self.env.reset()
+        observation = np.copy(self.state)
+        return observation
+
+    def step(self, action, use_states=None):
+        action = np.clip(action, *self.action_space.bounds)
+        if use_states is not None:
+            next_observation = self.dynamics.predict([use_states], [action])[0]
+            obs_dim = self.env.observation_space.shape[0]
+            next_observation[:obs_dim] = np.clip(next_observation[:obs_dim], *self.observation_space.bounds)
+            next_observation[:obs_dim] = np.clip(next_observation[:obs_dim], -1e5, 1e5)
+        else:
+            next_observation = self.dynamics.predict([self.state], [action])[0]
+            next_observation = np.clip(next_observation, *self.observation_space.bounds)
+            next_observation = np.clip(next_observation, -1e5, 1e5)
+
+        if hasattr(self.inner_env, "env"):
+            reward = - self.inner_env.env.cost_np_vec(self.state[None], action[None], np.array([next_observation]))[0]
+        else:
+            reward = - self.inner_env.cost_np_vec(self.state[None], action[None], np.array([next_observation]))[0]
+
+        done = self.is_done(self.state[None], next_observation)[0]
+        self.state = np.reshape(next_observation, -1)
+        return self.inner_env.step({"next_obs": next_observation, "reward": reward, "done": done})
+
+    def render(self):
+        print('current state:', self.state)
+
+    def vec_env_executor(self, n_envs, max_path_length):
+        return VecSimpleEnv(env=self, inner_env=self.inner_env, n_envs=n_envs, max_path_length=max_path_length)
+
+    def terminate(self):
+        self.env.terminate()
+
+
+class VecSimpleEnv(object):
+
+    def __init__(self, env, inner_env, n_envs, max_path_length):
+        self.env = env
+        self.inner_env = inner_env
+        self.n_envs = n_envs
+        self.num_envs = n_envs
+        self.ts = np.zeros((self.n_envs,))
+        self.max_path_length = max_path_length
+        self.obs_dim = env.observation_space.shape[0]
+        self.states = np.zeros((self.n_envs, self.obs_dim))
+
+    def reset(self, dones=None):
+        if dones is None:
+            dones = np.asarray([True] * self.n_envs)
+        else:
+            dones = np.cast['bool'](dones)
+        for i, done in enumerate(dones):
+            if done:
+                self.states[i] = self.env.reset()
+        self.ts[dones] = 0
+        return self.states[dones]
+
+    def step(self, actions, use_states=None):
+        self.ts += 1
+        actions = np.clip(actions, *self.env.action_space.bounds)
+        next_observations = self.get_next_observation(actions, use_states=use_states)
+        if use_states is not None:
+            obs_dim = self.env.observation_space.shape[0]
+            next_observations[:, :obs_dim] = np.clip(next_observations[:, :obs_dim], *self.env.observation_space.bounds)
+            next_observations[:, :obs_dim] = np.clip(next_observations[:, :obs_dim], -1e5, 1e5)
+        else:
+            next_observations = np.clip(next_observations, *self.env.observation_space.bounds)
+            next_observations = np.clip(next_observations, -1e5, 1e5)
+        if hasattr(self.env.inner_env, "cost_np_vec"):
+            rewards = - self.env.inner_env.cost_np_vec(self.states, actions, next_observations)
+        else:
+            rewards = - self.env.inner_env.env.cost_np_vec(self.states, actions, next_observations)
+        self.states = next_observations
+        dones = self.env.is_done(self.states, next_observations)
+        dones[self.ts >= self.max_path_length] = True
+        if np.any(dones):
+            self.reset(dones)
+        return self.states, rewards, dones, dict()
+
+    def get_next_observation(self, actions, use_states=None):
+        if use_states is not None:
+            return self.env.dynamics.predict(use_states, actions)
+        return self.env.dynamics.predict(self.states, actions)
+
+    def terminate(self):
+        self.env.terminate()
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/proxy_env.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/proxy_env.py
new file mode 100644
index 0000000..6381ab4
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/proxy_env.py
@@ -0,0 +1,89 @@
+from gym.core import Env
+from gym.spaces import Box as GymBox
+from gym.wrappers.monitoring import Monitor
+import numpy as np
+import tensorflow as tf
+
+
+class Box:
+
+    def __init__(self, gym_box: GymBox):
+        self.gym_box = gym_box
+
+    @property
+    def flat_dim(self):
+        return np.prod(self.gym_box.shape)
+
+    @property
+    def shape(self):
+        return self.gym_box.shape
+
+    @property
+    def dtype(self):
+        return tf.float32
+
+    @property
+    def bounds(self):
+        return self.gym_box.low, self.gym_box.high
+
+    def flatten_n(self, xs):
+        xs = np.asarray(xs)
+        return xs.reshape((xs.shape[0], -1))
+
+    def sample(self):
+        return self.gym_box.sample()
+
+    def flatten(self, x):
+        return np.asarray(x).flatten()
+
+    def __repr__(self):
+        return "Box Wrapper of shape {}".format(self.shape)
+
+    def __eq__(self, other):
+        return self.gym_box.__eq__(other)
+
+
+class ProxyEnv(Env):
+
+    def __init__(self, wrapped_env: Env):
+        self._wrapped_env = wrapped_env
+        self._wrapped_observation_space = Box(wrapped_env.observation_space)
+        self._wrapped_action_space = Box(wrapped_env.action_space)
+
+    @property
+    def wrapped_env(self):
+        return self._wrapped_env
+
+    def reset(self, **kwargs):
+        return self._wrapped_env.reset(**kwargs)
+
+    @property
+    def action_space(self):
+        return self._wrapped_action_space
+
+    @property
+    def observation_space(self):
+        return self._wrapped_observation_space
+
+    def step(self, action, **kwargs):
+        return self._wrapped_env.step(action, **kwargs)
+
+    def render(self, *args, **kwargs):
+        return self._wrapped_env.render(*args, **kwargs)
+
+    def log_diagnostics(self, paths, *args, **kwargs):
+        self._wrapped_env.log_diagnostics(paths, *args, **kwargs)
+
+    @property
+    def horizon(self):
+        return self._wrapped_env.horizon
+
+    def terminate(self):
+        if isinstance(self._wrapped_env, Monitor):
+            self._wrapped_env._close()
+
+    def get_param_values(self):
+        return self._wrapped_env.get_param_values()
+
+    def set_param_values(self, params):
+        self._wrapped_env.set_param_values(params)
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/vec_env.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/vec_env.py
new file mode 100644
index 0000000..d0369e1
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/vec_env.py
@@ -0,0 +1,45 @@
+import numpy as np
+from libs.misc import tensor_utils
+
+
+class VecEnvExecutor(object):
+    def __init__(self, envs, max_path_length, **kwargs):
+        self.envs = envs
+        self._action_space = envs[0].action_space
+        self._observation_space = envs[0].observation_space
+        self.ts = np.zeros(len(self.envs), dtype='int')
+        self.max_path_length = max_path_length
+
+    def step(self, action_n, **kwargs):
+        all_results = [env.step(a) for (a, env) in zip(action_n, self.envs)]
+        obs, rewards, dones, env_infos = list(map(list, list(zip(*all_results))))
+        dones = np.asarray(dones)
+        rewards = np.asarray(rewards)
+        self.ts += 1
+        if self.max_path_length is not None:
+            dones[self.ts >= self.max_path_length] = True
+        for (i, done) in enumerate(dones):
+            if done:
+                obs[i] = self.envs[i].reset()
+                self.ts[i] = 0
+        return obs, rewards, dones, tensor_utils.stack_tensor_dict_list(env_infos)
+
+    def reset(self):
+        results = [env.reset() for env in self.envs]
+        self.ts[:] = 0
+        return results
+
+    @property
+    def num_envs(self):
+        return len(self.envs)
+
+    @property
+    def action_space(self):
+        return self._action_space
+
+    @property
+    def observation_space(self):
+        return self._observation_space
+
+    def terminate(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/__init__.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/__init__.py
new file mode 100644
index 0000000..5c7f19c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/__init__.py
@@ -0,0 +1 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/ant_env.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/ant_env.py
new file mode 100644
index 0000000..2a300af
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/ant_env.py
@@ -0,0 +1,50 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from rllab.envs.mujoco import ant_env
+from rllab.envs.base import Step
+from slbo.envs import BaseModelBasedEnv
+
+
+class AntEnv(ant_env.AntEnv, BaseModelBasedEnv):
+    def get_current_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat,  # 15
+            self.model.data.qvel.flat,  # 14
+            # np.clip(self.model.data.cfrc_ext, -1, 1).flat,  # 84
+            self.get_body_xmat("torso").flat,  # 9
+            self.get_body_com("torso"),  # 9
+            self.get_body_comvel("torso"),  # 3
+        ]).reshape(-1)
+
+    def step(self, action):
+        self.forward_dynamics(action)
+        comvel = self.get_body_comvel("torso")
+        forward_reward = comvel[0]
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+        ctrl_cost = 0.5 * 1e-2 * np.sum(np.square(action / scaling))
+        contact_cost = 0.
+        # contact_cost = 0.5 * 1e-3 * np.sum(
+        #     np.square(np.clip(self.model.data.cfrc_ext, -1, 1))),
+        survive_reward = 0.05
+        reward = forward_reward - ctrl_cost - contact_cost + survive_reward
+        state = self._state
+        notdone = np.isfinite(state).all() and state[2] >= 0.2 and state[2] <= 1.0
+        done = not notdone
+        ob = self.get_current_obs()
+        return Step(ob, float(reward), done)
+
+    def mb_step(self, states: np.ndarray, actions: np.ndarray, next_states: np.ndarray):
+        comvel = next_states[..., -3:]
+        forward_reward = comvel[..., 0]
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+        ctrl_cost = 0.5 * 1e-2 * np.sum(np.square(actions / scaling), axis=-1)
+        contact_cost = 0.
+        # contact_cost = 0.5 * 1e-3 * np.sum(
+        #     np.square(np.clip(self.model.data.cfrc_ext, -1, 1))),
+        survive_reward = 0.05
+        reward = forward_reward - ctrl_cost - contact_cost + survive_reward
+        notdone = np.all([next_states[..., 2] >= 0.2, next_states[..., 2] <= 1.0], axis=0)
+        return reward, 1. - notdone
+
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/half_cheetah_env.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/half_cheetah_env.py
new file mode 100644
index 0000000..29b1502
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/half_cheetah_env.py
@@ -0,0 +1,20 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from rllab.envs.mujoco import half_cheetah_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HalfCheetahEnv(half_cheetah_env.HalfCheetahEnv, BaseModelBasedEnv):
+    def get_current_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat,  # 9
+            self.model.data.qvel.flat,  # 9
+            self.get_body_com("torso").flat,  # 3
+            self.get_body_comvel("torso").flat,  # 3
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        actions = np.clip(actions, *self.action_bounds)
+        reward_ctrl = -0.05 * np.sum(np.square(actions), axis=-1)
+        reward_fwd = next_states[..., 21]
+        return reward_ctrl + reward_fwd, np.zeros_like(reward_fwd, dtype=np.bool)
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/hopper_env.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/hopper_env.py
new file mode 100644
index 0000000..23c9c50
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/hopper_env.py
@@ -0,0 +1,26 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from rllab.envs.mujoco import hopper_env
+from rllab.envs.base import Step
+from slbo.envs import BaseModelBasedEnv
+
+
+class HopperEnv(hopper_env.HopperEnv, BaseModelBasedEnv):
+    def get_current_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat,  # 6
+            self.model.data.qvel.flat,  # 6
+            self.get_body_com("torso").flat,  # 3
+            self.get_body_comvel("torso"),  # 3
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+        vel = next_states[:, -3]
+        reward = vel + self.alive_coeff - 0.5 * self.ctrl_cost_coeff * np.sum(np.square(actions / scaling), axis=-1)
+
+        done = ~((next_states[:, 3:12] < 100).all(axis=-1) &
+                 (next_states[:, 0] > 0.7) &
+                 (np.abs(next_states[:, 2]) < 0.2))
+        return reward, done
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/humanoid_env.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/humanoid_env.py
new file mode 100644
index 0000000..a5e55ae
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/humanoid_env.py
@@ -0,0 +1,53 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from rllab.envs.mujoco import simple_humanoid_env
+from rllab.envs.base import Step
+import numpy as np
+from slbo.envs import BaseModelBasedEnv
+
+
+class HumanoidEnv(simple_humanoid_env.SimpleHumanoidEnv, BaseModelBasedEnv):
+    def get_current_obs(self):
+        data = self.model.data
+        return np.concatenate([
+            data.qpos.flat,  # 17
+            data.qvel.flat,  # 16
+            self.get_body_com("torso").flat,  # 3
+            self.get_body_comvel("torso").flat,  # 3
+        ])
+
+    def step(self, action):
+        self.forward_dynamics(action)
+        alive_bonus = self.alive_bonus
+        data = self.model.data
+
+        comvel = self.get_body_comvel("torso")
+        lin_vel_reward = comvel[0]
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+        ctrl_cost = .5 * self.ctrl_cost_coeff * np.sum(
+            np.square(action / scaling))
+        impact_cost = 0.
+        vel_deviation_cost = 0.5 * self.vel_deviation_cost_coeff * np.sum(
+            np.square(comvel[1:]))
+        reward = lin_vel_reward + alive_bonus - ctrl_cost - \
+            impact_cost - vel_deviation_cost
+        pos = data.qpos.flat[2]
+        done = pos < 0.8 or pos > 2.0
+
+        next_obs = self.get_current_obs()
+        return Step(next_obs, reward, done)
+
+    def mb_step(self, states, actions, next_states):
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+
+        alive_bonus = 0.2
+        lin_vel_reward = next_states[:, 36]
+        ctrl_cost = 5.e-4 * np.square(actions / scaling).sum(axis=1)
+        impact_cost = 0.
+        vel_deviation_cost = 5.e-3 * np.square(next_states[:, 37:39]).sum(axis=1)
+        reward = lin_vel_reward + alive_bonus - ctrl_cost - impact_cost - vel_deviation_cost
+
+        dones = (next_states[:, 2] < 0.8) | (next_states[:, 2] > 2.0)
+        return reward, dones
+
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/swimmer_env.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/swimmer_env.py
new file mode 100644
index 0000000..f39bee4
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/swimmer_env.py
@@ -0,0 +1,22 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from rllab.envs.mujoco import swimmer_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class SwimmerEnv(swimmer_env.SwimmerEnv, BaseModelBasedEnv):
+    def get_current_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat,  # 5
+            self.model.data.qvel.flat,  # 5
+            self.get_body_com("torso").flat,  # 3
+            self.get_body_comvel("torso"),  # 3
+        ]).reshape(-1)
+
+    def mb_step(self, states: np.ndarray, actions: np.ndarray, next_states: np.ndarray):
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+        ctrl_cost = 0.5 * self.ctrl_cost_coeff * np.sum(np.square(actions / scaling), axis=-1)
+        forward_reward = next_states[:, -3]
+        reward = forward_reward - ctrl_cost
+        return reward, np.zeros_like(reward, dtype=np.bool)
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/walker2d_env.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/walker2d_env.py
new file mode 100644
index 0000000..8eb16bc
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/walker2d_env.py
@@ -0,0 +1,43 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from rllab.envs.mujoco import walker2d_env
+from rllab.envs.base import Step
+from slbo.envs import BaseModelBasedEnv
+
+
+class Walker2DEnv(walker2d_env.Walker2DEnv, BaseModelBasedEnv):
+    def get_current_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat,
+            self.model.data.qvel.flat,
+            self.get_body_com("torso").flat,
+            self.get_body_comvel("torso").flat
+        ])
+
+    def step(self, action):
+        self.forward_dynamics(action)
+        forward_reward = self.get_body_comvel("torso")[0]
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+        ctrl_cost = 1e-3 * np.sum(np.square(action / scaling))
+        alive_bonus = 1.
+        reward = forward_reward - ctrl_cost + alive_bonus
+        qpos = self.model.data.qpos
+        done = not (qpos[0] > 0.8 and qpos[0] < 2.0 and qpos[2] > -1.0 and qpos[2] < 1.0)
+        next_obs = self.get_current_obs()
+        return Step(next_obs, reward, done)
+
+    def mb_step(self, states, actions, next_states):
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+
+        reward_ctrl = -0.001 * np.sum(np.square(actions / scaling), axis=-1)
+        reward_fwd = next_states[:, 21]
+        alive_bonus = 1.
+        rewards = reward_ctrl + reward_fwd + alive_bonus
+
+        dones = ~((next_states[:, 0] > 0.8) &
+                  (next_states[:, 0] < 2.0) &
+                  (next_states[:, 2] > -1.0) &
+                  (next_states[:, 2] < 1.0))
+        return rewards, dones
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/__init__.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/__init__.py
new file mode 100644
index 0000000..95924e3
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/__init__.py
@@ -0,0 +1,81 @@
+"""
+Mujoco Maze
+----------
+
+A maze environment using mujoco that supports custom tasks and robots.
+"""
+
+
+import gym
+
+from slbo.envs.mujoco_maze.ant import AntEnv
+from slbo.envs.mujoco_maze.maze_task import TaskRegistry
+from slbo.envs.mujoco_maze.point import PointEnv
+from slbo.envs.mujoco_maze.reacher import ReacherEnv
+from slbo.envs.mujoco_maze.swimmer import SwimmerEnv
+
+for maze_id in TaskRegistry.keys():
+    for i, task_cls in enumerate(TaskRegistry.tasks(maze_id)):
+        point_scale = task_cls.MAZE_SIZE_SCALING.point
+        if point_scale is not None:
+            # Point
+            gym.envs.register(
+                id=f"Point{maze_id}-v{i}",
+                entry_point="slbo.envs.mujoco_maze.maze_env:MazeEnv",
+                kwargs=dict(
+                    model_cls=PointEnv,
+                    maze_task=task_cls,
+                    maze_size_scaling=point_scale,
+                    inner_reward_scaling=task_cls.INNER_REWARD_SCALING,
+                ),
+                max_episode_steps=1000,
+                reward_threshold=task_cls.REWARD_THRESHOLD,
+            )
+
+        ant_scale = task_cls.MAZE_SIZE_SCALING.ant
+        if ant_scale is not None:
+            # Ant
+            gym.envs.register(
+                id=f"Ant{maze_id}-v{i}",
+                entry_point="slbo.envs.mujoco_maze.maze_env:MazeEnv",
+                kwargs=dict(
+                    model_cls=AntEnv,
+                    maze_task=task_cls,
+                    maze_size_scaling=ant_scale,
+                    inner_reward_scaling=task_cls.INNER_REWARD_SCALING,
+                ),
+                max_episode_steps=1000,
+                reward_threshold=task_cls.REWARD_THRESHOLD,
+            )
+
+        swimmer_scale = task_cls.MAZE_SIZE_SCALING.swimmer
+        if swimmer_scale is not None:
+            # Reacher
+            gym.envs.register(
+                id=f"Reacher{maze_id}-v{i}",
+                entry_point="mujoco_maze.maze_env:MazeEnv",
+                kwargs=dict(
+                    model_cls=ReacherEnv,
+                    maze_task=task_cls,
+                    maze_size_scaling=task_cls.MAZE_SIZE_SCALING.swimmer,
+                    inner_reward_scaling=task_cls.INNER_REWARD_SCALING,
+                ),
+                max_episode_steps=1000,
+                reward_threshold=task_cls.REWARD_THRESHOLD,
+            )
+            # Swimmer
+            gym.envs.register(
+                id=f"Swimmer{maze_id}-v{i}",
+                entry_point="mujoco_maze.maze_env:MazeEnv",
+                kwargs=dict(
+                    model_cls=SwimmerEnv,
+                    maze_task=task_cls,
+                    maze_size_scaling=task_cls.MAZE_SIZE_SCALING.swimmer,
+                    inner_reward_scaling=task_cls.INNER_REWARD_SCALING,
+                ),
+                max_episode_steps=1000,
+                reward_threshold=task_cls.REWARD_THRESHOLD,
+            )
+
+
+__version__ = "0.1.0"
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/agent_model.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/agent_model.py
new file mode 100644
index 0000000..d209d78
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/agent_model.py
@@ -0,0 +1,42 @@
+"""Common APIs for defining mujoco robot.
+"""
+from abc import ABC, abstractmethod
+from typing import Optional
+
+import numpy as np
+from gym.envs.mujoco.mujoco_env import MujocoEnv
+from gym.utils import EzPickle
+
+
+class AgentModel(ABC, MujocoEnv, EzPickle):
+    FILE: str
+    MANUAL_COLLISION: bool
+    ORI_IND: int
+    RADIUS: Optional[float] = None
+
+    def __init__(self, file_path: str, frame_skip: int) -> None:
+        MujocoEnv.__init__(self, file_path, frame_skip)
+        EzPickle.__init__(self)
+
+    def close(self):
+        if self.viewer is not None and hasattr(self.viewer, "window"):
+            import glfw
+
+            glfw.destroy_window(self.viewer.window)
+        super().close()
+
+    @abstractmethod
+    def _get_obs(self) -> np.ndarray:
+        """Returns the observation from the model.
+        """
+        pass
+
+    def get_xy(self) -> np.ndarray:
+        """Returns the coordinate of the agent.
+        """
+        pass
+
+    def set_xy(self, xy: np.ndarray) -> None:
+        """Set the coordinate of the agent.
+        """
+        pass
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/ant.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/ant.py
new file mode 100644
index 0000000..b6a725a
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/ant.py
@@ -0,0 +1,108 @@
+"""
+A four-legged robot as an explorer in the maze.
+Based on `models`_ and `gym`_ (both ant and ant-v3).
+
+.. _models: https://github.com/tensorflow/models/tree/master/research/efficient-hrl
+.. _gym: https://github.com/openai/gym
+"""
+
+from typing import Callable, Tuple
+
+import numpy as np
+
+from slbo.envs.mujoco_maze.agent_model import AgentModel
+
+ForwardRewardFn = Callable[[float, float], float]
+
+
+def forward_reward_vabs(xy_velocity: float) -> float:
+    return np.sum(np.abs(xy_velocity))
+
+
+def forward_reward_vnorm(xy_velocity: float) -> float:
+    return np.linalg.norm(xy_velocity)
+
+
+def q_inv(a):
+    return [a[0], -a[1], -a[2], -a[3]]
+
+
+def q_mult(a, b):  # multiply two quaternion
+    w = a[0] * b[0] - a[1] * b[1] - a[2] * b[2] - a[3] * b[3]
+    i = a[0] * b[1] + a[1] * b[0] + a[2] * b[3] - a[3] * b[2]
+    j = a[0] * b[2] - a[1] * b[3] + a[2] * b[0] + a[3] * b[1]
+    k = a[0] * b[3] + a[1] * b[2] - a[2] * b[1] + a[3] * b[0]
+    return [w, i, j, k]
+
+
+class AntEnv(AgentModel):
+    FILE: str = "ant.xml"
+    ORI_IND: int = 3
+    MANUAL_COLLISION: bool = False
+
+    def __init__(
+        self,
+        file_path: str,
+        forward_reward_weight: float = 1.0,
+        ctrl_cost_weight: float = 1e-4,
+        forward_reward_fn: ForwardRewardFn = forward_reward_vnorm,
+    ) -> None:
+        self._forward_reward_weight = forward_reward_weight
+        self._ctrl_cost_weight = ctrl_cost_weight
+        self._forward_reward_fn = forward_reward_fn
+        super().__init__(file_path, 5)
+
+    def _forward_reward(self, xy_pos_before: np.ndarray) -> Tuple[float, np.ndarray]:
+        xy_pos_after = self.sim.data.qpos[:2].copy()
+        xy_velocity = (xy_pos_after - xy_pos_before) / self.dt
+        return self._forward_reward_fn(xy_velocity)
+
+    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, dict]:
+        xy_pos_before = self.sim.data.qpos[:2].copy()
+        self.do_simulation(action, self.frame_skip)
+
+        forward_reward = self._forward_reward(xy_pos_before)
+        ctrl_cost = self._ctrl_cost_weight * np.square(action).sum()
+
+        return (
+            self._get_obs(),
+            self._forward_reward_weight * forward_reward - ctrl_cost,
+            False,
+            dict(reward_forward=forward_reward, reward_ctrl=-ctrl_cost),
+        )
+
+    def _get_obs(self):
+        # No cfrc observation
+        return np.concatenate(
+            [
+                self.sim.data.qpos.flat[:15],  # Ensures only ant obs.
+                self.sim.data.qvel.flat[:14],
+            ]
+        )
+
+    def reset_model(self):
+        qpos = self.init_qpos + self.np_random.uniform(
+            size=self.model.nq, low=-0.1, high=0.1,
+        )
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * 0.1
+
+        # Set everything other than ant to original position and 0 velocity.
+        qpos[15:] = self.init_qpos[15:]
+        qvel[14:] = 0.0
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def get_ori(self) -> np.ndarray:
+        ori = [0, 1, 0, 0]
+        rot = self.sim.data.qpos[self.ORI_IND : self.ORI_IND + 4]  # take the quaternion
+        ori = q_mult(q_mult(rot, ori), q_inv(rot))[1:3]  # project onto x-y plane
+        ori = np.arctan2(ori[1], ori[0])
+        return ori
+
+    def set_xy(self, xy: np.ndarray) -> None:
+        qpos = self.sim.data.qpos.copy()
+        qpos[:2] = xy
+        self.set_state(qpos, self.sim.data.qvel)
+
+    def get_xy(self) -> np.ndarray:
+        return np.copy(self.sim.data.qpos[:2])
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/ant.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/ant.xml
new file mode 100755
index 0000000..ffe156b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/ant.xml
@@ -0,0 +1,80 @@
+<mujoco model="ant">
+  <compiler inertiafromgeom="true" angle="degree" coordinate="local" />
+  <option timestep="0.02" integrator="RK4" />
+  <custom>
+    <numeric name="init_qpos" data="0.0 0.0 0.55 1.0 0.0 0.0 0.0 0.0 1.0 0.0 -1.0 0.0 -1.0 0.0 1.0" />
+  </custom>
+  <default>
+    <joint limited="true" armature="1" damping="1" />
+    <geom condim="3" conaffinity="0" margin="0.01" friction="1 0.5 0.5" solref=".02 1" solimp=".8 .8 .01" rgba="0.8 0.6 0.4 1" density="5.0" />
+  </default>
+  <asset>
+    <texture type="skybox" builtin="gradient" width="100" height="100" rgb1="1 1 1" rgb2="0 0 0" />
+    <texture name="texgeom" type="cube" builtin="flat" mark="cross" width="127" height="1278" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" markrgb="1 1 1" random="0.01" />
+    <texture name="texplane" type="2d" builtin="checker" rgb1="0.2 0.3 0.4" rgb2="0.1 0.2 0.3" width="100" height="100" />
+    <material name='MatPlane' texture="texplane" shininess="1" texrepeat="60 60" specular="1"  reflectance="0.5" />
+    <material name='geom' texture="texgeom" texuniform="true" />
+  </asset>
+  <worldbody>
+    <light directional="true" cutoff="100" exponent="1" diffuse="1 1 1" specular=".1 .1 .1" pos="0 0 1.3" dir="-0 0 -1.3" />
+    <geom name="floor" material="MatPlane" pos="0 0 0" size="40 40 40" type="plane" conaffinity="1" rgba="0.8 0.9 0.8 1" condim="3" />
+    <body name="torso" pos="0 0 0.75">
+      <geom name="torso_geom" type="sphere" size="0.25" pos="0 0 0" />
+      <joint name="root" type="free" limited="false" pos="0 0 0" axis="0 0 1" margin="0.01" armature="0" damping="0" />
+      <body name="front_left_leg" pos="0 0 0">
+        <geom name="aux_1_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 0.2 0.2 0.0" />
+        <body name="aux_1" pos="0.2 0.2 0">
+          <joint name="hip_1" type="hinge" pos="0.0 0.0 0.0" axis="0 0 1" range="-30 30" />
+          <geom name="left_leg_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 0.2 0.2 0.0" />
+          <body pos="0.2 0.2 0">
+            <joint name="ankle_1" type="hinge" pos="0.0 0.0 0.0" axis="-1 1 0" range="30 70" />
+            <geom name="left_ankle_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 0.4 0.4 0.0" />
+          </body>
+        </body>
+      </body>
+      <body name="front_right_leg" pos="0 0 0">
+        <geom name="aux_2_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 -0.2 0.2 0.0" />
+        <body name="aux_2" pos="-0.2 0.2 0">
+          <joint name="hip_2" type="hinge" pos="0.0 0.0 0.0" axis="0 0 1" range="-30 30" />
+          <geom name="right_leg_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 -0.2 0.2 0.0" />
+          <body pos="-0.2 0.2 0">
+            <joint name="ankle_2" type="hinge" pos="0.0 0.0 0.0" axis="1 1 0" range="-70 -30" />
+            <geom name="right_ankle_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 -0.4 0.4 0.0" />
+          </body>
+        </body>
+      </body>
+      <body name="back_leg" pos="0 0 0">
+        <geom name="aux_3_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 -0.2 -0.2 0.0" />
+        <body name="aux_3" pos="-0.2 -0.2 0">
+          <joint name="hip_3" type="hinge" pos="0.0 0.0 0.0" axis="0 0 1" range="-30 30" />
+          <geom name="back_leg_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 -0.2 -0.2 0.0" />
+          <body pos="-0.2 -0.2 0">
+            <joint name="ankle_3" type="hinge" pos="0.0 0.0 0.0" axis="-1 1 0" range="-70 -30" />
+            <geom name="third_ankle_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 -0.4 -0.4 0.0" />
+          </body>
+        </body>
+      </body>
+      <body name="right_back_leg" pos="0 0 0">
+        <geom name="aux_4_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 0.2 -0.2 0.0" />
+        <body name="aux_4" pos="0.2 -0.2 0">
+          <joint name="hip_4" type="hinge" pos="0.0 0.0 0.0" axis="0 0 1" range="-30 30" />
+          <geom name="rightback_leg_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 0.2 -0.2 0.0" />
+          <body pos="0.2 -0.2 0">
+            <joint name="ankle_4" type="hinge" pos="0.0 0.0 0.0" axis="1 1 0" range="30 70" />
+            <geom name="fourth_ankle_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 0.4 -0.4 0.0" />
+          </body>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor joint="hip_4" ctrlrange="-30.0 30.0" ctrllimited="true" />
+    <motor joint="ankle_4" ctrlrange="-30.0 30.0" ctrllimited="true" />
+    <motor joint="hip_1" ctrlrange="-30.0 30.0" ctrllimited="true" />
+    <motor joint="ankle_1" ctrlrange="-30.0 30.0" ctrllimited="true" />
+    <motor joint="hip_2" ctrlrange="-30.0 30.0" ctrllimited="true" />
+    <motor joint="ankle_2" ctrlrange="-30.0 30.0" ctrllimited="true" />
+    <motor joint="hip_3" ctrlrange="-30.0 30.0" ctrllimited="true" />
+    <motor joint="ankle_3" ctrlrange="-30.0 30.0" ctrllimited="true" />
+  </actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/point.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/point.xml
new file mode 100755
index 0000000..4c06cb1
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/point.xml
@@ -0,0 +1,33 @@
+<mujoco>
+    <compiler inertiafromgeom="true" angle="degree" coordinate="local" />
+    <option timestep="0.02" integrator="RK4" />
+    <default>
+        <joint limited="false" armature="0" damping="0" />
+        <geom condim="3" conaffinity="0" margin="0" friction="1.0 0.5 0.5" rgba="0.8 0.6 0.4 1" density="100" />
+    </default>
+    <asset>
+        <texture type="skybox" builtin="gradient" width="100" height="100" rgb1="1 1 1" rgb2="0 0 0" />
+        <texture name="texgeom" type="cube" builtin="flat" mark="cross" width="127" height="1278" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" markrgb="1 1 1" random="0.01" />
+        <texture name="texplane" type="2d" builtin="checker" rgb1="0.2 0.3 0.4" rgb2="0.1 0.2 0.3" width="100" height="100" />
+        <material name="MatPlane" texture="texplane" shininess="1" texrepeat="30 30" specular="1"  reflectance="0.5" />
+        <material name="geom" texture="texgeom" texuniform="true" />
+    </asset>
+    <worldbody>
+        <light directional="true" cutoff="100" exponent="1" diffuse="1 1 1" specular=".1 .1 .1" pos="0 0 1.3" dir="-0 0 -1.3" />
+        <geom name="floor" material="MatPlane" pos="0 0 0" size="40 40 40" type="plane" conaffinity="1" rgba="0.8 0.9 0.8 1" condim="3" />
+        <!--  ================= Point ================= /-->
+        <!--  Note that the solimp is modified from rllab to prevent the point from going through the wall /-->
+        <body name="torso" pos="0 0 0">
+            <geom name="pointbody" type="sphere" size="0.5" pos="0 0 0.5" rgba="0.8 0.4 0.1 1" solimp="0.9 0.99 0.001" />
+            <geom name="pointarrow" type="box" size="0.5 0.1 0.1" pos="0.6 0 0.5" rgba="0.8 0.4 0.1 1" solimp="0.9 0.99 0.001" />
+            <joint name="ballx" type="slide" axis="1 0 0" pos="0 0 0" />
+            <joint name="bally" type="slide" axis="0 1 0" pos="0 0 0" />
+            <joint name="rot" type="hinge" axis="0 0 1" pos="0 0 0" limited="false" />
+        </body>
+    </worldbody>
+    <actuator>
+        <!-- Those are just dummy actuators for providing ranges -->
+        <motor joint="ballx" ctrlrange="-1 1" ctrllimited="true" />
+        <motor joint="rot" ctrlrange="-0.25 0.25" ctrllimited="true" />
+    </actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/reacher.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/reacher.xml
new file mode 100644
index 0000000..0d238c8
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/reacher.xml
@@ -0,0 +1,34 @@
+<mujoco model="swimmer">
+  <compiler angle="degree" coordinate="local" inertiafromgeom="true" />
+  <option collision="predefined" density="4000" integrator="RK4" timestep="0.01" viscosity="0.1" />
+  <default>
+    <geom conaffinity="1" condim="1" contype="1" material="geom" rgba="0.8 0.6 .4 1" />
+    <joint armature="0.1" />
+  </default>
+  <asset>
+    <texture type="skybox" builtin="gradient" width="100" height="100" rgb1="1 1 1" rgb2="0 0 0" />
+    <texture name="texgeom" type="cube" builtin="flat" mark="cross" width="127" height="1278" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" markrgb="1 1 1" random="0.01" />
+    <texture name="texplane" type="2d" builtin="checker" rgb1="0.2 0.3 0.4" rgb2="0.1 0.2 0.3" width="100" height="100" />
+    <material name='MatPlane' texture="texplane" shininess="1" texrepeat="60 60" specular="1"  reflectance="0.5" />
+    <material name='geom' texture="texgeom" texuniform="true" />
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0s 1.3" specular=".1 .1 .1" />
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 -0.1" rgba="0.8 0.9 0.8 1" size="40 40 0.1" type="plane" />
+    <!-- Reacher -->
+    <body name="torso" pos="0 0 0">
+      <camera name="track" mode="trackcom" pos="0 -3 3" xyaxes="1 0 0 0 1 1" />
+      <geom name="frontbody" density="1000" fromto="1.5 0 0 0.5 0 0" size="0.1" type="capsule" />
+      <joint axis="1 0 0" name="slider1" pos="0 0 0" type="slide" />
+      <joint axis="0 1 0" name="slider2" pos="0 0 0" type="slide" />
+      <joint axis="0 0 1" name="rot" pos="0 0 0" type="hinge" />
+      <body name="mid" pos="0.5 0 0">
+        <geom name="midbody" density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule" />
+        <joint axis="0 0 1" limited="true" name="rot2" pos="0 0 0" range="-100 100" type="hinge" />
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot2" />
+  </actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/swimmer.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/swimmer.xml
new file mode 100644
index 0000000..3c6c21a
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/swimmer.xml
@@ -0,0 +1,39 @@
+<mujoco model="swimmer">
+  <compiler angle="degree" coordinate="local" inertiafromgeom="true" />
+  <option collision="predefined" density="4000" integrator="RK4" timestep="0.01" viscosity="0.1" />
+  <default>
+    <geom conaffinity="1" condim="1" contype="1" material="geom" rgba="0.8 0.6 .4 1" />
+    <joint armature="0.1" />
+  </default>
+  <asset>
+    <texture type="skybox" builtin="gradient" width="100" height="100" rgb1="1 1 1" rgb2="0 0 0" />
+    <texture name="texgeom" type="cube" builtin="flat" mark="cross" width="127" height="1278" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" markrgb="1 1 1" random="0.01" />
+    <texture name="texplane" type="2d" builtin="checker" rgb1="0.2 0.3 0.4" rgb2="0.1 0.2 0.3" width="100" height="100" />
+    <material name='MatPlane' texture="texplane" shininess="1" texrepeat="60 60" specular="1"  reflectance="0.5" />
+    <material name='geom' texture="texgeom" texuniform="true" />
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0s 1.3" specular=".1 .1 .1" />
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 -0.1" rgba="0.8 0.9 0.8 1" size="40 40 0.1" type="plane" />
+    <!--  ================= SWIMMER ================= /-->
+    <body name="torso" pos="0 0 0">
+      <camera name="track" mode="trackcom" pos="0 -3 3" xyaxes="1 0 0 0 1 1" />
+      <geom name="frontbody" density="1000" fromto="1.5 0 0 0.5 0 0" size="0.1" type="capsule" />
+      <joint axis="1 0 0" name="slider1" pos="0 0 0" type="slide" />
+      <joint axis="0 1 0" name="slider2" pos="0 0 0" type="slide" />
+      <joint axis="0 0 1" name="rot" pos="0 0 0" type="hinge" />
+      <body name="mid" pos="0.5 0 0">
+        <geom name="midbody" density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule" />
+        <joint axis="0 0 1" limited="true" name="rot2" pos="0 0 0" range="-100 100" type="hinge" />
+        <body name="back" pos="-1 0 0">
+          <geom name="backbody" density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule" />
+          <joint axis="0 0 1" limited="true" name="rot3" pos="0 0 0" range="-100 100" type="hinge" />
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot2" />
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot3" />
+  </actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_env.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_env.py
new file mode 100644
index 0000000..dd11988
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_env.py
@@ -0,0 +1,595 @@
+"""
+Mujoco Maze environment.
+Based on `models`_ and `rllab`_.
+
+.. _models: https://github.com/tensorflow/models/tree/master/research/efficient-hrl
+.. _rllab: https://github.com/rll/rllab
+"""
+
+import itertools as it
+import os
+import tempfile
+import xml.etree.ElementTree as ET
+from typing import List, Tuple, Type
+
+import gym
+import numpy as np
+
+from slbo.envs.mujoco_maze import maze_env_utils, maze_task
+from slbo.envs.mujoco_maze.agent_model import AgentModel
+from slbo.utils.dataset import Dataset, gen_dtype
+
+
+# Directory that contains mujoco xml files.
+MODEL_DIR = os.path.dirname(os.path.abspath(__file__)) + "/assets"
+
+
+class MazeEnv(gym.Env):
+    def __init__(
+        self,
+        model_cls: Type[AgentModel],
+        maze_task: Type[maze_task.MazeTask] = maze_task.MazeTask,
+        top_down_view: float = False,
+        maze_height: float = 0.5,
+        maze_size_scaling: float = 4.0,
+        inner_reward_scaling: float = 1.0,
+        restitution_coef: float = 0.8,
+        task_kwargs: dict = {},
+        *args,
+        **kwargs,
+    ) -> None:
+        self._task = maze_task(maze_size_scaling, **task_kwargs)
+
+        xml_path = os.path.join(MODEL_DIR, model_cls.FILE)
+        tree = ET.parse(xml_path)
+        worldbody = tree.find(".//worldbody")
+
+        self._maze_height = height = maze_height
+        self._maze_size_scaling = size_scaling = maze_size_scaling
+        self._inner_reward_scaling = inner_reward_scaling
+        self.t = 0  # time steps
+        self._observe_blocks = self._task.OBSERVE_BLOCKS
+        self._put_spin_near_agent = self._task.PUT_SPIN_NEAR_AGENT
+        # Observe other objectives
+        self._observe_balls = self._task.OBSERVE_BALLS
+        self._top_down_view = self._task.TOP_DOWN_VIEW
+        self._restitution_coef = restitution_coef
+
+        self._maze_structure = structure = self._task.create_maze()
+        # Elevate the maze to allow for falling.
+        self.elevated = any(maze_env_utils.MazeCell.CHASM in row for row in structure)
+        # Are there any movable blocks?
+        self.blocks = any(any(r.can_move() for r in row) for row in structure)
+
+        torso_x, torso_y = self._find_robot()
+        self._init_torso_x = torso_x
+        self._init_torso_y = torso_y
+        self._init_positions = [
+            (x - torso_x, y - torso_y) for x, y in self._find_all_robots()
+        ]
+
+        if model_cls.MANUAL_COLLISION:
+            if model_cls.RADIUS is None:
+                raise ValueError("Manual collision needs radius of the model")
+            self._collision = maze_env_utils.CollisionDetector(
+                structure, size_scaling, torso_x, torso_y, model_cls.RADIUS,
+            )
+            # Now all object balls have size=1.0
+            self._objball_collision = maze_env_utils.CollisionDetector(
+                structure, size_scaling, torso_x, torso_y, self._task.OBJECT_BALL_SIZE,
+            )
+        else:
+            self._collision = None
+
+        self._xy_to_rowcol = lambda x, y: (
+            2 + (y + size_scaling / 2) / size_scaling,
+            2 + (x + size_scaling / 2) / size_scaling,
+        )
+        # walls (immovable), chasms (fall), movable blocks
+        self._view = np.zeros([5, 5, 3])
+
+        height_offset = 0.0
+        if self.elevated:
+            # Increase initial z-pos of ant.
+            height_offset = height * size_scaling
+            torso = tree.find(".//body[@name='torso']")
+            torso.set("pos", f"0 0 {0.75 + height_offset:.2f}")
+        if self.blocks:
+            # If there are movable blocks, change simulation settings to perform
+            # better contact detection.
+            default = tree.find(".//default")
+            default.find(".//geom").set("solimp", ".995 .995 .01")
+
+        self.movable_blocks = []
+        self.object_balls = []
+        for i in range(len(structure)):
+            for j in range(len(structure[0])):
+                struct = structure[i][j]
+                if struct.is_robot() and self._put_spin_near_agent:
+                    struct = maze_env_utils.MazeCell.SPIN
+                x, y = j * size_scaling - torso_x, i * size_scaling - torso_y
+                h = height / 2 * size_scaling
+                size = size_scaling * 0.5
+                if self.elevated and not struct.is_chasm():
+                    # Create elevated platform.
+                    ET.SubElement(
+                        worldbody,
+                        "geom",
+                        name=f"elevated_{i}_{j}",
+                        pos=f"{x} {y} {h}",
+                        size=f"{size} {size} {h}",
+                        type="box",
+                        material="",
+                        contype="1",
+                        conaffinity="1",
+                        rgba="0.9 0.9 0.9 1",
+                    )
+                if struct.is_block():
+                    # Unmovable block.
+                    # Offset all coordinates so that robot starts at the origin.
+                    ET.SubElement(
+                        worldbody,
+                        "geom",
+                        name=f"block_{i}_{j}",
+                        pos=f"{x} {y} {h + height_offset}",
+                        size=f"{size} {size} {h}",
+                        type="box",
+                        material="",
+                        contype="1",
+                        conaffinity="1",
+                        rgba="0.4 0.4 0.4 1",
+                    )
+                elif struct.can_move():
+                    # Movable block.
+                    self.movable_blocks.append(f"movable_{i}_{j}")
+                    _add_movable_block(
+                        worldbody, struct, i, j, size_scaling, x, y, h, height_offset,
+                    )
+                elif struct.is_object_ball():
+                    # Movable Ball
+                    self.object_balls.append(f"objball_{i}_{j}")
+                    _add_object_ball(worldbody, i, j, x, y, self._task.OBJECT_BALL_SIZE)
+
+        torso = tree.find(".//body[@name='torso']")
+        geoms = torso.findall(".//geom")
+        for geom in geoms:
+            if "name" not in geom.attrib:
+                raise Exception("Every geom of the torso must have a name")
+
+        # Set goals
+        for i, goal in enumerate(self._task.goals):
+            z = goal.pos[2] if goal.dim >= 3 else 0.0
+            if goal.custom_size is None:
+                size = f"{maze_size_scaling * 0.1}"
+            else:
+                size = f"{goal.custom_size}"
+            ET.SubElement(
+                worldbody,
+                "site",
+                name=f"goal_site{i}",
+                pos=f"{goal.pos[0]} {goal.pos[1]} {z}",
+                size=f"{maze_size_scaling * 0.1}",
+                rgba=goal.rgb.rgba_str(),
+            )
+
+        _, file_path = tempfile.mkstemp(text=True, suffix=".xml")
+        tree.write(file_path)
+        self.world_tree = tree
+        self.wrapped_env = model_cls(*args, file_path=file_path, **kwargs)
+        self.observation_space = self._get_obs_space()
+
+    @property
+    def has_extended_obs(self) -> bool:
+        return self._top_down_view or self._observe_blocks or self._observe_balls
+
+    def get_ori(self) -> float:
+        return self.wrapped_env.get_ori()
+
+    def _get_obs_space(self) -> gym.spaces.Box:
+        shape = self._get_obs().shape
+        high = np.inf * np.ones(shape, dtype=np.float32)
+        low = -high
+        # Set velocity limits
+        wrapped_obs_space = self.wrapped_env.observation_space
+        high[: wrapped_obs_space.shape[0]] = wrapped_obs_space.high
+        low[: wrapped_obs_space.shape[0]] = wrapped_obs_space.low
+        # Set coordinate limits
+        low[0], high[0], low[1], high[1] = self._xy_limits()
+        # Set orientation limits
+        return gym.spaces.Box(low, high)
+
+    def _xy_limits(self) -> Tuple[float, float, float, float]:
+        xmin, ymin, xmax, ymax = 100, 100, -100, -100
+        structure = self._maze_structure
+        for i, j in it.product(range(len(structure)), range(len(structure[0]))):
+            if structure[i][j].is_block():
+                continue
+            xmin, xmax = min(xmin, j), max(xmax, j)
+            ymin, ymax = min(ymin, i), max(ymax, i)
+        x0, y0 = self._init_torso_x, self._init_torso_y
+        scaling = self._maze_size_scaling
+        xmin, xmax = (xmin - 0.5) * scaling - x0, (xmax + 0.5) * scaling - x0
+        ymin, ymax = (ymin - 0.5) * scaling - y0, (ymax + 0.5) * scaling - y0
+        return xmin, xmax, ymin, ymax
+
+    def get_top_down_view(self) -> np.ndarray:
+        self._view = np.zeros_like(self._view)
+
+        def valid(row, col):
+            return self._view.shape[0] > row >= 0 and self._view.shape[1] > col >= 0
+
+        def update_view(x, y, d, row=None, col=None):
+            if row is None or col is None:
+                x = x - self._robot_x
+                y = y - self._robot_y
+
+                row, col = self._xy_to_rowcol(x, y)
+                update_view(x, y, d, row=row, col=col)
+                return
+
+            row, row_frac, col, col_frac = int(row), row % 1, int(col), col % 1
+            if row_frac < 0:
+                row_frac += 1
+            if col_frac < 0:
+                col_frac += 1
+
+            if valid(row, col):
+                self._view[row, col, d] += (
+                    min(1.0, row_frac + 0.5) - max(0.0, row_frac - 0.5)
+                ) * (min(1.0, col_frac + 0.5) - max(0.0, col_frac - 0.5))
+            if valid(row - 1, col):
+                self._view[row - 1, col, d] += (max(0.0, 0.5 - row_frac)) * (
+                    min(1.0, col_frac + 0.5) - max(0.0, col_frac - 0.5)
+                )
+            if valid(row + 1, col):
+                self._view[row + 1, col, d] += (max(0.0, row_frac - 0.5)) * (
+                    min(1.0, col_frac + 0.5) - max(0.0, col_frac - 0.5)
+                )
+            if valid(row, col - 1):
+                self._view[row, col - 1, d] += (
+                    min(1.0, row_frac + 0.5) - max(0.0, row_frac - 0.5)
+                ) * (max(0.0, 0.5 - col_frac))
+            if valid(row, col + 1):
+                self._view[row, col + 1, d] += (
+                    min(1.0, row_frac + 0.5) - max(0.0, row_frac - 0.5)
+                ) * (max(0.0, col_frac - 0.5))
+            if valid(row - 1, col - 1):
+                self._view[row - 1, col - 1, d] += (max(0.0, 0.5 - row_frac)) * max(
+                    0.0, 0.5 - col_frac
+                )
+            if valid(row - 1, col + 1):
+                self._view[row - 1, col + 1, d] += (max(0.0, 0.5 - row_frac)) * max(
+                    0.0, col_frac - 0.5
+                )
+            if valid(row + 1, col + 1):
+                self._view[row + 1, col + 1, d] += (max(0.0, row_frac - 0.5)) * max(
+                    0.0, col_frac - 0.5
+                )
+            if valid(row + 1, col - 1):
+                self._view[row + 1, col - 1, d] += (max(0.0, row_frac - 0.5)) * max(
+                    0.0, 0.5 - col_frac
+                )
+
+        # Draw ant.
+        robot_x, robot_y = self.wrapped_env.get_body_com("torso")[:2]
+        self._robot_x = robot_x
+        self._robot_y = robot_y
+
+        structure = self._maze_structure
+        size_scaling = self._maze_size_scaling
+
+        # Draw immovable blocks and chasms.
+        for i in range(len(structure)):
+            for j in range(len(structure[0])):
+                if structure[i][j].is_block():  # Wall.
+                    update_view(
+                        j * size_scaling - self._init_torso_x,
+                        i * size_scaling - self._init_torso_y,
+                        0,
+                    )
+                if structure[i][j].is_chasm():  # Chasm.
+                    update_view(
+                        j * size_scaling - self._init_torso_x,
+                        i * size_scaling - self._init_torso_y,
+                        1,
+                    )
+
+        # Draw movable blocks.
+        for block_name in self.movable_blocks:
+            block_x, block_y = self.wrapped_env.get_body_com(block_name)[:2]
+            update_view(block_x, block_y, 2)
+
+        return self._view
+
+    def _get_obs(self) -> np.ndarray:
+        wrapped_obs = self.wrapped_env._get_obs()
+        if self._top_down_view:
+            view = [self.get_top_down_view().flat]
+        else:
+            view = []
+
+        additional_obs = []
+
+        if self._observe_balls:
+            for name in self.object_balls:
+                additional_obs.append(self.wrapped_env.get_body_com(name))
+
+        if self._observe_blocks:
+            for name in self.movable_blocks:
+                additional_obs.append(self.wrapped_env.get_body_com(name))
+
+        obs = np.concatenate([wrapped_obs[:3]] + additional_obs + [wrapped_obs[3:]])
+        return np.concatenate([obs, *view, np.array([self.t * 0.001])])
+
+    def reset(self) -> np.ndarray:
+        self.t = 0
+        self.wrapped_env.reset()
+        # Samples a new goal
+        if self._task.sample_goals():
+            self.set_marker()
+        # Samples a new start position
+        if len(self._init_positions) > 1:
+            xy = np.random.choice(self._init_positions)
+            self.wrapped_env.set_xy(xy)
+        return self._get_obs()
+
+    def set_marker(self) -> None:
+        for i, goal in enumerate(self._task.goals):
+            idx = self.model.site_name2id(f"goal{i}")
+            self.data.site_xpos[idx][: len(goal.pos)] = goal.pos
+
+    @property
+    def viewer(self):
+        return self.wrapped_env.viewer
+
+    def render(self, *args, **kwargs):
+        return self.wrapped_env.render(*args, **kwargs)
+
+    @property
+    def action_space(self):
+        return self.wrapped_env.action_space
+
+    def _find_robot(self) -> Tuple[float, float]:
+        structure = self._maze_structure
+        size_scaling = self._maze_size_scaling
+        for i, j in it.product(range(len(structure)), range(len(structure[0]))):
+            if structure[i][j].is_robot():
+                return j * size_scaling, i * size_scaling
+        raise ValueError("No robot in maze specification.")
+
+    def _find_all_robots(self) -> List[Tuple[float, float]]:
+        structure = self._maze_structure
+        size_scaling = self._maze_size_scaling
+        coords = []
+        for i, j in it.product(range(len(structure)), range(len(structure[0]))):
+            if structure[i][j].is_robot():
+                coords.append((j * size_scaling, i * size_scaling))
+        return coords
+
+    def _objball_positions(self) -> None:
+        return [
+            self.wrapped_env.get_body_com(name)[:2].copy() for name in self.object_balls
+        ]
+
+    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, dict]:
+        action = np.clip(action, self.action_space.low, self.action_space.high)
+        self.t += 1
+        if self.wrapped_env.MANUAL_COLLISION:
+            old_pos = self.wrapped_env.get_xy()
+            old_objballs = self._objball_positions()
+            inner_next_obs, inner_reward, _, info = self.wrapped_env.step(action)
+            new_pos = self.wrapped_env.get_xy()
+            new_objballs = self._objball_positions()
+            # Checks that the new_position is in the wall
+            collision = self._collision.detect(old_pos, new_pos)
+            if collision is not None:
+                pos = collision.point + self._restitution_coef * collision.rest()
+                if self._collision.detect(old_pos, pos) is not None:
+                    # If pos is also not in the wall, we give up computing the position
+                    self.wrapped_env.set_xy(old_pos)
+                else:
+                    self.wrapped_env.set_xy(pos)
+            # Do the same check for object balls
+            for name, old, new in zip(self.object_balls, old_objballs, new_objballs):
+                collision = self._objball_collision.detect(old, new)
+                if collision is not None:
+                    pos = collision.point + self._restitution_coef * collision.rest()
+                    if self._objball_collision.detect(old, pos) is not None:
+                        pos = old
+                    idx = self.wrapped_env.model.body_name2id(name)
+                    self.wrapped_env.data.xipos[idx][:2] = pos
+        else:
+            inner_next_obs, inner_reward, _, info = self.wrapped_env.step(action)
+        next_obs = self._get_obs()
+        inner_reward = self._inner_reward_scaling * inner_reward
+        outer_reward = self._task.reward(next_obs)
+        done = self._task.termination(next_obs)
+        info["position"] = self.wrapped_env.get_xy()
+        return next_obs, inner_reward + outer_reward, done, info
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        rewards = []
+        dones = []
+        for i in range(len(next_states)):
+            rewards.append(self._task.reward(next_states[i]))
+            dones.append(self._task.termination(next_states[i]))
+        inner_rewards = np.linalg.norm((states[:,:2] - next_states[:,:2])/self.wrapped_env.dt,axis=-1)
+        reward_ctrl = self.wrapped_env._ctrl_cost_weight * np.sum(np.square(actions), axis=-1)
+        #print(inner_.rewards)
+        rewards = np.array(rewards)
+        assert rewards.shape == inner_rewards.shape
+        assert inner_rewards.shape == reward_ctrl.shape
+        rewards = rewards + self._inner_reward_scaling * (inner_rewards - reward_ctrl)
+        #print(rewards)
+        return rewards, np.array(dones, dtype=np.bool)
+
+
+    def verify(self, n=2000, eps=1e-4):
+        print(self._inner_reward_scaling)
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        print(dataset.reward)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        print('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
+
+
+    def close(self) -> None:
+        self.wrapped_env.close()
+
+
+def _add_object_ball(
+    worldbody: ET.Element, i: str, j: str, x: float, y: float, size: float
+) -> None:
+    body = ET.SubElement(worldbody, "body", name=f"objball_{i}_{j}", pos=f"{x} {y} 0")
+    mass = 0.0001 * (size ** 3)
+    ET.SubElement(
+        body,
+        "geom",
+        type="sphere",
+        name=f"objball_{i}_{j}_geom",
+        size=f"{size}",  # Radius
+        pos=f"0.0 0.0 {size}",  # Z = size so that this ball can move!!
+        rgba=maze_task.BLUE.rgba_str(),
+        contype="1",
+        conaffinity="1",
+        solimp="0.9 0.99 0.001",
+        mass=f"{mass}",
+    )
+    ET.SubElement(
+        body,
+        "joint",
+        name=f"objball_{i}_{j}_x",
+        axis="1 0 0",
+        pos="0 0 0.0",
+        type="slide",
+    )
+    ET.SubElement(
+        body,
+        "joint",
+        name=f"objball_{i}_{j}_y",
+        axis="0 1 0",
+        pos="0 0 0",
+        type="slide",
+    )
+    ET.SubElement(
+        body,
+        "joint",
+        name=f"objball_{i}_{j}_rot",
+        axis="0 0 1",
+        pos="0 0 0",
+        type="hinge",
+        limited="false",
+    )
+
+
+def _add_movable_block(
+    worldbody: ET.Element,
+    struct: maze_env_utils.MazeCell,
+    i: str,
+    j: str,
+    size_scaling: float,
+    x: float,
+    y: float,
+    h: float,
+    height_offset: float,
+) -> None:
+    falling = struct.can_move_z()
+    if struct.can_spin():
+        h *= 0.1
+        x += size_scaling * 0.25
+        shrink = 0.1
+    elif falling:
+        # The "falling" blocks are shrunk slightly and increased in mass to
+        # ensure it can fall easily through a gap in the platform blocks.
+        shrink = 0.99
+    elif struct.is_half_block():
+        shrink = 0.5
+    else:
+        shrink = 1.0
+    size = size_scaling * 0.5 * shrink
+    movable_body = ET.SubElement(
+        worldbody, "body", name=f"movable_{i}_{j}", pos=f"{x} {y} {h}",
+    )
+    ET.SubElement(
+        movable_body,
+        "geom",
+        name=f"block_{i}_{j}",
+        pos="0 0 0",
+        size=f"{size} {size} {h}",
+        type="box",
+        material="",
+        mass="0.001" if falling else "0.0002",
+        contype="1",
+        conaffinity="1",
+        rgba="0.9 0.1 0.1 1",
+    )
+    if struct.can_move_x():
+        ET.SubElement(
+            movable_body,
+            "joint",
+            axis="1 0 0",
+            name=f"movable_x_{i}_{j}",
+            armature="0",
+            damping="0.0",
+            limited="true" if falling else "false",
+            range=f"{-size_scaling} {size_scaling}",
+            margin="0.01",
+            pos="0 0 0",
+            type="slide",
+        )
+    if struct.can_move_y():
+        ET.SubElement(
+            movable_body,
+            "joint",
+            armature="0",
+            axis="0 1 0",
+            damping="0.0",
+            limited="true" if falling else "false",
+            range=f"{-size_scaling} {size_scaling}",
+            margin="0.01",
+            name=f"movable_y_{i}_{j}",
+            pos="0 0 0",
+            type="slide",
+        )
+    if struct.can_move_z():
+        ET.SubElement(
+            movable_body,
+            "joint",
+            armature="0",
+            axis="0 0 1",
+            damping="0.0",
+            limited="true",
+            range=f"{-height_offset} 0",
+            margin="0.01",
+            name=f"movable_z_{i}_{j}",
+            pos="0 0 0",
+            type="slide",
+        )
+    if struct.can_spin():
+        ET.SubElement(
+            movable_body,
+            "joint",
+            armature="0",
+            axis="0 0 1",
+            damping="0.0",
+            limited="false",
+            name=f"spinable_{i}_{j}",
+            pos="0 0 0",
+            type="ball",
+        )
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_env_utils.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_env_utils.py
new file mode 100644
index 0000000..348b88c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_env_utils.py
@@ -0,0 +1,205 @@
+"""
+Utilities for creating maze.
+Based on `models`_ and `rllab`_.
+
+.. _models: https://github.com/tensorflow/models/tree/master/research/efficient-hrl
+.. _rllab: https://github.com/rll/rllab
+"""
+
+import itertools as it
+from enum import Enum
+from typing import Any, List, Optional, Sequence, Tuple, Union
+
+import numpy as np
+
+Self = Any
+Point = np.complex
+
+
+class MazeCell(Enum):
+    # Robot: Start position
+    ROBOT = -1
+    # Blocks
+    EMPTY = 0
+    BLOCK = 1
+    CHASM = 2
+    OBJECT_BALL = 3
+    # Moves
+    XY_BLOCK = 14
+    XZ_BLOCK = 15
+    YZ_BLOCK = 16
+    XYZ_BLOCK = 17
+    XY_HALF_BLOCK = 18
+    SPIN = 19
+
+    def is_block(self) -> bool:
+        return self == self.BLOCK
+
+    def is_chasm(self) -> bool:
+        return self == self.CHASM
+
+    def is_object_ball(self) -> bool:
+        return self == self.OBJECT_BALL
+
+    def is_empty(self) -> bool:
+        return self == self.ROBOT or self == self.EMPTY
+
+    def is_robot(self) -> bool:
+        return self == self.ROBOT
+
+    def is_wall_or_chasm(self) -> bool:
+        return self in [self.BLOCK, self.CHASM]
+
+    def can_move_x(self) -> bool:
+        return self in [
+            self.XY_BLOCK,
+            self.XY_HALF_BLOCK,
+            self.XZ_BLOCK,
+            self.XYZ_BLOCK,
+            self.SPIN,
+        ]
+
+    def can_move_y(self) -> bool:
+        return self in [
+            self.XY_BLOCK,
+            self.XY_HALF_BLOCK,
+            self.YZ_BLOCK,
+            self.XYZ_BLOCK,
+            self.SPIN,
+        ]
+
+    def can_move_z(self) -> bool:
+        return self in [self.XZ_BLOCK, self.YZ_BLOCK, self.XYZ_BLOCK]
+
+    def can_spin(self) -> bool:
+        return self == self.SPIN
+
+    def can_move(self) -> bool:
+        return self.can_move_x() or self.can_move_y() or self.can_move_z()
+
+    def is_half_block(self) -> bool:
+        return self in [self.XY_HALF_BLOCK]
+
+
+class Line:
+    def __init__(
+        self, p1: Union[Sequence[float], Point], p2: Union[Sequence[float], Point],
+    ) -> None:
+        self.p1 = p1 if isinstance(p1, Point) else np.complex(*p1)
+        self.p2 = p2 if isinstance(p2, Point) else np.complex(*p2)
+        self.v1 = self.p2 - self.p1
+        self.conj_v1 = np.conjugate(self.v1)
+        self.norm = np.absolute(self.v1)
+
+    def _intersect(self, other: Self) -> bool:
+        v2 = other.p1 - self.p1
+        v3 = other.p2 - self.p1
+        return (self.conj_v1 * v2).imag * (self.conj_v1 * v3).imag <= 0.0
+
+    def _projection(self, p: Point) -> Point:
+        nv1 = -self.v1
+        nv1_norm = np.absolute(nv1) ** 2
+        scale = np.real(np.conjugate(p - self.p1) * nv1) / nv1_norm
+        return self.p1 + nv1 * scale
+
+    def reflection(self, p: Point) -> Point:
+        return p + 2.0 * (self._projection(p) - p)
+
+    def distance(self, p: Point) -> float:
+        return np.absolute(p - self._projection(p))
+
+    def intersect(self, other: Self) -> Point:
+        if self._intersect(other) and other._intersect(self):
+            return self._cross_point(other)
+        else:
+            return None
+
+    def _cross_point(self, other: Self) -> Optional[Point]:
+        v2 = other.p2 - other.p1
+        v3 = self.p2 - other.p1
+        a, b = (self.conj_v1 * v2).imag, (self.conj_v1 * v3).imag
+        return other.p1 + b / a * v2
+
+    def __repr__(self) -> str:
+        x1, y1 = self.p1.real, self.p1.imag
+        x2, y2 = self.p2.real, self.p2.imag
+        return f"Line(({x1}, {y1}) -> ({x2}, {y2}))"
+
+
+class Collision:
+    def __init__(self, point: Point, reflection: Point) -> None:
+        self._point = point
+        self._reflection = reflection
+
+    @property
+    def point(self) -> np.ndarray:
+        return np.array([self._point.real, self._point.imag])
+
+    def rest(self) -> np.ndarray:
+        p = self._reflection - self._point
+        return np.array([p.real, p.imag])
+
+
+class CollisionDetector:
+    """For manual collision detection.
+    """
+
+    EPS: float = 0.05
+    NEIGHBORS: List[Tuple[int, int]] = [[0, -1], [-1, 0], [0, 1], [1, 0]]
+
+    def __init__(
+        self,
+        structure: list,
+        size_scaling: float,
+        torso_x: float,
+        torso_y: float,
+        radius: float,
+    ) -> None:
+        h, w = len(structure), len(structure[0])
+        self.lines = []
+
+        def is_empty(i, j) -> bool:
+            if 0 <= i < h and 0 <= j < w:
+                return structure[i][j].is_empty()
+            else:
+                return False
+
+        for i, j in it.product(range(len(structure)), range(len(structure[0]))):
+            if not structure[i][j].is_block():
+                continue
+            y_base = i * size_scaling - torso_y
+            x_base = j * size_scaling - torso_x
+            offset = size_scaling * 0.5 + radius
+            min_y, max_y = y_base - offset, y_base + offset
+            min_x, max_x = x_base - offset, x_base + offset
+            for dx, dy in self.NEIGHBORS:
+                if not is_empty(i + dy, j + dx):
+                    continue
+                self.lines.append(
+                    Line(
+                        (max_x if dx == 1 else min_x, max_y if dy == 1 else min_y),
+                        (min_x if dx == -1 else max_x, min_y if dy == -1 else max_y),
+                    )
+                )
+
+    def detect(self, old_pos: np.ndarray, new_pos: np.ndarray) -> Optional[Collision]:
+        move = Line(old_pos, new_pos)
+        # First, checks that it actually moved
+        if move.norm <= 1e-8:
+            return None
+        # Next, checks that the trajectory cross the wall or not
+        collisions = []
+        for line in self.lines:
+            intersection = line.intersect(move)
+            if intersection is not None:
+                reflection = line.reflection(move.p2)
+                collisions.append(Collision(intersection, reflection))
+        if len(collisions) == 0:
+            return None
+        col = collisions[0]
+        dist = np.absolute(col._point - move.p1)
+        for collision in collisions[1:]:
+            new_dist = np.absolute(collision._point - move.p1)
+            if new_dist < dist:
+                col, dist = collision, new_dist
+        return col
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_task.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_task.py
new file mode 100644
index 0000000..77fca6b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_task.py
@@ -0,0 +1,477 @@
+"""Maze tasks that are defined by their map, termination condition, and goals.
+"""
+
+from abc import ABC, abstractmethod
+from typing import Dict, List, NamedTuple, Optional, Tuple, Type
+
+import numpy as np
+
+from slbo.envs.mujoco_maze.maze_env_utils import MazeCell
+
+
+class Rgb(NamedTuple):
+    red: float
+    green: float
+    blue: float
+
+    def rgba_str(self) -> str:
+        return f"{self.red} {self.green} {self.blue} 1"
+
+
+RED = Rgb(0.7, 0.1, 0.1)
+GREEN = Rgb(0.1, 0.7, 0.1)
+BLUE = Rgb(0.1, 0.1, 0.7)
+
+
+class MazeGoal:
+    def __init__(
+        self,
+        pos: np.ndarray,
+        reward_scale: float = 1.0,
+        rgb: Rgb = RED,
+        threshold: float = 0.6,
+        custom_size: Optional[float] = None,
+    ) -> None:
+        assert 0.0 <= reward_scale <= 1.0
+        self.pos = pos
+        self.dim = pos.shape[0]
+        self.reward_scale = reward_scale
+        self.rgb = rgb
+        self.threshold = threshold
+        self.custom_size = custom_size
+
+    def neighbor(self, obs: np.ndarray) -> float:
+        return np.linalg.norm(obs[: self.dim] - self.pos) <= self.threshold
+
+    def euc_dist(self, obs: np.ndarray) -> float:
+        return np.sum(np.square(obs[: self.dim] - self.pos)) ** 0.5
+
+
+class Scaling(NamedTuple):
+    ant: Optional[float]
+    point: Optional[float]
+    swimmer: Optional[float]
+
+
+class MazeTask(ABC):
+    REWARD_THRESHOLD: float
+    PENALTY: Optional[float] = None
+    MAZE_SIZE_SCALING: Scaling = Scaling(8.0, 4.0, 4.0)
+    INNER_REWARD_SCALING: float = 0.01
+    # For Fall/Push/BlockMaze
+    OBSERVE_BLOCKS: bool = False
+    # For Billiard
+    OBSERVE_BALLS: bool = False
+    OBJECT_BALL_SIZE: float = 1.0
+    # Unused now
+    PUT_SPIN_NEAR_AGENT: bool = False
+    TOP_DOWN_VIEW: bool = False
+
+    def __init__(self, scale: float) -> None:
+        self.goals = []
+        self.scale = scale
+
+    def sample_goals(self) -> bool:
+        return False
+
+    def termination(self, obs: np.ndarray) -> bool:
+        for goal in self.goals:
+            if goal.neighbor(obs):
+                return True
+        return False
+
+    @abstractmethod
+    def reward(self, obs: np.ndarray) -> float:
+        pass
+
+    @staticmethod
+    @abstractmethod
+    def create_maze() -> List[List[MazeCell]]:
+        pass
+
+
+class DistRewardMixIn:
+    REWARD_THRESHOLD: float = -1000.0
+    goals: List[MazeGoal]
+    scale: float
+
+    def reward(self, obs: np.ndarray) -> float:
+        return -self.goals[0].euc_dist(obs) / self.scale
+
+
+class GoalRewardUMaze(MazeTask):
+    REWARD_THRESHOLD: float = 0.9
+    PENALTY: float = -0.0001
+
+    def __init__(self, scale: float) -> None:
+        super().__init__(scale)
+        self.goals = [MazeGoal(np.array([0.0, 2.0 * scale]))]
+
+    def reward(self, obs: np.ndarray) -> float:
+        return 100.0 if self.termination(obs) else self.PENALTY
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B, R = MazeCell.EMPTY, MazeCell.BLOCK, MazeCell.ROBOT
+        return [
+            [B, B, B, B, B],
+            [B, R, E, E, B],
+            [B, B, B, E, B],
+            [B, E, E, E, B],
+            [B, B, B, B, B],
+        ]
+
+
+class DistRewardUMaze(GoalRewardUMaze, DistRewardMixIn):
+    pass
+
+
+class GoalRewardSimpleRoom(GoalRewardUMaze):
+    def __init__(self, scale: float) -> None:
+        super().__init__(scale)
+        self.goals = [MazeGoal(np.array([2.0 * scale, 0.0]))]
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B, R = MazeCell.EMPTY, MazeCell.BLOCK, MazeCell.ROBOT
+        return [
+            [B, B, B, B, B],
+            [B, R, E, E, B],
+            [B, B, B, B, B],
+        ]
+
+
+class DistRewardSimpleRoom(GoalRewardSimpleRoom, DistRewardMixIn):
+    pass
+
+
+class GoalRewardPush(GoalRewardUMaze):
+    OBSERVE_BLOCKS: bool = True
+
+    def __init__(self, scale: float) -> None:
+        super().__init__(scale)
+        self.goals = [MazeGoal(np.array([0.0, 2.375 * scale]))]
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B, R, M = MazeCell.EMPTY, MazeCell.BLOCK, MazeCell.ROBOT, MazeCell.XY_BLOCK
+        return [
+            [B, B, B, B, B],
+            [B, E, R, B, B],
+            [B, E, M, E, B],
+            [B, B, E, B, B],
+            [B, B, B, B, B],
+        ]
+
+
+class DistRewardPush(GoalRewardPush, DistRewardMixIn):
+    pass
+
+
+class GoalRewardFall(GoalRewardUMaze):
+    OBSERVE_BLOCKS: bool = True
+
+    def __init__(self, scale: float) -> None:
+        super().__init__(scale)
+        self.goals = [MazeGoal(np.array([0.0, 3.375 * scale, 4.5]))]
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B, C, R = MazeCell.EMPTY, MazeCell.BLOCK, MazeCell.CHASM, MazeCell.ROBOT
+        M = MazeCell.YZ_BLOCK
+        return [
+            [B, B, B, B],
+            [B, R, E, B],
+            [B, E, M, B],
+            [B, C, C, B],
+            [B, E, E, B],
+            [B, B, B, B],
+        ]
+
+
+class DistRewardFall(GoalRewardFall, DistRewardMixIn):
+    pass
+
+
+class GoalReward2Rooms(MazeTask):
+    REWARD_THRESHOLD: float = 0.9
+    PENALTY: float = -0.0001
+    MAZE_SIZE_SCALING: Scaling = Scaling(4.0, 4.0, 4.0)
+
+    def __init__(self, scale: float, goal: Tuple[int, int] = (4.0, -2.0)) -> None:
+        super().__init__(scale)
+        self.goals = [MazeGoal(np.array(goal) * scale)]
+
+    def reward(self, obs: np.ndarray) -> float:
+        for goal in self.goals:
+            if goal.neighbor(obs):
+                return goal.reward_scale
+        return self.PENALTY
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B, R = MazeCell.EMPTY, MazeCell.BLOCK, MazeCell.ROBOT
+        return [
+            [B, B, B, B, B, B, B, B],
+            [B, E, E, E, B, E, E, B],
+            [B, E, E, E, B, E, E, B],
+            [B, E, R, E, B, E, E, B],
+            [B, E, E, E, B, E, E, B],
+            [B, E, E, E, E, E, E, B],
+            [B, B, B, B, B, B, B, B],
+        ]
+
+
+class DistReward2Rooms(GoalReward2Rooms, DistRewardMixIn):
+    pass
+
+
+class SubGoal2Rooms(GoalReward2Rooms):
+    def __init__(
+        self,
+        scale: float,
+        primary_goal: Tuple[float, float] = (4.0, -2.0),
+        subgoals: List[Tuple[float, float]] = [(1.0, -2.0), (-1.0, 2.0)],
+    ) -> None:
+        super().__init__(scale, primary_goal)
+        for subgoal in subgoals:
+            self.goals.append(
+                MazeGoal(np.array(subgoal) * scale, reward_scale=0.5, rgb=GREEN)
+            )
+
+
+class GoalReward4Rooms(MazeTask):
+    REWARD_THRESHOLD: float = 0.9
+    PENALTY: float = -0.0001
+    MAZE_SIZE_SCALING: Scaling = Scaling(4.0, 4.0, 4.0)
+
+    def __init__(self, scale: float) -> None:
+        super().__init__(scale)
+        self.goals = [MazeGoal(np.array([6.0 * scale, -6.0 * scale]))]
+
+    def reward(self, obs: np.ndarray) -> float:
+        for goal in self.goals:
+            if goal.neighbor(obs):
+                return goal.reward_scale
+        return self.PENALTY
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B, R = MazeCell.EMPTY, MazeCell.BLOCK, MazeCell.ROBOT
+        return [
+            [B, B, B, B, B, B, B, B, B],
+            [B, E, E, E, B, E, E, E, B],
+            [B, E, E, E, E, E, E, E, B],
+            [B, E, E, E, B, E, E, E, B],
+            [B, B, E, B, B, B, E, B, B],
+            [B, E, E, E, B, E, E, E, B],
+            [B, E, E, E, E, E, E, E, B],
+            [B, R, E, E, B, E, E, E, B],
+            [B, B, B, B, B, B, B, B, B],
+        ]
+
+
+class DistReward4Rooms(GoalReward4Rooms, DistRewardMixIn):
+    pass
+
+
+class SubGoal4Rooms(GoalReward4Rooms):
+    def __init__(self, scale: float) -> None:
+        super().__init__(scale)
+        self.goals += [
+            MazeGoal(np.array([0.0 * scale, -6.0 * scale]), 0.5, GREEN),
+            MazeGoal(np.array([6.0 * scale, 0.0 * scale]), 0.5, GREEN),
+        ]
+
+
+class GoalRewardTRoom(MazeTask):
+    REWARD_THRESHOLD: float = 0.9
+    PENALTY: float = -0.0001
+    MAZE_SIZE_SCALING: Scaling = Scaling(4.0, 4.0, 4.0)
+
+    def __init__(self, scale: float, goal: Tuple[float, float] = (2.0, -3.0)) -> None:
+        super().__init__(scale)
+        self.goals = [MazeGoal(np.array(goal) * scale)]
+
+    def reward(self, obs: np.ndarray) -> float:
+        for goal in self.goals:
+            if goal.neighbor(obs):
+                return goal.reward_scale
+        return self.PENALTY
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B, R = MazeCell.EMPTY, MazeCell.BLOCK, MazeCell.ROBOT
+        return [
+            [B, B, B, B, B, B, B],
+            [B, E, E, B, E, E, B],
+            [B, E, E, B, E, E, B],
+            [B, E, B, B, B, E, B],
+            [B, E, E, R, E, E, B],
+            [B, B, B, B, B, B, B],
+        ]
+
+
+class DistRewardTRoom(GoalRewardTRoom, DistRewardMixIn):
+    pass
+
+
+class SubGoalTRoom(GoalRewardTRoom):
+    def __init__(
+        self,
+        scale: float,
+        primary_goal: Tuple[float, float] = (2.0, -3.0),
+        subgoal: Tuple[float, float] = (-2.0, -3.0),
+    ) -> None:
+        super().__init__(scale, primary_goal)
+        self.goals.append(
+            MazeGoal(np.array(subgoal) * scale, reward_scale=0.5, rgb=GREEN)
+        )
+
+
+class GoalRewardBlockMaze(GoalRewardUMaze):
+    MAZE_SIZE_SCALING: Scaling = Scaling(8.0, 4.0, None)
+    OBSERVE_BLOCKS: bool = True
+
+    def __init__(self, scale: float) -> None:
+        super().__init__(scale)
+        self.goals = [MazeGoal(np.array([0.0, 3.0 * scale]))]
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B, R = MazeCell.EMPTY, MazeCell.BLOCK, MazeCell.ROBOT
+        M = MazeCell.XY_BLOCK
+        return [
+            [B, B, B, B, B],
+            [B, R, E, E, B],
+            [B, B, B, M, B],
+            [B, E, E, E, B],
+            [B, E, E, E, B],
+            [B, B, B, B, B],
+        ]
+
+
+class DistRewardBlockMaze(GoalRewardBlockMaze, DistRewardMixIn):
+    pass
+
+
+class GoalRewardBilliard(MazeTask):
+    REWARD_THRESHOLD: float = 0.9
+    PENALTY: float = -0.0001
+    MAZE_SIZE_SCALING: Scaling = Scaling(None, 3.0, None)
+    OBSERVE_BALLS: bool = True
+    GOAL_SIZE: float = 0.3
+
+    def __init__(self, scale: float, goal: Tuple[float, float] = (2.0, -3.0)) -> None:
+        super().__init__(scale)
+        goal = np.array(goal) * scale
+        self.goals.append(
+            MazeGoal(goal, threshold=self._threshold(), custom_size=self.GOAL_SIZE)
+        )
+
+    def _threshold(self) -> float:
+        return self.OBJECT_BALL_SIZE + self.GOAL_SIZE
+
+    def reward(self, obs: np.ndarray) -> float:
+        object_pos = obs[3:6]
+        for goal in self.goals:
+            if goal.neighbor(object_pos):
+                return goal.reward_scale
+        return self.PENALTY
+
+    def termination(self, obs: np.ndarray) -> bool:
+        object_pos = obs[3:6]
+        for goal in self.goals:
+            if goal.neighbor(object_pos):
+                return True
+        return False
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B = MazeCell.EMPTY, MazeCell.BLOCK
+        R, M = MazeCell.ROBOT, MazeCell.OBJECT_BALL
+        return [
+            [B, B, B, B, B, B, B],
+            [B, E, E, E, E, E, B],
+            [B, E, E, E, E, E, B],
+            [B, E, E, M, E, E, B],
+            [B, E, E, R, E, E, B],
+            [B, E, E, E, E, E, B],
+            [B, B, B, B, B, B, B],
+        ]
+
+
+class DistRewardBilliard(GoalRewardBilliard):
+    def reward(self, obs: np.ndarray) -> float:
+        return -self.goals[0].euc_dist(obs[3:6]) / self.scale
+
+
+class SubGoalBilliard(GoalRewardBilliard):
+    def __init__(
+        self,
+        scale: float,
+        primary_goal: Tuple[float, float] = (2.0, -3.0),
+        subgoals: List[Tuple[float, float]] = [(-2.0, -3.0), (-2.0, 1.0), (2.0, 1.0)],
+    ) -> None:
+        super().__init__(scale, primary_goal)
+        for subgoal in subgoals:
+            self.goals.append(
+                MazeGoal(
+                    np.array(subgoal) * scale,
+                    reward_scale=0.5,
+                    rgb=GREEN,
+                    threshold=self._threshold(),
+                    custom_size=self.GOAL_SIZE,
+                )
+            )
+
+
+class BanditBilliard(SubGoalBilliard):
+    def __init__(
+        self,
+        scale: float,
+        primary_goal: Tuple[float, float] = (4.0, -2.0),
+        subgoals: List[Tuple[float, float]] = [(4.0, 2.0)],
+    ) -> None:
+        super().__init__(scale, primary_goal, subgoals)
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B = MazeCell.EMPTY, MazeCell.BLOCK
+        R, M = MazeCell.ROBOT, MazeCell.OBJECT_BALL
+        return [
+            [B, B, B, B, B, B, B],
+            [B, E, E, B, B, E, B],
+            [B, E, E, E, E, E, B],
+            [B, R, M, E, B, B, B],
+            [B, E, E, E, E, E, B],
+            [B, E, E, E, E, E, B],
+            [B, B, B, B, B, B, B],
+        ]
+
+
+class TaskRegistry:
+    REGISTRY: Dict[str, List[Type[MazeTask]]] = {
+        "SimpleRoom": [DistRewardSimpleRoom, GoalRewardSimpleRoom],
+        "UMaze": [DistRewardUMaze, GoalRewardUMaze],
+        "Push": [DistRewardPush, GoalRewardPush],
+        "Fall": [DistRewardFall, GoalRewardFall],
+        "2Rooms": [DistReward2Rooms, GoalReward2Rooms, SubGoal2Rooms],
+        "4Rooms": [DistReward4Rooms, GoalReward4Rooms, SubGoal4Rooms],
+        "TRoom": [DistRewardTRoom, GoalRewardTRoom, SubGoalTRoom],
+        "BlockMaze": [DistRewardBlockMaze, GoalRewardBlockMaze],
+        "Billiard": [
+            DistRewardBilliard,
+            GoalRewardBilliard,
+            SubGoalBilliard,
+            BanditBilliard,
+        ],
+    }
+
+    @staticmethod
+    def keys() -> List[str]:
+        return list(TaskRegistry.REGISTRY.keys())
+
+    @staticmethod
+    def tasks(key: str) -> List[Type[MazeTask]]:
+        return TaskRegistry.REGISTRY[key]
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/point.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/point.py
new file mode 100644
index 0000000..4602746
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/point.py
@@ -0,0 +1,81 @@
+"""
+A ball-like robot as an explorer in the maze.
+Based on `models`_ and `rllab`_.
+
+.. _models: https://github.com/tensorflow/models/tree/master/research/efficient-hrl
+.. _rllab: https://github.com/rll/rllab
+"""
+
+from typing import Optional, Tuple
+
+import gym
+import numpy as np
+
+from slbo.envs.mujoco_maze.agent_model import AgentModel
+
+
+class PointEnv(AgentModel):
+    FILE: str = "point.xml"
+    ORI_IND: int = 2
+    MANUAL_COLLISION: bool = True
+    RADIUS: float = 0.4
+
+    VELOCITY_LIMITS: float = 10.0
+
+    def __init__(self, file_path: Optional[str] = None):
+        super().__init__(file_path, 1)
+        high = np.inf * np.ones(6, dtype=np.float32)
+        high[3:] = self.VELOCITY_LIMITS * 1.2
+        high[self.ORI_IND] = np.pi
+        low = -high
+        self.observation_space = gym.spaces.Box(low, high)
+
+    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, dict]:
+        qpos = self.sim.data.qpos.copy()
+        qpos[2] += action[1]
+        # Clip orientation
+        if qpos[2] < -np.pi:
+            qpos[2] += np.pi * 2
+        elif np.pi < qpos[2]:
+            qpos[2] -= np.pi * 2
+        ori = qpos[2]
+        # Compute increment in each direction
+        qpos[0] += np.cos(ori) * action[0]
+        qpos[1] += np.sin(ori) * action[0]
+        qvel = np.clip(self.sim.data.qvel, -self.VELOCITY_LIMITS, self.VELOCITY_LIMITS)
+        self.set_state(qpos, qvel)
+        for _ in range(0, self.frame_skip):
+            self.sim.step()
+        next_obs = self._get_obs()
+        return next_obs, 0.0, False, {}
+
+    def _get_obs(self):
+        return np.concatenate(
+            [
+                self.sim.data.qpos.flat[:3],  # Only point-relevant coords.
+                self.sim.data.qvel.flat[:3],
+            ]
+        )
+
+    def reset_model(self):
+        qpos = self.init_qpos + self.np_random.uniform(
+            size=self.sim.model.nq, low=-0.1, high=0.1
+        )
+        qvel = self.init_qvel + self.np_random.randn(self.sim.model.nv) * 0.1
+
+        # Set everything other than point to original position and 0 velocity.
+        qpos[3:] = self.init_qpos[3:]
+        qvel[3:] = 0.0
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def get_xy(self):
+        return self.sim.data.qpos[:2].copy()
+
+    def set_xy(self, xy: np.ndarray) -> None:
+        qpos = self.sim.data.qpos.copy()
+        qpos[:2] = xy
+        self.set_state(qpos, self.sim.data.qvel)
+
+    def get_ori(self):
+        return self.sim.data.qpos[self.ORI_IND]
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/reacher.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/reacher.py
new file mode 100644
index 0000000..d2db6aa
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/reacher.py
@@ -0,0 +1,72 @@
+"""
+Based on the reacher in `dm_control`_.
+
+.. _gym: https://github.com/openai/gym
+"""
+
+from typing import Tuple
+
+import numpy as np
+
+from slbo.envs.mujoco_maze.agent_model import AgentModel
+from slbo.envs.mujoco_maze.ant import ForwardRewardFn, forward_reward_vnorm
+
+
+class ReacherEnv(AgentModel):
+    FILE: str = "reacher.xml"
+    MANUAL_COLLISION: bool = False
+
+    def __init__(
+        self,
+        file_path: str = None,
+        forward_reward_weight: float = 1.0,
+        ctrl_cost_weight: float = 1e-4,
+        forward_reward_fn: ForwardRewardFn = forward_reward_vnorm,
+    ) -> None:
+        self._forward_reward_weight = forward_reward_weight
+        self._ctrl_cost_weight = ctrl_cost_weight
+        self._forward_reward_fn = forward_reward_fn
+        super().__init__(file_path, 4)
+
+    def _forward_reward(self, xy_pos_before: np.ndarray) -> Tuple[float, np.ndarray]:
+        xy_pos_after = self.sim.data.qpos[:2].copy()
+        xy_velocity = (xy_pos_after - xy_pos_before) / self.dt
+        return self._forward_reward_fn(xy_velocity)
+
+    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, dict]:
+        xy_pos_before = self.sim.data.qpos[:2].copy()
+        self.do_simulation(action, self.frame_skip)
+
+        forward_reward = self._forward_reward(xy_pos_before)
+        ctrl_cost = self._ctrl_cost_weight * np.sum(np.square(action))
+        return (
+            self._get_obs(),
+            self._forward_reward_weight * forward_reward - ctrl_cost,
+            False,
+            dict(reward_forward=forward_reward, reward_ctrl=-ctrl_cost),
+        )
+
+    def _get_obs(self) -> np.ndarray:
+        position = self.sim.data.qpos.flat.copy()
+        velocity = self.sim.data.qvel.flat.copy()
+        observation = np.concatenate([position, velocity]).ravel()
+        return observation
+
+    def reset_model(self) -> np.ndarray:
+        qpos = self.init_qpos + self.np_random.uniform(
+            low=-0.1, high=0.1, size=self.model.nq,
+        )
+        qvel = self.init_qvel + self.np_random.uniform(
+            low=-0.1, high=0.1, size=self.model.nv,
+        )
+
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def set_xy(self, xy: np.ndarray) -> None:
+        qpos = self.sim.data.qpos.copy()
+        qpos[:2] = xy
+        self.set_state(qpos, self.sim.data.qvel)
+
+    def get_xy(self) -> np.ndarray:
+        return np.copy(self.sim.data.qpos[:2])
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/swimmer.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/swimmer.py
new file mode 100644
index 0000000..cd825df
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/swimmer.py
@@ -0,0 +1,73 @@
+"""
+Swimmer robot as an explorer in the maze.
+Based on `gym`_ (swimmer-v3).
+
+.. _gym: https://github.com/openai/gym
+"""
+
+from typing import Tuple
+
+import numpy as np
+
+from slbo.envs.mujoco_maze.agent_model import AgentModel
+from slbo.envs.mujoco_maze.ant import ForwardRewardFn, forward_reward_vnorm
+
+
+class SwimmerEnv(AgentModel):
+    FILE: str = "swimmer.xml"
+    MANUAL_COLLISION: bool = False
+
+    def __init__(
+        self,
+        file_path: str = None,
+        forward_reward_weight: float = 1.0,
+        ctrl_cost_weight: float = 1e-4,
+        forward_reward_fn: ForwardRewardFn = forward_reward_vnorm,
+    ) -> None:
+        self._forward_reward_weight = forward_reward_weight
+        self._ctrl_cost_weight = ctrl_cost_weight
+        self._forward_reward_fn = forward_reward_fn
+        super().__init__(file_path, 4)
+
+    def _forward_reward(self, xy_pos_before: np.ndarray) -> Tuple[float, np.ndarray]:
+        xy_pos_after = self.sim.data.qpos[:2].copy()
+        xy_velocity = (xy_pos_after - xy_pos_before) / self.dt
+        return self._forward_reward_fn(xy_velocity)
+
+    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, dict]:
+        xy_pos_before = self.sim.data.qpos[:2].copy()
+        self.do_simulation(action, self.frame_skip)
+
+        forward_reward = self._forward_reward(xy_pos_before)
+        ctrl_cost = self._ctrl_cost_weight * np.sum(np.square(action))
+        return (
+            self._get_obs(),
+            self._forward_reward_weight * forward_reward - ctrl_cost,
+            False,
+            dict(reward_forward=forward_reward, reward_ctrl=-ctrl_cost),
+        )
+
+    def _get_obs(self) -> np.ndarray:
+        position = self.sim.data.qpos.flat.copy()
+        velocity = self.sim.data.qvel.flat.copy()
+        observation = np.concatenate([position, velocity]).ravel()
+        return observation
+
+    def reset_model(self) -> np.ndarray:
+        qpos = self.init_qpos + self.np_random.uniform(
+            low=-0.1, high=0.1, size=self.model.nq,
+        )
+        qvel = self.init_qvel + self.np_random.uniform(
+            low=-0.1, high=0.1, size=self.model.nv,
+        )
+
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def set_xy(self, xy: np.ndarray) -> None:
+        qpos = self.sim.data.qpos.copy()
+        qpos[:2] = xy
+        self.set_state(qpos, self.sim.data.qvel)
+
+    def get_xy(self) -> np.ndarray:
+        return np.copy(self.sim.data.qpos[:2])
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/LICENSE.md b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/LICENSE.md
new file mode 100644
index 0000000..22ce901
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/LICENSE.md
@@ -0,0 +1,222 @@
+# Fetch Robotics
+The model of the [Fetch](http://fetchrobotics.com/platforms-research-development/) is based on [models provided by Fetch](https://github.com/fetchrobotics/fetch_ros/tree/indigo-devel/fetch_description). It was adapted and refined by OpenAI.
+
+# ShadowHand
+The model of the [ShadowHand](https://www.shadowrobot.com/products/dexterous-hand/) is based on [models provided by ShadowRobot](https://github.com/shadow-robot/sr_common/tree/kinetic-devel/sr_description/hand/model), and on code used under the following license:
+
+(C) Vikash Kumar, CSE, UW. Licensed under Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
+
+	                                 Apache License
+	                           Version 2.0, January 2004
+	                        http://www.apache.org/licenses/
+
+	   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+	   1. Definitions.
+
+	      "License" shall mean the terms and conditions for use, reproduction,
+	      and distribution as defined by Sections 1 through 9 of this document.
+
+	      "Licensor" shall mean the copyright owner or entity authorized by
+	      the copyright owner that is granting the License.
+
+	      "Legal Entity" shall mean the union of the acting entity and all
+	      other entities that control, are controlled by, or are under common
+	      control with that entity. For the purposes of this definition,
+	      "control" means (i) the power, direct or indirect, to cause the
+	      direction or management of such entity, whether by contract or
+	      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+	      outstanding shares, or (iii) beneficial ownership of such entity.
+
+	      "You" (or "Your") shall mean an individual or Legal Entity
+	      exercising permissions granted by this License.
+
+	      "Source" form shall mean the preferred form for making modifications,
+	      including but not limited to software source code, documentation
+	      source, and configuration files.
+
+	      "Object" form shall mean any form resulting from mechanical
+	      transformation or translation of a Source form, including but
+	      not limited to compiled object code, generated documentation,
+	      and conversions to other media types.
+
+	      "Work" shall mean the work of authorship, whether in Source or
+	      Object form, made available under the License, as indicated by a
+	      copyright notice that is included in or attached to the work
+	      (an example is provided in the Appendix below).
+
+	      "Derivative Works" shall mean any work, whether in Source or Object
+	      form, that is based on (or derived from) the Work and for which the
+	      editorial revisions, annotations, elaborations, or other modifications
+	      represent, as a whole, an original work of authorship. For the purposes
+	      of this License, Derivative Works shall not include works that remain
+	      separable from, or merely link (or bind by name) to the interfaces of,
+	      the Work and Derivative Works thereof.
+
+	      "Contribution" shall mean any work of authorship, including
+	      the original version of the Work and any modifications or additions
+	      to that Work or Derivative Works thereof, that is intentionally
+	      submitted to Licensor for inclusion in the Work by the copyright owner
+	      or by an individual or Legal Entity authorized to submit on behalf of
+	      the copyright owner. For the purposes of this definition, "submitted"
+	      means any form of electronic, verbal, or written communication sent
+	      to the Licensor or its representatives, including but not limited to
+	      communication on electronic mailing lists, source code control systems,
+	      and issue tracking systems that are managed by, or on behalf of, the
+	      Licensor for the purpose of discussing and improving the Work, but
+	      excluding communication that is conspicuously marked or otherwise
+	      designated in writing by the copyright owner as "Not a Contribution."
+
+	      "Contributor" shall mean Licensor and any individual or Legal Entity
+	      on behalf of whom a Contribution has been received by Licensor and
+	      subsequently incorporated within the Work.
+
+	   2. Grant of Copyright License. Subject to the terms and conditions of
+	      this License, each Contributor hereby grants to You a perpetual,
+	      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+	      copyright license to reproduce, prepare Derivative Works of,
+	      publicly display, publicly perform, sublicense, and distribute the
+	      Work and such Derivative Works in Source or Object form.
+
+	   3. Grant of Patent License. Subject to the terms and conditions of
+	      this License, each Contributor hereby grants to You a perpetual,
+	      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+	      (except as stated in this section) patent license to make, have made,
+	      use, offer to sell, sell, import, and otherwise transfer the Work,
+	      where such license applies only to those patent claims licensable
+	      by such Contributor that are necessarily infringed by their
+	      Contribution(s) alone or by combination of their Contribution(s)
+	      with the Work to which such Contribution(s) was submitted. If You
+	      institute patent litigation against any entity (including a
+	      cross-claim or counterclaim in a lawsuit) alleging that the Work
+	      or a Contribution incorporated within the Work constitutes direct
+	      or contributory patent infringement, then any patent licenses
+	      granted to You under this License for that Work shall terminate
+	      as of the date such litigation is filed.
+
+	   4. Redistribution. You may reproduce and distribute copies of the
+	      Work or Derivative Works thereof in any medium, with or without
+	      modifications, and in Source or Object form, provided that You
+	      meet the following conditions:
+
+	      (a) You must give any other recipients of the Work or
+	          Derivative Works a copy of this License; and
+
+	      (b) You must cause any modified files to carry prominent notices
+	          stating that You changed the files; and
+
+	      (c) You must retain, in the Source form of any Derivative Works
+	          that You distribute, all copyright, patent, trademark, and
+	          attribution notices from the Source form of the Work,
+	          excluding those notices that do not pertain to any part of
+	          the Derivative Works; and
+
+	      (d) If the Work includes a "NOTICE" text file as part of its
+	          distribution, then any Derivative Works that You distribute must
+	          include a readable copy of the attribution notices contained
+	          within such NOTICE file, excluding those notices that do not
+	          pertain to any part of the Derivative Works, in at least one
+	          of the following places: within a NOTICE text file distributed
+	          as part of the Derivative Works; within the Source form or
+	          documentation, if provided along with the Derivative Works; or,
+	          within a display generated by the Derivative Works, if and
+	          wherever such third-party notices normally appear. The contents
+	          of the NOTICE file are for informational purposes only and
+	          do not modify the License. You may add Your own attribution
+	          notices within Derivative Works that You distribute, alongside
+	          or as an addendum to the NOTICE text from the Work, provided
+	          that such additional attribution notices cannot be construed
+	          as modifying the License.
+
+	      You may add Your own copyright statement to Your modifications and
+	      may provide additional or different license terms and conditions
+	      for use, reproduction, or distribution of Your modifications, or
+	      for any such Derivative Works as a whole, provided Your use,
+	      reproduction, and distribution of the Work otherwise complies with
+	      the conditions stated in this License.
+
+	   5. Submission of Contributions. Unless You explicitly state otherwise,
+	      any Contribution intentionally submitted for inclusion in the Work
+	      by You to the Licensor shall be under the terms and conditions of
+	      this License, without any additional terms or conditions.
+	      Notwithstanding the above, nothing herein shall supersede or modify
+	      the terms of any separate license agreement you may have executed
+	      with Licensor regarding such Contributions.
+
+	   6. Trademarks. This License does not grant permission to use the trade
+	      names, trademarks, service marks, or product names of the Licensor,
+	      except as required for reasonable and customary use in describing the
+	      origin of the Work and reproducing the content of the NOTICE file.
+
+	   7. Disclaimer of Warranty. Unless required by applicable law or
+	      agreed to in writing, Licensor provides the Work (and each
+	      Contributor provides its Contributions) on an "AS IS" BASIS,
+	      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+	      implied, including, without limitation, any warranties or conditions
+	      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+	      PARTICULAR PURPOSE. You are solely responsible for determining the
+	      appropriateness of using or redistributing the Work and assume any
+	      risks associated with Your exercise of permissions under this License.
+
+	   8. Limitation of Liability. In no event and under no legal theory,
+	      whether in tort (including negligence), contract, or otherwise,
+	      unless required by applicable law (such as deliberate and grossly
+	      negligent acts) or agreed to in writing, shall any Contributor be
+	      liable to You for damages, including any direct, indirect, special,
+	      incidental, or consequential damages of any character arising as a
+	      result of this License or out of the use or inability to use the
+	      Work (including but not limited to damages for loss of goodwill,
+	      work stoppage, computer failure or malfunction, or any and all
+	      other commercial damages or losses), even if such Contributor
+	      has been advised of the possibility of such damages.
+
+	   9. Accepting Warranty or Additional Liability. While redistributing
+	      the Work or Derivative Works thereof, You may choose to offer,
+	      and charge a fee for, acceptance of support, warranty, indemnity,
+	      or other liability obligations and/or rights consistent with this
+	      License. However, in accepting such obligations, You may act only
+	      on Your own behalf and on Your sole responsibility, not on behalf
+	      of any other Contributor, and only if You agree to indemnify,
+	      defend, and hold each Contributor harmless for any liability
+	      incurred by, or claims asserted against, such Contributor by reason
+	      of your accepting any such warranty or additional liability.
+
+	   END OF TERMS AND CONDITIONS
+
+	   APPENDIX: How to apply the Apache License to your work.
+
+	      To apply the Apache License to your work, attach the following
+	      boilerplate notice, with the fields enclosed by brackets "[]"
+	      replaced with your own identifying information. (Don't include
+	      the brackets!)  The text should be enclosed in the appropriate
+	      comment syntax for the file format. We also recommend that a
+	      file or class name and description of purpose be included on the
+	      same "printed page" as the copyright notice for easier
+	      identification within third-party archives.
+
+	   Copyright [yyyy] [name of copyright owner]
+
+	   Licensed under the Apache License, Version 2.0 (the "License");
+	   you may not use this file except in compliance with the License.
+	   You may obtain a copy of the License at
+
+	       http://www.apache.org/licenses/LICENSE-2.0
+
+	   Unless required by applicable law or agreed to in writing, software
+	   distributed under the License is distributed on an "AS IS" BASIS,
+	   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+	   See the License for the specific language governing permissions and
+	   limitations under the License.
+
+Additional license notices:
+
+	Sources		: 1) Manipulator and Manipulation in High Dimensional Spaces. Vikash Kumar, Ph.D. Thesis, CSE, Univ. of Washington. 2016.
+
+	Mujoco		:: Advanced physics simulation engine
+		Source		: www.roboti.us
+		Version		: 1.40
+		Released 	: 17Jan'17
+
+	Author		:: Vikash Kumar
+		Contacts 	: vikash@openai.com
+		Last edits 	: 3Apr'17
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/pick_and_place.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/pick_and_place.xml
new file mode 100644
index 0000000..337032a
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/pick_and_place.xml
@@ -0,0 +1,35 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+	<compiler angle="radian" coordinate="local" meshdir="../stls/fetch" texturedir="../textures"></compiler>
+	<option timestep="0.002">
+		<flag warmstart="enable"></flag>
+	</option>
+
+	<include file="shared.xml"></include>
+	
+	<worldbody>
+		<geom name="floor0" pos="0.8 0.75 0" size="0.85 0.7 1" type="plane" condim="3" material="floor_mat"></geom>
+		<body name="floor0" pos="0.8 0.75 0">
+			<site name="target0" pos="0 0 0.5" size="0.02 0.02 0.02" rgba="1 0 0 1" type="sphere"></site>
+		</body>
+
+		<include file="robot.xml"></include>
+		
+		<body pos="1.3 0.75 0.2" name="table0">
+			<geom size="0.25 0.35 0.2" type="box" mass="2000" material="table_mat"></geom>
+		</body>
+		
+		<body name="object0" pos="0.025 0.025 0.025">
+			<joint name="object0:joint" type="free" damping="0.01"></joint>
+			<geom size="0.025 0.025 0.025" type="box" condim="3" name="object0" material="block_mat" mass="2"></geom>
+			<site name="object0" pos="0 0 0" size="0.02 0.02 0.02" rgba="1 0 0 1" type="sphere"></site>
+		</body>
+
+		<light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 0 4" dir="0 0 -1" name="light0"></light>
+	</worldbody>
+
+	<actuator>
+		<position ctrllimited="true" ctrlrange="0 0.2" joint="robot0:l_gripper_finger_joint" kp="30000" name="robot0:l_gripper_finger_joint" user="1"></position>
+		<position ctrllimited="true" ctrlrange="0 0.2" joint="robot0:r_gripper_finger_joint" kp="30000" name="robot0:r_gripper_finger_joint" user="1"></position>
+	</actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/push.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/push.xml
new file mode 100644
index 0000000..8e12db2
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/push.xml
@@ -0,0 +1,32 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+	<compiler angle="radian" coordinate="local" meshdir="../stls/fetch" texturedir="../textures"></compiler>
+	<option timestep="0.002">
+		<flag warmstart="enable"></flag>
+	</option>
+
+	<include file="shared.xml"></include>
+	
+	<worldbody>
+		<geom name="floor0" pos="0.8 0.75 0" size="0.85 0.70 1" type="plane" condim="3" material="floor_mat"></geom>
+		<body name="floor0" pos="0.8 0.75 0">
+			<site name="target0" pos="0 0 0.5" size="0.02 0.02 0.02" rgba="1 0 0 1" type="sphere"></site>
+		</body>
+
+		<include file="robot.xml"></include>
+
+		<body pos="1.3 0.75 0.2" name="table0">
+			<geom size="0.25 0.35 0.2" type="box" mass="2000" material="table_mat"></geom>
+		</body>
+		
+		<body name="object0" pos="0.025 0.025 0.025">
+			<joint name="object0:joint" type="free" damping="0.01"></joint>
+			<geom size="0.025 0.025 0.025" type="box" condim="3" name="object0" material="block_mat" mass="2"></geom>
+			<site name="object0" pos="0 0 0" size="0.02 0.02 0.02" rgba="1 0 0 1" type="sphere"></site>
+		</body>
+
+		<light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 0 4" dir="0 0 -1" name="light0"></light>
+	</worldbody>
+	
+	<actuator></actuator>
+</mujoco>
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/reach.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/reach.xml
new file mode 100644
index 0000000..c73d624
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/reach.xml
@@ -0,0 +1,26 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+	<compiler angle="radian" coordinate="local" meshdir="../stls/fetch" texturedir="../textures"></compiler>
+	<option timestep="0.002">
+		<flag warmstart="enable"></flag>
+	</option>
+
+	<include file="shared.xml"></include>
+	
+	<worldbody>
+		<geom name="floor0" pos="0.8 0.75 0" size="0.85 0.7 1" type="plane" condim="3" material="floor_mat"></geom>
+		<body name="floor0" pos="0.8 0.75 0">
+			<site name="target0" pos="0 0 0.5" size="0.02 0.02 0.02" rgba="1 0 0 1" type="sphere"></site>
+		</body>
+
+		<include file="robot.xml"></include>
+		
+		<body pos="1.3 0.75 0.2" name="table0">
+			<geom size="0.25 0.35 0.2" type="box" mass="2000" material="table_mat"></geom>
+		</body>
+
+		<light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 0 4" dir="0 0 -1" name="light0"></light>
+	</worldbody>
+	
+	<actuator></actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/robot.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/robot.xml
new file mode 100644
index 0000000..b627d49
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/robot.xml
@@ -0,0 +1,123 @@
+<mujoco>
+	<body mocap="true" name="robot0:mocap" pos="0 0 0">
+		<geom conaffinity="0" contype="0" pos="0 0 0" rgba="0 0.5 0 0.7" size="0.005 0.005 0.005" type="box"></geom>
+		<geom conaffinity="0" contype="0" pos="0 0 0" rgba="0 0.5 0 0.1" size="1 0.005 0.005" type="box"></geom>
+		<geom conaffinity="0" contype="0" pos="0 0 0" rgba="0 0.5 0 0.1" size="0.005 1 0.001" type="box"></geom>
+		<geom conaffinity="0" contype="0" pos="0 0 0" rgba="0 0.5 0 0.1" size="0.005 0.005 1" type="box"></geom>
+	</body>
+	<body childclass="robot0:fetch" name="robot0:base_link" pos="0.2869 0.2641 0">
+		<joint armature="0.0001" axis="1 0 0" damping="1e+11" name="robot0:slide0" pos="0 0 0" type="slide"></joint>
+		<joint armature="0.0001" axis="0 1 0" damping="1e+11" name="robot0:slide1" pos="0 0 0" type="slide"></joint>
+		<joint armature="0.0001" axis="0 0 1" damping="1e+11" name="robot0:slide2" pos="0 0 0" type="slide"></joint>
+		<inertial diaginertia="1.2869 1.2236 0.9868" mass="70.1294" pos="-0.0036 0 0.0014" quat="0.7605 -0.0133 -0.0061 0.6491"></inertial>
+		<geom mesh="robot0:base_link" name="robot0:base_link" material="robot0:base_mat" class="robot0:grey"></geom>
+		<body name="robot0:torso_lift_link" pos="-0.0869 0 0.3774">
+			<inertial diaginertia="0.3365 0.3354 0.0943" mass="10.7796" pos="-0.0013 -0.0009 0.2935" quat="0.9993 -0.0006 0.0336 0.0185"></inertial>
+			<joint axis="0 0 1" damping="1e+07" name="robot0:torso_lift_joint" range="0.0386 0.3861" type="slide"></joint>
+			<geom mesh="robot0:torso_lift_link" name="robot0:torso_lift_link" material="robot0:torso_mat"></geom>
+			<body name="robot0:head_pan_link" pos="0.0531 0 0.603">
+				<inertial diaginertia="0.0185 0.0128 0.0095" mass="2.2556" pos="0.0321 0.0161 0.039" quat="0.5148 0.5451 -0.453 0.4823"></inertial>
+				<joint axis="0 0 1" name="robot0:head_pan_joint" range="-1.57 1.57"></joint>
+				<geom mesh="robot0:head_pan_link" name="robot0:head_pan_link" material="robot0:head_mat" class="robot0:grey"></geom>
+				<body name="robot0:head_tilt_link" pos="0.1425 0 0.058">
+					<inertial diaginertia="0.0063 0.0059 0.0014" mass="0.9087" pos="0.0081 0.0025 0.0113" quat="0.6458 0.66 -0.274 0.2689"></inertial>
+					<joint axis="0 1 0" damping="1000" name="robot0:head_tilt_joint" range="-0.76 1.45" ref="0.06"></joint>
+					<geom mesh="robot0:head_tilt_link" name="robot0:head_tilt_link" material="robot0:head_mat" class="robot0:blue"></geom>
+					<body name="robot0:head_camera_link" pos="0.055 0 0.0225">
+						<inertial diaginertia="0 0 0" mass="0" pos="0.055 0 0.0225"></inertial>
+						<body name="robot0:head_camera_rgb_frame" pos="0 0.02 0">
+							<inertial diaginertia="0 0 0" mass="0" pos="0 0.02 0"></inertial>
+							<body name="robot0:head_camera_rgb_optical_frame" pos="0 0 0" quat="0.5 -0.5 0.5 -0.5">
+								<inertial diaginertia="0 0 0" mass="0" pos="0 0 0" quat="0.5 -0.5 0.5 -0.5"></inertial>
+								<camera euler="3.1415 0 0" fovy="50" name="head_camera_rgb" pos="0 0 0"></camera>
+							</body>
+						</body>
+						<body name="robot0:head_camera_depth_frame" pos="0 0.045 0">
+							<inertial diaginertia="0 0 0" mass="0" pos="0 0.045 0"></inertial>
+							<body name="robot0:head_camera_depth_optical_frame" pos="0 0 0" quat="0.5 -0.5 0.5 -0.5">
+								<inertial diaginertia="0 0 0" mass="0" pos="0 0 0" quat="0.5 -0.5 0.5 -0.5"></inertial>
+							</body>
+						</body>
+					</body>
+				</body>
+			</body>
+			<body name="robot0:shoulder_pan_link" pos="0.1195 0 0.3486">
+				<inertial diaginertia="0.009 0.0086 0.0041" mass="2.5587" pos="0.0927 -0.0056 0.0564" quat="-0.1364 0.7624 -0.1562 0.613"></inertial>
+				<joint axis="0 0 1" name="robot0:shoulder_pan_joint" range="-1.6056 1.6056"></joint>
+				<geom mesh="robot0:shoulder_pan_link" name="robot0:shoulder_pan_link" material="robot0:arm_mat"></geom>
+				<body name="robot0:shoulder_lift_link" pos="0.117 0 0.06">
+					<inertial diaginertia="0.0116 0.0112 0.0023" mass="2.6615" pos="0.1432 0.0072 -0.0001" quat="0.4382 0.4382 0.555 0.555"></inertial>
+					<joint axis="0 1 0" name="robot0:shoulder_lift_joint" range="-1.221 1.518"></joint>
+					<geom mesh="robot0:shoulder_lift_link" name="robot0:shoulder_lift_link" material="robot0:arm_mat" class="robot0:blue"></geom>
+					<body name="robot0:upperarm_roll_link" pos="0.219 0 0">
+						<inertial diaginertia="0.0047 0.0045 0.0019" mass="2.3311" pos="0.1165 0.0014 0" quat="-0.0136 0.707 0.0136 0.707"></inertial>
+						<joint axis="1 0 0" limited="false" name="robot0:upperarm_roll_joint"></joint>
+						<geom mesh="robot0:upperarm_roll_link" name="robot0:upperarm_roll_link" material="robot0:arm_mat"></geom>
+						<body name="robot0:elbow_flex_link" pos="0.133 0 0">
+							<inertial diaginertia="0.0086 0.0084 0.002" mass="2.1299" pos="0.1279 0.0073 0" quat="0.4332 0.4332 0.5589 0.5589"></inertial>
+							<joint axis="0 1 0" name="robot0:elbow_flex_joint" range="-2.251 2.251"></joint>
+							<geom mesh="robot0:elbow_flex_link" name="robot0:elbow_flex_link" material="robot0:arm_mat" class="robot0:blue"></geom>
+							<body name="robot0:forearm_roll_link" pos="0.197 0 0">
+								<inertial diaginertia="0.0035 0.0031 0.0015" mass="1.6563" pos="0.1097 -0.0266 0" quat="-0.0715 0.7035 0.0715 0.7035"></inertial>
+								<joint armature="2.7538" axis="1 0 0" damping="3.5247" frictionloss="0" limited="false" name="robot0:forearm_roll_joint" stiffness="10"></joint>
+								<geom mesh="robot0:forearm_roll_link" name="robot0:forearm_roll_link" material="robot0:arm_mat"></geom>
+								<body name="robot0:wrist_flex_link" pos="0.1245 0 0">
+									<inertial diaginertia="0.0042 0.0042 0.0018" mass="1.725" pos="0.0882 0.0009 -0.0001" quat="0.4895 0.4895 0.5103 0.5103"></inertial>
+									<joint axis="0 1 0" name="robot0:wrist_flex_joint" range="-2.16 2.16"></joint>
+									<geom mesh="robot0:wrist_flex_link" name="robot0:wrist_flex_link" material="robot0:arm_mat" class="robot0:blue"></geom>
+									<body name="robot0:wrist_roll_link" pos="0.1385 0 0">
+										<inertial diaginertia="0.0001 0.0001 0.0001" mass="0.1354" pos="0.0095 0.0004 -0.0002"></inertial>
+										<joint axis="1 0 0" limited="false" name="robot0:wrist_roll_joint"></joint>
+										<geom mesh="robot0:wrist_roll_link" name="robot0:wrist_roll_link" material="robot0:arm_mat"></geom>
+										<body euler="0 0 0" name="robot0:gripper_link" pos="0.1664 0 0">
+											<inertial diaginertia="0.0024 0.0019 0.0013" mass="1.5175" pos="-0.09 -0.0001 -0.0017" quat="0 0.7071 0 0.7071"></inertial>
+											<geom mesh="robot0:gripper_link" name="robot0:gripper_link" material="robot0:gripper_mat"></geom>
+											<body name="robot0:gripper_camera_link" pos="0.055 0 0.0225">
+												<body name="robot0:gripper_camera_rgb_frame" pos="0 0.02 0">
+													<body name="robot0:gripper_camera_rgb_optical_frame" pos="0 0 0" quat="0.5 -0.5 0.5 -0.5">
+														<camera euler="3.1415 0 0" fovy="50" name="gripper_camera_rgb" pos="0 0 0"></camera>
+													</body>
+												</body>
+												<body name="robot0:gripper_camera_depth_frame" pos="0 0.045 0">
+													<body name="robot0:gripper_camera_depth_optical_frame" pos="0 0 0" quat="0.5 -0.5 0.5 -0.5"></body>
+												</body>
+											</body>
+
+											<body childclass="robot0:fetchGripper" name="robot0:r_gripper_finger_link" pos="0 0.0159 0">
+												<inertial diaginertia="0.1 0.1 0.1" mass="4" pos="-0.01 0 0"></inertial>
+												<joint axis="0 1 0" name="robot0:r_gripper_finger_joint" range="0 0.05"></joint>
+												<geom pos="0 -0.008 0" size="0.0385 0.007 0.0135" type="box" name="robot0:r_gripper_finger_link" material="robot0:gripper_finger_mat" condim="4" friction="1 0.05 0.01"></geom>
+											</body>
+											<body childclass="robot0:fetchGripper" name="robot0:l_gripper_finger_link" pos="0 -0.0159 0">
+												<inertial diaginertia="0.1 0.1 0.1" mass="4" pos="-0.01 0 0"></inertial>
+												<joint axis="0 -1 0" name="robot0:l_gripper_finger_joint" range="0 0.05"></joint>
+												<geom pos="0 0.008 0" size="0.0385 0.007 0.0135" type="box" name="robot0:l_gripper_finger_link" material="robot0:gripper_finger_mat" condim="4" friction="1 0.05 0.01"></geom>
+											</body>
+											<site name="robot0:grip" pos="0.02 0 0" rgba="0 0 0 0" size="0.02 0.02 0.02"></site>
+										</body>
+									</body>
+								</body>
+							</body>
+						</body>
+					</body>
+				</body>
+			</body>
+		</body>
+		<body name="robot0:estop_link" pos="-0.1246 0.2389 0.3113" quat="0.7071 0.7071 0 0">
+			<inertial diaginertia="0 0 0" mass="0.002" pos="0.0024 -0.0033 0.0067" quat="0.3774 -0.1814 0.1375 0.8977"></inertial>
+			<geom mesh="robot0:estop_link" rgba="0.8 0 0 1" name="robot0:estop_link"></geom>
+		</body>
+		<body name="robot0:laser_link" pos="0.235 0 0.2878" quat="0 1 0 0">
+			<inertial diaginertia="0 0 0" mass="0.0083" pos="-0.0306 0.0007 0.0552" quat="0.5878 0.5378 -0.4578 0.3945"></inertial>
+			<geom mesh="robot0:laser_link" rgba="0.7922 0.8196 0.9333 1" name="robot0:laser_link"></geom>
+			<camera euler="1.55 -1.55 3.14" fovy="25" name="lidar" pos="0 0 0.02"></camera>
+		</body>
+		<body name="robot0:torso_fixed_link" pos="-0.0869 0 0.3774">
+			<inertial diaginertia="0.3865 0.3394 0.1009" mass="13.2775" pos="-0.0722 0.0057 0.2656" quat="0.9995 0.0249 0.0177 0.011"></inertial>
+			<geom mesh="robot0:torso_fixed_link" name="robot0:torso_fixed_link" class="robot0:blue"></geom>
+		</body>
+		<body name="robot0:external_camera_body_0" pos="0 0 0">
+			<camera euler="0 0.75 1.57" fovy="43.3" name="external_camera_0" pos="1.3 0 1.2"></camera>
+		</body>
+	</body>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/shared.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/shared.xml
new file mode 100644
index 0000000..5d61fef
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/shared.xml
@@ -0,0 +1,66 @@
+<mujoco>
+    <asset>
+        <texture type="skybox" builtin="gradient" rgb1="0.44 0.85 0.56" rgb2="0.46 0.87 0.58" width="32" height="32"></texture>
+        <texture name="texture_block" file="block.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+
+        <material name="floor_mat" specular="0" shininess="0.5" reflectance="0" rgba="0.2 0.2 0.2 1"></material>
+        <material name="table_mat" specular="0" shininess="0.5" reflectance="0" rgba="0.93 0.93 0.93 1"></material>
+        <material name="block_mat" specular="0" shininess="0.5" reflectance="0" rgba="0.2 0.2 0.2 1"></material>
+        <material name="puck_mat" specular="0" shininess="0.5" reflectance="0" rgba="0.2 0.2 0.2 1"></material>
+        <material name="robot0:geomMat" shininess="0.03" specular="0.4"></material>
+        <material name="robot0:gripper_finger_mat" shininess="0.03" specular="0.4" reflectance="0"></material>
+        <material name="robot0:gripper_mat" shininess="0.03" specular="0.4" reflectance="0"></material>
+        <material name="robot0:arm_mat" shininess="0.03" specular="0.4" reflectance="0"></material>
+        <material name="robot0:head_mat" shininess="0.03" specular="0.4" reflectance="0"></material>
+        <material name="robot0:torso_mat" shininess="0.03" specular="0.4" reflectance="0"></material>
+        <material name="robot0:base_mat" shininess="0.03" specular="0.4" reflectance="0"></material>
+        
+        <mesh file="base_link_collision.stl" name="robot0:base_link"></mesh>
+        <mesh file="bellows_link_collision.stl" name="robot0:bellows_link"></mesh>
+        <mesh file="elbow_flex_link_collision.stl" name="robot0:elbow_flex_link"></mesh>
+        <mesh file="estop_link.stl" name="robot0:estop_link"></mesh>
+        <mesh file="forearm_roll_link_collision.stl" name="robot0:forearm_roll_link"></mesh>
+        <mesh file="gripper_link.stl" name="robot0:gripper_link"></mesh>
+        <mesh file="head_pan_link_collision.stl" name="robot0:head_pan_link"></mesh>
+        <mesh file="head_tilt_link_collision.stl" name="robot0:head_tilt_link"></mesh>
+        <mesh file="l_wheel_link_collision.stl" name="robot0:l_wheel_link"></mesh>
+        <mesh file="laser_link.stl" name="robot0:laser_link"></mesh>
+        <mesh file="r_wheel_link_collision.stl" name="robot0:r_wheel_link"></mesh>
+        <mesh file="torso_lift_link_collision.stl" name="robot0:torso_lift_link"></mesh>
+        <mesh file="shoulder_pan_link_collision.stl" name="robot0:shoulder_pan_link"></mesh>
+        <mesh file="shoulder_lift_link_collision.stl" name="robot0:shoulder_lift_link"></mesh>
+        <mesh file="upperarm_roll_link_collision.stl" name="robot0:upperarm_roll_link"></mesh>
+        <mesh file="wrist_flex_link_collision.stl" name="robot0:wrist_flex_link"></mesh>
+        <mesh file="wrist_roll_link_collision.stl" name="robot0:wrist_roll_link"></mesh>
+        <mesh file="torso_fixed_link.stl" name="robot0:torso_fixed_link"></mesh>
+    </asset>
+
+    <equality>
+        <weld body1="robot0:mocap" body2="robot0:gripper_link" solimp="0.9 0.95 0.001" solref="0.02 1"></weld>
+    </equality>
+    
+    <contact>
+        <exclude body1="robot0:r_gripper_finger_link" body2="robot0:l_gripper_finger_link"></exclude>
+        <exclude body1="robot0:torso_lift_link" body2="robot0:torso_fixed_link"></exclude>
+        <exclude body1="robot0:torso_lift_link" body2="robot0:shoulder_pan_link"></exclude>
+    </contact>
+    
+    <default>
+        <default class="robot0:fetch">
+            <geom margin="0.001" material="robot0:geomMat" rgba="1 1 1 1" solimp="0.99 0.99 0.01" solref="0.01 1" type="mesh" user="0"></geom>
+            <joint armature="1" damping="50" frictionloss="0" stiffness="0"></joint>
+            
+            <default class="robot0:fetchGripper">
+                <geom condim="4" margin="0.001" type="box" user="0" rgba="0.356 0.361 0.376 1.0"></geom>
+                <joint armature="100" damping="1000" limited="true" solimplimit="0.99 0.999 0.01" solreflimit="0.01 1" type="slide"></joint>
+            </default>
+
+            <default class="robot0:grey">
+                <geom rgba="0.356 0.361 0.376 1.0"></geom>
+            </default>
+            <default class="robot0:blue">
+                <geom rgba="0.086 0.506 0.767 1.0"></geom>
+            </default>
+        </default>
+    </default>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/slide.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/slide.xml
new file mode 100644
index 0000000..efbfb51
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/slide.xml
@@ -0,0 +1,32 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+	<compiler angle="radian" coordinate="local" meshdir="../stls/fetch" texturedir="../textures"></compiler>
+	<option timestep="0.002">
+		<flag warmstart="enable"></flag>
+	</option>
+
+	<include file="shared.xml"></include>
+	
+	<worldbody>
+		<geom name="floor0" pos="1 0.75 0" size="1.05 0.7 1" type="plane" condim="3" material="floor_mat"></geom>
+		<body name="floor0" pos="1 0.75 0">
+			<site name="target0" pos="0 0 0.5" size="0.02 0.02 0.02" rgba="1 0 0 1" type="sphere"></site>
+		</body>
+
+		<include file="robot.xml"></include>
+		
+		<body name="table0" pos="1.32441906 0.75018422 0.2">
+			<geom size="0.625 0.45 0.2" type="box" condim="3" name="table0" material="table_mat" mass="2000" friction="0.1 0.005 0.0001"></geom>
+		</body>
+
+		<body name="object0" pos="0.025 0.025 0.02">
+			<joint name="object0:joint" type="free" damping="0.01"></joint>
+			<geom size="0.025 0.02" type="cylinder" condim="3" name="object0" material="puck_mat" friction="0.1 0.005 0.0001" mass="2"></geom>
+			<site name="object0" pos="0 0 0" size="0.02 0.02 0.02" rgba="1 0 0 1" type="sphere"></site>
+		</body>
+
+		<light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 0 4" dir="0 0 -1" name="light0"></light>
+	</worldbody>
+
+	<actuator></actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_block.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_block.xml
new file mode 100644
index 0000000..83a6517
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_block.xml
@@ -0,0 +1,41 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+    <compiler angle="radian" coordinate="local" meshdir="../stls/hand" texturedir="../textures"></compiler>
+    <option timestep="0.002" iterations="20" apirate="200">
+        <flag warmstart="enable"></flag>
+    </option>
+
+    <include file="shared.xml"></include>
+
+    <asset>
+        <include file="shared_asset.xml"></include>
+
+        <texture name="texture:object" file="block.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+        <texture name="texture:hidden" file="block_hidden.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+
+        <material name="material:object" texture="texture:object" specular="1" shininess="0.3" reflectance="0"></material>
+        <material name="material:hidden" texture="texture:hidden" specular="1" shininess="0.3" reflectance="0"></material>
+        <material name="material:target" texture="texture:object" specular="1" shininess="0.3" reflectance="0" rgba="1 1 1 0.5"></material>
+    </asset>
+
+    <worldbody>
+        <geom name="floor0" pos="1 1 0" size="1 1 1" type="plane" condim="3" material="floor_mat"></geom>
+        <body name="floor0" pos="1 1 0"></body>
+
+        <include file="robot.xml"></include>
+
+        <body name="object" pos="1 0.87 0.2">
+            <geom name="object" type="box" size="0.025 0.025 0.025" material="material:object" condim="4" density="567"></geom>
+            <geom name="object_hidden" type="box" size="0.024 0.024 0.024" material="material:hidden" condim="4" contype="0" conaffinity="0" mass="0"></geom>
+            <site name="object:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <joint name="object:joint" type="free" damping="0.01"></joint>
+        </body>
+        <body name="target" pos="1 0.87 0.2">
+            <geom name="target" type="box" size="0.025 0.025 0.025" material="material:target" condim="4" group="2" contype="0" conaffinity="0"></geom>
+            <site name="target:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <joint name="target:joint" type="free" damping="0.01"></joint>
+        </body>
+        
+        <light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 1 4" dir="0 0 -1" name="light0"></light>
+    </worldbody>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_block_touch_sensors.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_block_touch_sensors.xml
new file mode 100644
index 0000000..b649f10
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_block_touch_sensors.xml
@@ -0,0 +1,42 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+    <compiler angle="radian" coordinate="local" meshdir="../stls/hand" texturedir="../textures"></compiler>
+    <option timestep="0.002" iterations="20" apirate="200">
+        <flag warmstart="enable"></flag>
+    </option>
+
+    <include file="shared.xml"></include>
+    <include file="shared_touch_sensors_92.xml"></include>
+
+    <asset>
+        <include file="shared_asset.xml"></include>
+
+        <texture name="texture:object" file="block.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+        <texture name="texture:hidden" file="block_hidden.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+
+        <material name="material:object" texture="texture:object" specular="1" shininess="0.3" reflectance="0"></material>
+        <material name="material:hidden" texture="texture:hidden" specular="1" shininess="0.3" reflectance="0"></material>
+        <material name="material:target" texture="texture:object" specular="1" shininess="0.3" reflectance="0" rgba="1 1 1 0.5"></material>
+    </asset>
+
+    <worldbody>
+        <geom name="floor0" pos="1 1 0" size="1 1 1" type="plane" condim="3" material="floor_mat"></geom>
+        <body name="floor0" pos="1 1 0"></body>
+
+        <include file="robot_touch_sensors_92.xml"></include>
+
+        <body name="object" pos="1 0.87 0.2">
+            <geom name="object" type="box" size="0.025 0.025 0.025" material="material:object" condim="4" density="567"></geom>
+            <geom name="object_hidden" type="box" size="0.024 0.024 0.024" material="material:hidden" condim="4" contype="0" conaffinity="0" mass="0"></geom>
+            <site name="object:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <joint name="object:joint" type="free" damping="0.01"></joint>
+        </body>
+        <body name="target" pos="1 0.87 0.2">
+            <geom name="target" type="box" size="0.025 0.025 0.025" material="material:target" condim="4" group="2" contype="0" conaffinity="0"></geom>
+            <site name="target:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <joint name="target:joint" type="free" damping="0.01"></joint>
+        </body>
+
+        <light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 1 4" dir="0 0 -1" name="light0"></light>
+    </worldbody>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_egg.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_egg.xml
new file mode 100644
index 0000000..d60217f
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_egg.xml
@@ -0,0 +1,41 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+    <compiler angle="radian" coordinate="local" meshdir="../stls/hand" texturedir="../textures"></compiler>
+    <option timestep="0.002" iterations="20" apirate="200">
+        <flag warmstart="enable"></flag>
+    </option>
+
+    <include file="shared.xml"></include>
+
+    <asset>
+        <include file="shared_asset.xml"></include>
+
+        <texture name="texture:object" file="block.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+        <texture name="texture:hidden" file="block_hidden.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+
+        <material name="material:object" texture="texture:object" specular="1" shininess="0.3" reflectance="0"></material>
+        <material name="material:hidden" texture="texture:hidden" specular="1" shininess="0.3" reflectance="0"></material>
+        <material name="material:target" texture="texture:object" specular="1" shininess="0.3" reflectance="0" rgba="1 1 1 0.5"></material>
+    </asset>
+
+    <worldbody>
+        <geom name="floor0" pos="1 1 0" size="1 1 1" type="plane" condim="3" material="floor_mat"></geom>
+        <body name="floor0" pos="1 1 0"></body>
+
+        <include file="robot.xml"></include>
+
+        <body name="object" pos="1 0.87 0.2">
+            <geom name="object" type="ellipsoid" size="0.03 0.03 0.04" material="material:object" condim="4"></geom>
+            <geom name="object_hidden" type="ellipsoid" size="0.029 0.029 0.03" material="material:hidden" condim="4" contype="0" conaffinity="0" mass="0"></geom>
+            <site name="object:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <joint name="object:joint" type="free" damping="0.01"></joint>
+        </body>
+        <body name="target" pos="1 0.87 0.2">
+            <geom name="target" type="ellipsoid" size="0.03 0.03 0.04" material="material:target" condim="4" group="2" contype="0" conaffinity="0"></geom>
+            <site name="target:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <joint name="target:joint" type="free" damping="0.01"></joint>
+        </body>
+        
+        <light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 1 4" dir="0 0 -1" name="light0"></light>
+    </worldbody>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_egg_touch_sensors.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_egg_touch_sensors.xml
new file mode 100644
index 0000000..73af83c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_egg_touch_sensors.xml
@@ -0,0 +1,42 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+    <compiler angle="radian" coordinate="local" meshdir="../stls/hand" texturedir="../textures"></compiler>
+    <option timestep="0.002" iterations="20" apirate="200">
+        <flag warmstart="enable"></flag>
+    </option>
+
+    <include file="shared.xml"></include>
+    <include file="shared_touch_sensors_92.xml"></include>
+
+    <asset>
+        <include file="shared_asset.xml"></include>
+
+        <texture name="texture:object" file="block.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+        <texture name="texture:hidden" file="block_hidden.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+
+        <material name="material:object" texture="texture:object" specular="1" shininess="0.3" reflectance="0"></material>
+        <material name="material:hidden" texture="texture:hidden" specular="1" shininess="0.3" reflectance="0"></material>
+        <material name="material:target" texture="texture:object" specular="1" shininess="0.3" reflectance="0" rgba="1 1 1 0.5"></material>
+    </asset>
+
+    <worldbody>
+        <geom name="floor0" pos="1 1 0" size="1 1 1" type="plane" condim="3" material="floor_mat"></geom>
+        <body name="floor0" pos="1 1 0"></body>
+
+        <include file="robot_touch_sensors_92.xml"></include>
+
+        <body name="object" pos="1 0.87 0.2">
+            <geom name="object" type="ellipsoid" size="0.03 0.03 0.04" material="material:object" condim="4"></geom>
+            <geom name="object_hidden" type="ellipsoid" size="0.029 0.029 0.03" material="material:hidden" condim="4" contype="0" conaffinity="0" mass="0"></geom>
+            <site name="object:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <joint name="object:joint" type="free" damping="0.01"></joint>
+        </body>
+        <body name="target" pos="1 0.87 0.2">
+            <geom name="target" type="ellipsoid" size="0.03 0.03 0.04" material="material:target" condim="4" group="2" contype="0" conaffinity="0"></geom>
+            <site name="target:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <joint name="target:joint" type="free" damping="0.01"></joint>
+        </body>
+
+        <light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 1 4" dir="0 0 -1" name="light0"></light>
+    </worldbody>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_pen.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_pen.xml
new file mode 100644
index 0000000..20a6fb5
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_pen.xml
@@ -0,0 +1,40 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+    <compiler angle="radian" coordinate="local" meshdir="../stls/hand" texturedir="../textures"></compiler>
+    <option timestep="0.002" iterations="20" apirate="200">
+        <flag warmstart="enable"></flag>
+    </option>
+
+    <include file="shared.xml"></include>
+
+    <asset>
+        <include file="shared_asset.xml"></include>
+
+        <material name="material:object" specular="0" shininess="0.5" reflectance="0.0" rgba="0.46 0.81 0.88 1.0"></material>
+        <material name="material:target" specular="0" shininess="0.5" reflectance="0.0" rgba="0.46 0.81 0.88 0.5"></material>
+    </asset>
+
+    <worldbody>
+        <geom name="floor0" pos="1 1 -0.2" size="1 1 1" type="plane" condim="3" material="floor_mat"></geom>
+        <body name="floor0" pos="1 1 0"></body>
+
+        <include file="robot.xml"></include>
+
+        <body name="object" pos="1 0.87 0.2" euler="-1 1 0">
+            <geom name="object" type="capsule" size="0.008 0.1" material="material:object" condim="4"></geom>
+            <site name="object:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <site name="object:top" pos="0 0 0.1" rgba="1 0 0 1" size="0.0081"></site>
+            <site name="object:bottom" pos="0 0 -0.1" rgba="0 1 0 1" size="0.0081"></site>
+            <joint name="object:joint" type="free" damping="0.01"></joint>
+        </body>
+        <body name="target" pos="1 0.87 0.2" euler="-1 1 0">
+            <geom name="target" type="capsule" size="0.008 0.1" material="material:target" condim="4" group="2" contype="0" conaffinity="0"></geom>
+            <site name="target:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <site name="target:top" pos="0 0 0.1" rgba="1 0 0 0.5" size="0.0081"></site>
+            <site name="target:bottom" pos="0 0 -0.1" rgba="0 1 0 0.5" size="0.0081"></site>
+            <joint name="target:joint" type="free" damping="0.01"></joint>
+        </body>
+        
+        <light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 1 4" dir="0 0 -1" name="light0"></light>
+    </worldbody>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_pen_touch_sensors.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_pen_touch_sensors.xml
new file mode 100644
index 0000000..758839b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_pen_touch_sensors.xml
@@ -0,0 +1,41 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+    <compiler angle="radian" coordinate="local" meshdir="../stls/hand" texturedir="../textures"></compiler>
+    <option timestep="0.002" iterations="20" apirate="200">
+        <flag warmstart="enable"></flag>
+    </option>
+
+    <include file="shared.xml"></include>
+    <include file="shared_touch_sensors_92.xml"></include>
+
+    <asset>
+        <include file="shared_asset.xml"></include>
+
+        <material name="material:object" specular="0" shininess="0.5" reflectance="0.0" rgba="0.46 0.81 0.88 1.0"></material>
+        <material name="material:target" specular="0" shininess="0.5" reflectance="0.0" rgba="0.46 0.81 0.88 0.5"></material>
+    </asset>
+
+    <worldbody>
+        <geom name="floor0" pos="1 1 -0.2" size="1 1 1" type="plane" condim="3" material="floor_mat"></geom>
+        <body name="floor0" pos="1 1 0"></body>
+
+        <include file="robot_touch_sensors_92.xml"></include>
+
+        <body name="object" pos="1 0.87 0.2" euler="-1 1 0">
+            <geom name="object" type="capsule" size="0.008 0.1" material="material:object" condim="4"></geom>
+            <site name="object:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <site name="object:top" pos="0 0 0.1" rgba="1 0 0 1" size="0.0081"></site>
+            <site name="object:bottom" pos="0 0 -0.1" rgba="0 1 0 1" size="0.0081"></site>
+            <joint name="object:joint" type="free" damping="0.01"></joint>
+        </body>
+        <body name="target" pos="1 0.87 0.2" euler="-1 1 0">
+            <geom name="target" type="capsule" size="0.008 0.1" material="material:target" condim="4" group="2" contype="0" conaffinity="0"></geom>
+            <site name="target:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <site name="target:top" pos="0 0 0.1" rgba="1 0 0 0.5" size="0.0081"></site>
+            <site name="target:bottom" pos="0 0 -0.1" rgba="0 1 0 0.5" size="0.0081"></site>
+            <joint name="target:joint" type="free" damping="0.01"></joint>
+        </body>
+
+        <light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 1 4" dir="0 0 -1" name="light0"></light>
+    </worldbody>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/reach.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/reach.xml
new file mode 100644
index 0000000..71f6dfe
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/reach.xml
@@ -0,0 +1,34 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+    <compiler angle="radian" coordinate="local" meshdir="../stls/hand" texturedir="../textures"></compiler>
+    <option timestep="0.002" iterations="20" apirate="200">
+        <flag warmstart="enable"></flag>
+    </option>
+
+    <include file="shared.xml"></include>
+
+    <asset>
+        <include file="shared_asset.xml"></include>
+    </asset>
+
+    <worldbody>
+        <geom name="floor0" pos="1 1 0" size="1 1 1" type="plane" condim="3" material="floor_mat"></geom>
+        <body name="floor0" pos="1 1 0">
+            <site name="target0" pos="0 0 0" size="0.005" rgba="1 0 0 1" type="sphere"></site>
+            <site name="target1" pos="0 0 0" size="0.005" rgba="0 1 0 1" type="sphere"></site>
+            <site name="target2" pos="0 0 0" size="0.005" rgba="0 0 1 1" type="sphere"></site>
+            <site name="target3" pos="0 0 0" size="0.005" rgba="1 1 0 1" type="sphere"></site>
+            <site name="target4" pos="0 0 0" size="0.005" rgba="1 0 1 1" type="sphere"></site>
+
+            <site name="finger0" pos="0 0 0" size="0.01" rgba="1 0 0 0.2" type="sphere"></site>
+            <site name="finger1" pos="0 0 0" size="0.01" rgba="0 1 0 0.2" type="sphere"></site>
+            <site name="finger2" pos="0 0 0" size="0.01" rgba="0 0 1 0.2" type="sphere"></site>
+            <site name="finger3" pos="0 0 0" size="0.01" rgba="1 1 0 0.2" type="sphere"></site>
+            <site name="finger4" pos="0 0 0" size="0.01" rgba="1 0 1 0.2" type="sphere"></site>
+        </body>
+
+        <include file="robot.xml"></include>
+        
+        <light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 0 4" dir="0 0 -1" name="light0"></light>
+    </worldbody>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/robot.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/robot.xml
new file mode 100644
index 0000000..dbb9e43
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/robot.xml
@@ -0,0 +1,160 @@
+<!-- See LICENSE.md for legal notices. LICENSE.md must be kept together with this file. -->
+<mujoco>
+    <body name="robot0:hand mount" pos="1 1.25 0.15" euler="1.5708 0 3.14159">
+        <inertial mass="0.1" pos="0 0 0" diaginertia="0.001 0.001 0.001"></inertial>
+        <body childclass="robot0:asset_class" name="robot0:forearm" pos="0 0.01 0" euler="0 0 0">
+            <inertial pos="0.001 -0.002 0.29" quat="0.982 -0.016 0 -0.188" mass="4" diaginertia="0.01 0.01 0.0075"></inertial>
+            <geom class="robot0:D_Vizual" pos="0 0.01 0.04" name="robot0:V_forearm" mesh="robot0:forearm" euler="0 0 1.57"></geom>
+            <geom class="robot0:DC_Hand" name="robot0:C_forearm" type="mesh" mesh="robot0:forearm_cvx" pos="0 0.01 0.04" euler="0 0 1.57" rgba="0.4 0.5 0.6 0.7"></geom>
+            <body name="robot0:wrist" pos="0 0 0.256">
+                <inertial pos="0.003 0 0.016" quat="0.504 0.496 0.495 0.504" mass="0.3" diaginertia="0.001 0.001 0.001"></inertial>
+                <joint name="robot0:WRJ1" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.489 0.14" damping="0.5" armature="0.005" user="1123"></joint>
+                <geom class="robot0:D_Vizual" name="robot0:V_wrist" mesh="robot0:wrist"></geom>
+                <geom class="robot0:DC_Hand" name="robot0:C_wrist" type="capsule" pos="0 0 0" quat="0.707 0.707 0 0" size="0.015 0.01" rgba="0.4 0.5 0.6 0.1"></geom>
+                <body name="robot0:palm" pos="0 0 0.034">
+                    <inertial pos="0.006 0 0.036" quat="0.716 0.044 0.075 0.693" mass="0.3" diaginertia="0.001 0.001 0.001"></inertial>
+                    <joint name="robot0:WRJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="-0.698 0.489" damping="0.5" armature="0.005" user="1122"></joint>
+                    <geom class="robot0:D_Vizual" name="robot0:V_palm" mesh="robot0:palm"></geom>
+                    <geom class="robot0:DC_Hand" name="robot0:C_palm0" type="box" pos="0.011 0 0.038" size="0.032 0.0111 0.049" rgba="0.4 0.5 0.6 0.1"></geom>
+                    <geom class="robot0:DC_Hand" name="robot0:C_palm1" type="box" pos="-0.032 0 0.014" size="0.011 0.0111 0.025" rgba="0.4 0.5 0.6 0.1"></geom>
+                    <body name="robot0:ffknuckle" pos="0.033 0 0.095">
+                        <inertial pos="0 0 0" quat="0.52 0.854 0.006 -0.003" mass="0.008" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:FFJ3" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.349 0.349" user="1103"></joint>
+                        <geom class="robot0:D_Vizual" name="robot0:V_ffknuckle" mesh="robot0:knuckle"></geom>
+                        <body name="robot0:ffproximal" pos="0 0 0">
+                            <inertial pos="0 0 0.023" quat="0.707 -0.004 0.004 0.707" mass="0.014" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:FFJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1102"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_ffproximal" mesh="robot0:F3"></geom>
+                            <geom class="robot0:DC_Hand" name="robot0:C_ffproximal" type="capsule" pos="0 0 0.0225" size="0.01 0.0225"></geom>
+                            <body name="robot0:ffmiddle" pos="0 0 0.045">
+                                <inertial pos="0 0 0.011" quat="0.707 0 0 0.707" mass="0.012" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:FFJ1" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1101"></joint>
+                                <geom class="robot0:D_Vizual" name="robot0:V_ffmiddle" mesh="robot0:F2"></geom>
+                                <geom class="robot0:DC_Hand" name="robot0:C_ffmiddle" type="capsule" pos="0 0 0.0125" size="0.00805 0.0125"></geom>
+                                <body name="robot0:ffdistal" pos="0 0 0.025">
+                                    <inertial pos="0 0 0.015" quat="0.707 -0.003 0.003 0.707" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:FFJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1100"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_ffdistal" pos="0 0 0.001" mesh="robot0:F1"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_ffdistal" type="capsule" pos="0 0 0.012" size="0.00705 0.012" condim="4"></geom>
+                                    <site name="robot0:S_fftip" pos="0 0 0.026" group="3"></site>
+                                    <site class="robot0:D_Touch" name="robot0:Tch_fftip"></site>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                    <body name="robot0:mfknuckle" pos="0.011 0 0.099">
+                        <inertial pos="0 0 0" quat="0.52 0.854 0.006 -0.003" mass="0.008" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:MFJ3" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.349 0.349" user="1107"></joint>
+                        <geom class="robot0:D_Vizual" name="robot0:V_mfknuckle" mesh="robot0:knuckle"></geom>
+                        <body name="robot0:mfproximal" pos="0 0 0">
+                            <inertial pos="0 0 0.023" quat="0.707 -0.004 0.004 0.707" mass="0.014" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:MFJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1106"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_mfproximal" mesh="robot0:F3"></geom>
+                            <geom class="robot0:DC_Hand" name="robot0:C_mfproximal" type="capsule" pos="0 0 0.0225" size="0.01 0.0225"></geom>
+                            <body name="robot0:mfmiddle" pos="0 0 0.045">
+                                <inertial pos="0 0 0.012" quat="0.707 0 0 0.707" mass="0.012" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:MFJ1" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1105"></joint>
+                                <geom class="robot0:D_Vizual" name="robot0:V_mfmiddle" mesh="robot0:F2"></geom>
+                                <geom class="robot0:DC_Hand" name="robot0:C_mfmiddle" type="capsule" pos="0 0 0.0125" size="0.00805 0.0125"></geom>
+                                <body name="robot0:mfdistal" pos="0 0 0.025">
+                                    <inertial pos="0 0 0.015" quat="0.707 -0.003 0.003 0.707" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:MFJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1104"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_mfdistal" mesh="robot0:F1"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_mfdistal" type="capsule" pos="0 0 0.012" size="0.00705 0.012" condim="4"></geom>
+                                    <site name="robot0:S_mftip" pos="0 0 0.026" group="3"></site>
+                                    <site class="robot0:D_Touch" name="robot0:Tch_mftip"></site>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                    <body name="robot0:rfknuckle" pos="-0.011 0 0.095">
+                        <inertial pos="0 0 0" quat="0.52 0.854 0.006 -0.003" mass="0.008" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:RFJ3" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.349 0.349" user="1111"></joint>
+                        <geom class="robot0:D_Vizual" name="robot0:V_rfknuckle" mesh="robot0:knuckle"></geom>
+                        <body name="robot0:rfproximal" pos="0 0 0">
+                            <inertial pos="0 0 0.023" quat="0.707 -0.004 0.004 0.707" mass="0.014" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:RFJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1110"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_rfproximal" mesh="robot0:F3"></geom>
+                            <geom class="robot0:DC_Hand" name="robot0:C_rfproximal" type="capsule" pos="0 0 0.0225" size="0.01 0.0225"></geom>
+                            <body name="robot0:rfmiddle" pos="0 0 0.045">
+                                <inertial pos="0 0 0.012" quat="0.707 0 0 0.707" mass="0.012" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:RFJ1" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1109"></joint>
+                                <geom class="robot0:D_Vizual" name="robot0:V_rfmiddle" mesh="robot0:F2"></geom>
+                                <geom class="robot0:DC_Hand" name="robot0:C_rfmiddle" type="capsule" pos="0 0 0.0125" size="0.00805 0.0125"></geom>
+                                <body name="robot0:rfdistal" pos="0 0 0.025">
+                                    <inertial pos="0 0 0.015" quat="0.707 -0.003 0.003 0.707" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:RFJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1108"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_rfdistal" mesh="robot0:F1" pos="0 0 0.001"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_rfdistal" type="capsule" pos="0 0 0.012" size="0.00705 0.012" condim="4"></geom>
+                                    <site name="robot0:S_rftip" pos="0 0 0.026" group="3"></site>
+                                    <site class="robot0:D_Touch" name="robot0:Tch_rftip"></site>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                    <body name="robot0:lfmetacarpal" pos="-0.017 0 0.044">
+                        <inertial pos="-0.014 0.001 0.014" quat="0.709 -0.092 -0.063 0.696" mass="0.075" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:LFJ4" type="hinge" pos="0 0 0" axis="0.571 0 0.821" range="0 0.785" user="1116"></joint>
+                        <geom class="robot0:D_Vizual" name="robot0:V_lfmetacarpal" pos="-0.016 0 -0.023" mesh="robot0:lfmetacarpal"></geom>
+                        <geom class="robot0:DC_Hand" name="robot0:C_lfmetacarpal" type="box" pos="-0.0165 0 0.01" size="0.0095 0.0111 0.025" rgba="0.4 0.5 0.6 0.2"></geom>
+                        <body name="robot0:lfknuckle" pos="-0.017 0 0.044">
+                            <inertial pos="0 0 0" quat="0.52 0.854 0.006 -0.003" mass="0.008" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:LFJ3" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.349 0.349" user="1115"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_lfknuckle" mesh="robot0:knuckle"></geom>
+                            <body name="robot0:lfproximal" pos="0 0 0">
+                                <inertial pos="0 0 0.023" quat="0.707 -0.004 0.004 0.707" mass="0.014" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:LFJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1114"></joint>
+                                <geom class="robot0:D_Vizual" name="robot0:V_lfproximal" mesh="robot0:F3"></geom>
+                                <geom class="robot0:DC_Hand" name="robot0:C_lfproximal" type="capsule" pos="0 0 0.0225" size="0.01 0.0225"></geom>
+                                <body name="robot0:lfmiddle" pos="0 0 0.045">
+                                    <inertial pos="0 0 0.012" quat="0.707 0 0 0.707" mass="0.012" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:LFJ1" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1113"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_lfmiddle" mesh="robot0:F2"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_lfmiddle" type="capsule" pos="0 0 0.0125" size="0.00805 0.0125"></geom>
+                                    <body name="robot0:lfdistal" pos="0 0 0.025">
+                                        <inertial pos="0 0 0.015" quat="0.707 -0.003 0.003 0.707" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                        <joint name="robot0:LFJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1112"></joint>
+                                        <geom class="robot0:D_Vizual" name="robot0:V_lfdistal" mesh="robot0:F1" pos="0 0 0.001"></geom>
+                                        <geom class="robot0:DC_Hand" name="robot0:C_lfdistal" type="capsule" pos="0 0 0.012" size="0.00705 0.012" condim="4"></geom>
+                                        <site name="robot0:S_lftip" pos="0 0 0.026" group="3"></site>
+                                        <site class="robot0:D_Touch" name="robot0:Tch_lftip"></site>
+                                    </body>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                    <body name="robot0:thbase" pos="0.034 -0.009 0.029" axisangle="0 1 0 0.785">
+                        <inertial pos="0 0 0" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:THJ4" type="hinge" pos="0 0 0" axis="0 0 -1" range="-1.047 1.047" user="1121"></joint>
+                        <geom name="robot0:V_thbase" type="box" group="1" pos="0 0 0" size="0.001 0.001 0.001"></geom>
+                        <body name="robot0:thproximal" pos="0 0 0">
+                            <inertial pos="0 0 0.017" quat="0.982 0 0.001 0.191" mass="0.016" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:THJ3" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.222" user="1120"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_thproximal" mesh="robot0:TH3_z"></geom>
+                            <geom class="robot0:DC_Hand" name="robot0:C_thproximal" type="capsule" pos="0 0 0.019" size="0.013 0.019" rgba="0.4 0.5 0.6 0.1"></geom>
+                            <body name="robot0:thhub" pos="0 0 0.038">
+                                <inertial pos="0 0 0" mass="0.002" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:THJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="-0.209 0.209" user="1119"></joint>
+                                <geom name="robot0:V_thhub" type="box" group="1" pos="0 0 0" size="0.001 0.001 0.001"></geom>
+                                <body name="robot0:thmiddle" pos="0 0 0">
+                                    <inertial pos="0 0 0.016" quat="1 -0.001 -0.007 0.003" mass="0.016" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:THJ1" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.524 0.524" user="1118"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_thmiddle" mesh="robot0:TH2_z"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_thmiddle" type="capsule" pos="0 0 0.016" size="0.011 0.016"></geom>
+                                    <body name="robot0:thdistal" pos="0 0 0.032">
+                                        <inertial pos="0 0 0.016" quat="0.999 -0.005 -0.047 0.005" mass="0.016" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                        <joint name="robot0:THJ0" type="hinge" pos="0 0 0" axis="0 1 0" range="-1.571 0" user="1117"></joint>
+                                        <geom class="robot0:D_Vizual" name="robot0:V_thdistal" mesh="robot0:TH1_z"></geom>
+                                        <geom class="robot0:DC_Hand" name="robot0:C_thdistal" type="capsule" pos="0 0 0.013" size="0.00918 0.013" condim="4"></geom>
+                                        <site name="robot0:S_thtip" pos="0 0 0.0275" group="3"></site>
+                                        <site class="robot0:D_Touch" name="robot0:Tch_thtip" size="0.005 0.011 0.016" pos="-0.005 0 0.02"></site>
+                                    </body>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                </body>
+            </body>
+        </body>
+    </body>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/robot_touch_sensors_92.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/robot_touch_sensors_92.xml
new file mode 100644
index 0000000..fa6d41c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/robot_touch_sensors_92.xml
@@ -0,0 +1,252 @@
+<!-- See LICENSE.md for legal notices. LICENSE.md must be kept together with this file. -->
+<mujoco>
+    <body name="robot0:hand mount" pos="1 1.25 0.15" euler="1.5708 0 3.14159">
+        <inertial mass="0.1" pos="0 0 0" diaginertia="0.001 0.001 0.001"></inertial>
+        <body childclass="robot0:asset_class" name="robot0:forearm" pos="0 0.01 0" euler="0 0 0">
+            <inertial pos="0.001 -0.002 0.29" quat="0.982 -0.016 0 -0.188" mass="4" diaginertia="0.01 0.01 0.0075"></inertial>
+            <geom class="robot0:D_Vizual" pos="0 0.01 0.04" name="robot0:V_forearm" mesh="robot0:forearm" euler="0 0 1.57"></geom>
+            <geom class="robot0:DC_Hand" name="robot0:C_forearm" type="mesh" mesh="robot0:forearm_cvx" pos="0 0.01 0.04" euler="0 0 1.57" rgba="0.4 0.5 0.6 0.7"></geom>
+            <body name="robot0:wrist" pos="0 0 0.256">
+                <inertial pos="0.003 0 0.016" quat="0.504 0.496 0.495 0.504" mass="0.3" diaginertia="0.001 0.001 0.001"></inertial>
+                <joint name="robot0:WRJ1" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.489 0.14" damping="0.5" armature="0.005" user="1123"></joint>
+                <geom class="robot0:D_Vizual" name="robot0:V_wrist" mesh="robot0:wrist"></geom>
+                <geom class="robot0:DC_Hand" name="robot0:C_wrist" type="capsule" pos="0 0 0" quat="0.707 0.707 0 0" size="0.015 0.01" rgba="0.4 0.5 0.6 0.1"></geom>
+                <body name="robot0:palm" pos="0 0 0.034">
+                    <inertial pos="0.006 0 0.036" quat="0.716 0.044 0.075 0.693" mass="0.3" diaginertia="0.001 0.001 0.001"></inertial>
+                    <joint name="robot0:WRJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="-0.698 0.489" damping="0.5" armature="0.005" user="1122"></joint>
+                    <geom class="robot0:D_Vizual" name="robot0:V_palm" mesh="robot0:palm"></geom>
+                    <geom class="robot0:DC_Hand" name="robot0:C_palm0" type="box" pos="0.011 0 0.038" size="0.032 0.0111 0.049" rgba="0.4 0.5 0.6 0.1"></geom>
+                    <site name="robot0:T_palm_b0" type="box" pos="0.011 -0.005 0.006"  size="0.033 0.007 0.019" rgba="0 0 0 0.33"/>
+                    <site name="robot0:T_palm_bl" type="box" pos="-0.011 -0.005 0.041" size="0.011 0.007 0.016" rgba="1 0 0 0.33"/>
+                    <site name="robot0:T_palm_bm" type="box" pos="0.011 -0.005 0.041"  size="0.011 0.007 0.016" rgba="0 0 0 0.33"/>
+                    <site name="robot0:T_palm_br" type="box" pos="0.033 -0.005 0.041"  size="0.011 0.007 0.016" rgba="1 0 0 0.33"/>
+                    <site name="robot0:T_palm_fl" type="box" pos="-0.011 -0.005 0.073" size="0.011 0.007 0.016" rgba="0 0 0 0.33"/>
+                    <site name="robot0:T_palm_fm" type="box" pos="0.011 -0.005 0.073"  size="0.011 0.007 0.016" rgba="1 0 0 0.33"/>
+                    <site name="robot0:T_palm_fr" type="box" pos="0.033 -0.005 0.073"  size="0.011 0.007 0.016" rgba="0 0 0 0.33"/>
+                    <geom class="robot0:DC_Hand" name="robot0:C_palm1" type="box" pos="-0.032 0 0.014" size="0.011 0.0111 0.025" rgba="0.4 0.5 0.6 0.1"></geom>
+                    <site name="robot0:T_palm_b1" type="box" pos="-0.0325 -0.005 0.014" size="0.012 0.007 0.027" rgba="0 0 0 0.33"/>
+                    <body name="robot0:ffknuckle" pos="0.033 0 0.095">
+                        <inertial pos="0 0 0" quat="0.52 0.854 0.006 -0.003" mass="0.008" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:FFJ3" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.349 0.349" user="1103"></joint>
+                        <geom class="robot0:D_Vizual" name="robot0:V_ffknuckle" mesh="robot0:knuckle"></geom>
+                        <body name="robot0:ffproximal" pos="0 0 0">
+                            <inertial pos="0 0 0.023" quat="0.707 -0.004 0.004 0.707" mass="0.014" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:FFJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1102"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_ffproximal" mesh="robot0:F3"></geom>
+                            <geom class="robot0:DC_Hand" name="robot0:C_ffproximal" type="capsule" pos="0 0 0.0225" size="0.01 0.0225"></geom>
+                            <site name="robot0:T_ffproximal_front_left_bottom"  type="box"    pos="-0.005 -0.005 0.00625" size="0.005 0.005 0.01625" rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_ffproximal_front_right_bottom" type="box"    pos="0.005 -0.005 0.00625"  size="0.005 0.005 0.01625" rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_ffproximal_front_left_top"     type="box"    pos="-0.005 -0.005 0.03875" size="0.005 0.005 0.01625" rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_ffproximal_front_right_top"    type="box"    pos="0.005 -0.005 0.03875"  size="0.005 0.005 0.01625" rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_ffproximal_back_left"          type="box"    pos="-0.005 0.005 0.0225"   size="0.005 0.005 0.0325"  rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_ffproximal_back_right"         type="box"    pos="0.005 0.005 0.0225"    size="0.005 0.005 0.0325"  rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_ffproximal_tip"                type="sphere" pos="0 0 0.045"             size="0.011"               rgba="1 1 0 0.33"/>
+                            <body name="robot0:ffmiddle" pos="0 0 0.045">
+                                <inertial pos="0 0 0.011" quat="0.707 0 0 0.707" mass="0.012" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:FFJ1" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1101"></joint>
+                                <geom class="robot0:D_Vizual" name="robot0:V_ffmiddle" mesh="robot0:F2"></geom>
+                                <geom class="robot0:DC_Hand" name="robot0:C_ffmiddle" type="capsule" pos="0 0 0.0125" size="0.00805 0.0125"></geom>
+                                <site name="robot0:T_ffmiddle_front_left"  type="box"     pos="-0.00451 -0.00451 0.0125" size="0.00451 0.00451 0.02055" rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_ffmiddle_front_right" type="box"     pos="0.00451 -0.00451 0.0125"  size="0.00451 0.00451 0.02055" rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_ffmiddle_back_left"   type="box"     pos="-0.00451 0.00451 0.0125"  size="0.00451 0.00451 0.02055" rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_ffmiddle_back_right"  type="box"     pos="0.00451 0.00451 0.0125"   size="0.00451 0.00451 0.02055" rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_ffmiddle_tip"         type="sphere"  pos="0 0 0.025"                size="0.009"                   rgba="1 1 0 0.33"/>
+                                <body name="robot0:ffdistal" pos="0 0 0.025">
+                                    <inertial pos="0 0 0.015" quat="0.707 -0.003 0.003 0.707" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:FFJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1100"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_ffdistal" pos="0 0 0.001" mesh="robot0:F1"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_ffdistal" type="capsule" pos="0 0 0.012" size="0.00705 0.012" condim="4"></geom>
+                                    <site name="robot0:S_fftip" pos="0 0 0.026" group="3"></site>
+                                    <site class="robot0:D_Touch" name="robot0:Tch_fftip"></site>
+                                    <site name="robot0:T_fftip_front_left"  type="box"      pos="-0.00451 -0.00451 0.012" size="0.00451 0.00451 0.01905" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_fftip_front_right" type="box"      pos="0.00451 -0.00451 0.012"  size="0.00451 0.00451 0.01905" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_fftip_back_left"   type="box"      pos="-0.00451 0.00451 0.012"  size="0.00451 0.00451 0.01905" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_fftip_back_right"  type="box"      pos="0.00451 0.00451 0.012"   size="0.00451 0.00451 0.01905" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_fftip_tip"         type="sphere"   pos="0 0 0.024"               size="0.008"                   rgba="1 1 0 0.33"/>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                    <body name="robot0:mfknuckle" pos="0.011 0 0.099">
+                        <inertial pos="0 0 0" quat="0.52 0.854 0.006 -0.003" mass="0.008" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:MFJ3" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.349 0.349" user="1107"></joint>
+                        <geom class="robot0:D_Vizual" name="robot0:V_mfknuckle" mesh="robot0:knuckle"></geom>
+                        <body name="robot0:mfproximal" pos="0 0 0">
+                            <inertial pos="0 0 0.023" quat="0.707 -0.004 0.004 0.707" mass="0.014" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:MFJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1106"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_mfproximal" mesh="robot0:F3"></geom>
+                            <geom class="robot0:DC_Hand" name="robot0:C_mfproximal" type="capsule" pos="0 0 0.0225" size="0.01 0.0225"></geom>
+                            <site name="robot0:T_mfproximal_front_left_bottom"  type="box"    pos="-0.005 -0.005 0.00625" size="0.005 0.005 0.01625" rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_mfproximal_front_right_bottom" type="box"    pos="0.005 -0.005 0.00625"  size="0.005 0.005 0.01625" rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_mfproximal_front_left_top"     type="box"    pos="-0.005 -0.005 0.03875" size="0.005 0.005 0.01625" rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_mfproximal_front_right_top"    type="box"    pos="0.005 -0.005 0.03875"  size="0.005 0.005 0.01625" rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_mfproximal_back_left"          type="box"    pos="-0.005 0.005 0.0225"   size="0.005 0.005 0.0325"  rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_mfproximal_back_right"         type="box"    pos="0.005 0.005 0.0225"    size="0.005 0.005 0.0325"  rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_mfproximal_tip"                type="sphere" pos="0 0 0.045"             size="0.011"               rgba="1 1 0 0.33"/>
+                            <body name="robot0:mfmiddle" pos="0 0 0.045">
+                                <inertial pos="0 0 0.012" quat="0.707 0 0 0.707" mass="0.012" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:MFJ1" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1105"></joint>
+                                <geom class="robot0:D_Vizual" name="robot0:V_mfmiddle" mesh="robot0:F2"></geom>
+                                <geom class="robot0:DC_Hand" name="robot0:C_mfmiddle" type="capsule" pos="0 0 0.0125" size="0.00805 0.0125"></geom>
+                                <site name="robot0:T_mfmiddle_front_left"  type="box"    pos="-0.00451 -0.00451 0.0125" size="0.00451 0.00451 0.02055" rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_mfmiddle_front_right" type="box"    pos="0.00451 -0.00451 0.0125"  size="0.00451 0.00451 0.02055" rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_mfmiddle_back_left"   type="box"    pos="-0.00451 0.00451 0.0125"  size="0.00451 0.00451 0.02055" rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_mfmiddle_back_right"  type="box"    pos="0.00451 0.00451 0.0125"   size="0.00451 0.00451 0.02055" rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_mfmiddle_tip"         type="sphere" pos="0 0 0.025"                size="0.009"                   rgba="1 1 0 0.33"/>
+                                <body name="robot0:mfdistal" pos="0 0 0.025">
+                                    <inertial pos="0 0 0.015" quat="0.707 -0.003 0.003 0.707" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:MFJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1104"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_mfdistal" mesh="robot0:F1"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_mfdistal" type="capsule" pos="0 0 0.012" size="0.00705 0.012" condim="4"></geom>
+                                    <site name="robot0:S_mftip" pos="0 0 0.026" group="3"></site>
+                                    <site class="robot0:D_Touch" name="robot0:Tch_mftip"></site>
+                                    <site name="robot0:T_mftip_front_left"  type="box"    pos="-0.00451 -0.00451 0.012" size="0.00451 0.00451 0.01905" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_mftip_front_right" type="box"    pos="0.00451 -0.00451 0.012"  size="0.00451 0.00451 0.01905" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_mftip_back_left"   type="box"    pos="-0.00451 0.00451 0.012"  size="0.00451 0.00451 0.01905" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_mftip_back_right"  type="box"    pos="0.00451 0.00451 0.012"   size="0.00451 0.00451 0.01905" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_mftip_tip"         type="sphere" pos="0 0 0.024"               size="0.008"                   rgba="1 1 0 0.33"/>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                    <body name="robot0:rfknuckle" pos="-0.011 0 0.095">
+                        <inertial pos="0 0 0" quat="0.52 0.854 0.006 -0.003" mass="0.008" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:RFJ3" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.349 0.349" user="1111"></joint>
+                        <geom class="robot0:D_Vizual" name="robot0:V_rfknuckle" mesh="robot0:knuckle"></geom>
+                        <body name="robot0:rfproximal" pos="0 0 0">
+                            <inertial pos="0 0 0.023" quat="0.707 -0.004 0.004 0.707" mass="0.014" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:RFJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1110"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_rfproximal" mesh="robot0:F3"></geom>
+                            <geom class="robot0:DC_Hand" name="robot0:C_rfproximal" type="capsule" pos="0 0 0.0225" size="0.01 0.0225"></geom>
+                            <site name="robot0:T_rfproximal_front_left_bottom"  type="box"    pos="-0.005 -0.005 0.00625" size="0.005 0.005 0.01625" rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_rfproximal_front_right_bottom" type="box"    pos="0.005 -0.005 0.00625"  size="0.005 0.005 0.01625" rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_rfproximal_front_left_top"     type="box"    pos="-0.005 -0.005 0.03875" size="0.005 0.005 0.01625" rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_rfproximal_front_right_top"    type="box"    pos="0.005 -0.005 0.03875"  size="0.005 0.005 0.01625" rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_rfproximal_back_left"          type="box"    pos="-0.005 0.005 0.0225"   size="0.005 0.005 0.0325"  rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_rfproximal_back_right"         type="box"    pos="0.005 0.005 0.0225"    size="0.005 0.005 0.0325"  rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_rfproximal_tip"                type="sphere" pos="0 0 0.045"             size="0.011"               rgba="1 1 0 0.33"/>
+                            <body name="robot0:rfmiddle" pos="0 0 0.045">
+                                <inertial pos="0 0 0.012" quat="0.707 0 0 0.707" mass="0.012" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:RFJ1" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1109"></joint>
+                                <geom class="robot0:D_Vizual" name="robot0:V_rfmiddle" mesh="robot0:F2"></geom>
+                                <geom class="robot0:DC_Hand" name="robot0:C_rfmiddle" type="capsule" pos="0 0 0.0125" size="0.00805 0.0125"></geom>
+                                <site name="robot0:T_rfmiddle_front_left"  type="box"    pos="-0.00451 -0.00451 0.0125" size="0.00451 0.00451 0.02055" rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_rfmiddle_front_right" type="box"    pos="0.00451 -0.00451 0.0125"  size="0.00451 0.00451 0.02055" rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_rfmiddle_back_left"   type="box"    pos="-0.00451 0.00451 0.0125"  size="0.00451 0.00451 0.02055" rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_rfmiddle_back_right"  type="box"    pos="0.00451 0.00451 0.0125"   size="0.00451 0.00451 0.02055" rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_rfmiddle_tip"         type="sphere" pos="0 0 0.025"                size="0.009"                   rgba="1 1 0 0.33"/>
+                                <body name="robot0:rfdistal" pos="0 0 0.025">
+                                    <inertial pos="0 0 0.015" quat="0.707 -0.003 0.003 0.707" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:RFJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1108"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_rfdistal" mesh="robot0:F1" pos="0 0 0.001"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_rfdistal" type="capsule" pos="0 0 0.012" size="0.00705 0.012" condim="4"></geom>
+                                    <site name="robot0:S_rftip" pos="0 0 0.026" group="3"></site>
+                                    <site class="robot0:D_Touch" name="robot0:Tch_rftip"></site>
+                                    <site name="robot0:T_rftip_front_left"  type="box"    pos="-0.00451 -0.00451 0.012" size="0.00451 0.00451 0.01905" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_rftip_front_right" type="box"    pos="0.00451 -0.00451 0.012"  size="0.00451 0.00451 0.01905" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_rftip_back_left"   type="box"    pos="-0.00451 0.00451 0.012"  size="0.00451 0.00451 0.01905" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_rftip_back_right"  type="box"    pos="0.00451 0.00451 0.012"   size="0.00451 0.00451 0.01905" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_rftip_tip"         type="sphere" pos="0 0 0.024"               size="0.008"                   rgba="1 1 0 0.33"/>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                    <body name="robot0:lfmetacarpal" pos="-0.017 0 0.044">
+                        <inertial pos="-0.014 0.001 0.014" quat="0.709 -0.092 -0.063 0.696" mass="0.075" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:LFJ4" type="hinge" pos="0 0 0" axis="0.571 0 0.821" range="0 0.785" user="1116"></joint>
+                        <geom class="robot0:D_Vizual" name="robot0:V_lfmetacarpal" pos="-0.016 0 -0.023" mesh="robot0:lfmetacarpal"></geom>
+                        <geom class="robot0:DC_Hand" name="robot0:C_lfmetacarpal" type="box" pos="-0.0165 0 0.01" size="0.0095 0.0111 0.025" rgba="0.4 0.5 0.6 0.2"></geom>
+                        <site name="robot0:T_lfmetacarpal_front" type="box" pos="-0.0165 -0.005 0.00975" size="0.01 0.007 0.026"  rgba="1 0 0 0.33"/>
+                        <body name="robot0:lfknuckle" pos="-0.017 0 0.044">
+                            <inertial pos="0 0 0" quat="0.52 0.854 0.006 -0.003" mass="0.008" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:LFJ3" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.349 0.349" user="1115"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_lfknuckle" mesh="robot0:knuckle"></geom>
+                            <body name="robot0:lfproximal" pos="0 0 0">
+                                <inertial pos="0 0 0.023" quat="0.707 -0.004 0.004 0.707" mass="0.014" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:LFJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1114"></joint>
+                                <geom class="robot0:D_Vizual" name="robot0:V_lfproximal" mesh="robot0:F3"></geom>
+                                <geom class="robot0:DC_Hand" name="robot0:C_lfproximal" type="capsule" pos="0 0 0.0225" size="0.01 0.0225"></geom>
+                                <site name="robot0:T_lfproximal_front_left_bottom"  type="box"    pos="-0.005 -0.005 0.00625" size="0.005 0.005 0.01625" rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_lfproximal_front_right_bottom" type="box"    pos="0.005 -0.005 0.00625"  size="0.005 0.005 0.01625" rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_lfproximal_front_left_top"     type="box"    pos="-0.005 -0.005 0.03875" size="0.005 0.005 0.01625" rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_lfproximal_front_right_top"    type="box"    pos="0.005 -0.005 0.03875"  size="0.005 0.005 0.01625" rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_lfproximal_back_left"          type="box"    pos="-0.005 0.005 0.0225"   size="0.005 0.005 0.0325"  rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_lfproximal_back_right"         type="box"    pos="0.005 0.005 0.0225"    size="0.005 0.005 0.0325"  rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_lfproximal_tip"                type="sphere" pos="0 0 0.045"             size="0.011"               rgba="1 1 0 0.33"/>
+                                <body name="robot0:lfmiddle" pos="0 0 0.045">
+                                    <inertial pos="0 0 0.012" quat="0.707 0 0 0.707" mass="0.012" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:LFJ1" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1113"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_lfmiddle" mesh="robot0:F2"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_lfmiddle" type="capsule" pos="0 0 0.0125" size="0.00805 0.0125"></geom>
+                                    <site name="robot0:T_lfmiddle_front_left"  type="box"    pos="-0.00451 -0.00451 0.0125" size="0.00451 0.00451 0.02055" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_lfmiddle_front_right" type="box"    pos="0.00451 -0.00451 0.0125"  size="0.00451 0.00451 0.02055" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_lfmiddle_back_left"   type="box"    pos="-0.00451 0.00451 0.0125"  size="0.00451 0.00451 0.02055" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_lfmiddle_back_right"  type="box"    pos="0.00451 0.00451 0.0125"   size="0.00451 0.00451 0.02055" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_lfmiddle_tip"         type="sphere" pos="0 0 0.025"                size="0.009"                   rgba="1 1 0 0.33"/>
+                                    <body name="robot0:lfdistal" pos="0 0 0.025">
+                                        <inertial pos="0 0 0.015" quat="0.707 -0.003 0.003 0.707" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                        <joint name="robot0:LFJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1112"></joint>
+                                        <geom class="robot0:D_Vizual" name="robot0:V_lfdistal" mesh="robot0:F1" pos="0 0 0.001"></geom>
+                                        <geom class="robot0:DC_Hand" name="robot0:C_lfdistal" type="capsule" pos="0 0 0.012" size="0.00705 0.012" condim="4"></geom>
+                                        <site name="robot0:S_lftip" pos="0 0 0.026" group="3"></site>
+                                        <site class="robot0:D_Touch" name="robot0:Tch_lftip"></site>
+                                        <site name="robot0:T_lftip_front_left"  type="box"    pos="-0.00451 -0.00451 0.012" size="0.00451 0.00451 0.01905" rgba="1 0 0 0.33"/>
+                                        <site name="robot0:T_lftip_front_right" type="box"    pos="0.00451 -0.00451 0.012"  size="0.00451 0.00451 0.01905" rgba="0 0 0 0.33"/>
+                                        <site name="robot0:T_lftip_back_left"   type="box"    pos="-0.00451 0.00451 0.012"  size="0.00451 0.00451 0.01905" rgba="0 0 0 0.33"/>
+                                        <site name="robot0:T_lftip_back_right"  type="box"    pos="0.00451 0.00451 0.012"   size="0.00451 0.00451 0.01905" rgba="1 0 0 0.33"/>
+                                        <site name="robot0:T_lftip_tip"         type="sphere" pos="0 0 0.024"               size="0.008"                   rgba="1 1 0 0.33"/>
+                                    </body>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                    <body name="robot0:thbase" pos="0.034 -0.009 0.029" axisangle="0 1 0 0.785">
+                        <inertial pos="0 0 0" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:THJ4" type="hinge" pos="0 0 0" axis="0 0 -1" range="-1.047 1.047" user="1121"></joint>
+                        <geom name="robot0:V_thbase" type="box" group="1" pos="0 0 0" size="0.001 0.001 0.001"></geom>
+                        <body name="robot0:thproximal" pos="0 0 0">
+                            <inertial pos="0 0 0.017" quat="0.982 0 0.001 0.191" mass="0.016" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:THJ3" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.222" user="1120"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_thproximal" mesh="robot0:TH3_z"></geom>
+                            <geom class="robot0:DC_Hand" name="robot0:C_thproximal" type="capsule" pos="0 0 0.019" size="0.013 0.019" rgba="0.4 0.5 0.6 0.1"></geom>
+                            <site name="robot0:T_thproximal_front_left"  type="box"    pos="-0.007 -0.007 0.019" size="0.007 0.007 0.032" rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_thproximal_front_right" type="box"    pos="0.007 -0.007 0.019"  size="0.007 0.007 0.032" rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_thproximal_back_left"   type="box"    pos="-0.007 0.007 0.019"  size="0.007 0.007 0.032" rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_thproximal_back_right"  type="box"    pos="0.007 0.007 0.019"   size="0.007 0.007 0.032" rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_thproximal_tip"         type="sphere" pos="0 0 0.038"           size="0.014"             rgba="1 1 0 0.33"/>
+                            <body name="robot0:thhub" pos="0 0 0.038">
+                                <inertial pos="0 0 0" mass="0.002" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:THJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="-0.209 0.209" user="1119"></joint>
+                                <geom name="robot0:V_thhub" type="box" group="1" pos="0 0 0" size="0.001 0.001 0.001"></geom>
+                                <body name="robot0:thmiddle" pos="0 0 0">
+                                    <inertial pos="0 0 0.016" quat="1 -0.001 -0.007 0.003" mass="0.016" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:THJ1" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.524 0.524" user="1118"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_thmiddle" mesh="robot0:TH2_z"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_thmiddle" type="capsule" pos="0 0 0.016" size="0.011 0.016"></geom>
+                                    <site name="robot0:T_thmiddle_front_left"  type="box"    pos="-0.006 -0.006 0.016" size="0.006 0.006 0.027" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_thmiddle_front_right" type="box"    pos="0.006 -0.006 0.016"  size="0.006 0.006 0.027" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_thmiddle_back_left"   type="box"    pos="-0.006 0.006 0.016"  size="0.006 0.006 0.027" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_thmiddle_back_right"  type="box"    pos="0.006 0.006 0.016"   size="0.006 0.006 0.027" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_thmiddle_tip"         type="sphere" pos="0 0 0.032"           size="0.012"             rgba="1 1 0 0.33"/>
+                                    <body name="robot0:thdistal" pos="0 0 0.032">
+                                        <inertial pos="0 0 0.016" quat="0.999 -0.005 -0.047 0.005" mass="0.016" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                        <joint name="robot0:THJ0" type="hinge" pos="0 0 0" axis="0 1 0" range="-1.571 0" user="1117"></joint>
+                                        <geom class="robot0:D_Vizual" name="robot0:V_thdistal" mesh="robot0:TH1_z"></geom>
+                                        <geom class="robot0:DC_Hand" name="robot0:C_thdistal" type="capsule" pos="0 0 0.013" size="0.00918 0.013" condim="4"></geom>
+                                        <site name="robot0:S_thtip" pos="0 0 0.0275" group="3"></site>
+                                        <site class="robot0:D_Touch" name="robot0:Tch_thtip" size="0.005 0.011 0.016" pos="-0.005 0 0.02"></site>
+                                        <site name="robot0:T_thtip_front_left"  type="box"    pos="-0.0056 -0.0056 0.013" size="0.0056 0.0056 0.02218" rgba="0 0 0 0.33"/>
+                                        <site name="robot0:T_thtip_front_right" type="box"    pos="0.0056 -0.0056 0.013"  size="0.0056 0.0056 0.02218" rgba="1 0 0 0.33"/>
+                                        <site name="robot0:T_thtip_back_left"   type="box"    pos="-0.0056 0.0056 0.013"  size="0.0056 0.0056 0.02218" rgba="1 0 0 0.33"/>
+                                        <site name="robot0:T_thtip_back_right"  type="box"    pos="0.0056 0.0056 0.013"   size="0.0056 0.0056 0.02218" rgba="0 0 0 0.33"/>
+                                        <site name="robot0:T_thtip_tip"         type="sphere" pos="0 0 0.026"             size="0.01"                  rgba="1 1 0 0.33"/>
+                                    </body>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                </body>
+            </body>
+        </body>
+    </body>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared.xml
new file mode 100644
index 0000000..f27f265
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared.xml
@@ -0,0 +1,254 @@
+<!-- See LICENSE.md for legal notices. LICENSE.md must be kept together with this file. -->
+<mujoco>
+    <size njmax="500" nconmax="100" nuser_jnt="1" nuser_site="1" nuser_tendon="1" nuser_sensor="1" nuser_actuator="16" nstack="600000"></size>
+
+    <visual>
+        <map fogstart="3" fogend="5" force="0.1"></map>
+        <quality shadowsize="4096"></quality>
+    </visual>
+
+    <default>
+        <default class="robot0:asset_class">
+            <geom friction="1 0.005 0.001" condim="3" margin="0.0005" contype="1" conaffinity="1"></geom>
+            <joint limited="true" damping="0.1" armature="0.001" margin="0.01" frictionloss="0.001"></joint>
+            <site size="0.005" rgba="0.4 0.9 0.4 1"></site>
+            <general ctrllimited="true" forcelimited="true"></general>
+        </default>
+        <default class="robot0:D_Touch">
+            <site type="box" size="0.009 0.004 0.013" pos="0 -0.004 0.018" rgba="0.8 0.8 0.8 0.15" group="4"></site>
+        </default>
+        <default class="robot0:DC_Hand">
+            <geom material="robot0:MatColl" contype="1" conaffinity="0" group="4"></geom>
+        </default>
+        <default class="robot0:D_Vizual">
+            <geom material="robot0:MatViz" contype="0" conaffinity="0" group="1" type="mesh"></geom>
+        </default>
+        <default class="robot0:free">
+            <joint type="free" damping="0" armature="0" limited="false"></joint>
+        </default>
+    </default>
+
+    <contact>
+        <pair geom1="robot0:C_ffdistal" geom2="robot0:C_thdistal" condim="1"></pair>
+        <pair geom1="robot0:C_ffmiddle" geom2="robot0:C_thdistal" condim="1"></pair>
+        <pair geom1="robot0:C_ffproximal" geom2="robot0:C_thdistal" condim="1"></pair>
+        <pair geom1="robot0:C_mfproximal" geom2="robot0:C_thdistal" condim="1"></pair>
+        <pair geom1="robot0:C_mfdistal" geom2="robot0:C_thdistal" condim="1"></pair>
+        <pair geom1="robot0:C_rfdistal" geom2="robot0:C_thdistal" condim="1"></pair>
+        <pair geom1="robot0:C_lfdistal" geom2="robot0:C_thdistal" condim="1"></pair>
+        <pair geom1="robot0:C_palm0" geom2="robot0:C_thdistal" condim="1"></pair>
+        <pair geom1="robot0:C_mfdistal" geom2="robot0:C_ffdistal" condim="1"></pair>
+        <pair geom1="robot0:C_rfdistal" geom2="robot0:C_mfdistal" condim="1"></pair>
+        <pair geom1="robot0:C_lfdistal" geom2="robot0:C_rfdistal" condim="1"></pair>
+        <pair geom1="robot0:C_mfproximal" geom2="robot0:C_ffproximal" condim="1"></pair>
+        <pair geom1="robot0:C_rfproximal" geom2="robot0:C_mfproximal" condim="1"></pair>
+        <pair geom1="robot0:C_lfproximal" geom2="robot0:C_rfproximal" condim="1"></pair>
+        <pair geom1="robot0:C_lfdistal" geom2="robot0:C_rfdistal" condim="1"></pair>
+        <pair geom1="robot0:C_lfdistal" geom2="robot0:C_mfdistal" condim="1"></pair>
+        <pair geom1="robot0:C_lfdistal" geom2="robot0:C_rfmiddle" condim="1"></pair>
+        <pair geom1="robot0:C_lfmiddle" geom2="robot0:C_rfdistal" condim="1"></pair>
+        <pair geom1="robot0:C_lfmiddle" geom2="robot0:C_rfmiddle" condim="1"></pair>
+    </contact>
+
+    <tendon>
+        <fixed name="robot0:T_WRJ1r" limited="true" range="-0.032 0.032" user="1236">
+            <joint joint="robot0:WRJ1" coef="0.0325"></joint>
+        </fixed>
+        <fixed name="robot0:T_WRJ1l" limited="true" range="-0.032 0.032" user="1237">
+            <joint joint="robot0:WRJ1" coef="-0.0325"></joint>
+        </fixed>
+        <fixed name="robot0:T_WRJ0u" limited="true" range="-0.032 0.032" user="1236">
+            <joint joint="robot0:WRJ0" coef="0.0175"></joint>
+        </fixed>
+        <fixed name="robot0:T_WRJ0d" limited="true" range="-0.032 0.032" user="1237">
+            <joint joint="robot0:WRJ0" coef="-0.0175"></joint>
+        </fixed>
+        <fixed name="robot0:T_FFJ3r" limited="true" range="-0.018 0.018" user="1204">
+            <joint joint="robot0:FFJ3" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_FFJ3l" limited="true" range="-0.018 0.018" user="1205">
+            <joint joint="robot0:FFJ3" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_FFJ2u" limited="true" range="-0.007 0.03" user="1202">
+            <joint joint="robot0:FFJ2" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_FFJ2d" limited="true" range="-0.03 0.007" user="1203">
+            <joint joint="robot0:FFJ2" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_FFJ1c" limited="true" range="-0.001 0.001">
+            <joint joint="robot0:FFJ0" coef="0.00705"></joint>
+            <joint joint="robot0:FFJ1" coef="-0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_FFJ1u" limited="true" range="-0.007 0.03" user="1200">
+            <joint joint="robot0:FFJ0" coef="0.00705"></joint>
+            <joint joint="robot0:FFJ1" coef="0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_FFJ1d" limited="true" range="-0.03 0.007" user="1201">
+            <joint joint="robot0:FFJ0" coef="-0.00705"></joint>
+            <joint joint="robot0:FFJ1" coef="-0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_MFJ3r" limited="true" range="-0.018 0.018" user="1210">
+            <joint joint="robot0:MFJ3" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_MFJ3l" limited="true" range="-0.018 0.018" user="1211">
+            <joint joint="robot0:MFJ3" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_MFJ2u" limited="true" range="-0.007 0.03" user="1208">
+            <joint joint="robot0:MFJ2" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_MFJ2d" limited="true" range="-0.03 0.007" user="1209">
+            <joint joint="robot0:MFJ2" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_MFJ1c" limited="true" range="-0.001 0.001">
+            <joint joint="robot0:MFJ0" coef="0.00705"></joint>
+            <joint joint="robot0:MFJ1" coef="-0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_MFJ1u" limited="true" range="-0.007 0.03" user="1206">
+            <joint joint="robot0:MFJ0" coef="0.00705"></joint>
+            <joint joint="robot0:MFJ1" coef="0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_MFJ1d" limited="true" range="-0.03 0.007" user="1207">
+            <joint joint="robot0:MFJ0" coef="-0.00705"></joint>
+            <joint joint="robot0:MFJ1" coef="-0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_RFJ3r" limited="true" range="-0.018 0.018" user="1216">
+            <joint joint="robot0:RFJ3" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_RFJ3l" limited="true" range="-0.018 0.018" user="1217">
+            <joint joint="robot0:RFJ3" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_RFJ2u" limited="true" range="-0.007 0.03" user="1214">
+            <joint joint="robot0:RFJ2" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_RFJ2d" limited="true" range="-0.03 0.007" user="1215">
+            <joint joint="robot0:RFJ2" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_RFJ1c" limited="true" range="-0.001 0.001">
+            <joint joint="robot0:RFJ0" coef="0.00705"></joint>
+            <joint joint="robot0:RFJ1" coef="-0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_RFJ1u" limited="true" range="-0.007 0.03" user="1212">
+            <joint joint="robot0:RFJ0" coef="0.00705"></joint>
+            <joint joint="robot0:RFJ1" coef="0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_RFJ1d" limited="true" range="-0.03 0.007" user="1213">
+            <joint joint="robot0:RFJ0" coef="-0.00705"></joint>
+            <joint joint="robot0:RFJ1" coef="-0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ4u" limited="true" range="-0.007 0.03" user="1224">
+            <joint joint="robot0:LFJ4" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ4d" limited="true" range="-0.03 0.007" user="1225">
+            <joint joint="robot0:LFJ4" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ3r" limited="true" range="-0.018 0.018" user="1222">
+            <joint joint="robot0:LFJ3" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ3l" limited="true" range="-0.018 0.018" user="1223">
+            <joint joint="robot0:LFJ3" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ2u" limited="true" range="-0.007 0.03" user="1220">
+            <joint joint="robot0:LFJ2" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ2d" limited="true" range="-0.03 0.007" user="1221">
+            <joint joint="robot0:LFJ2" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ1c" limited="true" range="-0.001 0.001">
+            <joint joint="robot0:LFJ0" coef="0.00705"></joint>
+            <joint joint="robot0:LFJ1" coef="-0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ1u" limited="true" range="-0.007 0.03" user="1218">
+            <joint joint="robot0:LFJ0" coef="0.00705"></joint>
+            <joint joint="robot0:LFJ1" coef="0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ1d" limited="true" range="-0.03 0.007" user="1219">
+            <joint joint="robot0:LFJ0" coef="-0.00705"></joint>
+            <joint joint="robot0:LFJ1" coef="-0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ4a" limited="true" range="-0.018 0.018" user="1234">
+            <joint joint="robot0:THJ4" coef="0.01636"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ4c" limited="true" range="-0.018 0.018" user="1235">
+            <joint joint="robot0:THJ4" coef="-0.01636"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ3u" limited="true" range="-0.007 0.03" user="1232">
+            <joint joint="robot0:THJ3" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ3d" limited="true" range="-0.03 0.007" user="1233">
+            <joint joint="robot0:THJ3" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ2u" limited="true" range="-0.018 0.018" user="1230">
+            <joint joint="robot0:THJ2" coef="0.011"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ2d" limited="true" range="-0.018 0.018" user="1231">
+            <joint joint="robot0:THJ2" coef="-0.011"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ1r" limited="true" range="-0.018 0.018" user="1228">
+            <joint joint="robot0:THJ1" coef="0.011"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ1l" limited="true" range="-0.018 0.018" user="1229">
+            <joint joint="robot0:THJ1" coef="-0.011"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ0r" limited="true" range="-0.03 0.007" user="1226">
+            <joint joint="robot0:THJ0" coef="0.009"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ0l" limited="true" range="-0.007 0.03" user="1227">
+            <joint joint="robot0:THJ0" coef="-0.009"></joint>
+        </fixed>
+    </tendon>
+
+    <sensor>
+        <jointpos name="robot0:Sjp_WRJ1" joint="robot0:WRJ1"></jointpos>
+        <jointpos name="robot0:Sjp_WRJ0" joint="robot0:WRJ0"></jointpos>
+        <jointpos name="robot0:Sjp_FFJ3" joint="robot0:FFJ3"></jointpos>
+        <jointpos name="robot0:Sjp_FFJ2" joint="robot0:FFJ2"></jointpos>
+        <jointpos name="robot0:Sjp_FFJ1" joint="robot0:FFJ1"></jointpos>
+        <jointpos name="robot0:Sjp_FFJ0" joint="robot0:FFJ0"></jointpos>
+        <jointpos name="robot0:Sjp_MFJ3" joint="robot0:MFJ3"></jointpos>
+        <jointpos name="robot0:Sjp_MFJ2" joint="robot0:MFJ2"></jointpos>
+        <jointpos name="robot0:Sjp_MFJ1" joint="robot0:MFJ1"></jointpos>
+        <jointpos name="robot0:Sjp_MFJ0" joint="robot0:MFJ0"></jointpos>
+        <jointpos name="robot0:Sjp_RFJ3" joint="robot0:RFJ3"></jointpos>
+        <jointpos name="robot0:Sjp_RFJ2" joint="robot0:RFJ2"></jointpos>
+        <jointpos name="robot0:Sjp_RFJ1" joint="robot0:RFJ1"></jointpos>
+        <jointpos name="robot0:Sjp_RFJ0" joint="robot0:RFJ0"></jointpos>
+        <jointpos name="robot0:Sjp_LFJ4" joint="robot0:LFJ4"></jointpos>
+        <jointpos name="robot0:Sjp_LFJ3" joint="robot0:LFJ3"></jointpos>
+        <jointpos name="robot0:Sjp_LFJ2" joint="robot0:LFJ2"></jointpos>
+        <jointpos name="robot0:Sjp_LFJ1" joint="robot0:LFJ1"></jointpos>
+        <jointpos name="robot0:Sjp_LFJ0" joint="robot0:LFJ0"></jointpos>
+        <jointpos name="robot0:Sjp_THJ4" joint="robot0:THJ4"></jointpos>
+        <jointpos name="robot0:Sjp_THJ3" joint="robot0:THJ3"></jointpos>
+        <jointpos name="robot0:Sjp_THJ2" joint="robot0:THJ2"></jointpos>
+        <jointpos name="robot0:Sjp_THJ1" joint="robot0:THJ1"></jointpos>
+        <jointpos name="robot0:Sjp_THJ0" joint="robot0:THJ0"></jointpos>
+        <touch name="robot0:ST_Tch_fftip" site="robot0:Tch_fftip"></touch>
+        <touch name="robot0:ST_Tch_mftip" site="robot0:Tch_mftip"></touch>
+        <touch name="robot0:ST_Tch_rftip" site="robot0:Tch_rftip"></touch>
+        <touch name="robot0:ST_Tch_lftip" site="robot0:Tch_lftip"></touch>
+        <touch name="robot0:ST_Tch_thtip" site="robot0:Tch_thtip"></touch>
+    </sensor>
+
+    <actuator>
+        <position name="robot0:A_WRJ1" class="robot0:asset_class" user="2038" joint="robot0:WRJ1" ctrlrange="-0.489 0.14" kp="5" forcerange="-4.785 4.785"></position>
+        <position name="robot0:A_WRJ0" class="robot0:asset_class" user="2036" joint="robot0:WRJ0" ctrlrange="-0.698 0.489" kp="5" forcerange="-2.175 2.175"></position>
+        <position name="robot0:A_FFJ3" class="robot0:asset_class" user="2004" joint="robot0:FFJ3" ctrlrange="-0.349 0.349" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_FFJ2" class="robot0:asset_class" user="2002" joint="robot0:FFJ2" ctrlrange="0 1.571" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_FFJ1" class="robot0:asset_class" user="2000" joint="robot0:FFJ1" ctrlrange="0 1.571" kp="1" forcerange="-0.7245 0.7245"></position>
+        <position name="robot0:A_MFJ3" class="robot0:asset_class" user="2010" joint="robot0:MFJ3" ctrlrange="-0.349 0.349" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_MFJ2" class="robot0:asset_class" user="2008" joint="robot0:MFJ2" ctrlrange="0 1.571" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_MFJ1" class="robot0:asset_class" user="2006" joint="robot0:MFJ1" ctrlrange="0 1.571" kp="1" forcerange="-0.7245 0.7245"></position>
+        <position name="robot0:A_RFJ3" class="robot0:asset_class" user="2016" joint="robot0:RFJ3" ctrlrange="-0.349 0.349" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_RFJ2" class="robot0:asset_class" user="2014" joint="robot0:RFJ2" ctrlrange="0 1.571" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_RFJ1" class="robot0:asset_class" user="2012" joint="robot0:RFJ1" ctrlrange="0 1.571" kp="1" forcerange="-0.7245 0.7245"></position>
+        <position name="robot0:A_LFJ4" class="robot0:asset_class" user="2024" joint="robot0:LFJ4" ctrlrange="0 0.785" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_LFJ3" class="robot0:asset_class" user="2022" joint="robot0:LFJ3" ctrlrange="-0.349 0.349" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_LFJ2" class="robot0:asset_class" user="2020" joint="robot0:LFJ2" ctrlrange="0 1.571" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_LFJ1" class="robot0:asset_class" user="2018" joint="robot0:LFJ1" ctrlrange="0 1.571" kp="1" forcerange="-0.7245 0.7245"></position>
+        <position name="robot0:A_THJ4" class="robot0:asset_class" user="2034" joint="robot0:THJ4" ctrlrange="-1.047 1.047" kp="1" forcerange="-2.3722 2.3722"></position>
+        <position name="robot0:A_THJ3" class="robot0:asset_class" user="2032" joint="robot0:THJ3" ctrlrange="0 1.222" kp="1" forcerange="-1.45 1.45"></position>
+        <position name="robot0:A_THJ2" class="robot0:asset_class" user="2030" joint="robot0:THJ2" ctrlrange="-0.209 0.209" kp="1" forcerange="-0.99 0.99"></position>
+        <position name="robot0:A_THJ1" class="robot0:asset_class" user="2028" joint="robot0:THJ1" ctrlrange="-0.524 0.524" kp="1" forcerange="-0.99 0.99"></position>
+        <position name="robot0:A_THJ0" class="robot0:asset_class" user="2026" joint="robot0:THJ0" ctrlrange="-1.571 0" kp="1" forcerange="-0.81 0.81"></position>
+    </actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared_asset.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared_asset.xml
new file mode 100644
index 0000000..ec9a0b0
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared_asset.xml
@@ -0,0 +1,26 @@
+<!-- See LICENSE.md for legal notices. LICENSE.md must be kept together with this file. -->
+<mujoco>
+    <texture type="skybox" builtin="gradient" rgb1="0.44 0.85 0.56" rgb2="0.46 0.87 0.58" width="32" height="32"></texture>
+
+    <texture name="robot0:texplane" type="2d" builtin="checker" rgb1="0.2 0.3 0.4" rgb2="0.1 0.15 0.2" width="512" height="512"></texture>
+    <texture name="robot0:texgeom" type="cube" builtin="flat" mark="cross" width="127" height="127" rgb1="0.3 0.6 0.5" rgb2="0.3 0.6 0.5" markrgb="0 0 0" random="0.01"></texture>
+
+    <material name="robot0:MatGnd" reflectance="0.5" texture="robot0:texplane" texrepeat="1 1" texuniform="true"></material>
+    <material name="robot0:MatColl" specular="1" shininess="0.3" reflectance="0.5" rgba="0.4 0.5 0.6 1"></material>
+    <material name="robot0:MatViz" specular="0.75" shininess="0.1" reflectance="0.5" rgba="0.93 0.93 0.93 1"></material>
+    <material name="robot0:object" texture="robot0:texgeom" texuniform="false"></material>
+    <material name="floor_mat" specular="0" shininess="0.5" reflectance="0" rgba="0.2 0.2 0.2 0"></material>
+
+    <mesh name="robot0:forearm" file="forearm_electric.stl"></mesh>
+    <mesh name="robot0:forearm_cvx" file="forearm_electric_cvx.stl"></mesh>
+    <mesh name="robot0:wrist" scale="0.001 0.001 0.001" file="wrist.stl"></mesh>
+    <mesh name="robot0:palm" scale="0.001 0.001 0.001" file="palm.stl"></mesh>
+    <mesh name="robot0:knuckle" scale="0.001 0.001 0.001" file="knuckle.stl"></mesh>
+    <mesh name="robot0:F3" scale="0.001 0.001 0.001" file="F3.stl"></mesh>
+    <mesh name="robot0:F2" scale="0.001 0.001 0.001" file="F2.stl"></mesh>
+    <mesh name="robot0:F1" scale="0.001 0.001 0.001" file="F1.stl"></mesh>
+    <mesh name="robot0:lfmetacarpal" scale="0.001 0.001 0.001" file="lfmetacarpal.stl"></mesh>
+    <mesh name="robot0:TH3_z" scale="0.001 0.001 0.001" file="TH3_z.stl"></mesh>
+    <mesh name="robot0:TH2_z" scale="0.001 0.001 0.001" file="TH2_z.stl"></mesh>
+    <mesh name="robot0:TH1_z" scale="0.001 0.001 0.001" file="TH1_z.stl"></mesh>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared_touch_sensors_92.xml b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared_touch_sensors_92.xml
new file mode 100644
index 0000000..472c84c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared_touch_sensors_92.xml
@@ -0,0 +1,120 @@
+<mujoco>
+    <sensor>
+
+        <!--PALM-->
+        <touch name="robot0:TS_palm_b0" site="robot0:T_palm_b0"></touch>
+        <touch name="robot0:TS_palm_bl" site="robot0:T_palm_bl"></touch>
+        <touch name="robot0:TS_palm_bm" site="robot0:T_palm_bm"></touch>
+        <touch name="robot0:TS_palm_br" site="robot0:T_palm_br"></touch>
+        <touch name="robot0:TS_palm_fl" site="robot0:T_palm_fl"></touch>
+        <touch name="robot0:TS_palm_fm" site="robot0:T_palm_fm"></touch>
+        <touch name="robot0:TS_palm_fr" site="robot0:T_palm_fr"></touch>
+        <touch name="robot0:TS_palm_b1" site="robot0:T_palm_b1"></touch>
+
+         <!--FOREFINGER-->
+        <touch name="robot0:TS_ffproximal_front_left_bottom"  site="robot0:T_ffproximal_front_left_bottom"></touch>
+        <touch name="robot0:TS_ffproximal_front_right_bottom" site="robot0:T_ffproximal_front_right_bottom"></touch>
+        <touch name="robot0:TS_ffproximal_front_left_top"     site="robot0:T_ffproximal_front_left_top"></touch>
+        <touch name="robot0:TS_ffproximal_front_right_top"    site="robot0:T_ffproximal_front_right_top"></touch>
+        <touch name="robot0:TS_ffproximal_back_left"          site="robot0:T_ffproximal_back_left"></touch>
+        <touch name="robot0:TS_ffproximal_back_right"         site="robot0:T_ffproximal_back_right"></touch>
+        <touch name="robot0:TS_ffproximal_tip"                site="robot0:T_ffproximal_tip"></touch>
+
+        <touch name="robot0:TS_ffmiddle_front_left"  site="robot0:T_ffmiddle_front_left"></touch>
+        <touch name="robot0:TS_ffmiddle_front_right" site="robot0:T_ffmiddle_front_right"></touch>
+        <touch name="robot0:TS_ffmiddle_back_left"   site="robot0:T_ffmiddle_back_left"></touch>
+        <touch name="robot0:TS_ffmiddle_back_right"  site="robot0:T_ffmiddle_back_right"></touch>
+        <touch name="robot0:TS_ffmiddle_tip"         site="robot0:T_ffmiddle_tip"></touch>
+
+        <touch name="robot0:TS_fftip_front_left"  site="robot0:T_fftip_front_left"></touch>
+        <touch name="robot0:TS_fftip_front_right" site="robot0:T_fftip_front_right"></touch>
+        <touch name="robot0:TS_fftip_back_left"   site="robot0:T_fftip_back_left"></touch>
+        <touch name="robot0:TS_fftip_back_right"  site="robot0:T_fftip_back_right"></touch>
+        <touch name="robot0:TS_fftip_tip"         site="robot0:T_fftip_tip"></touch>
+
+        <!-- MIDDLE FINGER -->
+        <touch name="robot0:TS_mfproximal_front_left_bottom"  site="robot0:T_mfproximal_front_left_bottom"></touch>
+        <touch name="robot0:TS_mfproximal_front_right_bottom" site="robot0:T_mfproximal_front_right_bottom"></touch>
+        <touch name="robot0:TS_mfproximal_front_left_top"     site="robot0:T_mfproximal_front_left_top"></touch>
+        <touch name="robot0:TS_mfproximal_front_right_top"    site="robot0:T_mfproximal_front_right_top"></touch>
+        <touch name="robot0:TS_mfproximal_back_left"          site="robot0:T_mfproximal_back_left"></touch>
+        <touch name="robot0:TS_mfproximal_back_right"         site="robot0:T_mfproximal_back_right"></touch>
+        <touch name="robot0:TS_mfproximal_tip"                site="robot0:T_mfproximal_tip"></touch>
+
+        <touch name="robot0:TS_mfmiddle_front_left"  site="robot0:T_mfmiddle_front_left"></touch>
+        <touch name="robot0:TS_mfmiddle_front_right" site="robot0:T_mfmiddle_front_right"></touch>
+        <touch name="robot0:TS_mfmiddle_back_left"   site="robot0:T_mfmiddle_back_left"></touch>
+        <touch name="robot0:TS_mfmiddle_back_right"  site="robot0:T_mfmiddle_back_right"></touch>
+        <touch name="robot0:TS_mfmiddle_tip"         site="robot0:T_mfmiddle_tip"></touch>
+
+        <touch name="robot0:TS_mftip_front_left"  site="robot0:T_mftip_front_left"></touch>
+        <touch name="robot0:TS_mftip_front_right" site="robot0:T_mftip_front_right"></touch>
+        <touch name="robot0:TS_mftip_back_left"   site="robot0:T_mftip_back_left"></touch>
+        <touch name="robot0:TS_mftip_back_right"  site="robot0:T_mftip_back_right"></touch>
+        <touch name="robot0:TS_mftip_tip"         site="robot0:T_mftip_tip"></touch>
+
+        <!-- RING FINGER -->
+        <touch name="robot0:TS_rfproximal_front_left_bottom"  site="robot0:T_rfproximal_front_left_bottom"></touch>
+        <touch name="robot0:TS_rfproximal_front_right_bottom" site="robot0:T_rfproximal_front_right_bottom"></touch>
+        <touch name="robot0:TS_rfproximal_front_left_top"     site="robot0:T_rfproximal_front_left_top"></touch>
+        <touch name="robot0:TS_rfproximal_front_right_top"    site="robot0:T_rfproximal_front_right_top"></touch>
+        <touch name="robot0:TS_rfproximal_back_left"          site="robot0:T_rfproximal_back_left"></touch>
+        <touch name="robot0:TS_rfproximal_back_right"         site="robot0:T_rfproximal_back_right"></touch>
+        <touch name="robot0:TS_rfproximal_tip"                site="robot0:T_rfproximal_tip"></touch>
+
+        <touch name="robot0:TS_rfmiddle_front_left"  site="robot0:T_rfmiddle_front_left"></touch>
+        <touch name="robot0:TS_rfmiddle_front_right" site="robot0:T_rfmiddle_front_right"></touch>
+        <touch name="robot0:TS_rfmiddle_back_left"   site="robot0:T_rfmiddle_back_left"></touch>
+        <touch name="robot0:TS_rfmiddle_back_right"  site="robot0:T_rfmiddle_back_right"></touch>
+        <touch name="robot0:TS_rfmiddle_tip"         site="robot0:T_rfmiddle_tip"></touch>
+
+        <touch name="robot0:TS_rftip_front_left"  site="robot0:T_rftip_front_left"></touch>
+        <touch name="robot0:TS_rftip_front_right" site="robot0:T_rftip_front_right"></touch>
+        <touch name="robot0:TS_rftip_back_left"   site="robot0:T_rftip_back_left"></touch>
+        <touch name="robot0:TS_rftip_back_right"  site="robot0:T_rftip_back_right"></touch>
+        <touch name="robot0:TS_rftip_tip"         site="robot0:T_rftip_tip"></touch>
+
+        <!-- LITTLE FINGER -->
+        <touch name="robot0:TS_lfmetacarpal_front" site="robot0:T_lfmetacarpal_front"></touch>
+
+        <touch name="robot0:TS_lfproximal_front_left_bottom"  site="robot0:T_lfproximal_front_left_bottom"></touch>
+        <touch name="robot0:TS_lfproximal_front_right_bottom" site="robot0:T_lfproximal_front_right_bottom"></touch>
+        <touch name="robot0:TS_lfproximal_front_left_top"     site="robot0:T_lfproximal_front_left_top"></touch>
+        <touch name="robot0:TS_lfproximal_front_right_top"    site="robot0:T_lfproximal_front_right_top"></touch>
+        <touch name="robot0:TS_lfproximal_back_left"          site="robot0:T_lfproximal_back_left"></touch>
+        <touch name="robot0:TS_lfproximal_back_right"         site="robot0:T_lfproximal_back_right"></touch>
+        <touch name="robot0:TS_lfproximal_tip"                site="robot0:T_lfproximal_tip"></touch>
+
+        <touch name="robot0:TS_lfmiddle_front_left"  site="robot0:T_lfmiddle_front_left"></touch>
+        <touch name="robot0:TS_lfmiddle_front_right" site="robot0:T_lfmiddle_front_right"></touch>
+        <touch name="robot0:TS_lfmiddle_back_left"   site="robot0:T_lfmiddle_back_left"></touch>
+        <touch name="robot0:TS_lfmiddle_back_right"  site="robot0:T_lfmiddle_back_right"></touch>
+        <touch name="robot0:TS_lfmiddle_tip"         site="robot0:T_lfmiddle_tip"></touch>
+
+        <touch name="robot0:TS_lftip_front_left"  site="robot0:T_lftip_front_left"></touch>
+        <touch name="robot0:TS_lftip_front_right" site="robot0:T_lftip_front_right"></touch>
+        <touch name="robot0:TS_lftip_back_left"   site="robot0:T_lftip_back_left"></touch>
+        <touch name="robot0:TS_lftip_back_right"  site="robot0:T_lftip_back_right"></touch>
+        <touch name="robot0:TS_lftip_tip"         site="robot0:T_lftip_tip"></touch>
+
+        <!--THUMB-->
+        <touch name="robot0:TS_thproximal_front_left"  site="robot0:T_thproximal_front_left"></touch>
+        <touch name="robot0:TS_thproximal_front_right" site="robot0:T_thproximal_front_right"></touch>
+        <touch name="robot0:TS_thproximal_back_left"   site="robot0:T_thproximal_back_left"></touch>
+        <touch name="robot0:TS_thproximal_back_right"  site="robot0:T_thproximal_back_right"></touch>
+        <touch name="robot0:TS_thproximal_tip"         site="robot0:T_thproximal_tip"></touch>
+
+        <touch name="robot0:TS_thmiddle_front_left"  site="robot0:T_thmiddle_front_left"></touch>
+        <touch name="robot0:TS_thmiddle_front_right" site="robot0:T_thmiddle_front_right"></touch>
+        <touch name="robot0:TS_thmiddle_back_left"   site="robot0:T_thmiddle_back_left"></touch>
+        <touch name="robot0:TS_thmiddle_back_right"  site="robot0:T_thmiddle_back_right"></touch>
+        <touch name="robot0:TS_thmiddle_tip"         site="robot0:T_thmiddle_tip"></touch>
+
+        <touch name="robot0:TS_thtip_front_left"  site="robot0:T_thtip_front_left"></touch>
+        <touch name="robot0:TS_thtip_front_right" site="robot0:T_thtip_front_right"></touch>
+        <touch name="robot0:TS_thtip_back_left"   site="robot0:T_thtip_back_left"></touch>
+        <touch name="robot0:TS_thtip_back_right"  site="robot0:T_thtip_back_right"></touch>
+        <touch name="robot0:TS_thtip_tip"         site="robot0:T_thtip_tip"></touch>
+
+    </sensor>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/base_link_collision.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/base_link_collision.stl
new file mode 100644
index 0000000..1ef459f
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/base_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/bellows_link_collision.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/bellows_link_collision.stl
new file mode 100644
index 0000000..a7e5ab7
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/bellows_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/elbow_flex_link_collision.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/elbow_flex_link_collision.stl
new file mode 100644
index 0000000..b0eea07
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/elbow_flex_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/estop_link.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/estop_link.stl
new file mode 100644
index 0000000..f6d1c72
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/estop_link.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/forearm_roll_link_collision.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/forearm_roll_link_collision.stl
new file mode 100644
index 0000000..fe468c5
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/forearm_roll_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/gripper_link.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/gripper_link.stl
new file mode 100644
index 0000000..8a14874
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/gripper_link.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/head_pan_link_collision.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/head_pan_link_collision.stl
new file mode 100644
index 0000000..c77b5b1
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/head_pan_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/head_tilt_link_collision.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/head_tilt_link_collision.stl
new file mode 100644
index 0000000..53c2ddc
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/head_tilt_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/l_wheel_link_collision.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/l_wheel_link_collision.stl
new file mode 100644
index 0000000..5c17524
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/l_wheel_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/laser_link.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/laser_link.stl
new file mode 100644
index 0000000..fa4882f
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/laser_link.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/r_wheel_link_collision.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/r_wheel_link_collision.stl
new file mode 100644
index 0000000..3742b24
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/r_wheel_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/shoulder_lift_link_collision.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/shoulder_lift_link_collision.stl
new file mode 100644
index 0000000..c9aff0d
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/shoulder_lift_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/shoulder_pan_link_collision.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/shoulder_pan_link_collision.stl
new file mode 100644
index 0000000..ac17a94
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/shoulder_pan_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/torso_fixed_link.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/torso_fixed_link.stl
new file mode 100644
index 0000000..7cf7fc1
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/torso_fixed_link.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/torso_lift_link_collision.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/torso_lift_link_collision.stl
new file mode 100644
index 0000000..4ce5fcf
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/torso_lift_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/upperarm_roll_link_collision.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/upperarm_roll_link_collision.stl
new file mode 100644
index 0000000..1207932
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/upperarm_roll_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/wrist_flex_link_collision.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/wrist_flex_link_collision.stl
new file mode 100644
index 0000000..3215d2e
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/wrist_flex_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/wrist_roll_link_collision.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/wrist_roll_link_collision.stl
new file mode 100644
index 0000000..742bdd9
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/wrist_roll_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F1.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F1.stl
new file mode 100644
index 0000000..515d3c9
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F1.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F2.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F2.stl
new file mode 100644
index 0000000..7bc5e20
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F2.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F3.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F3.stl
new file mode 100644
index 0000000..223f06f
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F3.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH1_z.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH1_z.stl
new file mode 100644
index 0000000..400ee2d
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH1_z.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH2_z.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH2_z.stl
new file mode 100644
index 0000000..5ace838
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH2_z.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH3_z.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH3_z.stl
new file mode 100644
index 0000000..23485ab
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH3_z.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/forearm_electric.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/forearm_electric.stl
new file mode 100644
index 0000000..80f6f3d
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/forearm_electric.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/forearm_electric_cvx.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/forearm_electric_cvx.stl
new file mode 100644
index 0000000..3c30f57
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/forearm_electric_cvx.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/knuckle.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/knuckle.stl
new file mode 100644
index 0000000..4faedd7
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/knuckle.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/lfmetacarpal.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/lfmetacarpal.stl
new file mode 100644
index 0000000..535cf4d
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/lfmetacarpal.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/palm.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/palm.stl
new file mode 100644
index 0000000..65e47eb
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/palm.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/wrist.stl b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/wrist.stl
new file mode 100644
index 0000000..420d5f9
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/wrist.stl differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/textures/block.png b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/textures/block.png
new file mode 100644
index 0000000..0243b8f
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/textures/block.png differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/textures/block_hidden.png b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/textures/block_hidden.png
new file mode 100644
index 0000000..e08b861
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/textures/block_hidden.png differ
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/__init__.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/pick_and_place.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/pick_and_place.py
new file mode 100644
index 0000000..c6c5e7e
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/pick_and_place.py
@@ -0,0 +1,23 @@
+import os
+from gym import utils
+from gym.envs.robotics import fetch_env
+
+
+# Ensure we get the path separator correct on windows
+MODEL_XML_PATH = os.path.join('fetch', 'pick_and_place.xml')
+
+
+class FetchPickAndPlaceEnv(fetch_env.FetchEnv, utils.EzPickle):
+    def __init__(self, reward_type='sparse'):
+        initial_qpos = {
+            'robot0:slide0': 0.405,
+            'robot0:slide1': 0.48,
+            'robot0:slide2': 0.0,
+            'object0:joint': [1.25, 0.53, 0.4, 1., 0., 0., 0.],
+        }
+        fetch_env.FetchEnv.__init__(
+            self, MODEL_XML_PATH, has_object=True, block_gripper=False, n_substeps=20,
+            gripper_extra_height=0.2, target_in_the_air=True, target_offset=0.0,
+            obj_range=0.15, target_range=0.15, distance_threshold=0.05,
+            initial_qpos=initial_qpos, reward_type=reward_type)
+        utils.EzPickle.__init__(self)
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/push.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/push.py
new file mode 100644
index 0000000..043d101
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/push.py
@@ -0,0 +1,23 @@
+import os
+from gym import utils
+from slbo.envs.robotics import fetch_env
+
+
+# Ensure we get the path separator correct on windows
+MODEL_XML_PATH = os.path.join('fetch', 'push.xml')
+
+
+class FetchPushEnv(fetch_env.FetchEnv, utils.EzPickle):
+    def __init__(self, reward_type='sparse'):
+        initial_qpos = {
+            'robot0:slide0': 0.405,
+            'robot0:slide1': 0.48,
+            'robot0:slide2': 0.0,
+            'object0:joint': [1.25, 0.53, 0.4, 1., 0., 0., 0.],
+        }
+        fetch_env.FetchEnv.__init__(
+            self, MODEL_XML_PATH, has_object=True, block_gripper=True, n_substeps=20,
+            gripper_extra_height=0.0, target_in_the_air=False, target_offset=0.0,
+            obj_range=0.15, target_range=0.15, distance_threshold=0.05,
+            initial_qpos=initial_qpos, reward_type=reward_type)
+        utils.EzPickle.__init__(self)
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/reach.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/reach.py
new file mode 100644
index 0000000..cc3fc46
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/reach.py
@@ -0,0 +1,22 @@
+import os
+from gym import utils
+from gym.envs.robotics import fetch_env
+
+
+# Ensure we get the path separator correct on windows
+MODEL_XML_PATH = os.path.join('fetch', 'reach.xml')
+
+
+class FetchReachEnv(fetch_env.FetchEnv, utils.EzPickle):
+    def __init__(self, reward_type='sparse'):
+        initial_qpos = {
+            'robot0:slide0': 0.4049,
+            'robot0:slide1': 0.48,
+            'robot0:slide2': 0.0,
+        }
+        fetch_env.FetchEnv.__init__(
+            self, MODEL_XML_PATH, has_object=False, block_gripper=True, n_substeps=20,
+            gripper_extra_height=0.2, target_in_the_air=True, target_offset=0.0,
+            obj_range=0.15, target_range=0.15, distance_threshold=0.05,
+            initial_qpos=initial_qpos, reward_type=reward_type)
+        utils.EzPickle.__init__(self)
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/slide.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/slide.py
new file mode 100644
index 0000000..63234db
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/slide.py
@@ -0,0 +1,25 @@
+import os
+import numpy as np
+
+from gym import utils
+from gym.envs.robotics import fetch_env
+
+
+# Ensure we get the path separator correct on windows
+MODEL_XML_PATH = os.path.join('fetch', 'slide.xml')
+
+
+class FetchSlideEnv(fetch_env.FetchEnv, utils.EzPickle):
+    def __init__(self, reward_type='sparse'):
+        initial_qpos = {
+            'robot0:slide0': 0.05,
+            'robot0:slide1': 0.48,
+            'robot0:slide2': 0.0,
+            'object0:joint': [1.7, 1.1, 0.41, 1., 0., 0., 0.],
+        }
+        fetch_env.FetchEnv.__init__(
+            self, MODEL_XML_PATH, has_object=True, block_gripper=True, n_substeps=20,
+            gripper_extra_height=-0.02, target_in_the_air=False, target_offset=np.array([0.4, 0.0, 0.0]),
+            obj_range=0.1, target_range=0.3, distance_threshold=0.05,
+            initial_qpos=initial_qpos, reward_type=reward_type)
+        utils.EzPickle.__init__(self)
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch_env.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch_env.py
new file mode 100644
index 0000000..8e19c42
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch_env.py
@@ -0,0 +1,197 @@
+import numpy as np
+
+from gym.envs.robotics import rotations, robot_env, utils
+
+
+def goal_distance(goal_a, goal_b):
+    assert goal_a.shape == goal_b.shape
+    return np.linalg.norm(goal_a - goal_b, axis=-1)
+
+
+class FetchEnv(robot_env.RobotEnv):
+    """Superclass for all Fetch environments.
+    """
+
+    def __init__(
+        self, model_path, n_substeps, gripper_extra_height, block_gripper,
+        has_object, target_in_the_air, target_offset, obj_range, target_range,
+        distance_threshold, initial_qpos, reward_type,
+    ):
+        """Initializes a new Fetch environment.
+
+        Args:
+            model_path (string): path to the environments XML file
+            n_substeps (int): number of substeps the simulation runs on every call to step
+            gripper_extra_height (float): additional height above the table when positioning the gripper
+            block_gripper (boolean): whether or not the gripper is blocked (i.e. not movable) or not
+            has_object (boolean): whether or not the environment has an object
+            target_in_the_air (boolean): whether or not the target should be in the air above the table or on the table surface
+            target_offset (float or array with 3 elements): offset of the target
+            obj_range (float): range of a uniform distribution for sampling initial object positions
+            target_range (float): range of a uniform distribution for sampling a target
+            distance_threshold (float): the threshold after which a goal is considered achieved
+            initial_qpos (dict): a dictionary of joint names and values that define the initial configuration
+            reward_type ('sparse' or 'dense'): the reward type, i.e. sparse or dense
+        """
+        self.gripper_extra_height = gripper_extra_height
+        self.block_gripper = block_gripper
+        self.has_object = has_object
+        self.target_in_the_air = target_in_the_air
+        self.target_offset = target_offset
+        self.obj_range = obj_range
+        self.target_range = target_range
+        self.distance_threshold = distance_threshold
+        self.reward_type = reward_type
+
+        super(FetchEnv, self).__init__(
+            model_path=model_path, n_substeps=n_substeps, n_actions=4,
+            initial_qpos=initial_qpos)
+
+    # GoalEnv methods
+    # ----------------------------
+
+    def compute_reward(self, achieved_goal, goal, info):
+        # Compute distance between goal and the achieved goal.
+        d = goal_distance(achieved_goal, goal)
+        return -(d > self.distance_threshold).astype(np.float32)
+
+    # RobotEnv methods
+    # ----------------------------
+
+    def _step_callback(self):
+        if self.block_gripper:
+            self.sim.data.set_joint_qpos('robot0:l_gripper_finger_joint', 0.)
+            self.sim.data.set_joint_qpos('robot0:r_gripper_finger_joint', 0.)
+            self.sim.forward()
+
+    def _set_action(self, action):
+        assert action.shape == (4,)
+        action = action.copy()  # ensure that we don't change the action outside of this scope
+        pos_ctrl, gripper_ctrl = action[:3], action[3]
+
+        pos_ctrl *= 0.05  # limit maximum change in position
+        rot_ctrl = [1., 0., 1., 0.]  # fixed rotation of the end effector, expressed as a quaternion
+        gripper_ctrl = np.array([gripper_ctrl, gripper_ctrl])
+        assert gripper_ctrl.shape == (2,)
+        if self.block_gripper:
+            gripper_ctrl = np.zeros_like(gripper_ctrl)
+        action = np.concatenate([pos_ctrl, rot_ctrl, gripper_ctrl])
+
+        # Apply action to simulation.
+        utils.ctrl_set_action(self.sim, action)
+        utils.mocap_set_action(self.sim, action)
+
+    def _get_obs(self):
+        # positions
+        grip_pos = self.sim.data.get_site_xpos('robot0:grip')
+        dt = self.sim.nsubsteps * self.sim.model.opt.timestep
+        grip_velp = self.sim.data.get_site_xvelp('robot0:grip') * dt
+        robot_qpos, robot_qvel = utils.robot_get_obs(self.sim)
+        if self.has_object:
+            object_pos = self.sim.data.get_site_xpos('object0')
+            # rotations
+            object_rot = rotations.mat2euler(self.sim.data.get_site_xmat('object0'))
+            # velocities
+            object_velp = self.sim.data.get_site_xvelp('object0') * dt
+            object_velr = self.sim.data.get_site_xvelr('object0') * dt
+            # gripper state
+            object_rel_pos = object_pos - grip_pos
+            object_velp -= grip_velp
+        else:
+            object_pos = object_rot = object_velp = object_velr = object_rel_pos = np.zeros(0)
+        gripper_state = robot_qpos[-2:]
+        gripper_vel = robot_qvel[-2:] * dt  # change to a scalar if the gripper is made symmetric
+
+        if not self.has_object:
+            achieved_goal = grip_pos.copy()
+        else:
+            achieved_goal = np.squeeze(object_pos.copy())
+        obs = np.concatenate([
+            grip_pos, object_pos.ravel(), object_rel_pos.ravel(), gripper_state, object_rot.ravel(),
+            object_velp.ravel(), object_velr.ravel(), grip_velp, gripper_vel,
+        ])
+
+        return {
+            'observation': obs.copy(),
+            'achieved_goal': achieved_goal.copy(),
+            'desired_goal': self.goal.copy(),
+        }
+
+    def mb_step(self, states, actions, next_states):
+        a = next_states[:,:3]
+        b = next_states[:,3:6]
+        d = np.linalg.norm(a - b, axis=-1)
+        rewards = -(d <= self.distance_threshold).astype(np.float32) 
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
+
+    def _viewer_setup(self):
+        body_id = self.sim.model.body_name2id('robot0:gripper_link')
+        lookat = self.sim.data.body_xpos[body_id]
+        for idx, value in enumerate(lookat):
+            self.viewer.cam.lookat[idx] = value
+        self.viewer.cam.distance = 2.5
+        self.viewer.cam.azimuth = 132.
+        self.viewer.cam.elevation = -14.
+
+    def _render_callback(self):
+        # Visualize target.
+        sites_offset = (self.sim.data.site_xpos - self.sim.model.site_pos).copy()
+        site_id = self.sim.model.site_name2id('target0')
+        self.sim.model.site_pos[site_id] = self.goal - sites_offset[0]
+        self.sim.forward()
+
+    def _reset_sim(self):
+        self.sim.set_state(self.initial_state)
+
+        # Randomize start position of object.
+        if self.has_object:
+            object_xpos = self.initial_gripper_xpos[:2]
+            while np.linalg.norm(object_xpos - self.initial_gripper_xpos[:2]) < 0.1:
+                object_xpos = self.initial_gripper_xpos[:2] + self.np_random.uniform(-self.obj_range, self.obj_range, size=2)
+            object_qpos = self.sim.data.get_joint_qpos('object0:joint')
+            assert object_qpos.shape == (7,)
+            object_qpos[:2] = object_xpos
+            self.sim.data.set_joint_qpos('object0:joint', object_qpos)
+
+        self.sim.forward()
+        return True
+
+    def _sample_goal(self):
+        if self.has_object:
+            goal = self.initial_gripper_xpos[:3] + self.np_random.uniform(-self.target_range, self.target_range, size=3)
+            goal += self.target_offset
+            goal[2] = self.height_offset
+            if self.target_in_the_air and self.np_random.uniform() < 0.5:
+                goal[2] += self.np_random.uniform(0, 0.45)
+        else:
+            goal = self.initial_gripper_xpos[:3] + self.np_random.uniform(-self.target_range, self.target_range, size=3)
+        return goal.copy()
+
+    def _is_success(self, achieved_goal, desired_goal):
+        d = goal_distance(achieved_goal, desired_goal)
+        return (d < self.distance_threshold).astype(np.float32)
+
+    def _env_setup(self, initial_qpos):
+        for name, value in initial_qpos.items():
+            self.sim.data.set_joint_qpos(name, value)
+        utils.reset_mocap_welds(self.sim)
+        self.sim.forward()
+
+        # Move end effector into position.
+        gripper_target = np.array([-0.498, 0.005, -0.431 + self.gripper_extra_height]) + self.sim.data.get_site_xpos('robot0:grip')
+        gripper_rotation = np.array([1., 0., 1., 0.])
+        self.sim.data.set_mocap_pos('robot0:mocap', gripper_target)
+        self.sim.data.set_mocap_quat('robot0:mocap', gripper_rotation)
+        for _ in range(10):
+            self.sim.step()
+
+        # Extract information for sampling goals.
+        self.initial_gripper_xpos = self.sim.data.get_site_xpos('robot0:grip').copy()
+        if self.has_object:
+            self.height_offset = self.sim.data.get_site_xpos('object0')[2]
+
+    def render(self, mode='human', width=500, height=500):
+        return super(FetchEnv, self).render(mode, width, height)
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand/__init__.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand/manipulate.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand/manipulate.py
new file mode 100644
index 0000000..85a7dd0
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand/manipulate.py
@@ -0,0 +1,344 @@
+import os
+import numpy as np
+
+from gym import utils, error
+from gym.envs.robotics import rotations, hand_env
+from gym.envs.robotics.utils import robot_get_obs
+from gym.wrappers import FlattenObservation
+
+
+from slbo.utils.dataset import Dataset, gen_dtype
+
+try:
+    import mujoco_py
+except ImportError as e:
+    raise error.DependencyNotInstalled("{}. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)".format(e))
+
+
+def quat_from_angle_and_axis(angle, axis):
+    assert axis.shape == (3,)
+    axis /= np.linalg.norm(axis)
+    quat = np.concatenate([[np.cos(angle / 2.)], np.sin(angle / 2.) * axis])
+    quat /= np.linalg.norm(quat)
+    return quat
+
+
+# Ensure we get the path separator correct on windows
+MANIPULATE_BLOCK_XML = os.path.join('hand', 'manipulate_block.xml')
+MANIPULATE_EGG_XML = os.path.join('hand', 'manipulate_egg.xml')
+MANIPULATE_PEN_XML = os.path.join('hand', 'manipulate_pen.xml')
+
+
+class ManipulateEnv(hand_env.HandEnv):
+    def __init__(
+        self, model_path, target_position, target_rotation,
+        target_position_range, reward_type, initial_qpos=None,
+        randomize_initial_position=True, randomize_initial_rotation=True,
+        distance_threshold=0.01, rotation_threshold=0.1, n_substeps=20, relative_control=False,
+        ignore_z_target_rotation=False,
+    ):
+        """Initializes a new Hand manipulation environment.
+
+        Args:
+            model_path (string): path to the environments XML file
+            target_position (string): the type of target position:
+                - ignore: target position is fully ignored, i.e. the object can be positioned arbitrarily
+                - fixed: target position is set to the initial position of the object
+                - random: target position is fully randomized according to target_position_range
+            target_rotation (string): the type of target rotation:
+                - ignore: target rotation is fully ignored, i.e. the object can be rotated arbitrarily
+                - fixed: target rotation is set to the initial rotation of the object
+                - xyz: fully randomized target rotation around the X, Y and Z axis
+                - z: fully randomized target rotation around the Z axis
+                - parallel: fully randomized target rotation around Z and axis-aligned rotation around X, Y
+            ignore_z_target_rotation (boolean): whether or not the Z axis of the target rotation is ignored
+            target_position_range (np.array of shape (3, 2)): range of the target_position randomization
+            reward_type ('sparse' or 'dense'): the reward type, i.e. sparse or dense
+            initial_qpos (dict): a dictionary of joint names and values that define the initial configuration
+            randomize_initial_position (boolean): whether or not to randomize the initial position of the object
+            randomize_initial_rotation (boolean): whether or not to randomize the initial rotation of the object
+            distance_threshold (float, in meters): the threshold after which the position of a goal is considered achieved
+            rotation_threshold (float, in radians): the threshold after which the rotation of a goal is considered achieved
+            n_substeps (int): number of substeps the simulation runs on every call to step
+            relative_control (boolean): whether or not the hand is actuated in absolute joint positions or relative to the current state
+        """
+        self.target_position = target_position
+        self.target_rotation = target_rotation
+        self.target_position_range = target_position_range
+        self.parallel_quats = [rotations.euler2quat(r) for r in rotations.get_parallel_rotations()]
+        self.randomize_initial_rotation = randomize_initial_rotation
+        self.randomize_initial_position = randomize_initial_position
+        self.distance_threshold = distance_threshold
+        self.rotation_threshold = rotation_threshold
+        self.reward_type = reward_type
+        self.ignore_z_target_rotation = ignore_z_target_rotation
+
+        assert self.target_position in ['ignore', 'fixed', 'random']
+        assert self.target_rotation in ['ignore', 'fixed', 'xyz', 'z', 'parallel']
+        initial_qpos = initial_qpos or {}
+
+        hand_env.HandEnv.__init__(
+            self, model_path, n_substeps=n_substeps, initial_qpos=initial_qpos,
+            relative_control=relative_control)
+
+
+    def _get_achieved_goal(self):
+        # Object position and rotation.
+        object_qpos = self.sim.data.get_joint_qpos('object:joint')
+        assert object_qpos.shape == (7,)
+        return object_qpos
+
+    def _goal_distance(self, goal_a, goal_b):
+        assert goal_a.shape == goal_b.shape
+        assert goal_a.shape[-1] == 7
+
+        d_pos = np.zeros_like(goal_a[..., 0])
+        d_rot = np.zeros_like(goal_b[..., 0])
+        if self.target_position != 'ignore':
+            delta_pos = goal_a[..., :3] - goal_b[..., :3]
+            d_pos = np.linalg.norm(delta_pos, axis=-1)
+
+        if self.target_rotation != 'ignore':
+            quat_a, quat_b = goal_a[..., 3:], goal_b[..., 3:]
+
+            if self.ignore_z_target_rotation:
+                # Special case: We want to ignore the Z component of the rotation.
+                # This code here assumes Euler angles with xyz convention. We first transform
+                # to euler, then set the Z component to be equal between the two, and finally
+                # transform back into quaternions.
+                euler_a = rotations.quat2euler(quat_a)
+                euler_b = rotations.quat2euler(quat_b)
+                euler_a[2] = euler_b[2]
+                quat_a = rotations.euler2quat(euler_a)
+
+            # Subtract quaternions and extract angle between them.
+            quat_diff = rotations.quat_mul(quat_a, rotations.quat_conjugate(quat_b))
+            angle_diff = 2 * np.arccos(np.clip(quat_diff[..., 0], -1., 1.))
+            d_rot = angle_diff
+        assert d_pos.shape == d_rot.shape
+        return d_pos, d_rot
+
+    # GoalEnv methods
+    # ----------------------------
+
+    def compute_reward(self, achieved_goal, goal, info):
+        if self.reward_type == 'sparse':
+            success = self._is_success(achieved_goal, goal).astype(np.float32)
+            return success 
+        else:
+            d_pos, d_rot = self._goal_distance(achieved_goal, goal)
+            # We weigh the difference in position to avoid that `d_pos` (in meters) is completely
+            # dominated by `d_rot` (in radians).
+            return -(10. * d_pos + d_rot)
+
+    # RobotEnv methods
+    # ----------------------------
+
+    def _is_success(self, achieved_goal, desired_goal):
+        d_pos, d_rot = self._goal_distance(achieved_goal, desired_goal)
+        achieved_pos = (d_pos < self.distance_threshold).astype(np.float32)
+        achieved_rot = (d_rot < self.rotation_threshold).astype(np.float32)
+        achieved_both = achieved_pos * achieved_rot
+        return achieved_both
+
+    def _env_setup(self, initial_qpos):
+        for name, value in initial_qpos.items():
+            self.sim.data.set_joint_qpos(name, value)
+        self.sim.forward()
+
+    def _reset_sim(self):
+        self.sim.set_state(self.initial_state)
+        self.sim.forward()
+
+        initial_qpos = self.sim.data.get_joint_qpos('object:joint').copy()
+        initial_pos, initial_quat = initial_qpos[:3], initial_qpos[3:]
+        assert initial_qpos.shape == (7,)
+        assert initial_pos.shape == (3,)
+        assert initial_quat.shape == (4,)
+        initial_qpos = None
+
+        # Randomization initial rotation.
+        if self.randomize_initial_rotation:
+            if self.target_rotation == 'z':
+                angle = self.np_random.uniform(-np.pi, np.pi)
+                axis = np.array([0., 0., 1.])
+                offset_quat = quat_from_angle_and_axis(angle, axis)
+                initial_quat = rotations.quat_mul(initial_quat, offset_quat)
+            elif self.target_rotation == 'parallel':
+                angle = self.np_random.uniform(-np.pi, np.pi)
+                axis = np.array([0., 0., 1.])
+                z_quat = quat_from_angle_and_axis(angle, axis)
+                parallel_quat = self.parallel_quats[self.np_random.randint(len(self.parallel_quats))]
+                offset_quat = rotations.quat_mul(z_quat, parallel_quat)
+                initial_quat = rotations.quat_mul(initial_quat, offset_quat)
+            elif self.target_rotation in ['xyz', 'ignore']:
+                angle = self.np_random.uniform(-np.pi, np.pi)
+                axis = self.np_random.uniform(-1., 1., size=3)
+                offset_quat = quat_from_angle_and_axis(angle, axis)
+                initial_quat = rotations.quat_mul(initial_quat, offset_quat)
+            elif self.target_rotation == 'fixed':
+                pass
+            else:
+                raise error.Error('Unknown target_rotation option "{}".'.format(self.target_rotation))
+
+        # Randomize initial position.
+        if self.randomize_initial_position:
+            if self.target_position != 'fixed':
+                initial_pos += self.np_random.normal(size=3, scale=0.005)
+
+        initial_quat /= np.linalg.norm(initial_quat)
+        initial_qpos = np.concatenate([initial_pos, initial_quat])
+        self.sim.data.set_joint_qpos('object:joint', initial_qpos)
+
+        def is_on_palm():
+            self.sim.forward()
+            cube_middle_idx = self.sim.model.site_name2id('object:center')
+            cube_middle_pos = self.sim.data.site_xpos[cube_middle_idx]
+            is_on_palm = (cube_middle_pos[2] > 0.04)
+            return is_on_palm
+
+        # Run the simulation for a bunch of timesteps to let everything settle in.
+        for _ in range(10):
+            self._set_action(np.zeros(20))
+            try:
+                self.sim.step()
+            except mujoco_py.MujocoException:
+                return False
+        return is_on_palm()
+
+    def _sample_goal(self):
+        # Select a goal for the object position.
+        target_pos = None
+        if self.target_position == 'random':
+            assert self.target_position_range.shape == (3, 2)
+            offset = self.np_random.uniform(self.target_position_range[:, 0], self.target_position_range[:, 1])
+            assert offset.shape == (3,)
+            target_pos = self.sim.data.get_joint_qpos('object:joint')[:3] + offset
+        elif self.target_position in ['ignore', 'fixed']:
+            target_pos = self.sim.data.get_joint_qpos('object:joint')[:3]
+        else:
+            raise error.Error('Unknown target_position option "{}".'.format(self.target_position))
+        assert target_pos is not None
+        assert target_pos.shape == (3,)
+
+        # Select a goal for the object rotation.
+        target_quat = None
+        if self.target_rotation == 'z':
+            angle = self.np_random.uniform(-np.pi, np.pi)
+            axis = np.array([0., 0., 1.])
+            target_quat = quat_from_angle_and_axis(angle, axis)
+        elif self.target_rotation == 'parallel':
+            angle = self.np_random.uniform(-np.pi, np.pi)
+            axis = np.array([0., 0., 1.])
+            target_quat = quat_from_angle_and_axis(angle, axis)
+            parallel_quat = self.parallel_quats[self.np_random.randint(len(self.parallel_quats))]
+            target_quat = rotations.quat_mul(target_quat, parallel_quat)
+        elif self.target_rotation == 'xyz':
+            angle = self.np_random.uniform(-np.pi, np.pi)
+            axis = self.np_random.uniform(-1., 1., size=3)
+            target_quat = quat_from_angle_and_axis(angle, axis)
+        elif self.target_rotation in ['ignore', 'fixed']:
+            target_quat = self.sim.data.get_joint_qpos('object:joint')[3:]
+        else:
+            raise error.Error('Unknown target_rotation option "{}".'.format(self.target_rotation))
+        assert target_quat is not None
+        assert target_quat.shape == (4,)
+
+        target_quat /= np.linalg.norm(target_quat)  # normalized quaternion
+        goal = np.concatenate([target_pos, target_quat])
+        return goal
+
+    def _render_callback(self):
+        # Assign current state to target object but offset a bit so that the actual object
+        # is not obscured.
+        goal = self.goal.copy()
+        assert goal.shape == (7,)
+        if self.target_position == 'ignore':
+            # Move the object to the side since we do not care about it's position.
+            goal[0] += 0.15
+        self.sim.data.set_joint_qpos('target:joint', goal)
+        self.sim.data.set_joint_qvel('target:joint', np.zeros(6))
+
+        if 'object_hidden' in self.sim.model.geom_names:
+            hidden_id = self.sim.model.geom_name2id('object_hidden')
+            self.sim.model.geom_rgba[hidden_id, 3] = 1.
+        self.sim.forward()
+
+    def _get_obs(self):
+        robot_qpos, robot_qvel = robot_get_obs(self.sim)
+        object_qvel = self.sim.data.get_joint_qvel('object:joint')
+        achieved_goal = self._get_achieved_goal().ravel()  # this contains the object position + rotation
+        observation = np.concatenate([robot_qpos, robot_qvel, object_qvel, achieved_goal])
+        return {
+            'observation': observation.copy(),
+            'achieved_goal': achieved_goal.copy(),
+            'desired_goal': self.goal.ravel().copy(),
+        }
+
+
+class HandBlockEnv(ManipulateEnv, utils.EzPickle):
+    def __init__(self, target_position='random', target_rotation='xyz', reward_type='sparse'):
+        utils.EzPickle.__init__(self, target_position, target_rotation, reward_type)
+        ManipulateEnv.__init__(self,
+            model_path=MANIPULATE_BLOCK_XML, target_position=target_position,
+            target_rotation=target_rotation,
+            target_position_range=np.array([(-0.04, 0.04), (-0.06, 0.02), (0.0, 0.06)]),
+            reward_type=reward_type)
+
+
+class HandEggEnv(ManipulateEnv, utils.EzPickle):
+    def __init__(self, target_position='random', target_rotation='fixed', reward_type='sparse'):
+        utils.EzPickle.__init__(self, target_position, target_rotation, reward_type)
+        ManipulateEnv.__init__(self,
+            model_path=MANIPULATE_EGG_XML, target_position=target_position,
+            target_rotation=target_rotation,
+            target_position_range=np.array([(-0.04, 0.04), (-0.06, 0.02), (0.0, 0.06)]),
+            reward_type=reward_type)
+
+
+    def verify(self, n=2000, eps=1e-4):
+        dummy = FlattenObservation(self)
+
+        dataset = Dataset(gen_dtype(dummy, 'state action next_state reward done'), n)
+        state = dummy.reset()
+        #print(state)
+        for _ in range(n):
+            action = dummy.action_space.sample()
+            next_state, reward, done, _ = dummy.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = dummy.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        # logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
+
+    def mb_step(self, states, actions, next_states):
+        a = next_states[:,:7]
+        b = next_states[:,7:14]
+        if self.reward_type == 'sparse':
+            success = self._is_success(a, b).astype(np.float32)
+            rewards = (success * 10 - 1.)
+        else:
+            d_pos, d_rot = self._goal_distance(a, b)
+            # We weigh the difference in position to avoid that `d_pos` (in meters) is completely
+            # dominated by `d_rot` (in radians).
+            rewards = -(10. * d_pos + d_rot)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+
+class HandPenEnv(ManipulateEnv, utils.EzPickle):
+    def __init__(self, target_position='random', target_rotation='xyz', reward_type='sparse'):
+        utils.EzPickle.__init__(self, target_position, target_rotation, reward_type)
+        ManipulateEnv.__init__(self,
+            model_path=MANIPULATE_PEN_XML, target_position=target_position,
+            target_rotation=target_rotation,
+            target_position_range=np.array([(-0.04, 0.04), (-0.06, 0.02), (0.0, 0.06)]),
+            randomize_initial_rotation=False, reward_type=reward_type,
+            ignore_z_target_rotation=True, distance_threshold=0.05)
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand/manipulate_touch_sensors.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand/manipulate_touch_sensors.py
new file mode 100644
index 0000000..c364868
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand/manipulate_touch_sensors.py
@@ -0,0 +1,131 @@
+import os
+import numpy as np
+
+from gym import utils, error, spaces
+from gym.envs.robotics.hand import manipulate
+
+# Ensure we get the path separator correct on windows
+MANIPULATE_BLOCK_XML = os.path.join('hand', 'manipulate_block_touch_sensors.xml')
+MANIPULATE_EGG_XML = os.path.join('hand', 'manipulate_egg_touch_sensors.xml')
+MANIPULATE_PEN_XML = os.path.join('hand', 'manipulate_pen_touch_sensors.xml')
+
+
+class ManipulateTouchSensorsEnv(manipulate.ManipulateEnv):
+    def __init__(
+        self, model_path, target_position, target_rotation,
+        target_position_range, reward_type, initial_qpos={},
+        randomize_initial_position=True, randomize_initial_rotation=True,
+        distance_threshold=0.01, rotation_threshold=0.1, n_substeps=20, relative_control=False,
+        ignore_z_target_rotation=False, touch_visualisation="on_touch", touch_get_obs="sensordata",
+    ):
+        """Initializes a new Hand manipulation environment with touch sensors.
+
+        Args:
+            touch_visualisation (string): how touch sensor sites are visualised
+                - "on_touch": shows touch sensor sites only when touch values > 0
+                - "always": always shows touch sensor sites
+                - "off" or else: does not show touch sensor sites
+            touch_get_obs (string): touch sensor readings
+                - "boolean": returns 1 if touch sensor reading != 0.0 else 0
+                - "sensordata": returns original touch sensor readings from self.sim.data.sensordata[id]
+                - "log": returns log(x+1) touch sensor readings from self.sim.data.sensordata[id]
+                - "off" or else: does not add touch sensor readings to the observation
+
+        """
+        self.touch_visualisation = touch_visualisation
+        self.touch_get_obs = touch_get_obs
+        self._touch_sensor_id_site_id = []
+        self._touch_sensor_id = []
+        self.touch_color = [1, 0, 0, 0.5]
+        self.notouch_color = [0, 0.5, 0, 0.2]
+
+        manipulate.ManipulateEnv.__init__(
+            self, model_path, target_position, target_rotation,
+            target_position_range, reward_type, initial_qpos=initial_qpos,
+            randomize_initial_position=randomize_initial_position, randomize_initial_rotation=randomize_initial_rotation,
+            distance_threshold=distance_threshold, rotation_threshold=rotation_threshold, n_substeps=n_substeps, relative_control=relative_control,
+            ignore_z_target_rotation=ignore_z_target_rotation,
+        )
+
+        for k, v in self.sim.model._sensor_name2id.items():  # get touch sensor site names and their ids
+            if 'robot0:TS_' in k:
+                self._touch_sensor_id_site_id.append((v, self.sim.model._site_name2id[k.replace('robot0:TS_', 'robot0:T_')]))
+                self._touch_sensor_id.append(v)
+
+        if self.touch_visualisation == 'off':  # set touch sensors rgba values
+            for _, site_id in self._touch_sensor_id_site_id:
+                self.sim.model.site_rgba[site_id][3] = 0.0
+        elif self.touch_visualisation == 'always':
+            pass
+
+        obs = self._get_obs()
+        self.observation_space = spaces.Dict(dict(
+            desired_goal=spaces.Box(-np.inf, np.inf, shape=obs['achieved_goal'].shape, dtype='float32'),
+            achieved_goal=spaces.Box(-np.inf, np.inf, shape=obs['achieved_goal'].shape, dtype='float32'),
+            observation=spaces.Box(-np.inf, np.inf, shape=obs['observation'].shape, dtype='float32'),
+        ))
+
+    def _render_callback(self):
+        super(ManipulateTouchSensorsEnv, self)._render_callback()
+        if self.touch_visualisation == 'on_touch':
+            for touch_sensor_id, site_id in self._touch_sensor_id_site_id:
+                if self.sim.data.sensordata[touch_sensor_id] != 0.0:
+                    self.sim.model.site_rgba[site_id] = self.touch_color
+                else:
+                    self.sim.model.site_rgba[site_id] = self.notouch_color
+
+    def _get_obs(self):
+        robot_qpos, robot_qvel = manipulate.robot_get_obs(self.sim)
+        object_qvel = self.sim.data.get_joint_qvel('object:joint')
+        achieved_goal = self._get_achieved_goal().ravel()  # this contains the object position + rotation
+        touch_values = []  # get touch sensor readings. if there is one, set value to 1
+        if self.touch_get_obs == 'sensordata':
+            touch_values = self.sim.data.sensordata[self._touch_sensor_id]
+        elif self.touch_get_obs == 'boolean':
+            touch_values = self.sim.data.sensordata[self._touch_sensor_id] > 0.0
+        elif self.touch_get_obs == 'log':
+            touch_values = np.log(self.sim.data.sensordata[self._touch_sensor_id] + 1.0)
+        observation = np.concatenate([robot_qpos, robot_qvel, object_qvel, touch_values, achieved_goal])
+
+        return {
+            'observation': observation.copy(),
+            'achieved_goal': achieved_goal.copy(),
+            'desired_goal': self.goal.ravel().copy(),
+        }
+
+
+class HandBlockTouchSensorsEnv(ManipulateTouchSensorsEnv, utils.EzPickle):
+    def __init__(self, target_position='random', target_rotation='xyz', touch_get_obs='sensordata', reward_type='sparse'):
+        utils.EzPickle.__init__(self, target_position, target_rotation, touch_get_obs, reward_type)
+        ManipulateTouchSensorsEnv.__init__(self,
+            model_path=MANIPULATE_BLOCK_XML,
+            touch_get_obs=touch_get_obs,
+            target_rotation=target_rotation,
+            target_position=target_position,
+            target_position_range=np.array([(-0.04, 0.04), (-0.06, 0.02), (0.0, 0.06)]),
+            reward_type=reward_type)
+
+
+class HandEggTouchSensorsEnv(ManipulateTouchSensorsEnv, utils.EzPickle):
+    def __init__(self, target_position='random', target_rotation='xyz', touch_get_obs='sensordata', reward_type='sparse'):
+        utils.EzPickle.__init__(self, target_position, target_rotation, touch_get_obs, reward_type)
+        ManipulateTouchSensorsEnv.__init__(self,
+            model_path=MANIPULATE_EGG_XML,
+            touch_get_obs=touch_get_obs,
+            target_rotation=target_rotation,
+            target_position=target_position,
+            target_position_range=np.array([(-0.04, 0.04), (-0.06, 0.02), (0.0, 0.06)]),
+            reward_type=reward_type)
+
+
+class HandPenTouchSensorsEnv(ManipulateTouchSensorsEnv, utils.EzPickle):
+    def __init__(self, target_position='random', target_rotation='xyz', touch_get_obs='sensordata', reward_type='sparse'):
+        utils.EzPickle.__init__(self, target_position, target_rotation, touch_get_obs, reward_type)
+        ManipulateTouchSensorsEnv.__init__(self,
+            model_path=MANIPULATE_PEN_XML,
+            touch_get_obs=touch_get_obs,
+            target_rotation=target_rotation,
+            target_position=target_position,
+            target_position_range=np.array([(-0.04, 0.04), (-0.06, 0.02), (0.0, 0.06)]),
+            randomize_initial_rotation=False, reward_type=reward_type,
+            ignore_z_target_rotation=True, distance_threshold=0.05)
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand/reach.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand/reach.py
new file mode 100644
index 0000000..1459e5a
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand/reach.py
@@ -0,0 +1,159 @@
+import os
+import numpy as np
+
+from gym import utils
+from slbo.envs.robotics import hand_env
+from gym.envs.robotics.utils import robot_get_obs
+
+
+FINGERTIP_SITE_NAMES = [
+    'robot0:S_fftip',
+    'robot0:S_mftip',
+    'robot0:S_rftip',
+    'robot0:S_lftip',
+    'robot0:S_thtip',
+]
+
+
+DEFAULT_INITIAL_QPOS = {
+    'robot0:WRJ1': -0.16514339750464327,
+    'robot0:WRJ0': -0.31973286565062153,
+    'robot0:FFJ3': 0.14340512546557435,
+    'robot0:FFJ2': 0.32028208333591573,
+    'robot0:FFJ1': 0.7126053607727917,
+    'robot0:FFJ0': 0.6705281001412586,
+    'robot0:MFJ3': 0.000246444303701037,
+    'robot0:MFJ2': 0.3152655251085491,
+    'robot0:MFJ1': 0.7659800313729842,
+    'robot0:MFJ0': 0.7323156897425923,
+    'robot0:RFJ3': 0.00038520700007378114,
+    'robot0:RFJ2': 0.36743546201985233,
+    'robot0:RFJ1': 0.7119514095008576,
+    'robot0:RFJ0': 0.6699446327514138,
+    'robot0:LFJ4': 0.0525442258033891,
+    'robot0:LFJ3': -0.13615534724474673,
+    'robot0:LFJ2': 0.39872030433433003,
+    'robot0:LFJ1': 0.7415570009679252,
+    'robot0:LFJ0': 0.704096378652974,
+    'robot0:THJ4': 0.003673823825070126,
+    'robot0:THJ3': 0.5506291436028695,
+    'robot0:THJ2': -0.014515151997119306,
+    'robot0:THJ1': -0.0015229223564485414,
+    'robot0:THJ0': -0.7894883021600622,
+}
+
+
+# Ensure we get the path separator correct on windows
+MODEL_XML_PATH = os.path.join('hand', 'reach.xml')
+
+
+def goal_distance(goal_a, goal_b):
+    assert goal_a.shape == goal_b.shape
+    return np.linalg.norm(goal_a - goal_b, axis=-1)
+
+
+class HandReachEnv(hand_env.HandEnv, utils.EzPickle):
+    def __init__(
+        self, distance_threshold=0.01, n_substeps=20, relative_control=False,
+        initial_qpos=DEFAULT_INITIAL_QPOS, reward_type='sparse',
+    ):
+        utils.EzPickle.__init__(**locals())
+        self.distance_threshold = distance_threshold
+        self.reward_type = 'sparse'
+
+        hand_env.HandEnv.__init__(
+            self, MODEL_XML_PATH, n_substeps=n_substeps, initial_qpos=initial_qpos,
+            relative_control=relative_control)
+
+    def _get_achieved_goal(self):
+        goal = [self.sim.data.get_site_xpos(name) for name in FINGERTIP_SITE_NAMES]
+        return np.array(goal).flatten()
+
+    # GoalEnv methods
+    # ----------------------------
+
+    def compute_reward(self, achieved_goal, goal, info):
+        d = goal_distance(achieved_goal, goal)
+        if self.reward_type == 'sparse':
+            return -(d > self.distance_threshold).astype(np.float32)
+        else:
+            return -d
+
+    # RobotEnv methods
+    # ----------------------------
+
+    def mb_step(self, states, actions, next_states):
+        a = next_states[:,:15]
+        b = next_states[:,15:30]
+        d = goal_distance(achieved_goal, goal)
+        if self.reward_type == 'sparse':
+            rewards =  -(d > self.distance_threshold).astype(np.float32)
+        else:
+            rewards =  -d
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def _env_setup(self, initial_qpos):
+        for name, value in initial_qpos.items():
+            self.sim.data.set_joint_qpos(name, value)
+        self.sim.forward()
+
+        self.initial_goal = self._get_achieved_goal().copy()
+        self.palm_xpos = self.sim.data.body_xpos[self.sim.model.body_name2id('robot0:palm')].copy()
+
+    def _get_obs(self):
+        robot_qpos, robot_qvel = robot_get_obs(self.sim)
+        achieved_goal = self._get_achieved_goal().ravel()
+        observation = np.concatenate([robot_qpos, robot_qvel, achieved_goal])
+        return {
+            'observation': observation.copy(),
+            'achieved_goal': achieved_goal.copy(),
+            'desired_goal': self.goal.copy(),
+        }
+
+    def _sample_goal(self):
+        thumb_name = 'robot0:S_thtip'
+        finger_names = [name for name in FINGERTIP_SITE_NAMES if name != thumb_name]
+        finger_name = self.np_random.choice(finger_names)
+
+        thumb_idx = FINGERTIP_SITE_NAMES.index(thumb_name)
+        finger_idx = FINGERTIP_SITE_NAMES.index(finger_name)
+        assert thumb_idx != finger_idx
+
+        # Pick a meeting point above the hand.
+        meeting_pos = self.palm_xpos + np.array([0.0, -0.09, 0.05])
+        meeting_pos += self.np_random.normal(scale=0.005, size=meeting_pos.shape)
+
+        # Slightly move meeting goal towards the respective finger to avoid that they
+        # overlap.
+        goal = self.initial_goal.copy().reshape(-1, 3)
+        for idx in [thumb_idx, finger_idx]:
+            offset_direction = (meeting_pos - goal[idx])
+            offset_direction /= np.linalg.norm(offset_direction)
+            goal[idx] = meeting_pos - 0.005 * offset_direction
+
+        if self.np_random.uniform() < 0.1:
+            # With some probability, ask all fingers to move back to the origin.
+            # This avoids that the thumb constantly stays near the goal position already.
+            goal = self.initial_goal.copy()
+        return goal.flatten()
+
+    def _is_success(self, achieved_goal, desired_goal):
+        d = goal_distance(achieved_goal, desired_goal)
+        return (d < self.distance_threshold).astype(np.float32)
+
+    def _render_callback(self):
+        # Visualize targets.
+        sites_offset = (self.sim.data.site_xpos - self.sim.model.site_pos).copy()
+        goal = self.goal.reshape(5, 3)
+        for finger_idx in range(5):
+            site_name = 'target{}'.format(finger_idx)
+            site_id = self.sim.model.site_name2id(site_name)
+            self.sim.model.site_pos[site_id] = goal[finger_idx] - sites_offset[site_id]
+
+        # Visualize finger positions.
+        achieved_goal = self._get_achieved_goal().reshape(5, 3)
+        for finger_idx in range(5):
+            site_name = 'finger{}'.format(finger_idx)
+            site_id = self.sim.model.site_name2id(site_name)
+            self.sim.model.site_pos[site_id] = achieved_goal[finger_idx] - sites_offset[site_id]
+        self.sim.forward()
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand_env.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand_env.py
new file mode 100644
index 0000000..a66e2cd
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand_env.py
@@ -0,0 +1,55 @@
+import os
+import copy
+import numpy as np
+
+import gym
+from gym import error, spaces
+from gym.utils import seeding
+from gym.envs.robotics import robot_env
+
+
+class HandEnv(robot_env.RobotEnv):
+    def __init__(self, model_path, n_substeps, initial_qpos, relative_control):
+        self.relative_control = relative_control
+
+        super(HandEnv, self).__init__(
+            model_path=model_path, n_substeps=n_substeps, n_actions=20,
+            initial_qpos=initial_qpos)
+
+    # RobotEnv methods
+    # ----------------------------
+
+    def _set_action(self, action):
+        assert action.shape == (20,)
+
+        ctrlrange = self.sim.model.actuator_ctrlrange
+        actuation_range = (ctrlrange[:, 1] - ctrlrange[:, 0]) / 2.
+        if self.relative_control:
+            actuation_center = np.zeros_like(action)
+            for i in range(self.sim.data.ctrl.shape[0]):
+                actuation_center[i] = self.sim.data.get_joint_qpos(
+                    self.sim.model.actuator_names[i].replace(':A_', ':'))
+            for joint_name in ['FF', 'MF', 'RF', 'LF']:
+                act_idx = self.sim.model.actuator_name2id(
+                    'robot0:A_{}J1'.format(joint_name))
+                actuation_center[act_idx] += self.sim.data.get_joint_qpos(
+                    'robot0:{}J0'.format(joint_name))
+        else:
+            actuation_center = (ctrlrange[:, 1] + ctrlrange[:, 0]) / 2.
+        self.sim.data.ctrl[:] = actuation_center + action * actuation_range
+        self.sim.data.ctrl[:] = np.clip(self.sim.data.ctrl, ctrlrange[:, 0], ctrlrange[:, 1])
+
+    def _viewer_setup(self):
+        body_id = self.sim.model.body_name2id('robot0:palm')
+        lookat = self.sim.data.body_xpos[body_id]
+        for idx, value in enumerate(lookat):
+            self.viewer.cam.lookat[idx] = value
+        self.viewer.cam.distance = 0.5
+        self.viewer.cam.azimuth = 55.
+        self.viewer.cam.elevation = -25.
+
+    def render(self, mode='human', width=500, height=500):
+        return super(HandEnv, self).render(mode, width, height)
+
+    def verify(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/robot_env.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/robot_env.py
new file mode 100644
index 0000000..584ab1d
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/robot_env.py
@@ -0,0 +1,170 @@
+import os
+import copy
+import numpy as np
+
+import gym
+from gym import error, spaces
+from gym.utils import seeding
+
+try:
+    import mujoco_py
+except ImportError as e:
+    raise error.DependencyNotInstalled("{}. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)".format(e))
+
+DEFAULT_SIZE = 500
+
+class RobotEnv(gym.GoalEnv):
+    def __init__(self, model_path, initial_qpos, n_actions, n_substeps):
+        if model_path.startswith('/'):
+            fullpath = model_path
+        else:
+            fullpath = os.path.join(os.path.dirname(__file__), 'assets', model_path)
+        if not os.path.exists(fullpath):
+            raise IOError('File {} does not exist'.format(fullpath))
+
+        model = mujoco_py.load_model_from_path(fullpath)
+        self.sim = mujoco_py.MjSim(model, nsubsteps=n_substeps)
+        self.viewer = None
+        self._viewers = {}
+
+        self.metadata = {
+            'render.modes': ['human', 'rgb_array'],
+            'video.frames_per_second': int(np.round(1.0 / self.dt))
+        }
+
+        self.seed()
+        self._env_setup(initial_qpos=initial_qpos)
+        self.initial_state = copy.deepcopy(self.sim.get_state())
+
+        self.goal = self._sample_goal()
+        obs = self._get_obs()
+        self.action_space = spaces.Box(-1., 1., shape=(n_actions,), dtype='float32')
+        self.observation_space = spaces.Dict(dict(
+            desired_goal=spaces.Box(-np.inf, np.inf, shape=obs['achieved_goal'].shape, dtype='float32'),
+            achieved_goal=spaces.Box(-np.inf, np.inf, shape=obs['achieved_goal'].shape, dtype='float32'),
+            observation=spaces.Box(-np.inf, np.inf, shape=obs['observation'].shape, dtype='float32'),
+        ))
+
+    @property
+    def dt(self):
+        return self.sim.model.opt.timestep * self.sim.nsubsteps
+
+    # Env methods
+    # ----------------------------
+
+    def seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def step(self, action):
+        action = np.clip(action, self.action_space.low, self.action_space.high)
+        self._set_action(action)
+        self.sim.step()
+        self._step_callback()
+        obs = self._get_obs()
+
+        done = False
+        info = {
+            'is_success': self._is_success(obs['achieved_goal'], self.goal),
+        }
+        reward = self.compute_reward(obs['achieved_goal'], self.goal, info)
+        return obs, reward, done, info
+
+    def reset(self):
+        # Attempt to reset the simulator. Since we randomize initial conditions, it
+        # is possible to get into a state with numerical issues (e.g. due to penetration or
+        # Gimbel lock) or we may not achieve an initial condition (e.g. an object is within the hand).
+        # In this case, we just keep randomizing until we eventually achieve a valid initial
+        # configuration.
+        super(RobotEnv, self).reset()
+        did_reset_sim = False
+        while not did_reset_sim:
+            did_reset_sim = self._reset_sim()
+        self.goal = self._sample_goal().copy()
+        obs = self._get_obs()
+        return obs
+
+    def close(self):
+        if self.viewer is not None:
+            # self.viewer.finish()
+            self.viewer = None
+            self._viewers = {}
+
+    def render(self, mode='human', width=DEFAULT_SIZE, height=DEFAULT_SIZE):
+        self._render_callback()
+        if mode == 'rgb_array':
+            self._get_viewer(mode).render(width, height)
+            # window size used for old mujoco-py:
+            data = self._get_viewer(mode).read_pixels(width, height, depth=False)
+            # original image is upside-down, so flip it
+            return data[::-1, :, :]
+        elif mode == 'human':
+            self._get_viewer(mode).render()
+
+    def _get_viewer(self, mode):
+        self.viewer = self._viewers.get(mode)
+        if self.viewer is None:
+            if mode == 'human':
+                self.viewer = mujoco_py.MjViewer(self.sim)
+            elif mode == 'rgb_array':
+                self.viewer = mujoco_py.MjRenderContextOffscreen(self.sim, device_id=-1)
+            self._viewer_setup()
+            self._viewers[mode] = self.viewer
+        return self.viewer
+
+    # Extension methods
+    # ----------------------------
+
+    def _reset_sim(self):
+        """Resets a simulation and indicates whether or not it was successful.
+        If a reset was unsuccessful (e.g. if a randomized state caused an error in the
+        simulation), this method should indicate such a failure by returning False.
+        In such a case, this method will be called again to attempt a the reset again.
+        """
+        self.sim.set_state(self.initial_state)
+        self.sim.forward()
+        return True
+
+    def _get_obs(self):
+        """Returns the observation.
+        """
+        raise NotImplementedError()
+
+    def _set_action(self, action):
+        """Applies the given action to the simulation.
+        """
+        raise NotImplementedError()
+
+    def _is_success(self, achieved_goal, desired_goal):
+        """Indicates whether or not the achieved goal successfully achieved the desired goal.
+        """
+        raise NotImplementedError()
+
+    def _sample_goal(self):
+        """Samples a new goal and returns it.
+        """
+        raise NotImplementedError()
+
+    def _env_setup(self, initial_qpos):
+        """Initial configuration of the environment. Can be used to configure initial state
+        and extract information from the simulation.
+        """
+        pass
+
+    def _viewer_setup(self):
+        """Initial configuration of the viewer. Can be used to set the camera position,
+        for example.
+        """
+        pass
+
+    def _render_callback(self):
+        """A custom callback that is called before rendering. Can be used
+        to implement custom visualizations.
+        """
+        pass
+
+    def _step_callback(self):
+        """A custom callback that is called after stepping the simulation. Can be used
+        to enforce additional constraints on the simulation state.
+        """
+        pass
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/rotations.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/rotations.py
new file mode 100644
index 0000000..4aafb64
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/rotations.py
@@ -0,0 +1,369 @@
+# Copyright (c) 2009-2017, Matthew Brett and Christoph Gohlke
+#    All rights reserved.
+#
+#    Redistribution and use in source and binary forms, with or without
+#    modification, are permitted provided that the following conditions are
+#    met:
+#
+#    1. Redistributions of source code must retain the above copyright notice,
+#    this list of conditions and the following disclaimer.
+#
+#    2. Redistributions in binary form must reproduce the above copyright
+#    notice, this list of conditions and the following disclaimer in the
+#    documentation and/or other materials provided with the distribution.
+#
+#    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
+#    IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
+#    THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+#    PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR
+#    CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+#    EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+#    PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#    PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#    LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#    NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#    SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+# Many methods borrow heavily or entirely from transforms3d:
+# https://github.com/matthew-brett/transforms3d
+# They have mostly been modified to support batched operations.
+
+import numpy as np
+import itertools
+
+'''
+Rotations
+=========
+
+Note: these have caused many subtle bugs in the past.
+Be careful while updating these methods and while using them in clever ways.
+
+See MuJoCo documentation here: http://mujoco.org/book/modeling.html#COrientation
+
+Conventions
+-----------
+    - All functions accept batches as well as individual rotations
+    - All rotation conventions match respective MuJoCo defaults
+    - All angles are in radians
+    - Matricies follow LR convention
+    - Euler Angles are all relative with 'xyz' axes ordering
+    - See specific representation for more information
+
+Representations
+---------------
+
+Euler
+    There are many euler angle frames -- here we will strive to use the default
+        in MuJoCo, which is eulerseq='xyz'.
+    This frame is a relative rotating frame, about x, y, and z axes in order.
+        Relative rotating means that after we rotate about x, then we use the
+        new (rotated) y, and the same for z.
+
+Quaternions
+    These are defined in terms of rotation (angle) about a unit vector (x, y, z)
+    We use the following <q0, q1, q2, q3> convention:
+            q0 = cos(angle / 2)
+            q1 = sin(angle / 2) * x
+            q2 = sin(angle / 2) * y
+            q3 = sin(angle / 2) * z
+        This is also sometimes called qw, qx, qy, qz.
+    Note that quaternions are ambiguous, because we can represent a rotation by
+        angle about vector <x, y, z> and -angle about vector <-x, -y, -z>.
+        To choose between these, we pick "first nonzero positive", where we
+        make the first nonzero element of the quaternion positive.
+    This can result in mismatches if you're converting an quaternion that is not
+        "first nonzero positive" to a different representation and back.
+
+Axis Angle
+    (Not currently implemented)
+    These are very straightforward.  Rotation is angle about a unit vector.
+
+XY Axes
+    (Not currently implemented)
+    We are given x axis and y axis, and z axis is cross product of x and y.
+
+Z Axis
+    This is NOT RECOMMENDED.  Defines a unit vector for the Z axis,
+        but rotation about this axis is not well defined.
+    Instead pick a fixed reference direction for another axis (e.g. X)
+        and calculate the other (e.g. Y = Z cross-product X),
+        then use XY Axes rotation instead.
+
+SO3
+    (Not currently implemented)
+    While not supported by MuJoCo, this representation has a lot of nice features.
+    We expect to add support for these in the future.
+
+TODO / Missing
+--------------
+    - Rotation integration or derivatives (e.g. velocity conversions)
+    - More representations (SO3, etc)
+    - Random sampling (e.g. sample uniform random rotation)
+    - Performance benchmarks/measurements
+    - (Maybe) define everything as to/from matricies, for simplicity
+'''
+
+# For testing whether a number is close to zero
+_FLOAT_EPS = np.finfo(np.float64).eps
+_EPS4 = _FLOAT_EPS * 4.0
+
+
+def euler2mat(euler):
+    """ Convert Euler Angles to Rotation Matrix.  See rotation.py for notes """
+    euler = np.asarray(euler, dtype=np.float64)
+    assert euler.shape[-1] == 3, "Invalid shaped euler {}".format(euler)
+
+    ai, aj, ak = -euler[..., 2], -euler[..., 1], -euler[..., 0]
+    si, sj, sk = np.sin(ai), np.sin(aj), np.sin(ak)
+    ci, cj, ck = np.cos(ai), np.cos(aj), np.cos(ak)
+    cc, cs = ci * ck, ci * sk
+    sc, ss = si * ck, si * sk
+
+    mat = np.empty(euler.shape[:-1] + (3, 3), dtype=np.float64)
+    mat[..., 2, 2] = cj * ck
+    mat[..., 2, 1] = sj * sc - cs
+    mat[..., 2, 0] = sj * cc + ss
+    mat[..., 1, 2] = cj * sk
+    mat[..., 1, 1] = sj * ss + cc
+    mat[..., 1, 0] = sj * cs - sc
+    mat[..., 0, 2] = -sj
+    mat[..., 0, 1] = cj * si
+    mat[..., 0, 0] = cj * ci
+    return mat
+
+
+def euler2quat(euler):
+    """ Convert Euler Angles to Quaternions.  See rotation.py for notes """
+    euler = np.asarray(euler, dtype=np.float64)
+    assert euler.shape[-1] == 3, "Invalid shape euler {}".format(euler)
+
+    ai, aj, ak = euler[..., 2] / 2, -euler[..., 1] / 2, euler[..., 0] / 2
+    si, sj, sk = np.sin(ai), np.sin(aj), np.sin(ak)
+    ci, cj, ck = np.cos(ai), np.cos(aj), np.cos(ak)
+    cc, cs = ci * ck, ci * sk
+    sc, ss = si * ck, si * sk
+
+    quat = np.empty(euler.shape[:-1] + (4,), dtype=np.float64)
+    quat[..., 0] = cj * cc + sj * ss
+    quat[..., 3] = cj * sc - sj * cs
+    quat[..., 2] = -(cj * ss + sj * cc)
+    quat[..., 1] = cj * cs - sj * sc
+    return quat
+
+
+def mat2euler(mat):
+    """ Convert Rotation Matrix to Euler Angles.  See rotation.py for notes """
+    mat = np.asarray(mat, dtype=np.float64)
+    assert mat.shape[-2:] == (3, 3), "Invalid shape matrix {}".format(mat)
+
+    cy = np.sqrt(mat[..., 2, 2] * mat[..., 2, 2] + mat[..., 1, 2] * mat[..., 1, 2])
+    condition = cy > _EPS4
+    euler = np.empty(mat.shape[:-1], dtype=np.float64)
+    euler[..., 2] = np.where(condition,
+                             -np.arctan2(mat[..., 0, 1], mat[..., 0, 0]),
+                             -np.arctan2(-mat[..., 1, 0], mat[..., 1, 1]))
+    euler[..., 1] = np.where(condition,
+                             -np.arctan2(-mat[..., 0, 2], cy),
+                             -np.arctan2(-mat[..., 0, 2], cy))
+    euler[..., 0] = np.where(condition,
+                             -np.arctan2(mat[..., 1, 2], mat[..., 2, 2]),
+                             0.0)
+    return euler
+
+
+def mat2quat(mat):
+    """ Convert Rotation Matrix to Quaternion.  See rotation.py for notes """
+    mat = np.asarray(mat, dtype=np.float64)
+    assert mat.shape[-2:] == (3, 3), "Invalid shape matrix {}".format(mat)
+
+    Qxx, Qyx, Qzx = mat[..., 0, 0], mat[..., 0, 1], mat[..., 0, 2]
+    Qxy, Qyy, Qzy = mat[..., 1, 0], mat[..., 1, 1], mat[..., 1, 2]
+    Qxz, Qyz, Qzz = mat[..., 2, 0], mat[..., 2, 1], mat[..., 2, 2]
+    # Fill only lower half of symmetric matrix
+    K = np.zeros(mat.shape[:-2] + (4, 4), dtype=np.float64)
+    K[..., 0, 0] = Qxx - Qyy - Qzz
+    K[..., 1, 0] = Qyx + Qxy
+    K[..., 1, 1] = Qyy - Qxx - Qzz
+    K[..., 2, 0] = Qzx + Qxz
+    K[..., 2, 1] = Qzy + Qyz
+    K[..., 2, 2] = Qzz - Qxx - Qyy
+    K[..., 3, 0] = Qyz - Qzy
+    K[..., 3, 1] = Qzx - Qxz
+    K[..., 3, 2] = Qxy - Qyx
+    K[..., 3, 3] = Qxx + Qyy + Qzz
+    K /= 3.0
+    # TODO: vectorize this -- probably could be made faster
+    q = np.empty(K.shape[:-2] + (4,))
+    it = np.nditer(q[..., 0], flags=['multi_index'])
+    while not it.finished:
+        # Use Hermitian eigenvectors, values for speed
+        vals, vecs = np.linalg.eigh(K[it.multi_index])
+        # Select largest eigenvector, reorder to w,x,y,z quaternion
+        q[it.multi_index] = vecs[[3, 0, 1, 2], np.argmax(vals)]
+        # Prefer quaternion with positive w
+        # (q * -1 corresponds to same rotation as q)
+        if q[it.multi_index][0] < 0:
+            q[it.multi_index] *= -1
+        it.iternext()
+    return q
+
+
+def quat2euler(quat):
+    """ Convert Quaternion to Euler Angles.  See rotation.py for notes """
+    return mat2euler(quat2mat(quat))
+
+
+def subtract_euler(e1, e2):
+    assert e1.shape == e2.shape
+    assert e1.shape[-1] == 3
+    q1 = euler2quat(e1)
+    q2 = euler2quat(e2)
+    q_diff = quat_mul(q1, quat_conjugate(q2))
+    return quat2euler(q_diff)
+
+
+def quat2mat(quat):
+    """ Convert Quaternion to Euler Angles.  See rotation.py for notes """
+    quat = np.asarray(quat, dtype=np.float64)
+    assert quat.shape[-1] == 4, "Invalid shape quat {}".format(quat)
+
+    w, x, y, z = quat[..., 0], quat[..., 1], quat[..., 2], quat[..., 3]
+    Nq = np.sum(quat * quat, axis=-1)
+    s = 2.0 / Nq
+    X, Y, Z = x * s, y * s, z * s
+    wX, wY, wZ = w * X, w * Y, w * Z
+    xX, xY, xZ = x * X, x * Y, x * Z
+    yY, yZ, zZ = y * Y, y * Z, z * Z
+
+    mat = np.empty(quat.shape[:-1] + (3, 3), dtype=np.float64)
+    mat[..., 0, 0] = 1.0 - (yY + zZ)
+    mat[..., 0, 1] = xY - wZ
+    mat[..., 0, 2] = xZ + wY
+    mat[..., 1, 0] = xY + wZ
+    mat[..., 1, 1] = 1.0 - (xX + zZ)
+    mat[..., 1, 2] = yZ - wX
+    mat[..., 2, 0] = xZ - wY
+    mat[..., 2, 1] = yZ + wX
+    mat[..., 2, 2] = 1.0 - (xX + yY)
+    return np.where((Nq > _FLOAT_EPS)[..., np.newaxis, np.newaxis], mat, np.eye(3))
+
+def quat_conjugate(q):
+    inv_q = -q
+    inv_q[..., 0] *= -1
+    return inv_q
+
+def quat_mul(q0, q1):
+    assert q0.shape == q1.shape
+    assert q0.shape[-1] == 4
+    assert q1.shape[-1] == 4
+
+    w0 = q0[..., 0]
+    x0 = q0[..., 1]
+    y0 = q0[..., 2]
+    z0 = q0[..., 3]
+
+    w1 = q1[..., 0]
+    x1 = q1[..., 1]
+    y1 = q1[..., 2]
+    z1 = q1[..., 3]
+
+    w = w0 * w1 - x0 * x1 - y0 * y1 - z0 * z1
+    x = w0 * x1 + x0 * w1 + y0 * z1 - z0 * y1
+    y = w0 * y1 + y0 * w1 + z0 * x1 - x0 * z1
+    z = w0 * z1 + z0 * w1 + x0 * y1 - y0 * x1
+    q = np.array([w, x, y, z])
+    if q.ndim == 2:
+        q = q.swapaxes(0, 1)
+    assert q.shape == q0.shape
+    return q
+
+def quat_rot_vec(q, v0):
+    q_v0 = np.array([0, v0[0], v0[1], v0[2]])
+    q_v = quat_mul(q, quat_mul(q_v0, quat_conjugate(q)))
+    v = q_v[1:]
+    return v
+
+def quat_identity():
+    return np.array([1, 0, 0, 0])
+
+def quat2axisangle(quat):
+    theta = 0;
+    axis = np.array([0, 0, 1]);
+    sin_theta = np.linalg.norm(quat[1:])
+
+    if (sin_theta > 0.0001):
+        theta = 2 * np.arcsin(sin_theta)
+        theta *= 1 if quat[0] >= 0 else -1
+        axis = quat[1:] / sin_theta
+
+    return axis, theta
+
+def euler2point_euler(euler):
+    _euler = euler.copy()
+    if len(_euler.shape) < 2:
+        _euler = np.expand_dims(_euler,0)
+    assert(_euler.shape[1] == 3)
+    _euler_sin = np.sin(_euler)
+    _euler_cos = np.cos(_euler)
+    return np.concatenate([_euler_sin, _euler_cos], axis=-1)
+
+def point_euler2euler(euler):
+    _euler = euler.copy()
+    if len(_euler.shape) < 2:
+        _euler = np.expand_dims(_euler,0)
+    assert(_euler.shape[1] == 6)
+    angle = np.arctan(_euler[..., :3] / _euler[..., 3:])
+    angle[_euler[..., 3:] < 0] += np.pi
+    return angle
+
+def quat2point_quat(quat):
+    # Should be in qw, qx, qy, qz
+    _quat = quat.copy()
+    if len(_quat.shape) < 2:
+        _quat = np.expand_dims(_quat, 0)
+    assert(_quat.shape[1] == 4)
+    angle = np.arccos(_quat[:,[0]]) * 2
+    xyz = _quat[:, 1:]
+    xyz[np.squeeze(np.abs(np.sin(angle/2))) >= 1e-5] = (xyz / np.sin(angle / 2))[np.squeeze(np.abs(np.sin(angle/2))) >= 1e-5]
+    return np.concatenate([np.sin(angle),np.cos(angle), xyz], axis=-1)
+
+def point_quat2quat(quat):
+    _quat = quat.copy()
+    if len(_quat.shape) < 2:
+        _quat = np.expand_dims(_quat, 0)
+    assert(_quat.shape[1] == 5)
+    angle = np.arctan(_quat[:,[0]] / _quat[:,[1]])
+    qw = np.cos(angle / 2)
+
+    qxyz = _quat[:, 2:]
+    qxyz[np.squeeze(np.abs(np.sin(angle/2))) >= 1e-5] = (qxyz * np.sin(angle/2))[np.squeeze(np.abs(np.sin(angle/2))) >= 1e-5]
+    return np.concatenate([qw, qxyz], axis=-1)
+
+def normalize_angles(angles):
+    '''Puts angles in [-pi, pi] range.'''
+    angles = angles.copy()
+    if angles.size > 0:
+        angles = (angles + np.pi) % (2 * np.pi) - np.pi
+        assert -np.pi-1e-6 <= angles.min() and angles.max() <= np.pi+1e-6
+    return angles
+
+def round_to_straight_angles(angles):
+    '''Returns closest angle modulo 90 degrees '''
+    angles = np.round(angles / (np.pi / 2)) * (np.pi / 2)
+    return normalize_angles(angles)
+
+def get_parallel_rotations():
+    mult90 = [0, np.pi/2, -np.pi/2, np.pi]
+    parallel_rotations = []
+    for euler in itertools.product(mult90, repeat=3):
+        canonical = mat2euler(euler2mat(euler))
+        canonical = np.round(canonical / (np.pi / 2))
+        if canonical[0] == -2:
+            canonical[0] = 2
+        if canonical[2] == -2:
+            canonical[2] = 2
+        canonical *= np.pi / 2
+        if all([(canonical != rot).any() for rot in parallel_rotations]):
+            parallel_rotations += [canonical]
+    assert len(parallel_rotations) == 24
+    return parallel_rotations
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/utils.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/utils.py
new file mode 100644
index 0000000..a73e5f6
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/robotics/utils.py
@@ -0,0 +1,96 @@
+import numpy as np
+
+from gym import error
+try:
+    import mujoco_py
+except ImportError as e:
+    raise error.DependencyNotInstalled("{}. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)".format(e))
+
+
+def robot_get_obs(sim):
+    """Returns all joint positions and velocities associated with
+    a robot.
+    """
+    if sim.data.qpos is not None and sim.model.joint_names:
+        names = [n for n in sim.model.joint_names if n.startswith('robot')]
+        return (
+            np.array([sim.data.get_joint_qpos(name) for name in names]),
+            np.array([sim.data.get_joint_qvel(name) for name in names]),
+        )
+    return np.zeros(0), np.zeros(0)
+
+
+def ctrl_set_action(sim, action):
+    """For torque actuators it copies the action into mujoco ctrl field.
+    For position actuators it sets the target relative to the current qpos.
+    """
+    if sim.model.nmocap > 0:
+        _, action = np.split(action, (sim.model.nmocap * 7, ))
+    if sim.data.ctrl is not None:
+        for i in range(action.shape[0]):
+            if sim.model.actuator_biastype[i] == 0:
+                sim.data.ctrl[i] = action[i]
+            else:
+                idx = sim.model.jnt_qposadr[sim.model.actuator_trnid[i, 0]]
+                sim.data.ctrl[i] = sim.data.qpos[idx] + action[i]
+
+
+def mocap_set_action(sim, action):
+    """The action controls the robot using mocaps. Specifically, bodies
+    on the robot (for example the gripper wrist) is controlled with
+    mocap bodies. In this case the action is the desired difference
+    in position and orientation (quaternion), in world coordinates,
+    of the of the target body. The mocap is positioned relative to
+    the target body according to the delta, and the MuJoCo equality
+    constraint optimizer tries to center the welded body on the mocap.
+    """
+    if sim.model.nmocap > 0:
+        action, _ = np.split(action, (sim.model.nmocap * 7, ))
+        action = action.reshape(sim.model.nmocap, 7)
+
+        pos_delta = action[:, :3]
+        quat_delta = action[:, 3:]
+
+        reset_mocap2body_xpos(sim)
+        sim.data.mocap_pos[:] = sim.data.mocap_pos + pos_delta
+        sim.data.mocap_quat[:] = sim.data.mocap_quat + quat_delta
+
+
+def reset_mocap_welds(sim):
+    """Resets the mocap welds that we use for actuation.
+    """
+    if sim.model.nmocap > 0 and sim.model.eq_data is not None:
+        for i in range(sim.model.eq_data.shape[0]):
+            if sim.model.eq_type[i] == mujoco_py.const.EQ_WELD:
+                sim.model.eq_data[i, :] = np.array(
+                    [0., 0., 0., 1., 0., 0., 0.])
+    sim.forward()
+
+
+def reset_mocap2body_xpos(sim):
+    """Resets the position and orientation of the mocap bodies to the same
+    values as the bodies they're welded to.
+    """
+
+    if (sim.model.eq_type is None or
+        sim.model.eq_obj1id is None or
+        sim.model.eq_obj2id is None):
+        return
+    for eq_type, obj1_id, obj2_id in zip(sim.model.eq_type,
+                                         sim.model.eq_obj1id,
+                                         sim.model.eq_obj2id):
+        if eq_type != mujoco_py.const.EQ_WELD:
+            continue
+
+        mocap_id = sim.model.body_mocapid[obj1_id]
+        if mocap_id != -1:
+            # obj1 is the mocap, obj2 is the welded body
+            body_idx = obj2_id
+        else:
+            # obj2 is the mocap, obj1 is the welded body
+            mocap_id = sim.model.body_mocapid[obj2_id]
+            body_idx = obj1_id
+
+        assert (mocap_id != -1)
+        sim.data.mocap_pos[mocap_id][:] = sim.data.body_xpos[body_idx]
+        sim.data.mocap_quat[mocap_id][:] = sim.data.body_xquat[body_idx]
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/virtual_env.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/virtual_env.py
new file mode 100644
index 0000000..2d0cc76
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/envs/virtual_env.py
@@ -0,0 +1,94 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from gym.spaces import Box
+from slbo.dynamics_model import DynamicsModel
+from slbo.random_net import RandomNet
+from slbo.envs import BaseBatchedEnv, BaseModelBasedEnv
+from slbo.utils.pc_utils import compute_cov_pi
+
+
+class VirtualEnv(BaseBatchedEnv):
+    _states: np.ndarray
+
+    def __init__(self, model: DynamicsModel, env: BaseModelBasedEnv, random_net:RandomNet,  n_envs: int, 
+                    feature_size: int, bonus_scale: float, lamb: float, opt_model=False):
+        super().__init__()
+        self.n_envs = n_envs
+        self.observation_space = env.observation_space  # ???
+
+        dim_state = env.observation_space.shape[0]
+        dim_action = env.action_space.shape[0]
+        if opt_model:
+            self.action_space = Box(low=np.r_[env.action_space.low, np.zeros(dim_state) - 1.],
+                                    high=np.r_[env.action_space.high, np.zeros(dim_state) + 1.],
+                                    dtype=np.float32)
+        else:
+            self.action_space = env.action_space
+
+        self._opt_model = opt_model
+        self._model = model
+        self._env = env
+        self._random_net = random_net
+
+        self._states = np.zeros((self.n_envs, dim_state), dtype=np.float32)
+
+        self.feature_size = feature_size
+        self.cov_pis = None
+        self.inv_cov = None
+        self.bonus_scale = bonus_scale
+        self.lamb = lamb
+        self.pre = True
+
+    def _scale_action(self, actions):
+        lo, hi = self.action_space.low, self.action_space.high
+        return lo + (actions + 1.) * 0.5 * (hi - lo)
+
+    def step(self, actions):
+        if self._opt_model:
+            actions = actions[..., :self._env.action_space.shape[0]]
+
+        next_states = self._model.eval('next_states', states=self._states, actions=actions)
+        features = self._random_net.eval('features', states=self._states, actions=actions)
+        #print(features.shape)
+        rewards, dones = self._env.mb_step(self._states, self._scale_action(actions), next_states)
+
+        if not self.pre:
+            bonus = self.compute_bonus(features)
+            rewards = rewards + self.bonus_scale * bonus
+
+        self._states = next_states
+        return self._states.copy(), rewards, dones, [{} for _ in range(self.n_envs)]
+
+    def reset(self):
+        return self.partial_reset(range(self.n_envs))
+
+    def partial_reset(self, indices):
+        initial_states = np.array([self._env.reset() for _ in indices])
+
+        self._states = self._states.copy()
+        self._states[indices] = initial_states
+
+        return initial_states.copy()
+
+    def set_state(self, states):
+        self._states = states.copy()
+
+    def render(self, mode='human'):
+        pass
+
+    def update_cov(self, states, actions):
+        features = self._random_net.eval('features', states=states, actions=actions)
+
+        if self.pre:
+            self.cov_pis = compute_cov_pi(features)
+            self.pre = False
+        else:
+            self.cov_pis = self.cov_pis + compute_cov_pi(features)
+        
+        cur_cov = self.lamb * np.identity(self.feature_size) + self.cov_pis
+        self.inv_cov = np.linalg.inv(cur_cov)
+
+
+    def compute_bonus(self,features):
+        bonus = np.sqrt(np.sum(np.dot(features, self.inv_cov)*features,1))
+        return bonus
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/loss/__init__.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/loss/__init__.py
new file mode 100644
index 0000000..5c7f19c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/loss/__init__.py
@@ -0,0 +1 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/loss/multi_step_loss.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/loss/multi_step_loss.py
new file mode 100644
index 0000000..a2aadb7
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/loss/multi_step_loss.py
@@ -0,0 +1,65 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi import Tensor
+from slbo.utils.normalizer import Normalizers
+
+
+class MultiStepLoss(nn.Module):
+    op_train: Tensor
+    op_grad_norm: Tensor
+    _step: int
+    _criterion: nn.Module
+    _normalizers: Normalizers
+    _model: nn.Module
+
+    def __init__(self, model: nn.Module, normalizers: Normalizers, dim_state: int, dim_action: int,
+                 criterion: nn.Module, step=4):
+        super().__init__()
+        self._step = step
+        self._criterion = criterion
+        self._model = model
+        self._normalizers = normalizers
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, shape=[step, None, dim_state])
+            self.op_actions = tf.placeholder(tf.float32, shape=[step, None, dim_action])
+            self.op_masks = tf.placeholder(tf.float32, shape=[step, None])
+            self.op_next_states_ = tf.placeholder(tf.float32, shape=[step, None, dim_state])
+
+        self.op_loss = self(self.op_states, self.op_actions, self.op_next_states_, self.op_masks)
+
+    def forward(self, states: Tensor, actions: Tensor, next_states_: Tensor, masks: Tensor):
+        """
+            All inputs have shape [num_steps, batch_size, xxx]
+        """
+
+        cur_states = states[0]
+        loss = []
+        for i in range(self._step):
+            next_states = self._model(cur_states, actions[i])
+            diffs = next_states - cur_states - next_states_[i] + states[i]
+            weighted_diffs = diffs / self._normalizers.diff.op_std.maximum(1e-6)
+            loss.append(self._criterion(weighted_diffs, 0, cur_states))
+
+            if i < self._step - 1:
+                cur_states = states[i + 1] + masks[i].expand_dims(-1) * (next_states - states[i + 1])
+
+        return tf.add_n(loss) / self._step
+
+    @nn.make_method(fetch='loss')
+    def get_loss(self, states, next_states_, actions, masks): pass
+
+    def build_backward(self, lr: float, weight_decay: float, max_grad_norm=2.):
+        loss = self.op_loss.reduce_mean(name='Loss')
+
+        optimizer = tf.train.AdamOptimizer(lr)
+        params = self._model.parameters()
+        regularization = weight_decay * tf.add_n([tf.nn.l2_loss(t) for t in params], name='regularization')
+
+        grads_and_vars = optimizer.compute_gradients(loss + regularization, var_list=params)
+        print([var.name for grad, var in grads_and_vars])
+        clip_grads, op_grad_norm = tf.clip_by_global_norm([grad for grad, _ in grads_and_vars], max_grad_norm)
+        clip_grads_and_vars = [(grad, var) for grad, (_, var) in zip(clip_grads, grads_and_vars)]
+        self.op_train = optimizer.apply_gradients(clip_grads_and_vars)
+        self.op_grad_norm = op_grad_norm
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/partial_envs.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/partial_envs.py
new file mode 100644
index 0000000..b9aee20
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/partial_envs.py
@@ -0,0 +1,95 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+
+import gym
+import slbo.envs.mujoco_maze
+
+from gym.wrappers import FlattenObservation
+
+
+from slbo.envs.bm_envs.gym.half_cheetah import HalfCheetahEnv
+from slbo.envs.bm_envs.gym.walker2d import Walker2dEnv
+# from slbo.envs.mujoco.humanoid_env import HumanoidEnv
+from slbo.envs.bm_envs.gym.ant import AntEnv
+from slbo.envs.bm_envs.gym.hopper import HopperEnv
+from slbo.envs.bm_envs.gym.swimmer import SwimmerEnv
+from slbo.envs.bm_envs.gym.reacher import ReacherEnv
+from slbo.envs.bm_envs.gym.pendulum import PendulumEnv
+from slbo.envs.bm_envs.gym.inverted_pendulum import InvertedPendulumEnv
+from slbo.envs.bm_envs.gym.acrobot import AcrobotEnv
+from slbo.envs.bm_envs.gym.cartpole import CartPoleEnv
+from slbo.envs.bm_envs.gym.mountain_car import Continuous_MountainCarEnv
+
+from slbo.envs.bm_envs.gym import gym_fswimmer
+from slbo.envs.bm_envs.gym import gym_fwalker2d
+from slbo.envs.bm_envs.gym import gym_fhopper
+from slbo.envs.bm_envs.gym import gym_fant
+
+from slbo.envs.bm_envs.gym import gym_cheetahA01
+from slbo.envs.bm_envs.gym import gym_cheetahA003
+from slbo.envs.bm_envs.gym import gym_cheetahO01
+from slbo.envs.bm_envs.gym import gym_cheetahO001
+from slbo.envs.bm_envs.gym import gym_pendulumO01
+from slbo.envs.bm_envs.gym import gym_pendulumO001
+from slbo.envs.bm_envs.gym import gym_cartpoleO01
+from slbo.envs.bm_envs.gym import gym_cartpoleO001
+
+from slbo.envs.bm_envs.gym import gym_humanoid
+from slbo.envs.bm_envs.gym import gym_nostopslimhumanoid
+from slbo.envs.bm_envs.gym import gym_slimhumanoid
+from slbo.envs.robotics.fetch.push import FetchPushEnv
+from slbo.envs.robotics.hand.reach import HandReachEnv
+from slbo.envs.robotics.hand.manipulate import HandEggEnv
+
+
+def make_env(id: str):
+    if "Point" in id or "Ant" in id:
+        env = gym.make(id)
+        env.seed(np.random.randint(2**60))
+    else:
+        envs = {
+            'HandEgg': HandEggEnv,
+            'FetchPush': FetchPushEnv,
+            'HandReach': HandReachEnv,
+            'HalfCheetah': HalfCheetahEnv,
+            'Walker2D': Walker2dEnv,
+            'Ant': AntEnv,
+            'Hopper': HopperEnv,
+            'Swimmer': SwimmerEnv,
+            'FixedSwimmer': gym_fswimmer.fixedSwimmerEnv,
+            'FixedWalker': gym_fwalker2d.Walker2dEnv,
+            'FixedHopper': gym_fhopper.HopperEnv,
+            'FixedAnt': gym_fant.AntEnv,
+            'Reacher': ReacherEnv,
+            'Pendulum': PendulumEnv,
+            'InvertedPendulum': InvertedPendulumEnv,
+            'Acrobot': AcrobotEnv,
+            'CartPole': CartPoleEnv,
+            'MountainCar': Continuous_MountainCarEnv,
+
+            'HalfCheetahO01': gym_cheetahO01.HalfCheetahEnv,
+            'HalfCheetahO001': gym_cheetahO001.HalfCheetahEnv,
+            'HalfCheetahA01': gym_cheetahA01.HalfCheetahEnv,
+            'HalfCheetahA003': gym_cheetahA003.HalfCheetahEnv,
+
+            'PendulumO01': gym_pendulumO01.PendulumEnv,
+            'PendulumO001': gym_pendulumO001.PendulumEnv,
+
+            'CartPoleO01': gym_cartpoleO01.CartPoleEnv,
+            'CartPoleO001': gym_cartpoleO001.CartPoleEnv,
+
+            'gym_humanoid': gym_humanoid.HumanoidEnv,
+            'gym_slimhumanoid': gym_slimhumanoid.HumanoidEnv,
+            'gym_nostopslimhumanoid': gym_nostopslimhumanoid.HumanoidEnv,
+        }
+        
+        env = envs[id]()
+        if not hasattr(env, 'reward_range'):
+            env.reward_range = (-np.inf, np.inf)
+        if not hasattr(env, 'metadata'):
+            env.metadata = {}
+        env.seed(np.random.randint(2**60))
+        if 'Fetch' in id or 'Hand' in id: 
+            env = FlattenObservation(env)
+
+    return env
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/policies/__init__.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/policies/__init__.py
new file mode 100644
index 0000000..3ffa40d
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/policies/__init__.py
@@ -0,0 +1,13 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import abc
+from typing import Union
+import lunzi.nn as nn
+
+
+class BasePolicy(abc.ABC):
+    @abc.abstractmethod
+    def get_actions(self, states):
+        pass
+
+
+BaseNNPolicy = Union[BasePolicy, nn.Module]  # should be Intersection, see PEP544
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/policies/gaussian_mlp_policy.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/policies/gaussian_mlp_policy.py
new file mode 100644
index 0000000..c6bcb27
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/policies/gaussian_mlp_policy.py
@@ -0,0 +1,69 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List
+import tensorflow as tf
+import numpy as np
+from lunzi import Tensor
+from lunzi import nn
+from baselines.common.tf_util import normc_initializer
+from slbo.utils.truncated_normal import LimitedEntNormal
+from . import BasePolicy
+from slbo.utils.normalizer import GaussianNormalizer
+
+
+class GaussianMLPPolicy(nn.Module, BasePolicy):
+    op_states: Tensor
+
+    def __init__(self, dim_state: int, dim_action: int, hidden_sizes: List[int], normalizer: GaussianNormalizer,
+                 init_std=1.):
+        super().__init__()
+        self.dim_state = dim_state
+        self.dim_action = dim_action
+        self.hidden_sizes = hidden_sizes
+        self.init_std = init_std
+        self.normalizer = normalizer
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, shape=[None, dim_state], name='states')
+            self.op_actions_ = tf.placeholder(tf.float32, shape=[None, dim_action], name='actions')
+
+            layers = []
+            # note that the placeholder has size 105.
+            all_sizes = [dim_state, *self.hidden_sizes]
+            for i, (in_features, out_features) in enumerate(zip(all_sizes[:-1], all_sizes[1:])):
+                layers.append(nn.Linear(in_features, out_features, weight_initializer=normc_initializer(1)))
+                layers.append(nn.Tanh())
+            layers.append(nn.Linear(all_sizes[-1], dim_action, weight_initializer=normc_initializer(0.01)))
+            self.net = nn.Sequential(*layers)
+
+            self.op_log_std = nn.Parameter(
+                tf.constant(np.log(self.init_std), shape=[self.dim_action], dtype=tf.float32), name='log_std')
+
+        self.distribution = self(self.op_states)
+        self.op_actions = self.distribution.sample()
+        self.op_actions_mean = self.distribution.mean()
+        self.op_actions_std = self.distribution.stddev()
+        self.op_nlls_ = -self.distribution.log_prob(self.op_actions_).reduce_sum(axis=1)
+
+        self.register_callable('[states] => [actions]', self.fast)
+
+    def forward(self, states):
+        states = self.normalizer(states)
+        actions_mean = self.net(states)
+        distribution = LimitedEntNormal(actions_mean, self.op_log_std.exp())
+
+        return distribution
+
+    @nn.make_method(fetch='actions')
+    def get_actions(self, states): pass
+
+    def fast(self, states, use_log_prob=False):
+        states = self.normalizer.fast(states)
+        actions_mean = self.net.fast(states)
+        noise = np.random.randn(*actions_mean.shape)
+        actions = actions_mean + noise * np.exp(self.op_log_std.numpy())
+        if use_log_prob:
+            log_prob = -noise**2 / 2 - np.log(2 * np.pi) / 2 - self.op_log_std.numpy()
+            return actions, log_prob.sum(axis=1)
+        return actions
+
+    def clone(self):
+        return GaussianMLPPolicy(self.dim_state, self.dim_action, self.hidden_sizes, self.normalizer, self.init_std)
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/policies/uniform_policy.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/policies/uniform_policy.py
new file mode 100644
index 0000000..ca9f821
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/policies/uniform_policy.py
@@ -0,0 +1,11 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from . import BasePolicy
+
+
+class UniformPolicy(BasePolicy):
+    def __init__(self, dim_action):
+        self.dim_action = dim_action
+
+    def get_actions(self, states):
+        return np.random.uniform(-1., 1., states.shape[:-1] + (self.dim_action,))
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/q_function/__init__.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/q_function/__init__.py
new file mode 100644
index 0000000..e8dfcba
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/q_function/__init__.py
@@ -0,0 +1,13 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import abc
+from typing import Union
+import lunzi.nn as nn
+
+
+class BaseQFunction(abc.ABC):
+    @abc.abstractmethod
+    def get_q(self, states, values):
+        pass
+
+
+BaseNNQFunction = Union[BaseQFunction, nn.Module]
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/q_function/mlp_q_function.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/q_function/mlp_q_function.py
new file mode 100644
index 0000000..c8eed83
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/q_function/mlp_q_function.py
@@ -0,0 +1,22 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List
+import tensorflow as tf
+from . import BaseQFunction
+import lunzi.nn as nn
+from slbo.utils.multi_layer_perceptron import MultiLayerPerceptron
+
+
+class MLPQFunction(MultiLayerPerceptron, BaseQFunction):
+    def __init__(self, dim_state: int, dim_action: int, hidden_states: List[int]):
+        super().__init__((dim_state + dim_action, *hidden_states, 1), squeeze=True)
+        self._dim_state = dim_state
+        self._dim_action = dim_action
+
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, [None, dim_state])
+            self.op_actions = tf.placeholder(tf.float32, [None, dim_action])
+
+        self.op_Q = self.forward(self.op_states, self.op_actions)
+
+    @nn.make_method(fetch='Q')
+    def get_q(self, states, actions): pass
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/random_net.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/random_net.py
new file mode 100644
index 0000000..c9a94e9
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/random_net.py
@@ -0,0 +1,41 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List
+import tensorflow as tf
+from lunzi import Tensor
+import lunzi.nn as nn
+from slbo.utils.normalizer import Normalizers
+from slbo.utils.multi_layer_perceptron import MultiLayerPerceptron
+
+
+class RandomNet(MultiLayerPerceptron):
+    op_loss: Tensor
+    op_train: Tensor
+    op_grad_norm: Tensor
+
+    def __init__(self, dim_state: int, dim_action: int, normalizers: Normalizers, hidden_sizes: List[int]):
+        initializer = tf.truncated_normal_initializer(mean=0.0, stddev=1e-5)
+
+        self.dim_state = dim_state
+        self.dim_action = dim_action
+        self.hidden_sizes = hidden_sizes
+        self.op_states = tf.placeholder(tf.float32, shape=[None, self.dim_state], name='states')
+        self.op_actions = tf.placeholder(tf.float32, shape=[None, self.dim_action], name='actions')
+        super().__init__([dim_state + dim_action, *hidden_sizes[:-1]],
+                         activation=nn.ReLU,
+                         weight_initializer=initializer, build=False)
+
+        self.normalizers = normalizers
+        self.build()
+
+    def build(self):
+        self.op_features = self.forward(self.op_states, self.op_actions)
+
+    def forward(self, states, actions):
+        assert actions.shape[-1] == self.dim_action
+        inputs = tf.concat([self.normalizers.state(states), actions.clip_by_value(-1., 1.)], axis=1)
+
+        features = super().forward(inputs)
+        return features
+
+    def clone(self):
+        return RandomNet(self.dim_state, self.dim_action, self.normalizers, self.hidden_sizes)
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/OU_noise.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/OU_noise.py
new file mode 100644
index 0000000..eae7158
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/OU_noise.py
@@ -0,0 +1,36 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from slbo.policies import BasePolicy
+
+
+class OUNoise(object):
+    _policy: BasePolicy
+
+    def __init__(self, action_space, mu=0.0, theta=0.15, sigma=0.3, shape=None):
+        self.mu = mu
+        self.theta = theta
+        self.sigma = sigma
+        self.action_space = action_space
+        self._state = None
+        if shape:
+            self.shape = shape
+        else:
+            self.shape = action_space.shape
+
+        self.reset()
+
+    def reset(self):
+        self._state = np.ones(self.shape) * self.mu
+
+    def next(self):
+        delta = self.theta * (self.mu - self._state) + self.sigma * np.random.randn(*self._state.shape)
+        self._state = self._state + delta
+        return self._state
+
+    def get_actions(self, states):
+        return self._policy.get_actions(states) + self.next()
+
+    def make(self, policy: BasePolicy):
+        self._policy = policy
+        return self
+
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/__init__.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/__init__.py
new file mode 100644
index 0000000..5c7f19c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/__init__.py
@@ -0,0 +1 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/average_meter.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/average_meter.py
new file mode 100644
index 0000000..b3e285c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/average_meter.py
@@ -0,0 +1,20 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+class AverageMeter(object):
+    sum: float
+    count: float
+
+    def __init__(self, discount=1.):
+        self.discount = discount
+        self.reset()
+
+    def update(self, value, count=1):
+        self.sum = self.sum * self.discount + value * count
+        self.count = self.count * self.discount + count
+        return self.get()
+
+    def get(self):
+        return self.sum / (self.count + 1.e-8)
+
+    def reset(self):
+        self.sum = 0.
+        self.count = 0.
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/dataset.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/dataset.py
new file mode 100644
index 0000000..c0046d2
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/dataset.py
@@ -0,0 +1,28 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+import lunzi.dataset as dataset
+import gym
+
+
+def gen_dtype(env: gym.Env, fields: str):
+    dtypes = {
+        'state': ('state', 'f8', env.observation_space.shape),
+        'action': ('action', 'f8', env.action_space.shape),
+        'next_state': ('next_state', 'f8', env.observation_space.shape),
+        'reward': ('reward', 'f8'),
+        'done': ('done', 'bool'),
+        'timeout': ('timeout', 'bool'),
+        'return_': ('return_', 'f8'),
+        'advantage': ('advantage', 'f8'),
+    }
+    return [dtypes[field] for field in fields.split(' ')]
+
+
+class Dataset(dataset.Dataset):
+    def sample_multi_step(self, size: int, n_env: int, n_step=1):
+        starts = np.random.randint(0, self._len, size=size)
+        batch = []
+        for step in range(n_step):
+            batch.append(self[(starts + step * n_env) % self._len])
+        return np.concatenate(batch).reshape(n_step, size).view(np.recarray)
+
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/flags.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/flags.py
new file mode 100644
index 0000000..8c7bb40
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/flags.py
@@ -0,0 +1,163 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import time
+import os
+import yaml
+from subprocess import check_output, CalledProcessError
+from lunzi.config import BaseFLAGS, expand, parse
+from lunzi.Logger import logger, FileSink
+
+
+class FLAGS(BaseFLAGS):
+    _initialized = False
+
+    use_prev = True
+    seed = 100
+    log_dir: str = None
+    run_id: str = None
+    algorithm = 'OLBO'  # possible options: OLBO, baseline, MF
+
+    class pc(BaseFLAGS):
+        bonus_scale = 1
+        lamb = 0.01
+        bonus_stop_time= 30
+
+    class slbo(BaseFLAGS):
+        n_iters = 50
+        n_policy_iters = 10
+        n_model_iters = 100
+        n_stages = 100
+        n_evaluate_iters = 5
+        opt_model = False
+        start = 'reset'  # possibly 'buffer'
+
+    class plan(BaseFLAGS):
+        max_steps = 1000
+        n_envs = None
+        n_trpo_samples = 4000
+
+        @classmethod
+        def finalize(cls):
+            if cls.n_envs is None:
+                cls.n_envs = cls.n_trpo_samples // cls.max_steps
+            assert cls.n_envs * cls.max_steps == cls.n_trpo_samples
+
+    class env(BaseFLAGS):
+        id = 'HalfCheetah-v2'
+
+    class rollout(BaseFLAGS):
+        normalizer = 'policy'
+        max_buf_size = 100000
+        n_train_samples = 4000
+        n_dev_samples = 0
+        n_test_samples = 10000
+
+        @classmethod
+        def finalize(cls):
+            cls.n_dev_samples = cls.n_dev_samples or cls.n_train_samples
+
+    class ckpt(BaseFLAGS):
+        n_save_stages = 10
+        model_load = None
+        policy_load = None
+        buf_load = None
+        buf_load_index = 0
+        base = '/tmp/mbrl/logs'
+        warm_up = None
+
+        @classmethod
+        def finalize(cls):
+            for key, value in cls.as_dict().items():
+                if isinstance(value, str):
+                    setattr(cls, key, expand(value))
+
+    class OUNoise(BaseFLAGS):
+        theta = 0.15
+        sigma = 0.3
+
+    class model(BaseFLAGS):
+        hidden_sizes = [500, 500]
+        loss = 'L2'  # possibly L1, L2, MSE, G
+        G_coef = 0.5
+        multi_step = 1
+        lr = 1e-3
+        weight_decay = 1e-5
+        validation_freq = 1
+        optimizer = 'Adam'
+        train_batch_size = 256
+        dev_batch_size = 1024
+
+    class policy(BaseFLAGS):
+        hidden_sizes = [32, 32]
+        init_std = 1.
+
+    class PPO(BaseFLAGS):
+        n_minibatches = 32
+        n_opt_epochs = 10
+        ent_coef = 0.005
+        lr = 3e-4
+        clip_range = 0.2
+
+    class TRPO(BaseFLAGS):
+        cg_damping = 0.1
+        n_cg_iters = 10
+        max_kl = 0.01
+        vf_lr = 1e-3
+        n_vf_iters = 5
+        ent_coef = 0.0
+
+    class runner(BaseFLAGS):
+        lambda_ = 0.95
+        gamma = 0.99
+        max_steps = 500
+
+    @classmethod
+    def set_seed(cls):
+        if cls.seed == 0:  # auto seed
+            cls.seed = int.from_bytes(os.urandom(3), 'little') + 1  # never use seed 0 for RNG, 0 is for `urandom`
+        logger.warning("Setting random seed to %s", cls.seed)
+
+        import numpy as np
+        import tensorflow as tf
+        import random
+        np.random.seed(cls.seed)
+        tf.set_random_seed(np.random.randint(2**30))
+        random.seed(np.random.randint(2**30))
+
+    @classmethod
+    def finalize(cls):
+        log_dir = cls.log_dir
+        if log_dir is None:
+            run_id = cls.run_id
+            if run_id is None:
+                run_id = time.strftime('%Y-%m-%d_%H-%M-%S')
+
+            log_dir = os.path.join(cls.ckpt.base, run_id)
+            cls.log_dir = log_dir
+
+        if not os.path.exists(log_dir):
+            os.makedirs(log_dir)
+
+        for t in range(60):
+            try:
+                cls.commit = check_output(['git', 'rev-parse', 'HEAD']).decode('utf-8').strip()
+                check_output(['git', 'add', '.'])
+                check_output(['git', 'checkout-index', '-a', '-f', f'--prefix={log_dir}/src/'])
+                break
+            except CalledProcessError:
+                pass
+            time.sleep(1)
+        else:
+            raise RuntimeError('Failed after 60 trials.')
+
+        yaml.dump(cls.as_dict(), open(os.path.join(log_dir, 'config.yml'), 'w'), default_flow_style=False)
+        open(os.path.join(log_dir, 'diff.patch'), 'w').write(
+            check_output(['git', '--no-pager', 'diff', 'HEAD']).decode('utf-8'))
+
+        logger.add_sink(FileSink(os.path.join(log_dir, 'log.json')))
+        logger.info("log_dir = %s", log_dir)
+
+        cls.set_frozen()
+
+
+parse(FLAGS)
+
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/multi_layer_perceptron.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/multi_layer_perceptron.py
new file mode 100644
index 0000000..0fe5dfe
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/multi_layer_perceptron.py
@@ -0,0 +1,51 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+
+
+class MultiLayerPerceptron(nn.Module):
+    def __init__(self, blocks, activation=nn.ReLU, squeeze=False, weight_initializer=None, build=True):
+        super().__init__()
+
+        self._blocks = blocks
+        if build:
+            self.op_inputs = tf.placeholder(tf.float32, [None, self._blocks[0]])
+
+        with self.scope:
+            kwargs = {}
+            if weight_initializer is not None:
+                kwargs['weight_initializer'] = weight_initializer
+            layers = []
+            for in_features, out_features in zip(blocks[:-1], blocks[1:]):
+                if layers:
+                    layers.append(activation())
+                layers.append(nn.Linear(in_features, out_features, **kwargs))
+            if squeeze:
+                layers.append(nn.Squeeze(axis=1))
+            self.net = nn.Sequential(*layers)
+
+        self._squeeze = squeeze
+        self._activation = activation
+
+        if build:
+            self.build()
+
+    def build(self):
+        self.op_outputs = self.forward(self.op_inputs)
+
+    def forward(self, *inputs):
+        if len(inputs) > 1:
+            inputs = tf.concat(inputs, axis=-1)
+        else:
+            inputs = inputs[0]
+        return self.net(inputs)
+
+    def fast(self, *inputs):
+        return self.net.fast(np.concatenate(inputs, axis=-1))
+
+    def clone(self):
+        return MultiLayerPerceptron(self._blocks, self._activation, self._squeeze)
+
+    def extra_repr(self):
+        return f'activation = {self._activation}, blocks = {self._blocks}, squeeze = {self._squeeze}'
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/normalizer.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/normalizer.py
new file mode 100644
index 0000000..3bb3685
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/normalizer.py
@@ -0,0 +1,66 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List
+import numpy as np
+import tensorflow as tf
+import lunzi.nn as nn
+from lunzi.Logger import logger
+from lunzi import Tensor
+from slbo.utils.np_utils import gaussian_kl
+
+
+class GaussianNormalizer(nn.Module):
+    def __init__(self, name: str, shape: List[int], eps=1e-8, verbose=False):  # batch_size x ...
+        super().__init__()
+
+        self.name = name
+        self.shape = shape
+        self.eps = eps
+        self._verbose = verbose
+
+        with self.scope:
+            self.op_mean = nn.Parameter(tf.zeros(shape, dtype=tf.float32), name='mean', trainable=False)
+            self.op_std = nn.Parameter(tf.ones(shape, dtype=tf.float32), name='std', trainable=False)
+            self.op_n = nn.Parameter(tf.zeros([], dtype=tf.int64), name='n', trainable=False)
+
+    def extra_repr(self):
+        return f'shape={self.shape}'
+
+    def forward(self, x: Tensor, inverse=False):
+        if inverse:
+            return x * self.op_std + self.op_mean
+        return (x - self.op_mean).div(self.op_std.maximum(self.eps))
+
+    def update(self, samples: np.ndarray):
+        old_mean, old_std, old_n = self.op_mean.numpy(), self.op_std.numpy(), self.op_n.numpy()
+        samples = samples - old_mean
+
+        m = samples.shape[0]
+        delta = samples.mean(axis=0)
+        new_n = old_n + m
+        new_mean = old_mean + delta * m / new_n
+        new_std = np.sqrt((old_std**2 * old_n + samples.var(axis=0) * m + delta**2 * old_n * m / new_n) / new_n)
+
+        kl_old_new = gaussian_kl(new_mean, new_std, old_mean, old_std).sum()
+        self.load_state_dict({'op_mean': new_mean, 'op_std': new_std, 'op_n': new_n})
+
+        if self._verbose:
+            logger.info("updating Normalizer<%s>, KL divergence = %.6f", self.name, kl_old_new)
+
+    def fast(self, samples: np.ndarray, inverse=False) -> np.ndarray:
+        mean, std = self.op_mean.numpy(), self.op_std.numpy()
+        if inverse:
+            return samples * std + mean
+        return (samples - mean) / np.maximum(std, self.eps)
+
+
+class Normalizers(nn.Module):
+    def __init__(self, dim_action: int, dim_state: int):
+        super().__init__()
+        self.action = GaussianNormalizer('action', [dim_action])
+        self.state = GaussianNormalizer('state', [dim_state])
+        self.diff = GaussianNormalizer('diff', [dim_state])
+
+    def forward(self):
+        pass
+
+
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/np_utils.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/np_utils.py
new file mode 100644
index 0000000..d170ed4
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/np_utils.py
@@ -0,0 +1,9 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+
+
+def gaussian_kl(mean_1: np.ndarray, std_1: np.ndarray, mean_2: np.ndarray, std_2: np.ndarray) -> np.ndarray:
+    eps = 1e-20
+    std_1 = np.maximum(std_1, eps)
+    std_2 = np.maximum(std_2, eps)
+    return np.log(std_2 / std_1) + (std_1**2 + (mean_1 - mean_2)**2) / std_2**2 / 2. - 0.5
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/pc_utils.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/pc_utils.py
new file mode 100644
index 0000000..d488ef2
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/pc_utils.py
@@ -0,0 +1,58 @@
+import numpy as np
+import copy
+import os
+import scipy.spatial
+import scipy.signal
+
+
+def median_trick(X, args):
+    #median trick for computing the bandwith for kernel regression.
+    N = X.shape[0]
+    #print(X.shape)
+    perm = np.random.choice(N, np.min([N,args.update_size * args.buffer_width]), replace=False)
+    dsample = X[perm]
+    pd = scipy.spatial.distance.pdist(dsample)
+    sigma = np.median(pd)
+    return sigma
+
+def compute_cov_pi(phi):
+    #cov = np.zeros((phi.shape[1],phi.shape[1]))
+
+    #for i in range(len(phi)):
+    #    cov += np.outer(phi[i],phi[i])
+    cov = np.dot(phi.T,phi)
+    cov /= phi.shape[0]
+
+    #print(cov)
+
+    return cov
+
+def discount_cumsum(x, discount):
+    """
+    magic from rllab for computing discounted cumulative sums of vectors.
+    input:
+        vector x,
+        [x0,
+         x1,
+         x2]
+    output:
+        [x0 + discount * x1 + discount^2 * x2,
+         x1 + discount * x2,
+         x2]
+    """
+    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]
+
+
+
+
+def soft_update_from_to(source, target, tau):
+    for target_param, param in zip(target.parameters(), source.parameters()):
+        target_param.data.copy_(
+            target_param.data * (1.0 - tau) + param.data * tau
+        )
+
+def copy_model_params_from_to(source, target):
+    for target_param, param in zip(target.parameters(), source.parameters()):
+        target_param.data.copy_(param.data)
+
+
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/runner.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/runner.py
new file mode 100644
index 0000000..774665d
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/runner.py
@@ -0,0 +1,98 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from lunzi.dataset import Dataset
+from slbo.envs import BaseBatchedEnv
+from slbo.policies import BasePolicy
+from slbo.utils.dataset import Dataset, gen_dtype
+from slbo.v_function import BaseVFunction
+
+
+class Runner(object):
+    _states: np.ndarray  # [np.float]
+    _n_steps: np.ndarray
+    _returns: np.ndarray
+
+    def __init__(self, env: BaseBatchedEnv, max_steps: int, gamma=0.99, lambda_=0.95, rescale_action=False):
+        self.env = env
+        self.n_envs = env.n_envs
+        self.gamma = gamma
+        self.lambda_ = lambda_
+        self.max_steps = max_steps
+        self.rescale_action = rescale_action
+        self._dtype = gen_dtype(env, 'state action next_state reward done timeout')
+
+        self.reset()
+
+    def reset(self):
+        self.set_state(self.env.reset(), set_env_state=False)
+
+    def set_state(self, states: np.ndarray, set_env_state=True):
+        self._states = states.copy()
+        if set_env_state:
+            self.env.set_state(states)
+        self._n_steps = np.zeros(self.n_envs, 'i4')
+        self._returns = np.zeros(self.n_envs, 'f8')
+
+    def get_state(self):
+        return self._states.copy()
+
+    def run(self, policy: BasePolicy, n_samples: int, render=False):
+        ep_infos = []
+        n_steps = n_samples // self.n_envs
+        assert n_steps * self.n_envs == n_samples
+        dataset = Dataset(self._dtype, n_samples)
+
+        for T in range(n_steps):
+            unscaled_actions = policy.get_actions(self._states)
+            if self.rescale_action:
+                lo, hi = self.env.action_space.low, self.env.action_space.high
+                actions = (lo + (unscaled_actions + 1.) * 0.5 * (hi - lo))
+            else:
+                actions = unscaled_actions
+
+            next_states, rewards, dones, infos = self.env.step(actions)
+            if render:
+                #print(actions)
+                self.env.render()
+            dones = dones.astype(bool)
+            self._returns += rewards
+            self._n_steps += 1
+            timeouts = self._n_steps == self.max_steps
+
+            steps = [self._states.copy(), unscaled_actions, next_states.copy(), rewards, dones, timeouts]
+            dataset.extend(np.rec.fromarrays(steps, dtype=self._dtype))
+
+            indices = np.where(dones | timeouts)[0]
+            if len(indices) > 0:
+                next_states = next_states.copy()
+                next_states[indices] = self.env.partial_reset(indices)
+                for index in indices:
+                    infos[index]['episode'] = {'return': self._returns[index], 'success': (self._returns[index] > 50)}
+                self._n_steps[indices] = 0
+                self._returns[indices] = 0.
+
+            self._states = next_states.copy()
+            ep_infos.extend([info['episode'] for info in infos if 'episode' in info])
+
+        if render:
+            self.env.close()
+
+        return dataset, ep_infos
+
+    def compute_advantage(self, vfn: BaseVFunction, samples: Dataset):
+        n_steps = len(samples) // self.n_envs
+        samples = samples.reshape((n_steps, self.n_envs))
+        use_next_vf = ~samples.done
+        use_next_adv = ~(samples.done | samples.timeout)
+
+        next_values = vfn.get_values(samples[-1].next_state)
+        values = vfn.get_values(samples.reshape(-1).state).reshape(n_steps, self.n_envs)
+        advantages = np.zeros((n_steps, self.n_envs), dtype=np.float32)
+        last_gae_lambda = 0
+
+        for t in reversed(range(n_steps)):
+            delta = samples[t].reward + self.gamma * next_values * use_next_vf[t] - values[t]
+            advantages[t] = last_gae_lambda = delta + self.gamma * self.lambda_ * last_gae_lambda * use_next_adv[t]
+            next_values = values[t]
+        return advantages.reshape(-1), values.reshape(-1)
+
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/tf_utils.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/tf_utils.py
new file mode 100644
index 0000000..154c60a
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/tf_utils.py
@@ -0,0 +1,20 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+
+
+def get_tf_config():
+    gpu_frac = 1
+
+    gpu_options = tf.GPUOptions(
+        per_process_gpu_memory_fraction=gpu_frac,
+        allow_growth=True,
+    )
+    config = tf.ConfigProto(
+        gpu_options=gpu_options,
+        log_device_placement=False,
+        allow_soft_placement=True,
+        inter_op_parallelism_threads=1,
+        intra_op_parallelism_threads=1,
+    )
+
+    return config
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/truncated_normal.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/truncated_normal.py
new file mode 100644
index 0000000..04a0edc
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/utils/truncated_normal.py
@@ -0,0 +1,12 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+import numpy as np
+
+
+class LimitedEntNormal(tf.distributions.Normal):
+    def _entropy(self):
+        limit = 2.
+        lo, hi = (-limit - self._loc) / self._scale / np.sqrt(2), (limit - self._loc) / self._scale / np.sqrt(2)
+        return 0.5 * (self._scale.log() + np.log(2 * np.pi) / 2) * (hi.erf() - lo.erf()) + 0.5 * \
+            (tf.exp(-hi * hi) * hi - tf.exp(-lo * lo) * lo)
+
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/v_function/__init__.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/v_function/__init__.py
new file mode 100644
index 0000000..7794a84
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/v_function/__init__.py
@@ -0,0 +1,13 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import abc
+from typing import Union
+import lunzi.nn as nn
+
+
+class BaseVFunction(abc.ABC):
+    @abc.abstractmethod
+    def get_values(self, states):
+        pass
+
+
+BaseNNVFunction = Union[BaseVFunction, nn.Module]  # in fact it should be Intersection
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/v_function/mlp_v_function.py b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/v_function/mlp_v_function.py
new file mode 100644
index 0000000..d9749e9
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/src/slbo/v_function/mlp_v_function.py
@@ -0,0 +1,24 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+from baselines.common.tf_util import normc_initializer
+from slbo.utils.multi_layer_perceptron import MultiLayerPerceptron
+import lunzi.nn as nn
+from . import BaseVFunction
+
+
+class MLPVFunction(BaseVFunction, nn.Module):
+    def __init__(self, dim_state, hidden_sizes, normalizer=None):
+        super().__init__()
+        self.mlp = MultiLayerPerceptron((dim_state, *hidden_sizes, 1), activation=nn.Tanh, squeeze=True,
+                                        weight_initializer=normc_initializer(1.), build=False)
+        self.normalizer = normalizer
+        self.op_states = tf.placeholder(tf.float32, shape=[None, dim_state])
+        self.op_values = self.forward(self.op_states)
+
+    def forward(self, states):
+        states = self.normalizer(states)
+        return self.mlp(states)
+
+    @nn.make_method(fetch='values')
+    def get_values(self, states): pass
+
diff --git a/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/stage-0.npy b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/stage-0.npy
new file mode 100644
index 0000000..3081364
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/experiments05/ant_umaze_1234/stage-0.npy differ
diff --git a/experiments02/ant_umaze_1234/src/lunzi/Logger.py b/experiments02/ant_umaze_1234/src/lunzi/Logger.py
new file mode 100644
index 0000000..9f41014
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/lunzi/Logger.py
@@ -0,0 +1,133 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from termcolor import colored
+import datetime
+import sys
+import os
+from collections import Counter, defaultdict
+import json_tricks
+
+
+def a():
+    pass
+
+
+_srcfile = os.path.normcase(a.__code__.co_filename)
+
+
+class BaseSink(object):
+    @staticmethod
+    def _time():
+        return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S,%f')
+
+    def info(self, fmt, *args, **kwargs):
+        raise NotImplementedError
+
+    def warning(self, fmt, *args, **kwargs):
+        self.info(fmt, *args, **kwargs)
+
+    def verbose(self, fmt, *args, **kwargs):
+        pass
+
+
+class StdoutSink(BaseSink):
+    def __init__(self):
+        self.freq_count = Counter()
+
+    def info(self, fmt, *args, freq=1, caller=None):
+        if args:
+            fmt = fmt % args
+        self.freq_count[caller] += 1
+        if self.freq_count[caller] % freq == 0:
+            print("%s - %s - %s" % (colored(self._time(), 'green'),
+                                    colored(caller, 'cyan'), fmt), flush=True)
+
+    def warning(self, fmt, *args, **kwargs):
+        if args:
+            fmt = fmt % args
+        self.info(colored(fmt, 'yellow'), **kwargs)
+
+
+class FileSink(BaseSink):
+    def __init__(self, fn):
+        self.log_file = open(fn, 'w')
+        self.callers = {}
+
+    def info(self, fmt, *args, **kwargs):
+        self._kv(level='info', fmt=fmt, args=args, **kwargs)
+
+    def warning(self, fmt, *args, **kwargs):
+        self._kv(level='warning', fmt=fmt, args=args, **kwargs)
+
+    def _kv(self, **kwargs):
+        kwargs.update(time=datetime.datetime.now())
+        self.log_file.write(json_tricks.dumps(kwargs, primitives=True) + '\n')
+        self.log_file.flush()
+
+    def verbose(self, fmt, *args, **kwargs):
+        self._kv(level='verbose', fmt=fmt, args=args, **kwargs)
+
+
+class LibLogger(object):
+    logfile = ""
+
+    def __init__(self, name='logger', is_root=True):
+        self.name = name
+        self.is_root = is_root
+        self.tab_keys = None
+        self.sinks = []
+        self.key_prior = defaultdict(np.random.randn)
+
+    def add_sink(self, sink):
+        self.sinks.append(sink)
+
+    def info(self, fmt, *args, **kwargs):
+        caller = self.find_caller()
+        for sink in self.sinks:
+            sink.info(fmt, *args, caller=caller, **kwargs)
+
+    def warning(self, fmt, *args, **kwargs):
+        caller = self.find_caller()
+        for sink in self.sinks:
+            sink.warning(fmt, *args, caller=caller, **kwargs)
+
+    def verbose(self, fmt, *args, **kwargs):
+        caller = self.find_caller()
+        for sink in self.sinks:
+            sink.verbose(fmt, *args, caller=caller, **kwargs)
+
+    def find_caller(self):
+        """
+        Copy from `python.logging` module
+
+        Find the stack frame of the caller so that we can note the source
+        file name, line number and function name.
+        """
+        f = sys._getframe(1)
+        if f is not None:
+            f = f.f_back
+        caller = ''
+        while hasattr(f, "f_code"):
+            co = f.f_code
+            filename = os.path.normcase(co.co_filename)
+            if filename == _srcfile:
+                f = f.f_back
+                continue
+            # if stack_info:
+            #     sio = io.StringIO()
+            #     sio.write('Stack (most recent call last):\n')
+            #     traceback.print_stack(f, file=sio)
+            #     sio.close()
+            # rv = (co.co_filename, f.f_lineno, co.co_name, sinfo)
+            rel_path = os.path.relpath(co.co_filename, '')
+            caller = f'{rel_path}:{f.f_lineno}'
+            break
+        return caller
+
+
+def get_logger(name):
+    return LibLogger(name)
+
+
+logger = get_logger('Logger')
+logger.add_sink(StdoutSink())
diff --git a/experiments02/ant_umaze_1234/src/lunzi/__init__.py b/experiments02/ant_umaze_1234/src/lunzi/__init__.py
new file mode 100644
index 0000000..0c5417b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/lunzi/__init__.py
@@ -0,0 +1,4 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from .stubs import Tensor
+import lunzi.nn
+import lunzi.Logger
diff --git a/experiments02/ant_umaze_1234/src/lunzi/config.py b/experiments02/ant_umaze_1234/src/lunzi/config.py
new file mode 100644
index 0000000..5c39b1e
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/lunzi/config.py
@@ -0,0 +1,97 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import argparse
+import os
+import yaml
+from lunzi.Logger import logger
+
+
+_frozen = False
+_initialized = False
+
+
+def expand(path):
+    return os.path.abspath(os.path.expanduser(path))
+
+
+class MetaFLAGS(type):
+    _initialized = False
+
+    def __setattr__(self, key, value):
+        assert not _frozen, 'Modifying FLAGS after dumping is not allowed!'
+        super().__setattr__(key, value)
+
+    def __getitem__(self, item):
+        return self.__dict__[item]
+
+    def __iter__(self):
+        for key, value in self.__dict__.items():
+            if not key.startswith('_') and not isinstance(value, classmethod):
+                if isinstance(value, MetaFLAGS):
+                    value = dict(value)
+                yield key, value
+
+    def as_dict(self):
+        return dict(self)
+
+    def merge(self, other: dict):
+        for key in other:
+            assert key in self.__dict__, f"Can't find key `{key}`"
+            if isinstance(self[key], MetaFLAGS) and isinstance(other[key], dict):
+                self[key].merge(other[key])
+            else:
+                setattr(self, key, other[key])
+
+    def set_value(self, path, value):
+        key, *rest = path
+        assert key in self.__dict__, f"Can't find key `{key}`"
+        if not rest:
+            setattr(self, key, value)
+        else:
+            self[key]: MetaFLAGS
+            self[key].set_value(rest, value)
+
+    @staticmethod
+    def set_frozen():
+        global _frozen
+        _frozen = True
+
+    def freeze(self):
+        for key, value in self.__dict__.items():
+            if not key.startswith('_'):
+                if isinstance(value, MetaFLAGS):
+                    value.freeze()
+        self.finalize()
+
+    def finalize(self):
+        pass
+
+
+class BaseFLAGS(metaclass=MetaFLAGS):
+    pass
+
+
+def parse(cls):
+    global _initialized
+
+    if _initialized:
+        return
+    parser = argparse.ArgumentParser(description='Stochastic Lower Bound Optimization')
+    parser.add_argument('-c', '--config', type=str, help='configuration file (YAML)', nargs='+', action='append')
+    parser.add_argument('-s', '--set', type=str, help='additional options', nargs='*', action='append')
+
+    args, unknown = parser.parse_known_args()
+    for a in unknown:
+        logger.info('unknown arguments: %s', a)
+    # logger.info('parsed arguments = %s, unknown arguments: %s', args, unknown)
+    if args.config:
+        for config in sum(args.config, []):
+            cls.merge(yaml.load(open(expand(config))))
+    else:
+        logger.info('no config file specified.')
+    if args.set:
+        for instruction in sum(args.set, []):
+            path, *value = instruction.split('=')
+            cls.set_value(path.split('.'), yaml.load('='.join(value)))
+
+    _initialized = True
+
diff --git a/experiments02/ant_umaze_1234/src/lunzi/dataset.py b/experiments02/ant_umaze_1234/src/lunzi/dataset.py
new file mode 100644
index 0000000..7cb5d5f
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/lunzi/dataset.py
@@ -0,0 +1,82 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+
+
+class Dataset(np.recarray):
+    """
+        Overallocation can be supported, by making examinations before
+        each `append` and `extend`.
+    """
+
+    @staticmethod
+    def fromarrays(array_lists, dtype):
+        array = np.rec.fromarrays(array_lists, dtype=dtype)
+        ret = Dataset(dtype, len(array))
+        ret.extend(array)
+        return ret
+
+    def __init__(self, dtype, max_size, verbose=False):
+        super().__init__()
+        self.max_size = max_size
+        self._index = 0
+        self._buf_size = 0
+        self._len = 0
+
+        self.resize(max_size)
+        self._buf_size = max_size
+
+    def __new__(cls, dtype, max_size):
+        return np.recarray.__new__(cls, max_size, dtype=dtype)
+
+    def size(self):
+        return self._len
+
+    def reserve(self, size):
+        cur_size = max(self._buf_size, 1)
+        while cur_size < size:
+            cur_size *= 2
+        if cur_size != self._buf_size:
+            self.resize(cur_size)
+
+    def clear(self):
+        self._index = 0
+        self._len = 0
+        return self
+
+    def append(self, item):
+        self[self._index] = item
+        self._index = (self._index + 1) % self.max_size
+        self._len = min(self._len + 1, self.max_size)
+        return self
+
+    def extend(self, items):
+        n_new = len(items)
+        if n_new > self.max_size:
+            items = items[-self.max_size:]
+            n_new = self.max_size
+
+        n_tail = self.max_size - self._index
+        if n_new <= n_tail:
+            self[self._index:self._index + n_new] = items
+        else:
+            n_head = n_new - n_tail
+            self[self._index:] = items[:n_tail]
+            self[:n_head] = items[n_tail:]
+
+        self._index = (self._index + n_new) % self.max_size
+        self._len = min(self._len + n_new, self.max_size)
+        return self
+
+    def sample(self, size, indices=None):
+        if indices is None:
+            indices = np.random.randint(0, self._len, size=size)
+        return self[indices]
+
+    def iterator(self, batch_size):
+        indices = np.arange(self._len, dtype=np.int32)
+        np.random.shuffle(indices)
+        index = 0
+        while index + batch_size <= self._len:
+            end = index + batch_size
+            yield self[indices[index:end]]
+            index = end
diff --git a/experiments02/ant_umaze_1234/src/lunzi/nn/__init__.py b/experiments02/ant_umaze_1234/src/lunzi/nn/__init__.py
new file mode 100644
index 0000000..2315809
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/lunzi/nn/__init__.py
@@ -0,0 +1,10 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from . import patch
+from .parameter import Parameter
+from .module import Module
+from .container import *
+from . import utils
+from .utils import make_method
+from .layers import *
+from .loss import *
+from .flat_param import FlatParam
diff --git a/experiments02/ant_umaze_1234/src/lunzi/nn/container.py b/experiments02/ant_umaze_1234/src/lunzi/nn/container.py
new file mode 100644
index 0000000..6dbafdc
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/lunzi/nn/container.py
@@ -0,0 +1,35 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import Dict, Any
+from .module import Module
+from .parameter import Parameter
+
+_dict_methods = ['__setitem__', '__getitem__', '__delitem__', '__len__', '__iter__', '__contains__',
+                 'update', 'keys', 'values', 'items', 'clear', 'pop']
+
+
+class ModuleDict(Module, dict):  # use dict for auto-complete
+    """
+        Essentially this exposes some methods of `Module._modules`.
+    """
+    def __init__(self, modules: Dict[Any, Module] = None):
+        super().__init__()
+        for method in _dict_methods:
+            setattr(self, method, getattr(self._modules, method))
+        if modules is not None:
+            self.update(modules)
+
+    def forward(self):
+        raise RuntimeError("ModuleDict is not callable")
+
+
+# Do we need a factory for it?
+class ParameterDict(Module, dict):
+    def __init__(self, parameters: Dict[Any, Parameter] = None):
+        super().__init__()
+        for method in _dict_methods:
+            setattr(self, method, getattr(self._modules, method))
+        if parameters is not None:
+            self.update(parameters)
+
+    def forward(self):
+        raise RuntimeError("ParameterDict is not callable")
diff --git a/experiments02/ant_umaze_1234/src/lunzi/nn/flat_param.py b/experiments02/ant_umaze_1234/src/lunzi/nn/flat_param.py
new file mode 100644
index 0000000..5c4bb52
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/lunzi/nn/flat_param.py
@@ -0,0 +1,34 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+from lunzi.Logger import logger
+from .module import Module
+from .utils import make_method, n_parameters, parameters_to_vector, vector_to_parameters
+
+
+class FlatParam(Module):
+    def __init__(self, parameters):
+        super().__init__()
+        self.params = parameters
+        self.op_feed_flat, self.op_set_flat, self.op_get_flat = \
+            self.enable_flat()
+
+    def enable_flat(self):
+        params = self.params
+        logger.info('Enabling flattening... %s', [p.name for p in params])
+        n_params = n_parameters(params)
+        feed_flat = tf.placeholder(tf.float32, [n_params])
+        get_flat = parameters_to_vector(params)
+        set_flat = tf.group(*[tf.assign(param, value) for param, value in
+                            zip(params, vector_to_parameters(feed_flat, params))])
+        return feed_flat, set_flat, get_flat
+
+    def forward(self):
+        return self.op_get_flat
+
+    @make_method(feed='feed_flat', fetch='set_flat')
+    def set_flat(self, feed_flat):
+        pass
+
+    @make_method(fetch='get_flat')
+    def get_flat(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/lunzi/nn/layers.py b/experiments02/ant_umaze_1234/src/lunzi/nn/layers.py
new file mode 100644
index 0000000..a0cd9c2
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/lunzi/nn/layers.py
@@ -0,0 +1,88 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+import numpy as np
+from .module import Module
+from .parameter import Parameter
+
+
+class Linear(Module):
+    def __init__(self, in_features: int, out_features: int, bias=True, weight_initializer=None):
+        super().__init__()
+        self.in_features = in_features
+        self.out_features = out_features
+        if weight_initializer is None:
+            init_range = tf.sqrt(6.0 / (in_features + out_features))
+            weight_initializer = tf.random_uniform_initializer(-init_range, init_range, dtype=tf.float32)
+
+        self.use_bias = bias
+        with self.scope:
+            self.op_input = tf.placeholder(dtype=tf.float32, shape=[None, in_features], name='input')
+            self.weight = Parameter(weight_initializer([in_features, out_features],dtype=tf.float32), name='weight')
+            if bias:
+                self.bias = Parameter(tf.zeros([out_features], dtype=tf.float32), name='bias')
+
+        self.op_output = self(self.op_input)
+
+    def forward(self, x):
+        shape = x.get_shape().as_list()
+        if len(shape) > 2:
+            y = tf.tensordot(x, self.weight, [[len(shape) - 1], [0]])
+        else:
+            y = x.matmul(self.weight)
+        if self.use_bias:
+            y = y + self.bias
+        return y
+
+    def fast(self, x):
+        x = x.dot(self.weight.numpy())
+        if self.use_bias:
+            x = x + self.bias.numpy()
+        return x
+
+    def extra_repr(self):
+        return f'in_features={self.in_features}, out_features={self.out_features}, bias={self.use_bias}'
+
+
+class Sequential(Module):
+    def __init__(self, *modules):
+        super().__init__()
+        for i, module in enumerate(modules):
+            self._modules[i] = module
+
+    def forward(self, x):
+        for module in self._modules.values():
+            x = module(x)
+        return x
+
+    def fast(self, x):
+        for module in self._modules.values():
+            x = module.fast(x)
+        return x
+
+
+class ReLU(Module):
+    def forward(self, x):
+        return tf.nn.relu(x)
+
+    def fast(self, x: np.ndarray):
+        return np.maximum(x, 0)
+
+
+class Tanh(Module):
+    def forward(self, x):
+        return tf.nn.tanh(x)
+
+    def fast(self, x: np.ndarray):
+        return np.tanh(x)
+
+
+class Squeeze(Module):
+    def __init__(self, axis=None):
+        super().__init__()
+        self._axis = axis
+
+    def forward(self, x):
+        return x.squeeze(axis=self._axis)
+
+    def fast(self, x):
+        return x.squeeze(axis=self._axis)
diff --git a/experiments02/ant_umaze_1234/src/lunzi/nn/loss.py b/experiments02/ant_umaze_1234/src/lunzi/nn/loss.py
new file mode 100644
index 0000000..26e662b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/lunzi/nn/loss.py
@@ -0,0 +1,40 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from lunzi import Tensor
+from .module import Module
+
+
+class PointwiseLoss(Module):
+    def __init__(self, size_average=True, reduce=True):
+        super().__init__()
+        self.size_average = size_average
+        self.reduce = reduce
+
+    def pointwise(self, output: Tensor, target: Tensor):
+        raise NotImplementedError
+
+    def forward(self, output: Tensor, target: Tensor, input: Tensor = None):
+        loss = self.pointwise(output, target)
+        if self.reduce and len(loss.shape) > 1:
+            if self.size_average:
+                loss = loss.reduce_mean(axis=1)
+            else:
+                loss = loss.reduce_sum(axis=1)
+        return loss
+
+
+class L1Loss(PointwiseLoss):
+    def pointwise(self, output: Tensor, target: Tensor):
+        return output.sub(target).abs()
+
+
+class L2Loss(PointwiseLoss):
+    def pointwise(self, output: Tensor, target: Tensor):
+        return output.sub(target).pow(2)
+
+    def forward(self, output: Tensor, target: Tensor, input: Tensor = None):
+        return super().forward(output, target).sqrt()
+
+
+class MSELoss(PointwiseLoss):
+    def pointwise(self, output: Tensor, target: Tensor):
+        return output.sub(target).pow(2)
diff --git a/experiments02/ant_umaze_1234/src/lunzi/nn/module.py b/experiments02/ant_umaze_1234/src/lunzi/nn/module.py
new file mode 100644
index 0000000..8eacf3b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/lunzi/nn/module.py
@@ -0,0 +1,158 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import Dict, Any, Callable, List
+from collections import Counter
+import tensorflow as tf
+import numpy as np
+
+from lunzi import Tensor
+from lunzi.Logger import logger
+from .parameter import Parameter
+
+
+class Module(object):
+    """
+        A front-end for TensorFlow, heavily inspired by PyTorch's design and implementation.
+
+        Deepcopy is not supported since I didn't find a good way to duplicate `tf.Variables` and `tf.variable_scope`.
+    """
+
+    # To generate unique name scope
+    # The only reason we keep variable scope here is that we want the variables have meaning names,
+    # since the internal operations always look messy, I put no hope maintaining their names,
+    # So let's just do it for variables.
+    prefix_count = Counter()
+
+    @staticmethod
+    def _create_uid(prefix: str) -> str:
+        scope = tf.get_variable_scope().name + '/'
+        uid = Module.prefix_count[scope + prefix]
+        Module.prefix_count[scope + prefix] += 1
+        if uid == 0:
+            return prefix
+        return f'{prefix}_{uid}'
+
+    def __init__(self):
+        scope = Module._create_uid(self.__class__.__name__)
+        with tf.variable_scope(scope, reuse=False) as self._scope:
+            pass
+        # Since we only plan to support Python 3.6+, in which dict is already ordered, we don't use OrderedDict here.
+        self._parameters: Dict[Any, Parameter] = {}
+        self._modules: Dict[Any, Module] = {}
+        self._callables: Dict[Any, Callable] = {}
+
+    def forward(self, *args: List[Any], **kwargs: Dict[str, Any]) -> Tensor:
+        raise NotImplementedError
+
+    def fast(self, *args, **kwargs):
+        pass
+
+    def __setattr__(self, key, value):
+        # dynamically maintain sub modules.
+        modules = self.__dict__.get('_modules')
+        if isinstance(value, Parameter):
+            self._parameters[key] = value
+        if isinstance(value, Module):
+            assert modules is not None, 'Call `super().__init__` before assigning modules'
+            modules[key] = value
+        else:
+            if modules and key in modules:
+                del modules[key]
+        object.__setattr__(self, key, value)
+
+    def __call__(self, *args, **kwargs):
+        return self.forward(*args, **kwargs)
+
+    def register_callable(self, key, callable):
+        self._callables[key] = callable
+
+    def eval(self, fetch: str, **feed: Dict[str, np.ndarray]):
+        cache_key = f'[{" ".join(feed.keys())}] => [{fetch}]'
+        if cache_key not in self._callables:
+            logger.info('[%s] is making TensorFlow callables, key = %s', self.__class__.__name__, cache_key)
+            feed_ops = []
+            for key in feed.keys():
+                feed_ops.append(self.__dict__['op_' + key])
+            if isinstance(fetch, str):
+                fetch_ops = [self.__dict__['op_' + key] for key in fetch.split(' ')]
+                if len(fetch_ops) == 1:
+                    fetch_ops = fetch_ops[0]
+            else:
+                fetch_ops = fetch
+            self.register_callable(cache_key, tf.get_default_session().make_callable(fetch_ops, feed_ops))
+        return self._callables[cache_key](*feed.values())
+
+    def parameters(self, trainable=True, non_trainable=False, recursive=True, out=None) -> List[Parameter]:
+        """
+            We don't introduce `buffers` here. PyTorch has it since it doesn't have non-trainable Parameter.
+            A tensor in `buffers` is essentially a non-trainable Parameter (part of state_dict but isn't
+            optimized over).
+        """
+        if out is None:
+            out = []
+        for param in self._parameters.values():
+            if param.trainable and trainable or not param.trainable and non_trainable:
+                out.append(param)
+        if recursive:
+            for module in self._modules.values():
+                module.parameters(trainable=trainable, non_trainable=non_trainable, recursive=True, out=out)
+        # probably we don't need to sort since we're using `OrderedDict`
+        return out
+
+    @property
+    def scope(self) -> tf.variable_scope:
+        return tf.variable_scope(self._scope, reuse=tf.AUTO_REUSE)
+
+    def extra_repr(self) -> str:
+        return ''
+
+    def named_modules(self) -> dict:
+        return self._modules
+
+    def __repr__(self):
+        def dfs(node, prefix):
+            root_info = node.__class__.__name__
+            modules = node.named_modules()
+            if not modules:
+                return root_info + f'({node.extra_repr()})'
+
+            root_info += '(\n'
+            for key, module in modules.items():
+                module_repr = dfs(module, prefix + '    ')
+                root_info += f'{prefix}    ({key}): {module_repr}\n'
+            root_info += prefix + ')'
+            return root_info
+        return dfs(self, '')
+
+    def state_dict(self, recursive=True):
+        """
+            A better option is to find all parameters and then sess.run(state) but I assume this can't be the
+            bottleneck.
+        """
+        state = {}
+        for key, parameter in self._parameters.items():
+            # although we can use `.numpy()` here, for safety I'd use `.eval()`
+            state[key] = parameter.eval()
+        if recursive:
+            for key, module in self._modules.items():
+                state[key] = module.state_dict()
+        return state
+
+    def load_state_dict(self, state_dict: Dict[Any, Any], recursive=True, strict=True):
+        for key, parameter in self._parameters.items():
+            if key in state_dict:
+                parameter.load(state_dict[key])
+                parameter.invalidate()
+            else:
+                assert not strict, f'Missing Parameter {key} in state_dict'
+        if recursive:
+            for key, module in self._modules.items():
+                if key in state_dict:
+                    module.load_state_dict(state_dict[key], recursive=recursive, strict=strict)
+                else:
+                    assert not strict, f'Missing Module {key} in state_dict.'
+
+    def apply(self, fn):
+        for module in self._modules.values():
+            module.apply(fn)
+        fn(self)
+        return self
diff --git a/experiments02/ant_umaze_1234/src/lunzi/nn/parameter.py b/experiments02/ant_umaze_1234/src/lunzi/nn/parameter.py
new file mode 100644
index 0000000..969d79f
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/lunzi/nn/parameter.py
@@ -0,0 +1,24 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+from lunzi import Tensor
+
+
+def numpy(self):
+    if self.__dict__.get('_numpy_cache', None) is None:
+        self._numpy_cache: Tensor = self.eval()
+    return self._numpy_cache
+
+
+def invalidate(self):
+    self._numpy_cache = None
+
+
+# Q: Why not inherit from `tf.Variable`?
+# A: Since TensorFlow 1.11, `tf.Variable` has a meta class VariableMetaClass, which overrides `__call__`.
+#    And it's `_variable_call` function doesn't explicitly call `tf.Variable` so the return value must
+#    be a `tf.Variable`, which makes inheritance impossible.
+Parameter = tf.Variable
+
+Parameter.numpy = numpy
+Parameter.invalidate = invalidate
+
diff --git a/experiments02/ant_umaze_1234/src/lunzi/nn/patch.py b/experiments02/ant_umaze_1234/src/lunzi/nn/patch.py
new file mode 100644
index 0000000..5db96d8
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/lunzi/nn/patch.py
@@ -0,0 +1,60 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+
+from lunzi.Logger import logger
+
+
+def find_monkey_patch_keys(avoid_set=None):
+    if avoid_set is None:
+        avoid_set = {"shape"}  # tf.shape conflicts with Tensor.shape
+    patched = []
+    for key, value in tf.__dict__.items():
+        if not callable(value) or key in avoid_set:
+            continue
+        doc = value.__doc__
+        if doc is None:
+            continue
+        loc = doc.find('Args:\n')
+        if loc == -1:
+            continue
+
+        # Am I doing NLP?
+        # It seems that PyTorch has better doc. They always write `x (Tensor): ...` which is much easier to parse.
+        first_arg_doc = doc[loc + 6:].split('\n')[0].split(': ')[1]
+        if first_arg_doc.startswith('A `Tensor`') or first_arg_doc.startswith('`Tensor`') or key.startswith('reduce_'):
+            patched.append(key)
+    logger.warning(f'Monkey patched TensorFlow: {patched}')
+    return patched
+
+
+def monkey_patch(avoid_set=None):
+    logger.warning('Monkey patching TensorFlow...')
+
+    patched = ['abs', 'acos', 'acosh', 'add', 'angle', 'argmax', 'argmin', 'asin', 'asinh', 'atan', 'atan2', 'atanh',
+            'betainc', 'cast', 'ceil', 'check_numerics', 'clip_by_average_norm', 'clip_by_norm', 'clip_by_value',
+            'complex', 'conj', 'cos', 'cosh', 'cross', 'cumprod', 'cumsum', 'dequantize', 'diag', 'digamma', 'div',
+            'equal', 'erf', 'erfc', 'exp', 'expand_dims', 'expm1', 'fill', 'floor', 'floor_div', 'floordiv', 'floormod',
+            'gather', 'gather_nd', 'greater', 'greater_equal', 'hessians', 'identity', 'igamma', 'igammac', 'imag',
+            'is_finite', 'is_inf', 'is_nan', 'less', 'less_equal', 'lgamma', 'log', 'log1p', 'logical_and',
+            'logical_not', 'logical_or', 'matmul', 'maximum', 'meshgrid', 'minimum', 'mod', 'multiply', 'negative',
+            'norm', 'not_equal', 'one_hot', 'ones_like', 'pad', 'polygamma', 'pow', 'quantize', 'real', 'realdiv',
+            'reciprocal', 'reduce_all', 'reduce_any', 'reduce_logsumexp', 'reduce_max', 'reduce_mean', 'reduce_min',
+            'reduce_prod', 'reduce_sum', 'reshape', 'reverse', 'rint', 'round', 'rsqrt', 'scatter_nd', 'sign', 'sin',
+            'sinh', 'size', 'slice', 'sqrt', 'square', 'squeeze', 'stop_gradient', 'subtract', 'tan', 'tensordot',
+            'tile', 'to_bfloat16', 'to_complex128', 'to_complex64', 'to_double', 'to_float', 'to_int32', 'to_int64',
+            'transpose', 'truediv', 'truncatediv', 'truncatemod', 'unique', 'where', 'zeros_like', 'zeta']
+    alias = {
+        'mul': 'multiply',
+        'sub': 'subtract',
+    }
+
+    # use the code below for more ops
+    # patched = find_monkey_patch_keys(avoid_set)
+
+    for key, method in list(zip(patched, patched)) + list(alias.items()):
+        value = tf.__dict__[method]
+        setattr(tf.Tensor, key, value)
+        setattr(tf.Variable, key, value)
+
+
+monkey_patch()
diff --git a/experiments02/ant_umaze_1234/src/lunzi/nn/utils.py b/experiments02/ant_umaze_1234/src/lunzi/nn/utils.py
new file mode 100644
index 0000000..9fa5707
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/lunzi/nn/utils.py
@@ -0,0 +1,85 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import Callable, List, Union
+import inspect
+from functools import wraps
+import tensorflow as tf
+import numpy as np
+from lunzi import Tensor
+
+from .parameter import Parameter
+
+
+def make_method(feed: str = None, fetch: str = ''):
+    """
+        The following code:
+
+            @make_method('. w', fetch='d)
+            def func(a, c):
+                pass
+
+        will be converted to
+
+            def func(a, c, fetch='d'):
+                return self.eval(fetch, a=a, w=c)
+
+        Note that `func(1, c=2, b=1)` is also supported. This is
+        useful when writing PyTorch-like object method.
+
+    """
+
+    def decorator(func: Callable):
+        arg_names = inspect.signature(func).parameters.keys()
+        arg_map = {}
+        if feed is None:
+            arg_map = {op_name: op_name for op_name in arg_names if op_name != 'self'}
+        else:
+            feeds = ['-'] + feed.split(' ')  # ignore first `self`
+            for op_name, arg_name in zip(feeds, arg_names):
+                if op_name == '.':
+                    arg_map[op_name] = op_name
+                elif op_name != '-':  # deprecated
+                    arg_map[op_name] = arg_name
+
+        @wraps(func)
+        def wrapper(self, *args, **kwargs):
+            cur_fetch = kwargs.pop('fetch', fetch)
+            call_args = inspect.getcallargs(func, self, *args, **kwargs)
+            feed_dict = {op_name: call_args[arg_name] for op_name, arg_name in arg_map.items()}
+            return self.eval(cur_fetch, **feed_dict)
+
+        return wrapper
+
+    return decorator
+
+
+def n_parameters(params: List[Parameter]) -> int:
+    return sum([np.prod(p.shape) for p in params])
+
+
+def parameters_to_vector(parameters: List[Union[Parameter, Tensor]]) -> Tensor:
+    return tf.concat([param.reshape([-1]) for param in parameters], axis=0)
+
+
+def vector_to_parameters(vec: Tensor, parameters: List[Parameter]) -> List[Tensor]:
+    params: List[Tensor] = []
+    start = 0
+    for p in parameters:
+        end = start + np.prod(p.shape)
+        params.append(vec[start:end].reshape(p.shape))
+        start = end
+    return params
+
+
+def hessian_vec_prod(ys: Tensor, xs: List[Parameter], vs: Tensor) -> Tensor:
+    grad = parameters_to_vector(tf.gradients(ys, xs))
+    aux = (grad * vs).reduce_sum()
+    return parameters_to_vector(tf.gradients(aux, xs))
+
+
+# credit to https://github.com/renmengye/tensorflow-forward-ad/issues/2#issue-234418055
+def jacobian_vec_prod(ys: Tensor, xs: List[Parameter], vs: Tensor) -> Tensor:
+    u = tf.zeros_like(ys)  # dummy variable
+    grad = tf.gradients(ys, xs, grad_ys=u)
+    return tf.gradients(grad, u, grad_ys=vs)
+
+
diff --git a/experiments02/ant_umaze_1234/src/lunzi/stubs.py b/experiments02/ant_umaze_1234/src/lunzi/stubs.py
new file mode 100644
index 0000000..644c477
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/lunzi/stubs.py
@@ -0,0 +1,6 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+
+
+class Tensor(tf.Tensor):
+    pass
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/lunzi/stubs.pyi b/experiments02/ant_umaze_1234/src/lunzi/stubs.pyi
new file mode 100644
index 0000000..7bab814
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/lunzi/stubs.pyi
@@ -0,0 +1,138 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+
+class Tensor(tf.Tensor):
+    def abs(x, name=None) -> Tensor: ...
+    def acos(x, name=None) -> Tensor: ...
+    def acosh(x, name=None) -> Tensor: ...
+    def add(x, y, name=None) -> Tensor: ...
+    def angle(input, name=None) -> Tensor: ...
+    def argmax(input, axis=None, name=None, dimension=None, output_type=tf.int64) -> Tensor: ...
+    def argmin(input, axis=None, name=None, dimension=None, output_type=tf.int64) -> Tensor: ...
+    def asin(x, name=None) -> Tensor: ...
+    def asinh(x, name=None) -> Tensor: ...
+    def atan(x, name=None) -> Tensor: ...
+    def atan2(y, x, name=None) -> Tensor: ...
+    def atanh(x, name=None) -> Tensor: ...
+    def betainc(a, b, x, name=None) -> Tensor: ...
+    def cast(x, dtype, name=None) -> Tensor: ...
+    def ceil(x, name=None) -> Tensor: ...
+    def check_numerics(tensor, message, name=None) -> Tensor: ...
+    def clip_by_average_norm(t, clip_norm, name=None) -> Tensor: ...
+    def clip_by_norm(t, clip_norm, axes=None, name=None) -> Tensor: ...
+    def clip_by_value(t, clip_value_min, clip_value_max, name=None) -> Tensor: ...
+    def complex(real, imag, name=None) -> Tensor: ...
+    def conj(x, name=None) -> Tensor: ...
+    def cos(x, name=None) -> Tensor: ...
+    def cosh(x, name=None) -> Tensor: ...
+    def cross(a, b, name=None) -> Tensor: ...
+    def cumprod(x, axis=0, exclusive=False, reverse=False, name=None) -> Tensor: ...
+    def cumsum(x, axis=0, exclusive=False, reverse=False, name=None) -> Tensor: ...
+    def dequantize(input, min_range, max_range, mode='MIN_COMBINED', name=None) -> Tensor: ...
+    def diag(diagonal, name=None) -> Tensor: ...
+    def digamma(x, name=None) -> Tensor: ...
+    def div(x, y, name=None) -> Tensor: ...
+    def equal(x, y, name=None) -> Tensor: ...
+    def erf(x, name=None) -> Tensor: ...
+    def erfc(x, name=None) -> Tensor: ...
+    def exp(x, name=None) -> Tensor: ...
+    def expand_dims(input, axis=None, name=None, dim=None) -> Tensor: ...
+    def expm1(x, name=None) -> Tensor: ...
+    def fill(dims, value, name=None) -> Tensor: ...
+    def floor(x, name=None) -> Tensor: ...
+    def floor_div(x, y, name=None) -> Tensor: ...
+    def floordiv(x, y, name=None) -> Tensor: ...
+    def floormod(x, y, name=None) -> Tensor: ...
+    def gather(params, indices, validate_indices=None, name=None, axis=0) -> Tensor: ...
+    def gather_nd(params, indices, name=None) -> Tensor: ...
+    def greater(x, y, name=None) -> Tensor: ...
+    def greater_equal(x, y, name=None) -> Tensor: ...
+    def hessians(ys, xs, name='hessians', colocate_gradients_with_ops=False, gate_gradients=False, aggregation_method=None) -> Tensor: ...
+    def identity(input, name=None) -> Tensor: ...
+    def igamma(a, x, name=None) -> Tensor: ...
+    def igammac(a, x, name=None) -> Tensor: ...
+    def imag(input, name=None) -> Tensor: ...
+    def is_finite(x, name=None) -> Tensor: ...
+    def is_inf(x, name=None) -> Tensor: ...
+    def is_nan(x, name=None) -> Tensor: ...
+    def less(x, y, name=None) -> Tensor: ...
+    def less_equal(x, y, name=None) -> Tensor: ...
+    def lgamma(x, name=None) -> Tensor: ...
+    def log(x, name=None) -> Tensor: ...
+    def log1p(x, name=None) -> Tensor: ...
+    def logical_and(x, y, name=None) -> Tensor: ...
+    def logical_not(x, name=None) -> Tensor: ...
+    def logical_or(x, y, name=None) -> Tensor: ...
+    def matmul(a, b, transpose_a=False, transpose_b=False, adjoint_a=False, adjoint_b=False, a_is_sparse=False, b_is_sparse=False, name=None) -> Tensor: ...
+    def maximum(x, y, name=None) -> Tensor: ...
+    def meshgrid(*args, **kwargs) -> Tensor: ...
+    def minimum(x, y, name=None) -> Tensor: ...
+    def mod(x, y, name=None) -> Tensor: ...
+    def mul(x, y, name=None) -> Tensor: ...
+    def multiply(x, y, name=None) -> Tensor: ...
+    def negative(x, name=None) -> Tensor: ...
+    def norm(tensor, ord='euclidean', axis=None, keepdims=None, name=None) -> Tensor: ...
+    def not_equal(x, y, name=None) -> Tensor: ...
+    def one_hot(indices, depth, on_value=None, off_value=None, axis=None, dtype=None, name=None) -> Tensor: ...
+    def ones_like(tensor, dtype=None, name=None, optimize=True) -> Tensor: ...
+    def pad(tensor, paddings, mode='CONSTANT', name=None, constant_values=0) -> Tensor: ...
+    def polygamma(a, x, name=None) -> Tensor: ...
+    def pow(x, y, name=None) -> Tensor: ...
+    def quantize(input, min_range, max_range, T, mode='MIN_COMBINED', round_mode='HALF_AWAY_FROM_ZERO', name=None) -> Tensor: ...
+    def real(input, name=None) -> Tensor: ...
+    def realdiv(x, y, name=None) -> Tensor: ...
+    def reciprocal(x, name=None) -> Tensor: ...
+    def reduce_all(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_any(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_logsumexp(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_max(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_mean(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_min(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_prod(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_sum(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reshape(tensor, shape, name=None) -> Tensor: ...
+    def reverse(tensor, axis, name=None) -> Tensor: ...
+    def rint(x, name=None) -> Tensor: ...
+    def round(x, name=None) -> Tensor: ...
+    def rsqrt(x, name=None) -> Tensor: ...
+    def scatter_nd(indices, updates, shape, name=None) -> Tensor: ...
+    def sign(x, name=None) -> Tensor: ...
+    def sin(x, name=None) -> Tensor: ...
+    def sinh(x, name=None) -> Tensor: ...
+    def size(input, name=None, out_type=tf.int32) -> Tensor: ...
+    def slice(input_, begin, size, name=None) -> Tensor: ...
+    def sqrt(x, name=None) -> Tensor: ...
+    def square(x, name=None) -> Tensor: ...
+    def squeeze(input, axis=None, name=None, squeeze_dims=None) -> Tensor: ...
+    def stop_gradient(input, name=None) -> Tensor: ...
+    def sub(x, y, name=None) -> Tensor: ...
+    def subtract(x, y, name=None) -> Tensor: ...
+    def tan(x, name=None) -> Tensor: ...
+    def tensordot(a, b, axes, name=None) -> Tensor: ...
+    def tile(input, multiples, name=None) -> Tensor: ...
+    def to_bfloat16(x, name='ToBFloat16') -> Tensor: ...
+    def to_complex128(x, name='ToComplex128') -> Tensor: ...
+    def to_complex64(x, name='ToComplex64') -> Tensor: ...
+    def to_double(x, name='ToDouble') -> Tensor: ...
+    def to_float(x, name='ToFloat') -> Tensor: ...
+    def to_int32(x, name='ToInt32') -> Tensor: ...
+    def to_int64(x, name='ToInt64') -> Tensor: ...
+    def transpose(a, perm=None, name='transpose', conjugate=False) -> Tensor: ...
+    def truediv(x, y, name=None) -> Tensor: ...
+    def truncatediv(x, y, name=None) -> Tensor: ...
+    def truncatemod(x, y, name=None) -> Tensor: ...
+    def unique(x, out_idx=tf.int32, name=None) -> Tensor: ...
+    def where(condition, x=None, y=None, name=None) -> Tensor: ...
+    def zeros_like(tensor, dtype=None, name=None, optimize=True) -> Tensor: ...
+    def zeta(x, q, name=None) -> Tensor: ...
+
+    def __add__(self, other) -> Tensor: ...
+    def __sub__(self, other) -> Tensor: ...
+    def __mul__(self, other) -> Tensor: ...
+    def __rdiv__(self, other) -> Tensor: ...
+    def __itruediv__(self, other) -> Tensor: ...
+    def __rsub__(self, other) -> Tensor: ...
+    def __isub__(self, other) -> Tensor: ...
+    def __imul__(self, other) -> Tensor: ...
+    def __rmul__(self, other) -> Tensor: ...
+    def __radd__(self, other) -> Tensor: ...
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/main.py b/experiments02/ant_umaze_1234/src/main.py
new file mode 100644
index 0000000..38aafb2
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/main.py
@@ -0,0 +1,232 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import pickle
+from collections import deque
+import tensorflow as tf
+import numpy as np
+from tqdm import tqdm
+
+import lunzi.nn as nn
+from lunzi.Logger import logger
+from slbo.utils.average_meter import AverageMeter
+from slbo.utils.flags import FLAGS
+from slbo.utils.dataset import Dataset, gen_dtype
+from slbo.utils.OU_noise import OUNoise
+from slbo.utils.normalizer import Normalizers
+from slbo.utils.tf_utils import get_tf_config
+from slbo.utils.runner import Runner
+from slbo.policies.gaussian_mlp_policy import GaussianMLPPolicy
+from slbo.envs.virtual_env import VirtualEnv
+from slbo.dynamics_model import DynamicsModel
+from slbo.v_function.mlp_v_function import MLPVFunction
+from slbo.partial_envs import make_env
+from slbo.loss.multi_step_loss import MultiStepLoss
+from slbo.algos.TRPO import TRPO
+from slbo.random_net import RandomNet
+
+
+def evaluate(settings, tag):
+    return_means = []
+    for runner, policy, name in settings:
+        runner.reset()
+        _, ep_infos = runner.run(policy, FLAGS.rollout.n_test_samples)
+        if name == 'Real Env':
+            returns = np.array([ep_info['success'] for ep_info in ep_infos])
+        else:
+            returns = np.array([ep_info['return'] for ep_info in ep_infos])
+        logger.info('Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f', tag, name,
+                    len(returns), np.mean(returns), np.std(returns))
+
+        return_means.append(np.mean(returns))
+
+    return return_means
+
+
+
+def add_multi_step(src: Dataset, dst: Dataset):
+    n_envs = 1
+    dst.extend(src[:-n_envs])
+
+    ending = src[-n_envs:].copy()
+    ending.timeout = True
+    dst.extend(ending)
+
+
+def make_real_runner(n_envs):
+    from slbo.envs.batched_env import BatchedEnv
+    batched_env = BatchedEnv([make_env(FLAGS.env.id) for _ in range(n_envs)])
+    return Runner(batched_env, rescale_action=True, **FLAGS.runner.as_dict())
+
+
+def main():
+    FLAGS.set_seed()
+    FLAGS.freeze()
+
+    env = make_env(FLAGS.env.id)
+    dim_state = int(np.prod(env.observation_space.shape))
+    dim_action = int(np.prod(env.action_space.shape))
+
+    env.verify()
+
+    normalizers = Normalizers(dim_action=dim_action, dim_state=dim_state)
+
+    dtype = gen_dtype(env, 'state action next_state reward done timeout')
+    train_set = Dataset(dtype, FLAGS.rollout.max_buf_size)
+    dev_set = Dataset(dtype, FLAGS.rollout.max_buf_size)
+
+    policy = GaussianMLPPolicy(dim_state, dim_action, normalizer=normalizers.state, **FLAGS.policy.as_dict())
+    # batched noises
+    #noise = OUNoise(env.action_space, theta=FLAGS.OUNoise.theta, sigma=FLAGS.OUNoise.sigma, shape=(1, dim_action))
+    vfn = MLPVFunction(dim_state, [64, 64], normalizers.state)
+    model = DynamicsModel(dim_state, dim_action, normalizers, FLAGS.model.hidden_sizes)
+    random_net = RandomNet(dim_state, dim_action, normalizers, FLAGS.model.hidden_sizes)
+
+    virt_env = VirtualEnv(model, make_env(FLAGS.env.id), random_net, FLAGS.plan.n_envs,FLAGS.model.hidden_sizes[-1], 
+                            FLAGS.pc.bonus_scale,FLAGS.pc.lamb, opt_model=FLAGS.slbo.opt_model)
+    virt_runner = Runner(virt_env, **{**FLAGS.runner.as_dict(), 'max_steps': FLAGS.plan.max_steps})
+
+    criterion_map = {
+        'L1': nn.L1Loss(),
+        'L2': nn.L2Loss(),
+        'MSE': nn.MSELoss(),
+    }
+    criterion = criterion_map[FLAGS.model.loss]
+    loss_mod = MultiStepLoss(model, normalizers, dim_state, dim_action, criterion, FLAGS.model.multi_step)
+    loss_mod.build_backward(FLAGS.model.lr, FLAGS.model.weight_decay)
+    algo = TRPO(vfn=vfn, policy=policy, dim_state=dim_state, dim_action=dim_action, **FLAGS.TRPO.as_dict())
+
+    tf.get_default_session().run(tf.global_variables_initializer())
+
+    runners = {
+        'test': make_real_runner(4),
+        'collect': make_real_runner(1),
+        'dev': make_real_runner(1),
+        'train': make_real_runner(FLAGS.plan.n_envs) if FLAGS.algorithm == 'MF' else virt_runner,
+    }
+    settings = [(runners['test'], policy, 'Real Env'), (runners['train'], policy, 'Virt Env')]
+
+    saver = nn.ModuleDict({'policy': policy, 'model': model, 'vfn': vfn})
+    print(saver)
+
+    eval_real_returns = []
+    timesteps = []
+
+    if FLAGS.ckpt.model_load:
+        saver.load_state_dict(np.load(FLAGS.ckpt.model_load)[()])
+        logger.warning('Load model from %s', FLAGS.ckpt.model_load)
+
+    if FLAGS.ckpt.buf_load:
+        n_samples = 0
+        for i in range(FLAGS.ckpt.buf_load_index):
+            data = pickle.load(open(f'{FLAGS.ckpt.buf_load}/stage-{i}.inc-buf.pkl', 'rb'))
+            add_multi_step(data, train_set)
+            n_samples += len(data)
+        logger.warning('Loading %d samples from %s', n_samples, FLAGS.ckpt.buf_load)
+
+    max_ent_coef = 0
+    print("!!!!!!!!")
+    print(virt_env.bonus_scale)
+    for T in range(FLAGS.slbo.n_stages):
+        logger.info('------ Starting Stage %d --------', T)
+        eval_returns = evaluate(settings, 'episode')
+        eval_real_returns.append(eval_returns[0])
+        timesteps.append(T*FLAGS.rollout.n_train_samples)
+
+        if not FLAGS.use_prev:
+            train_set.clear()
+            dev_set.clear()
+
+        # collect data
+        recent_train_set, ep_infos = runners['collect'].run(policy, FLAGS.rollout.n_train_samples, render=False)
+        add_multi_step(recent_train_set, train_set)
+        add_multi_step(
+            runners['dev'].run(policy, FLAGS.rollout.n_dev_samples)[0],
+            dev_set,
+        )
+
+        returns = np.array([ep_info['return'] for ep_info in ep_infos])
+
+        if len(returns) > 0:
+            logger.info("episode: %s", np.mean(returns))
+
+        if T == 0:  # check
+            samples = train_set.sample_multi_step(100, 1, FLAGS.model.multi_step)
+            for i in range(FLAGS.model.multi_step - 1):
+                masks = 1 - (samples.done[i] | samples.timeout[i])[..., np.newaxis]
+                assert np.allclose(samples.state[i + 1] * masks, samples.next_state[i] * masks)
+
+        # recent_states = obsvs
+        # ref_actions = policy.eval('actions_mean actions_std', states=recent_states)
+        if FLAGS.rollout.normalizer == 'policy' or FLAGS.rollout.normalizer == 'uniform' and T == 0:
+            normalizers.state.update(recent_train_set.state)
+            normalizers.action.update(recent_train_set.action)
+            normalizers.diff.update(recent_train_set.next_state - recent_train_set.state)
+        #print(recent_train_set.state.shape)
+        virt_env.update_cov(recent_train_set.state,recent_train_set.action)
+
+        #if T == FLAGS.pc.bonus_stop_time:
+        #    virt_env.bonus_scale = 0.
+
+        for i in range(FLAGS.slbo.n_iters):
+            #if i % FLAGS.slbo.n_evaluate_iters == 0 and i != 0:
+                # cur_actions = policy.eval('actions_mean actions_std', states=recent_states)
+                # kl_old_new = gaussian_kl(*ref_actions, *cur_actions).sum(axis=1).mean()
+                # logger.info('KL(old || cur) = %.6f', kl_old_new)
+            #    evaluate(settings, 'iteration')
+
+            losses = deque(maxlen=FLAGS.slbo.n_model_iters)
+            grad_norm_meter = AverageMeter()
+            n_model_iters = FLAGS.slbo.n_model_iters
+            for _ in range(n_model_iters):
+                samples = train_set.sample_multi_step(FLAGS.model.train_batch_size, 1, FLAGS.model.multi_step)
+                _, train_loss, grad_norm = loss_mod.get_loss(
+                    samples.state, samples.next_state, samples.action, ~samples.done & ~samples.timeout,
+                    fetch='train loss grad_norm')
+                losses.append(train_loss.mean())
+                grad_norm_meter.update(grad_norm)
+                # ideally, we should define an Optimizer class, which takes parameters as inputs.
+                # The `update` method of `Optimizer` will invalidate all parameters during updates.
+                for param in model.parameters():
+                    param.invalidate()
+
+            if i % FLAGS.model.validation_freq == 0:
+                samples = train_set.sample_multi_step(
+                    FLAGS.model.train_batch_size, 1, FLAGS.model.multi_step)
+                loss = loss_mod.get_loss(
+                    samples.state, samples.next_state, samples.action, ~samples.done & ~samples.timeout)
+                loss = loss.mean()
+                if np.isnan(loss) or np.isnan(np.mean(losses)):
+                    logger.info('nan! %s %s', np.isnan(loss), np.isnan(np.mean(losses)))
+                logger.info('# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f',
+                            i, np.mean(losses), loss, n_model_iters, grad_norm_meter.get())
+
+            for n_updates in tqdm(range(FLAGS.slbo.n_policy_iters)):
+                if FLAGS.algorithm != 'MF' and FLAGS.slbo.start == 'buffer':
+                    runners['train'].set_state(train_set.sample(FLAGS.plan.n_envs).state)
+                else:
+                    runners['train'].reset()
+
+                data, ep_infos = runners['train'].run(policy, FLAGS.plan.n_trpo_samples)
+                advantages, values = runners['train'].compute_advantage(vfn, data)
+                dist_mean, dist_std, vf_loss = algo.train(max_ent_coef, data, advantages, values)
+                returns = [info['return'] for info in ep_infos]
+                #logger.info('[TRPO] # %d: n_episodes = %d, returns: {mean = %.0f, std = %.0f}, '
+                #            'dist std = %.10f, dist mean = %.10f, vf_loss = %.3f',
+                #            n_updates, len(returns), np.mean(returns), np.std(returns) / np.sqrt(len(returns)),
+                #            dist_std, dist_mean, vf_loss)
+
+        if T % FLAGS.ckpt.n_save_stages == 0:
+            np.save(f'{FLAGS.log_dir}/stage-{T}', saver.state_dict())
+            np.save(f'{FLAGS.log_dir}/final', saver.state_dict())
+        if FLAGS.ckpt.n_save_stages == 1:
+            pickle.dump(recent_train_set, open(f'{FLAGS.log_dir}/stage-{T}.inc-buf.pkl', 'wb'))
+
+    eval_returns = evaluate(settings, 'episode')
+    eval_real_returns.append(eval_returns[0])
+    timesteps.append(T*FLAGS.rollout.n_train_samples)
+    np.save(f'{FLAGS.log_dir}/eval_real_returns', eval_real_returns)
+    np.save(f'{FLAGS.log_dir}/timesteps', timesteps)
+
+
+if __name__ == '__main__':
+    with tf.Session(config=get_tf_config()):
+        main()
diff --git a/experiments02/ant_umaze_1234/src/requirements.txt b/experiments02/ant_umaze_1234/src/requirements.txt
new file mode 100644
index 0000000..8991cc0
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/requirements.txt
@@ -0,0 +1,6 @@
+tensorflow
+numpy
+pyyaml
+termcolor
+gym
+json_tricks
diff --git a/experiments02/ant_umaze_1234/src/rllab_requirements.txt b/experiments02/ant_umaze_1234/src/rllab_requirements.txt
new file mode 100644
index 0000000..9fb2d37
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/rllab_requirements.txt
@@ -0,0 +1,7 @@
+theano
+cached_property
+pyopengl
+joblib
+mako
+mujoco_py
+
diff --git a/experiments02/ant_umaze_1234/src/run2.sh b/experiments02/ant_umaze_1234/src/run2.sh
new file mode 100644
index 0000000..9bfb0b2
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/run2.sh
@@ -0,0 +1,10 @@
+#!/usr/bin/env bash
+
+for env_name in $1; do
+    echo "=> Running environment ${env_name}"
+    #for random_seed in 1234 2314 2345 1235; do
+    for random_seed in 1234; do
+        python main.py -c configs/algos/slbo_bm_200k.yml configs/env_tingwu/${env_name}.yml \
+	    -s pc.bonus_scale=0.3 log_dir=./experiments02/${env_name}_${random_seed} seed=${random_seed}
+    done
+done
diff --git a/experiments02/ant_umaze_1234/src/run_experiments.sh b/experiments02/ant_umaze_1234/src/run_experiments.sh
new file mode 100644
index 0000000..e3f2ad7
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/run_experiments.sh
@@ -0,0 +1,10 @@
+#!/usr/bin/env bash
+
+for env_name in $1; do
+    echo "=> Running environment ${env_name}"
+    #for random_seed in 1234 2314 2345 1235; do
+    for random_seed in 1234; do
+        python main.py -c configs/algos/slbo_bm_200k.yml configs/env_tingwu/${env_name}.yml \
+	    -s pc.bonus_scale=0.5 log_dir=./experiments05/${env_name}_${random_seed} seed=${random_seed}
+    done
+done
diff --git a/experiments02/ant_umaze_1234/src/slbo/__init__.py b/experiments02/ant_umaze_1234/src/slbo/__init__.py
new file mode 100644
index 0000000..5c7f19c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/__init__.py
@@ -0,0 +1 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
diff --git a/experiments02/ant_umaze_1234/src/slbo/algos/TRPO.py b/experiments02/ant_umaze_1234/src/slbo/algos/TRPO.py
new file mode 100644
index 0000000..e8469e3
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/algos/TRPO.py
@@ -0,0 +1,183 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List, Callable
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi import Tensor
+from lunzi.Logger import logger
+from slbo.utils.dataset import Dataset
+from slbo.policies import BaseNNPolicy
+from slbo.v_function import BaseVFunction
+
+
+def average_l2_norm(x):
+    return np.sqrt((x**2).mean())
+
+
+# for damping, modify func_Ax
+def conj_grad(mat_mul_vec: Callable[[np.ndarray], np.ndarray], b, n_iters=10, residual_tol=1e-10, verbose=False):
+    p = b.copy()
+    r = b.copy()
+    x = np.zeros_like(b)
+    r_dot_r = r.dot(r)
+
+    for i in range(n_iters):
+        if verbose:
+            logger.info('[CG] iters = %d, |Res| = %.6f, |x| = %.6f', i, r_dot_r, np.linalg.norm(x))
+        z = mat_mul_vec(p)
+        v = r_dot_r / p.dot(z)
+        x += v * p
+        r -= v * z
+        new_r_dot_r = r.dot(r)
+        if new_r_dot_r < residual_tol:
+            break
+        mu = new_r_dot_r / r_dot_r
+        p = r + mu * p
+        r_dot_r = new_r_dot_r
+    return x
+
+
+class TRPO(nn.Module):
+    def __init__(self, dim_state: int, dim_action: int, policy: BaseNNPolicy, vfn: BaseVFunction, max_kl: float,
+                 n_cg_iters: int, ent_coef=0.0, cg_damping=0.01, vf_lr=3e-4, n_vf_iters=3):
+        super().__init__()
+        self.dim_state = dim_state
+        self.policy = policy
+        self.ent_coef = ent_coef
+        self.vf = vfn
+        self.n_cg_iters = n_cg_iters
+        self.max_kl = max_kl
+        self.cg_damping = cg_damping
+        self.n_vf_iters = n_vf_iters
+        self.vf_lr = vf_lr
+
+        # doing backtrace, so don't need to separate.
+        self.flatten = nn.FlatParam(self.policy.parameters())
+        self.old_policy: nn.Module = policy.clone()
+
+        with self.scope:
+            self.op_returns = tf.placeholder(dtype=tf.float32, shape=[None], name='returns')
+            self.op_advantages = tf.placeholder(dtype=tf.float32, shape=[None], name='advantages')
+            self.op_states = tf.placeholder(dtype=tf.float32, shape=[None, dim_state], name='states')
+            self.op_actions = tf.placeholder(dtype=tf.float32, shape=[None, dim_action], name='actions')
+            self.op_feed_params = tf.placeholder(dtype=tf.float32, shape=[None], name='feed_params')
+
+            self.op_tangents = tf.placeholder(
+                dtype=tf.float32, shape=[nn.utils.n_parameters(self.policy.parameters())])
+            self.op_ent_coef = tf.placeholder(dtype=tf.float32, shape=[], name='ent_coef')
+
+        self.op_mean_kl, self.op_loss, self.op_dist_std, self.op_dist_mean, self.op_policy_loss = \
+            self(self.op_states, self.op_actions, self.op_advantages, self.op_ent_coef)
+
+        self.op_sync_old, self.op_hessian_vec_prod, self.op_flat_grad = \
+            self.compute_natural_grad(self.op_loss, self.op_mean_kl, self.op_tangents)
+
+        self.op_vf_loss, self.op_train_vf = self.compute_vf(self.op_states, self.op_returns)
+
+    def forward(self, states, actions, advantages, ent_coef):
+        old_distribution: tf.distributions.Normal = self.old_policy(states)
+        distribution: tf.distributions.Normal = self.policy(states)
+        mean_kl = old_distribution.kl_divergence(distribution).reduce_sum(axis=1).reduce_mean()
+        entropy = distribution.entropy().reduce_sum(axis=1).reduce_mean()
+        entropy_bonus = ent_coef * entropy
+
+        ratios: Tensor = (distribution.log_prob(actions) - old_distribution.log_prob(actions)) \
+            .reduce_sum(axis=1).exp()
+        # didn't output op_policy_loss since in principle it should be 0.
+        policy_loss = ratios.mul(advantages).reduce_mean()
+
+        # We're doing Gradient Ascent so this is, in fact, gain.
+        loss = policy_loss + entropy_bonus
+
+        return mean_kl, loss, distribution.stddev().log().reduce_mean().exp(), \
+            distribution.mean().norm(axis=1).reduce_mean() / np.sqrt(10), policy_loss
+
+    def compute_natural_grad(self, loss, mean_kl, tangents):
+        params = self.policy.parameters()
+        old_params = self.old_policy.parameters()
+        hessian_vec_prod = nn.utils.hessian_vec_prod(mean_kl, params, tangents)
+        flat_grad = nn.utils.parameters_to_vector(tf.gradients(loss, params))
+        sync_old = tf.group(*[tf.assign(old_v, new_v) for old_v, new_v in zip(old_params, params)])
+
+        return sync_old, hessian_vec_prod, flat_grad
+
+    def compute_vf(self, states, returns):
+        vf_loss = nn.MSELoss()(self.vf(states), returns).reduce_mean()
+        optimizer = tf.train.AdamOptimizer(self.vf_lr)
+        train_vf = optimizer.minimize(vf_loss)
+
+        return vf_loss, train_vf
+
+    @nn.make_method()
+    def get_vf_loss(self, states, returns) -> List[np.ndarray]: pass
+
+    @nn.make_method(fetch='sync_old')
+    def sync_old(self) -> List[np.ndarray]: pass
+
+    @nn.make_method(fetch='hessian_vec_prod')
+    def get_hessian_vec_prod(self, states, tangents, actions) -> List[np.ndarray]: pass
+
+    @nn.make_method(fetch='loss')
+    def get_loss(self, states, actions, advantages, ent_coef) -> List[np.ndarray]: pass
+
+    def train(self, ent_coef, samples, advantages, values):
+        returns = advantages + values
+        advantages = (advantages - advantages.mean()) / np.maximum(advantages.std(), 1e-8)
+        assert np.isfinite(advantages).all()
+        self.sync_old()
+        old_loss, grad, dist_std, mean_kl, dist_mean = self.get_loss(
+            samples.state, samples.action, advantages, ent_coef, fetch='loss flat_grad dist_std mean_kl dist_mean')
+
+        if np.allclose(grad, 0):
+            logger.info('Zero gradient, not updating...')
+            return
+
+        def fisher_vec_prod(x):
+            return self.get_hessian_vec_prod(samples.state, x, samples.action) + self.cg_damping * x
+
+        assert np.isfinite(grad).all()
+        nat_grad = conj_grad(fisher_vec_prod, grad, n_iters=self.n_cg_iters, verbose=False)
+
+        assert np.isfinite(nat_grad).all()
+
+        old_params = self.flatten.get_flat()
+        step_size = np.sqrt(2 * self.max_kl / nat_grad.dot(fisher_vec_prod(nat_grad)))
+
+        for _ in range(10):
+            new_params = old_params + nat_grad * step_size
+            self.flatten.set_flat(new_params)
+            loss, mean_kl = self.get_loss(samples.state, samples.action, advantages, ent_coef, fetch='loss mean_kl')
+            improve = loss - old_loss
+            if not np.isfinite([loss, mean_kl]).all():
+                logger.info('Got non-finite loss.')
+            elif mean_kl > self.max_kl * 1.5:
+                logger.info('Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f',
+                            mean_kl, self.max_kl)
+            elif improve < 0:
+                logger.info("Surrogate didn't improve, shrinking step... %.6f => %.6f", old_loss, loss)
+            else:
+                break
+            step_size *= 0.5
+        else:
+            logger.info("Couldn't find a good step.")
+            self.flatten.set_flat(old_params)
+        for param in self.policy.parameters():
+            param.invalidate()
+
+        # optimize value function
+        vf_dataset = Dataset.fromarrays([samples.state, returns],
+                                        dtype=[('state', ('f8', self.dim_state)), ('return_', 'f8')])
+        vf_loss = self.train_vf(vf_dataset)
+
+        return dist_mean, dist_std, vf_loss
+
+    def train_vf(self, dataset: Dataset):
+        for _ in range(self.n_vf_iters):
+            for subset in dataset.iterator(64):
+                self.get_vf_loss(subset.state, subset.return_, fetch='train_vf vf_loss')
+        for param in self.parameters():
+            param.invalidate()
+        vf_loss = self.get_vf_loss(dataset.state, dataset.return_, fetch='vf_loss')
+        return vf_loss
+
+
diff --git a/experiments02/ant_umaze_1234/src/slbo/algos/__init__.py b/experiments02/ant_umaze_1234/src/slbo/algos/__init__.py
new file mode 100644
index 0000000..5c7f19c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/algos/__init__.py
@@ -0,0 +1 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
diff --git a/experiments02/ant_umaze_1234/src/slbo/dynamics_model.py b/experiments02/ant_umaze_1234/src/slbo/dynamics_model.py
new file mode 100644
index 0000000..19662ec
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/dynamics_model.py
@@ -0,0 +1,43 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List
+import tensorflow as tf
+from lunzi import Tensor
+import lunzi.nn as nn
+from slbo.utils.normalizer import Normalizers
+from slbo.utils.multi_layer_perceptron import MultiLayerPerceptron
+
+
+class DynamicsModel(MultiLayerPerceptron):
+    op_loss: Tensor
+    op_train: Tensor
+    op_grad_norm: Tensor
+
+    def __init__(self, dim_state: int, dim_action: int, normalizers: Normalizers, hidden_sizes: List[int]):
+        initializer = tf.truncated_normal_initializer(mean=0.0, stddev=1e-5)
+
+        self.dim_state = dim_state
+        self.dim_action = dim_action
+        self.hidden_sizes = hidden_sizes
+        self.op_states = tf.placeholder(tf.float32, shape=[None, self.dim_state], name='states')
+        self.op_actions = tf.placeholder(tf.float32, shape=[None, self.dim_action], name='actions')
+        super().__init__([dim_state + dim_action, *hidden_sizes, dim_state],
+                         activation=nn.ReLU,
+                         weight_initializer=initializer, build=False)
+
+        self.normalizers = normalizers
+        self.build()
+
+    def build(self):
+        self.op_next_states = self.forward(self.op_states, self.op_actions)
+
+    def forward(self, states, actions):
+        assert actions.shape[-1] == self.dim_action
+        inputs = tf.concat([self.normalizers.state(states), actions.clip_by_value(-1., 1.)], axis=1)
+
+        normalized_diffs = super().forward(inputs)
+        next_states = states + self.normalizers.diff(normalized_diffs, inverse=True)
+        next_states = self.normalizers.state(self.normalizers.state(next_states).clip_by_value(-100, 100), inverse=True)
+        return next_states
+
+    def clone(self):
+        return DynamicsModel(self.dim_state, self.dim_action, self.normalizers, self.hidden_sizes)
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/__init__.py b/experiments02/ant_umaze_1234/src/slbo/envs/__init__.py
new file mode 100644
index 0000000..7efdf01
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/__init__.py
@@ -0,0 +1,55 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+import abc
+import gym
+from slbo.utils.dataset import Dataset, gen_dtype
+from lunzi.Logger import logger
+
+
+class BaseBatchedEnv(gym.Env, abc.ABC):
+    # thought about using `@property @abc.abstractmethod` here but we don't need explicit `@property` function here.
+    n_envs: int
+
+    @abc.abstractmethod
+    def step(self, actions):
+        pass
+
+    def reset(self):
+        return self.partial_reset(range(self.n_envs))
+
+    @abc.abstractmethod
+    def partial_reset(self, indices):
+        pass
+
+    def set_state(self, state):
+        logger.warning('`set_state` is not implemented')
+
+
+class BaseModelBasedEnv(gym.Env, abc.ABC):
+    @abc.abstractmethod
+    def mb_step(self, states: np.ndarray, actions: np.ndarray, next_states: np.ndarray):
+        raise NotImplementedError
+
+    def verify(self, n=2000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
+
+    def seed(self, seed: int = None):
+        pass
+
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/batched_env.py b/experiments02/ant_umaze_1234/src/slbo/envs/batched_env.py
new file mode 100644
index 0000000..cbe4bac
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/batched_env.py
@@ -0,0 +1,37 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from gym import Wrapper
+from . import BaseBatchedEnv
+
+
+class BatchedEnv(BaseBatchedEnv, Wrapper):
+    def __init__(self, envs):
+        super().__init__(envs[0])
+        self.envs = envs
+        self.n_envs = len(envs)
+
+    def step(self, actions):
+
+        buf, infos = [], []
+        for env, action in zip(self.envs, actions):
+            next_state, reward, done, info = env.step(action)
+            buf.append((next_state, reward, done))
+            infos.append(info)
+
+        return [*(np.array(x) for x in zip(*buf)), infos]
+
+    def reset(self):
+        return self.partial_reset(range(self.n_envs))
+
+    def partial_reset(self, indices):
+        states = []
+        for index in indices:
+            states.append(self.envs[index].reset())
+        return np.array(states)
+
+    def __repr__(self):
+        return f'Batch<{self.n_envs}x {self.env}>'
+
+    def set_state(self, state):
+        pass
+
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/__init__.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/assets/ant.xml b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/assets/ant.xml
new file mode 100644
index 0000000..18ad38b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/assets/ant.xml
@@ -0,0 +1,80 @@
+<mujoco model="ant">
+  <compiler angle="degree" coordinate="local" inertiafromgeom="true"/>
+  <option integrator="RK4" timestep="0.01"/>
+  <custom>
+    <numeric data="0.0 0.0 0.55 1.0 0.0 0.0 0.0 0.0 1.0 0.0 -1.0 0.0 -1.0 0.0 1.0" name="init_qpos"/>
+  </custom>
+  <default>
+    <joint armature="1" damping="1" limited="true"/>
+    <geom conaffinity="0" condim="3" density="5.0" friction="1 0.5 0.5" margin="0.01" rgba="0.8 0.6 0.4 1"/>
+  </default>
+  <asset>
+    <texture builtin="gradient" height="100" rgb1="1 1 1" rgb2="0 0 0" type="skybox" width="100"/>
+    <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+    <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+    <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="60 60" texture="texplane"/>
+    <material name="geom" texture="texgeom" texuniform="true"/>
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="40 40 40" type="plane"/>
+    <body name="torso" pos="0 0 0.75">
+      <geom name="torso_geom" pos="0 0 0" size="0.25" type="sphere"/>
+      <joint armature="0" damping="0" limited="false" margin="0.01" name="root" pos="0 0 0" type="free"/>
+      <body name="front_left_leg" pos="0 0 0">
+        <geom fromto="0.0 0.0 0.0 0.2 0.2 0.0" name="aux_1_geom" size="0.08" type="capsule"/>
+        <body name="aux_1" pos="0.2 0.2 0">
+          <joint axis="0 0 1" name="hip_1" pos="0.0 0.0 0.0" range="-30 30" type="hinge"/>
+          <geom fromto="0.0 0.0 0.0 0.2 0.2 0.0" name="left_leg_geom" size="0.08" type="capsule"/>
+          <body pos="0.2 0.2 0">
+            <joint axis="-1 1 0" name="ankle_1" pos="0.0 0.0 0.0" range="30 70" type="hinge"/>
+            <geom fromto="0.0 0.0 0.0 0.4 0.4 0.0" name="left_ankle_geom" size="0.08" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <body name="front_right_leg" pos="0 0 0">
+        <geom fromto="0.0 0.0 0.0 -0.2 0.2 0.0" name="aux_2_geom" size="0.08" type="capsule"/>
+        <body name="aux_2" pos="-0.2 0.2 0">
+          <joint axis="0 0 1" name="hip_2" pos="0.0 0.0 0.0" range="-30 30" type="hinge"/>
+          <geom fromto="0.0 0.0 0.0 -0.2 0.2 0.0" name="right_leg_geom" size="0.08" type="capsule"/>
+          <body pos="-0.2 0.2 0">
+            <joint axis="1 1 0" name="ankle_2" pos="0.0 0.0 0.0" range="-70 -30" type="hinge"/>
+            <geom fromto="0.0 0.0 0.0 -0.4 0.4 0.0" name="right_ankle_geom" size="0.08" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <body name="back_leg" pos="0 0 0">
+        <geom fromto="0.0 0.0 0.0 -0.2 -0.2 0.0" name="aux_3_geom" size="0.08" type="capsule"/>
+        <body name="aux_3" pos="-0.2 -0.2 0">
+          <joint axis="0 0 1" name="hip_3" pos="0.0 0.0 0.0" range="-30 30" type="hinge"/>
+          <geom fromto="0.0 0.0 0.0 -0.2 -0.2 0.0" name="back_leg_geom" size="0.08" type="capsule"/>
+          <body pos="-0.2 -0.2 0">
+            <joint axis="-1 1 0" name="ankle_3" pos="0.0 0.0 0.0" range="-70 -30" type="hinge"/>
+            <geom fromto="0.0 0.0 0.0 -0.4 -0.4 0.0" name="third_ankle_geom" size="0.08" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <body name="right_back_leg" pos="0 0 0">
+        <geom fromto="0.0 0.0 0.0 0.2 -0.2 0.0" name="aux_4_geom" size="0.08" type="capsule"/>
+        <body name="aux_4" pos="0.2 -0.2 0">
+          <joint axis="0 0 1" name="hip_4" pos="0.0 0.0 0.0" range="-30 30" type="hinge"/>
+          <geom fromto="0.0 0.0 0.0 0.2 -0.2 0.0" name="rightback_leg_geom" size="0.08" type="capsule"/>
+          <body pos="0.2 -0.2 0">
+            <joint axis="1 1 0" name="ankle_4" pos="0.0 0.0 0.0" range="30 70" type="hinge"/>
+            <geom fromto="0.0 0.0 0.0 0.4 -0.4 0.0" name="fourth_ankle_geom" size="0.08" type="capsule"/>
+          </body>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="hip_4" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="ankle_4" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="hip_1" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="ankle_1" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="hip_2" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="ankle_2" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="hip_3" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="ankle_3" gear="150"/>
+  </actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/assets/half_cheetah.xml b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/assets/half_cheetah.xml
new file mode 100644
index 0000000..b07aada
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/assets/half_cheetah.xml
@@ -0,0 +1,95 @@
+<!-- Cheetah Model
+
+    The state space is populated with joints in the order that they are
+    defined in this file. The actuators also operate on joints.
+
+    State-Space (name/joint/parameter):
+        - rootx     slider      position (m)
+        - rootz     slider      position (m)
+        - rooty     hinge       angle (rad)
+        - bthigh    hinge       angle (rad)
+        - bshin     hinge       angle (rad)
+        - bfoot     hinge       angle (rad)
+        - fthigh    hinge       angle (rad)
+        - fshin     hinge       angle (rad)
+        - ffoot     hinge       angle (rad)
+        - rootx     slider      velocity (m/s)
+        - rootz     slider      velocity (m/s)
+        - rooty     hinge       angular velocity (rad/s)
+        - bthigh    hinge       angular velocity (rad/s)
+        - bshin     hinge       angular velocity (rad/s)
+        - bfoot     hinge       angular velocity (rad/s)
+        - fthigh    hinge       angular velocity (rad/s)
+        - fshin     hinge       angular velocity (rad/s)
+        - ffoot     hinge       angular velocity (rad/s)
+
+    Actuators (name/actuator/parameter):
+        - bthigh    hinge       torque (N m)
+        - bshin     hinge       torque (N m)
+        - bfoot     hinge       torque (N m)
+        - fthigh    hinge       torque (N m)
+        - fshin     hinge       torque (N m)
+        - ffoot     hinge       torque (N m)
+
+-->
+<mujoco model="cheetah">
+  <compiler angle="radian" coordinate="local" inertiafromgeom="true" settotalmass="14"/>
+  <default>
+    <joint armature=".1" damping=".01" limited="true" solimplimit="0 .8 .03" solreflimit=".02 1" stiffness="8"/>
+    <geom conaffinity="0" condim="3" contype="1" friction=".4 .1 .1" rgba="0.8 0.6 .4 1" solimp="0.0 0.8 0.01" solref="0.02 1"/>
+    <motor ctrllimited="true" ctrlrange="-1 1"/>
+  </default>
+  <size nstack="300000" nuser_geom="1"/>
+  <option gravity="0 0 -9.81" timestep="0.01"/>
+  <asset>
+    <texture builtin="gradient" height="100" rgb1="1 1 1" rgb2="0 0 0" type="skybox" width="100"/>
+    <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+    <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+    <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="60 60" texture="texplane"/>
+    <material name="geom" texture="texgeom" texuniform="true"/>
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="40 40 40" type="plane"/>
+    <body name="torso" pos="0 0 .7">
+      <joint armature="0" axis="1 0 0" damping="0" limited="false" name="rootx" pos="0 0 0" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 0 1" damping="0" limited="false" name="rootz" pos="0 0 0" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 1 0" damping="0" limited="false" name="rooty" pos="0 0 0" stiffness="0" type="hinge"/>
+      <geom fromto="-.5 0 0 .5 0 0" name="torso" size="0.046" type="capsule"/>
+      <geom axisangle="0 1 0 .87" name="head" pos=".6 0 .1" size="0.046 .15" type="capsule"/>
+      <!-- <site name='tip'  pos='.15 0 .11'/>-->
+      <body name="bthigh" pos="-.5 0 0">
+        <joint axis="0 1 0" damping="6" name="bthigh" pos="0 0 0" range="-.52 1.05" stiffness="240" type="hinge"/>
+        <geom axisangle="0 1 0 -3.8" name="bthigh" pos=".1 0 -.13" size="0.046 .145" type="capsule"/>
+        <body name="bshin" pos=".16 0 -.25">
+          <joint axis="0 1 0" damping="4.5" name="bshin" pos="0 0 0" range="-.785 .785" stiffness="180" type="hinge"/>
+          <geom axisangle="0 1 0 -2.03" name="bshin" pos="-.14 0 -.07" rgba="0.9 0.6 0.6 1" size="0.046 .15" type="capsule"/>
+          <body name="bfoot" pos="-.28 0 -.14">
+            <joint axis="0 1 0" damping="3" name="bfoot" pos="0 0 0" range="-.4 .785" stiffness="120" type="hinge"/>
+            <geom axisangle="0 1 0 -.27" name="bfoot" pos=".03 0 -.097" rgba="0.9 0.6 0.6 1" size="0.046 .094" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <body name="fthigh" pos=".5 0 0">
+        <joint axis="0 1 0" damping="4.5" name="fthigh" pos="0 0 0" range="-1 .7" stiffness="180" type="hinge"/>
+        <geom axisangle="0 1 0 .52" name="fthigh" pos="-.07 0 -.12" size="0.046 .133" type="capsule"/>
+        <body name="fshin" pos="-.14 0 -.24">
+          <joint axis="0 1 0" damping="3" name="fshin" pos="0 0 0" range="-1.2 .87" stiffness="120" type="hinge"/>
+          <geom axisangle="0 1 0 -.6" name="fshin" pos=".065 0 -.09" rgba="0.9 0.6 0.6 1" size="0.046 .106" type="capsule"/>
+          <body name="ffoot" pos=".13 0 -.18">
+            <joint axis="0 1 0" damping="1.5" name="ffoot" pos="0 0 0" range="-.5 .5" stiffness="60" type="hinge"/>
+            <geom axisangle="0 1 0 -.6" name="ffoot" pos=".045 0 -.07" rgba="0.9 0.6 0.6 1" size="0.046 .07" type="capsule"/>
+          </body>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor gear="120" joint="bthigh" name="bthigh"/>
+    <motor gear="90" joint="bshin" name="bshin"/>
+    <motor gear="60" joint="bfoot" name="bfoot"/>
+    <motor gear="120" joint="fthigh" name="fthigh"/>
+    <motor gear="60" joint="fshin" name="fshin"/>
+    <motor gear="30" joint="ffoot" name="ffoot"/>
+  </actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/assets/hopper.xml b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/assets/hopper.xml
new file mode 100644
index 0000000..b0ebc0e
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/assets/hopper.xml
@@ -0,0 +1,44 @@
+<mujoco model="hopper">
+  <compiler angle="degree" coordinate="global" inertiafromgeom="true"/>
+  <default>
+    <joint armature="1" damping="1" limited="true"/>
+    <geom conaffinity="1" condim="1" contype="1" margin="0.001" material="geom" rgba="0.8 0.6 .4 1" solimp=".8 .8 .01" solref=".02 1"/>
+    <motor ctrllimited="true" ctrlrange="-.4 .4"/>
+  </default>
+  <option integrator="RK4" timestep="0.002"/>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="20 20 .125" type="plane" material="MatPlane"/>
+    <body name="torso" pos="0 0 1.25">
+      <joint armature="0" axis="1 0 0" damping="0" limited="false" name="rootx" pos="0 0 0" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 0 1" damping="0" limited="false" name="rootz" pos="0 0 0" ref="1.25" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 1 0" damping="0" limited="false" name="rooty" pos="0 0 1.25" stiffness="0" type="hinge"/>
+      <geom friction="0.9" fromto="0 0 1.45 0 0 1.05" name="torso_geom" size="0.05" type="capsule"/>
+      <body name="thigh" pos="0 0 1.05">
+        <joint axis="0 -1 0" name="thigh_joint" pos="0 0 1.05" range="-150 0" type="hinge"/>
+        <geom friction="0.9" fromto="0 0 1.05 0 0 0.6" name="thigh_geom" size="0.05" type="capsule"/>
+        <body name="leg" pos="0 0 0.35">
+          <joint axis="0 -1 0" name="leg_joint" pos="0 0 0.6" range="-150 0" type="hinge"/>
+          <geom friction="0.9" fromto="0 0 0.6 0 0 0.1" name="leg_geom" size="0.04" type="capsule"/>
+          <body name="foot" pos="0.13/2 0 0.1">
+            <joint axis="0 -1 0" name="foot_joint" pos="0 0 0.1" range="-45 45" type="hinge"/>
+            <geom friction="2.0" fromto="-0.13 0 0.1 0.26 0 0.1" name="foot_geom" size="0.06" type="capsule"/>
+          </body>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="thigh_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="leg_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="foot_joint"/>
+  </actuator>
+    <asset>
+        <texture type="skybox" builtin="gradient" rgb1=".4 .5 .6" rgb2="0 0 0"
+            width="100" height="100"/>        
+        <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+        <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+        <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="60 60" texture="texplane"/>
+        <material name="geom" texture="texgeom" texuniform="true"/>
+    </asset>
+</mujoco>
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/assets/point.xml b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/assets/point.xml
new file mode 100644
index 0000000..e35ef3d
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/assets/point.xml
@@ -0,0 +1,31 @@
+<mujoco>
+  <compiler angle="degree" coordinate="local" inertiafromgeom="true"/>
+  <option integrator="RK4" timestep="0.02"/>
+  <default>
+    <joint armature="0" damping="0" limited="false"/>
+    <geom conaffinity="0" condim="3" density="100" friction="1 0.5 0.5" margin="0" rgba="0.8 0.6 0.4 1"/>
+  </default>
+  <asset>
+    <texture builtin="gradient" height="100" rgb1="1 1 1" rgb2="0 0 0" type="skybox" width="100"/>
+    <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+    <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+    <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="30 30" texture="texplane"/>
+    <material name="geom" texture="texgeom" texuniform="true"/>
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="40 40 40" type="plane"/>
+    <body name="torso" pos="0 0 0">
+      <geom name="pointbody" pos="0 0 0.5" size="0.5" type="sphere"/>
+      <geom name="pointarrow" pos="0.6 0 0.5" size="0.5 0.1 0.1" type="box"/>
+      <joint axis="1 0 0" name="ballx" pos="0 0 0" type="slide"/>
+      <joint axis="0 1 0" name="bally" pos="0 0 0" type="slide"/>
+      <joint axis="0 0 1" limited="false" name="rot" pos="0 0 0" type="hinge"/>
+    </body>
+  </worldbody>
+  <actuator>
+    <!-- Those are just dummy actuators for providing ranges -->
+    <motor ctrllimited="true" ctrlrange="-1 1" joint="ballx"/>
+    <motor ctrllimited="true" ctrlrange="-0.25 0.25" joint="rot"/>
+  </actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/assets/pusher.xml b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/assets/pusher.xml
new file mode 100644
index 0000000..31a5ef7
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/assets/pusher.xml
@@ -0,0 +1,91 @@
+<mujoco model="arm3d">
+  <compiler inertiafromgeom="true" angle="radian" coordinate="local"/>
+  <option timestep="0.01" gravity="0 0 0" iterations="20" integrator="Euler" />
+  
+  <default>
+    <joint armature='0.04' damping="1" limited="true"/>
+    <geom friction=".8 .1 .1" density="300" margin="0.002" condim="1" contype="0" conaffinity="0"/>
+  </default>
+  
+  <worldbody>
+    <light diffuse=".5 .5 .5" pos="0 0 3" dir="0 0 -1"/>
+    <geom name="table" type="plane" pos="0 0.5 -0.325" size="1 1 0.1" contype="1" conaffinity="1"/>
+
+    <body name="r_shoulder_pan_link" pos="0 -0.6 0">
+      <geom name="e1" type="sphere" rgba="0.6 0.6 0.6 1" pos="-0.06 0.05 0.2" size="0.05" />
+      <geom name="e2" type="sphere" rgba="0.6 0.6 0.6 1" pos=" 0.06 0.05 0.2" size="0.05" />
+      <geom name="e1p" type="sphere" rgba="0.1 0.1 0.1 1" pos="-0.06 0.09 0.2" size="0.03" />
+      <geom name="e2p" type="sphere" rgba="0.1 0.1 0.1 1" pos=" 0.06 0.09 0.2" size="0.03" />
+      <geom name="sp" type="capsule" fromto="0 0 -0.4 0 0 0.2" size="0.1" />
+      <joint name="r_shoulder_pan_joint" type="hinge" pos="0 0 0" axis="0 0 1" range="-2.2854 1.714602" damping="1.0" />
+
+      <body name="r_shoulder_lift_link" pos="0.1 0 0">
+        <geom name="sl" type="capsule" fromto="0 -0.1 0 0 0.1 0" size="0.1" />
+        <joint name="r_shoulder_lift_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.5236 1.3963" damping="1.0" />
+
+        <body name="r_upper_arm_roll_link" pos="0 0 0">
+          <geom name="uar" type="capsule" fromto="-0.1 0 0 0.1 0 0" size="0.02" />
+          <joint name="r_upper_arm_roll_joint" type="hinge" pos="0 0 0" axis="1 0 0" range="-1.5 1.7" damping="0.1" />
+
+          <body name="r_upper_arm_link" pos="0 0 0">
+            <geom name="ua" type="capsule" fromto="0 0 0 0.4 0 0" size="0.06" />
+
+            <body name="r_elbow_flex_link" pos="0.4 0 0">
+              <geom name="ef" type="capsule" fromto="0 -0.02 0 0.0 0.02 0" size="0.06" />
+              <joint name="r_elbow_flex_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-2.3213 0" damping="0.1" />
+
+              <body name="r_forearm_roll_link" pos="0 0 0">
+                <geom name="fr" type="capsule" fromto="-0.1 0 0 0.1 0 0" size="0.02" />
+                <joint name="r_forearm_roll_joint" type="hinge" limited="true" pos="0 0 0" axis="1 0 0" damping=".1" range="-1.5 1.5"/>
+
+                <body name="r_forearm_link" pos="0 0 0">
+                  <geom name="fa" type="capsule" fromto="0 0 0 0.291 0 0" size="0.05" />
+
+                  <body name="r_wrist_flex_link" pos="0.321 0 0">
+                    <geom name="wf" type="capsule" fromto="0 -0.02 0 0 0.02 0" size="0.01" />
+                    <joint name="r_wrist_flex_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-1.094 0" damping=".1" />
+
+                    <body name="r_wrist_roll_link" pos="0 0 0">
+                      <joint name="r_wrist_roll_joint" type="hinge" pos="0 0 0" limited="true" axis="1 0 0" damping="0.1" range="-1.5 1.5"/>
+                      <body name="tips_arm" pos="0 0 0">
+                        <geom name="tip_arml" type="sphere" pos="0.1 -0.1 0." size="0.01" />
+                        <geom name="tip_armr" type="sphere" pos="0.1 0.1 0." size="0.01" />
+                      </body>
+                      <geom type="capsule" fromto="0 -0.1 0. 0.0 +0.1 0" size="0.02" contype="1" conaffinity="1" />
+                      <geom type="capsule" fromto="0 -0.1 0. 0.1 -0.1 0" size="0.02" contype="1" conaffinity="1" />
+                      <geom type="capsule" fromto="0 +0.1 0. 0.1 +0.1 0." size="0.02" contype="1" conaffinity="1" />
+                    </body>
+                  </body>
+                </body>
+              </body>
+            </body>
+          </body>
+        </body>
+      </body>
+    </body>
+
+    <!--<body name="object" pos="0.55 -0.3 -0.275" >-->
+    <body name="object" pos="0.45 -0.05 -0.275" >
+      <geom rgba="1 1 1 0" type="sphere" size="0.05 0.05 0.05" density="0.00001" conaffinity="0"/>
+      <geom rgba="1 1 1 1" type="cylinder" size="0.05 0.05 0.05" density="0.00001" contype="1" conaffinity="0"/>
+      <joint name="obj_slidey" type="slide" pos="0 0 0" axis="0 1 0" range="-10.3213 10.3" damping="0.5"/>
+      <joint name="obj_slidex" type="slide" pos="0 0 0" axis="1 0 0" range="-10.3213 10.3" damping="0.5"/>
+    </body>
+
+    <body name="goal" pos="0.45 -0.05 -0.3230">
+      <geom rgba="1 0 0 1" type="cylinder" size="0.08 0.001 0.1" density='0.00001' contype="0" conaffinity="0"/>
+      <joint name="goal_slidey" type="slide" pos="0 0 0" axis="0 1 0" range="-10.3213 10.3" damping="0.5"/>
+      <joint name="goal_slidex" type="slide" pos="0 0 0" axis="1 0 0" range="-10.3213 10.3" damping="0.5"/> 
+    </body>
+  </worldbody>
+
+  <actuator>
+    <motor joint="r_shoulder_pan_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_shoulder_lift_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_upper_arm_roll_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_elbow_flex_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_forearm_roll_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_wrist_flex_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_wrist_roll_joint" ctrlrange="-2.0 2.0" ctrllimited="true"/>
+  </actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/assets/reacher.xml b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/assets/reacher.xml
new file mode 100644
index 0000000..64a67b9
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/assets/reacher.xml
@@ -0,0 +1,39 @@
+<mujoco model="reacher">
+	<compiler angle="radian" inertiafromgeom="true"/>
+	<default>
+		<joint armature="1" damping="1" limited="true"/>
+		<geom contype="0" friction="1 0.1 0.1" rgba="0.7 0.7 0 1"/>
+	</default>
+	<option gravity="0 0 -9.81" integrator="RK4" timestep="0.01"/>
+	<worldbody>
+		<!-- Arena -->
+		<geom conaffinity="0" contype="0" name="ground" pos="0 0 0" rgba="0.9 0.9 0.9 1" size="1 1 10" type="plane"/>
+		<geom conaffinity="0" fromto="-.3 -.3 .01 .3 -.3 .01" name="sideS" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
+		<geom conaffinity="0" fromto=" .3 -.3 .01 .3  .3 .01" name="sideE" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
+		<geom conaffinity="0" fromto="-.3  .3 .01 .3  .3 .01" name="sideN" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
+		<geom conaffinity="0" fromto="-.3 -.3 .01 -.3 .3 .01" name="sideW" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
+		<!-- Arm -->
+		<geom conaffinity="0" contype="0" fromto="0 0 0 0 0 0.02" name="root" rgba="0.9 0.4 0.6 1" size=".011" type="cylinder"/>
+		<body name="body0" pos="0 0 .01">
+			<geom fromto="0 0 0 0.1 0 0" name="link0" rgba="0.0 0.4 0.6 1" size=".01" type="capsule"/>
+			<joint axis="0 0 1" limited="false" name="joint0" pos="0 0 0" type="hinge"/>
+			<body name="body1" pos="0.1 0 0">
+				<joint axis="0 0 1" limited="true" name="joint1" pos="0 0 0" range="-3.0 3.0" type="hinge"/>
+				<geom fromto="0 0 0 0.1 0 0" name="link1" rgba="0.0 0.4 0.6 1" size=".01" type="capsule"/>
+				<body name="fingertip" pos="0.11 0 0">
+					<geom contype="0" name="fingertip" pos="0 0 0" rgba="0.0 0.8 0.6 1" size=".01" type="sphere"/>
+				</body>
+			</body>
+		</body>
+		<!-- Target -->
+		<body name="target" pos=".1 -.1 .01">
+			<joint armature="0" axis="1 0 0" damping="0" limited="true" name="target_x" pos="0 0 0" range="-.27 .27" ref=".1" stiffness="0" type="slide"/>
+			<joint armature="0" axis="0 1 0" damping="0" limited="true" name="target_y" pos="0 0 0" range="-.27 .27" ref="-.1" stiffness="0" type="slide"/>
+			<geom conaffinity="0" contype="0" name="target" pos="0 0 0" rgba="0.9 0.2 0.2 1" size=".009" type="sphere"/>
+		</body>
+	</worldbody>
+	<actuator>
+		<motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="joint0"/>
+		<motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="joint1"/>
+	</actuator>
+</mujoco>
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/assets/swimmer.xml b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/assets/swimmer.xml
new file mode 100644
index 0000000..cda25da
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/assets/swimmer.xml
@@ -0,0 +1,38 @@
+<mujoco model="swimmer">
+  <compiler angle="degree" coordinate="local" inertiafromgeom="true"/>
+  <option collision="predefined" density="4000" integrator="RK4" timestep="0.01" viscosity="0.1"/>
+  <default>
+    <geom conaffinity="1" condim="1" contype="1" material="geom" rgba="0.8 0.6 .4 1"/>
+    <joint armature='0.1'  />
+  </default>
+  <asset>
+    <texture builtin="gradient" height="100" rgb1="1 1 1" rgb2="0 0 0" type="skybox" width="100"/>
+    <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+    <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+    <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="30 30" texture="texplane"/>
+    <material name="geom" texture="texgeom" texuniform="true"/>
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 -0.1" rgba="0.8 0.9 0.8 1" size="40 40 0.1" type="plane"/>
+    <!--  ================= SWIMMER ================= /-->
+    <body name="torso" pos="0 0 0">
+      <geom density="1000" fromto="1.5 0 0 0.5 0 0" size="0.1" type="capsule"/>
+      <joint axis="1 0 0" name="slider1" pos="0 0 0" type="slide"/>
+      <joint axis="0 1 0" name="slider2" pos="0 0 0" type="slide"/>
+      <joint axis="0 0 1" name="rot" pos="0 0 0" type="hinge"/>
+      <body name="mid" pos="0.5 0 0">
+        <geom density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule"/>
+        <joint axis="0 0 1" limited="true" name="rot2" pos="0 0 0" range="-100 100" type="hinge"/>
+        <body name="back" pos="-1 0 0">
+          <geom density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule"/>
+          <joint axis="0 0 1" limited="true" name="rot3" pos="0 0 0" range="-100 100" type="hinge"/>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot2"/>
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot3"/>
+  </actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/assets/walker2d.xml b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/assets/walker2d.xml
new file mode 100644
index 0000000..cbc074d
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/assets/walker2d.xml
@@ -0,0 +1,61 @@
+<mujoco model="walker2d">
+  <compiler angle="degree" coordinate="global" inertiafromgeom="true"/>
+  <default>
+    <joint armature="0.01" damping=".1" limited="true"/>
+    <geom conaffinity="0" condim="3" contype="1" density="1000" friction=".7 .1 .1" rgba="0.8 0.6 .4 1"/>
+  </default>
+  <option integrator="RK4" timestep="0.002"/>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="40 40 40" type="plane" material="MatPlane"/>
+    <body name="torso" pos="0 0 1.25">
+      <joint armature="0" axis="1 0 0" damping="0" limited="false" name="rootx" pos="0 0 0" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 0 1" damping="0" limited="false" name="rootz" pos="0 0 0" ref="1.25" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 1 0" damping="0" limited="false" name="rooty" pos="0 0 1.25" stiffness="0" type="hinge"/>
+      <geom friction="0.9" fromto="0 0 1.45 0 0 1.05" name="torso_geom" size="0.05" type="capsule"/>
+      <body name="thigh" pos="0 0 1.05">
+        <joint axis="0 -1 0" name="thigh_joint" pos="0 0 1.05" range="-150 0" type="hinge"/>
+        <geom friction="0.9" fromto="0 0 1.05 0 0 0.6" name="thigh_geom" size="0.05" type="capsule"/>
+        <body name="leg" pos="0 0 0.35">
+          <joint axis="0 -1 0" name="leg_joint" pos="0 0 0.6" range="-150 0" type="hinge"/>
+          <geom friction="0.9" fromto="0 0 0.6 0 0 0.1" name="leg_geom" size="0.04" type="capsule"/>
+          <body name="foot" pos="0.2/2 0 0.1">
+            <joint axis="0 -1 0" name="foot_joint" pos="0 0 0.1" range="-45 45" type="hinge"/>
+            <geom friction="0.9" fromto="-0.0 0 0.1 0.2 0 0.1" name="foot_geom" size="0.06" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <!-- copied and then replace thigh->thigh_left, leg->leg_left, foot->foot_right -->
+      <body name="thigh_left" pos="0 0 1.05">
+        <joint axis="0 -1 0" name="thigh_left_joint" pos="0 0 1.05" range="-150 0" type="hinge"/>
+        <geom friction="0.9" fromto="0 0 1.05 0 0 0.6" name="thigh_left_geom" rgba=".7 .3 .6 1" size="0.05" type="capsule"/>
+        <body name="leg_left" pos="0 0 0.35">
+          <joint axis="0 -1 0" name="leg_left_joint" pos="0 0 0.6" range="-150 0" type="hinge"/>
+          <geom friction="0.9" fromto="0 0 0.6 0 0 0.1" name="leg_left_geom" rgba=".7 .3 .6 1" size="0.04" type="capsule"/>
+          <body name="foot_left" pos="0.2/2 0 0.1">
+            <joint axis="0 -1 0" name="foot_left_joint" pos="0 0 0.1" range="-45 45" type="hinge"/>
+            <geom friction="1.9" fromto="-0.0 0 0.1 0.2 0 0.1" name="foot_left_geom" rgba=".7 .3 .6 1" size="0.06" type="capsule"/>
+          </body>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <!-- <motor joint="torso_joint" ctrlrange="-100.0 100.0" isctrllimited="true"/>-->
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="100" joint="thigh_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="100" joint="leg_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="100" joint="foot_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="100" joint="thigh_left_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="100" joint="leg_left_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="100" joint="foot_left_joint"/>
+    <!-- <motor joint="finger2_rot" ctrlrange="-20.0 20.0" isctrllimited="true"/>-->
+  </actuator>
+    <asset>
+        <texture type="skybox" builtin="gradient" rgb1=".4 .5 .6" rgb2="0 0 0"
+            width="100" height="100"/>        
+        <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+        <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+        <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="60 60" texture="texplane"/>
+        <material name="geom" texture="texgeom" texuniform="true"/>
+    </asset>
+</mujoco>
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/__init__.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/__init__.py
new file mode 100644
index 0000000..eff23c0
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/__init__.py
@@ -0,0 +1,210 @@
+from gym.envs.registration import register
+
+register(
+    id='MBRLHalfCheetah-v0',
+    entry_point='envs.gym.half_cheetah:HalfCheetahEnv',
+    kwargs={'frame_skip': 5},
+    max_episode_steps=1000,
+)
+
+register(
+    id='MBRLWalker2d-v0',
+    entry_point='envs.gym.walker2d:Walker2dEnv',
+    kwargs={'frame_skip': 4},
+    max_episode_steps=1000,
+)
+
+register(
+    id='MBRLSwimmer-v0',
+    entry_point='envs.gym.swimmer:SwimmerEnv',
+    kwargs={'frame_skip': 4},
+    max_episode_steps=1000,
+)
+
+register(
+    id='MBRLAnt-v0',
+    entry_point='envs.gym.ant:AntEnv',
+    kwargs={'frame_skip': 5},
+    max_episode_steps=1000,
+)
+
+register(
+    id='MBRLHopper-v0',
+    entry_point='envs.gym.hopper:HopperEnv',
+    kwargs={'frame_skip': 4},
+    max_episode_steps=1000,
+)
+
+register(
+    id='MBRLReacher-v0',
+    entry_point='envs.gym.reacher:ReacherEnv',
+    max_episode_steps=50,
+)
+
+
+# second batch of environments
+
+register(
+    id='MBRLInvertedPendulum-v0',
+    entry_point='envs.gym.inverted_pendulum:InvertedPendulumEnv',
+    max_episode_steps=100,
+)
+register(
+    id='MBRLAcrobot-v0',
+    entry_point='envs.gym.acrobot:AcrobotEnv',
+    max_episode_steps=200,
+)
+register(
+    id='MBRLCartpole-v0',
+    entry_point='envs.gym.cartpole:CartPoleEnv',
+    max_episode_steps=200,
+
+)
+register(
+    id='MBRLMountain-v0',
+    entry_point='envs.gym.mountain_car:Continuous_MountainCarEnv',
+    max_episode_steps=200,
+
+)
+register(
+    id='MBRLPendulum-v0',
+    entry_point='envs.gym.pendulum:PendulumEnv',
+    max_episode_steps=200,
+)
+
+register(
+    id='gym_petsPusher-v0',
+    entry_point='envs.gym.pets_pusher:PusherEnv',
+    max_episode_steps=150,
+)
+register(
+    id='gym_petsReacher-v0',
+    entry_point='envs.gym.pets_reacher:Reacher3DEnv',
+    max_episode_steps=150,
+)
+register(
+    id='gym_petsCheetah-v0',
+    entry_point='envs.gym.pets_cheetah:HalfCheetahEnv',
+    max_episode_steps=1000,
+)
+
+# noisy env
+register(
+    id='gym_cheetahO01-v0',
+    entry_point='envs.gym.gym_cheetahO01:HalfCheetahEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_cheetahO001-v0',
+    entry_point='envs.gym.gym_cheetahO001:HalfCheetahEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_cheetahA01-v0',
+    entry_point='envs.gym.gym_cheetahA01:HalfCheetahEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_cheetahA003-v0',
+    entry_point='envs.gym.gym_cheetahA003:HalfCheetahEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_pendulumO01-v0',
+    entry_point='envs.gym.gym_pendulumO01:PendulumEnv',
+    max_episode_steps=200,
+)
+register(
+    id='gym_pendulumO001-v0',
+    entry_point='envs.gym.gym_pendulumO001:PendulumEnv',
+    max_episode_steps=200,
+)
+register(
+    id='gym_cartpoleO01-v0',
+    entry_point='envs.gym.gym_cartpoleO01:CartPoleEnv',
+    max_episode_steps=200,
+)
+register(
+    id='gym_cartpoleO001-v0',
+    entry_point='envs.gym.gym_cartpoleO001:CartPoleEnv',
+    max_episode_steps=200,
+)
+
+register(
+    id='gym_fant-v0',
+    entry_point='envs.gym.gym_fant:AntEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_fhopper-v0',
+    entry_point='envs.gym.gym_fhopper:HopperEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_fwalker2d-v0',
+    entry_point='envs.gym.gym_fwalker2d:Walker2dEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_fswimmer-v0',
+    entry_point='envs.gym.gym_fswimmer:fixedSwimmerEnv',
+    max_episode_steps=1000,
+)
+register(
+    id="gym_humanoid-v0",
+    entry_point='envs.gym.gym_humanoid:HumanoidEnv',
+    max_episode_steps=1000,
+)
+register(
+    id="gym_slimhumanoid-v0",
+    entry_point='envs.gym.gym_slimhumanoid:HumanoidEnv',
+    max_episode_steps=1000,
+)
+register(
+    id="gym_nostopslimhumanoid-v0",
+    entry_point='envs.gym.gym_nostopslimhumanoid:HumanoidEnv',
+    max_episode_steps=1000,
+)
+
+env_name_to_gym_registry = {
+    # first batch
+    "half_cheetah": "MBRLHalfCheetah-v0",
+    "swimmer": "MBRLSwimmer-v0",
+    "ant": "MBRLAnt-v0",
+    "hopper": "MBRLHopper-v0",
+    "reacher": "MBRLReacher-v0",
+    "walker2d": "MBRLWalker2d-v0",
+
+    # second batch
+    "invertedPendulum": "MBRLInvertedPendulum-v0",
+    "acrobot": 'MBRLAcrobot-v0',
+    "cartpole": 'MBRLCartpole-v0',
+    "mountain": 'MBRLMountain-v0',
+    "pendulum": 'MBRLPendulum-v0',
+
+    # the pets env
+    "gym_petsPusher": "gym_petsPusher-v0",
+    "gym_petsReacher": "gym_petsReacher-v0",
+    "gym_petsCheetah": "gym_petsCheetah-v0",
+
+    # the noise env
+    "gym_cheetahO01": "gym_cheetahO01-v0",
+    "gym_cheetahO001": "gym_cheetahO001-v0",
+    "gym_cheetahA01": "gym_cheetahA01-v0",
+    "gym_cheetahA003": "gym_cheetahA003-v0",
+
+    "gym_pendulumO01": "gym_pendulumO01-v0",
+    "gym_pendulumO001": "gym_pendulumO001-v0",
+
+    "gym_cartpoleO01": "gym_cartpoleO01-v0",
+    "gym_cartpoleO001": "gym_cartpoleO001-v0",
+
+    "gym_fant": "gym_fant-v0",
+    "gym_fswimmer": "gym_fswimmer-v0",
+    "gym_fhopper": "gym_fhopper-v0",
+    "gym_fwalker2d": "gym_fwalker2d-v0",
+
+    "gym_humanoid": "gym_humanoid-v0",
+    "gym_slimhumanoid": "gym_slimhumanoid-v0",
+    "gym_nostopslimhumanoid": "gym_nostopslimhumanoid-v0",
+}
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/acrobot.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/acrobot.py
new file mode 100644
index 0000000..1af57a6
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/acrobot.py
@@ -0,0 +1,357 @@
+"""classic Acrobot task"""
+from gym import core, spaces
+from gym.utils import seeding
+import numpy as np
+from numpy import sin, cos, pi
+from slbo.utils.dataset import Dataset, gen_dtype
+from lunzi.Logger import logger
+
+
+__copyright__ = "Copyright 2013, RLPy http://acl.mit.edu/RLPy"
+__credits__ = ["Alborz Geramifard", "Robert H. Klein", "Christoph Dann",
+               "William Dabney", "Jonathan P. How"]
+__license__ = "BSD 3-Clause"
+__author__ = "Christoph Dann <cdann@cdann.de>"
+
+# SOURCE:
+# https://github.com/rlpy/rlpy/blob/master/rlpy/Domains/Acrobot.py
+
+
+class AcrobotEnv(core.Env):
+
+    """
+    Acrobot is a 2-link pendulum with only the second joint actuated
+    Intitially, both links point downwards. The goal is to swing the
+    end-effector at a height at least the length of one link above the base.
+    Both links can swing freely and can pass by each other, i.e., they don't
+    collide when they have the same angle.
+    **STATE:**
+    The state consists of the two rotational joint angles and their velocities
+    [theta1 theta2 thetaDot1 thetaDot2]. An angle of 0 corresponds to corresponds
+    to the respective link pointing downwards (angles are in world coordinates).
+    **ACTIONS:**
+    The action is either applying +1, 0 or -1 torque on the joint between
+    the two pendulum links.
+    .. note::
+        The dynamics equations were missing some terms in the NIPS paper which
+        are present in the book. R. Sutton confirmed in personal correspondance
+        that the experimental results shown in the paper and the book were
+        generated with the equations shown in the book.
+        However, there is the option to run the domain with the paper equations
+        by setting book_or_nips = 'nips'
+    **REFERENCE:**
+    .. seealso::
+        R. Sutton: Generalization in Reinforcement Learning:
+        Successful Examples Using Sparse Coarse Coding (NIPS 1996)
+    .. seealso::
+        R. Sutton and A. G. Barto:
+        Reinforcement learning: An introduction.
+        Cambridge: MIT press, 1998.
+    .. warning::
+        This version of the domain uses the Runge-Kutta method for integrating
+        the system dynamics and is more realistic, but also considerably harder
+        than the original version which employs Euler integration,
+        see the AcrobotLegacy class.
+    """
+
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 15
+    }
+
+    dt = .2
+
+    LINK_LENGTH_1 = 1.  # [m]
+    LINK_LENGTH_2 = 1.  # [m]
+    LINK_MASS_1 = 1.  #: [kg] mass of link 1
+    LINK_MASS_2 = 1.  #: [kg] mass of link 2
+    LINK_COM_POS_1 = 0.5  #: [m] position of the center of mass of link 1
+    LINK_COM_POS_2 = 0.5  #: [m] position of the center of mass of link 2
+    LINK_MOI = 1.  #: moments of inertia for both links
+
+    MAX_VEL_1 = 4 * np.pi
+    MAX_VEL_2 = 9 * np.pi
+
+    AVAIL_TORQUE = [-1., 0., +1]
+
+    torque_noise_max = 0.
+
+    #: use dynamics equations from the nips paper or the book
+    book_or_nips = "book"
+    action_arrow = None
+    domain_fig = None
+    actions_num = 3
+
+    def __init__(self):
+        self.viewer = None
+        high = np.array([1.0, 1.0, 1.0, 1.0, self.MAX_VEL_1, self.MAX_VEL_2])
+        low = -high
+        self.observation_space = spaces.Box(low, high)
+        self.action_space = spaces.Box(np.array([-1.0]), np.array([1.0]))
+        self.state = None
+        self._seed()
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _reset(self):
+        self.state = self.np_random.uniform(low=-0.1, high=0.1, size=(4,))
+        return self._get_ob()
+
+    def _step(self, a):
+        # Discretize
+        if a[0] < -.33:
+            action = 0
+        elif a[0] < .33:
+            action = 1
+        else:
+            action = 2
+
+        s = self.state
+        reward = -np.cos(s[0]) - np.cos(s[1] + s[0])
+        torque = self.AVAIL_TORQUE[action]
+
+        # Add noise to the force action
+        if self.torque_noise_max > 0:
+            torque += self.np_random.uniform(-self.torque_noise_max, self.torque_noise_max)
+
+        # Now, augment the state with our force action so it can be passed to
+        # _dsdt
+        s_augmented = np.append(s, torque)
+
+        ns = rk4(self._dsdt, s_augmented, [0, self.dt])
+        # only care about final timestep of integration returned by integrator
+        ns = ns[-1]
+        ns = ns[:4]  # omit action
+        # ODEINT IS TOO SLOW!
+        # ns_continuous = integrate.odeint(self._dsdt, self.s_continuous, [0, self.dt])
+        # self.s_continuous = ns_continuous[-1] # We only care about the state
+        # at the ''final timestep'', self.dt
+
+        ns[0] = wrap(ns[0], -pi, pi)
+        ns[1] = wrap(ns[1], -pi, pi)
+        ns[2] = bound(ns[2], -self.MAX_VEL_1, self.MAX_VEL_1)
+        ns[3] = bound(ns[3], -self.MAX_VEL_2, self.MAX_VEL_2)
+        self.state = ns
+        # terminal = self._terminal()
+        terminal = False
+        # reward = -1. if not terminal else 0.
+        return (self._get_ob(), reward, terminal, {})
+
+    def _get_ob(self):
+        s = self.state
+        return np.array([cos(s[0]), np.sin(s[0]), cos(s[1]), sin(s[1]), s[2], s[3]])
+
+    def _terminal(self):
+        s = self.state
+        return bool(-np.cos(s[0]) - np.cos(s[1] + s[0]) > 1.)
+
+    def _dsdt(self, s_augmented, t):
+        m1 = self.LINK_MASS_1
+        m2 = self.LINK_MASS_2
+        l1 = self.LINK_LENGTH_1
+        lc1 = self.LINK_COM_POS_1
+        lc2 = self.LINK_COM_POS_2
+        I1 = self.LINK_MOI
+        I2 = self.LINK_MOI
+        g = 9.8
+        a = s_augmented[-1]
+        s = s_augmented[:-1]
+        theta1 = s[0]
+        theta2 = s[1]
+        dtheta1 = s[2]
+        dtheta2 = s[3]
+        d1 = m1 * lc1 ** 2 + m2 * \
+            (l1 ** 2 + lc2 ** 2 + 2 * l1 * lc2 * np.cos(theta2)) + I1 + I2
+        d2 = m2 * (lc2 ** 2 + l1 * lc2 * np.cos(theta2)) + I2
+        phi2 = m2 * lc2 * g * np.cos(theta1 + theta2 - np.pi / 2.)
+        phi1 = - m2 * l1 * lc2 * dtheta2 ** 2 * np.sin(theta2) \
+               - 2 * m2 * l1 * lc2 * dtheta2 * dtheta1 * np.sin(theta2)  \
+            + (m1 * lc1 + m2 * l1) * g * np.cos(theta1 - np.pi / 2) + phi2
+        if self.book_or_nips == "nips":
+            # the following line is consistent with the description in the
+            # paper
+            ddtheta2 = (a + d2 / d1 * phi1 - phi2) / \
+                (m2 * lc2 ** 2 + I2 - d2 ** 2 / d1)
+        else:
+            # the following line is consistent with the java implementation and the
+            # book
+            ddtheta2 = (a + d2 / d1 * phi1 - m2 * l1 * lc2 * dtheta1 ** 2 * np.sin(theta2) - phi2) \
+                / (m2 * lc2 ** 2 + I2 - d2 ** 2 / d1)
+        ddtheta1 = -(d2 * ddtheta2 + phi1) / d1
+        return (dtheta1, dtheta2, ddtheta1, ddtheta2, 0.)
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+        from gym.envs.classic_control import rendering
+
+        s = self.state
+
+        if self.viewer is None:
+            self.viewer = rendering.Viewer(500, 500)
+            self.viewer.set_bounds(-2.2, 2.2, -2.2, 2.2)
+
+        if s is None:
+            return None
+
+        p1 = [-self.LINK_LENGTH_1 *
+              np.cos(s[0]), self.LINK_LENGTH_1 * np.sin(s[0])]
+
+        p2 = [p1[0] - self.LINK_LENGTH_2 * np.cos(s[0] + s[1]),
+              p1[1] + self.LINK_LENGTH_2 * np.sin(s[0] + s[1])]
+
+        xys = np.array([[0, 0], p1, p2])[:, ::-1]
+        thetas = [s[0] - np.pi / 2, s[0] + s[1] - np.pi / 2]
+
+        self.viewer.draw_line((-2.2, 1), (2.2, 1))
+        for ((x, y), th) in zip(xys, thetas):
+            l, r, t, b = 0, 1, .1, -.1
+            jtransform = rendering.Transform(rotation=th, translation=(x, y))
+            link = self.viewer.draw_polygon([(l, b), (l, t), (r, t), (r, b)])
+            link.add_attr(jtransform)
+            link.set_color(0, .8, .8)
+            circ = self.viewer.draw_circle(.1)
+            circ.set_color(.8, .8, 0)
+            circ.add_attr(jtransform)
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+            def height(obs):
+                h1 = obs[0]  # Height of first arm
+                h2 = obs[0] * obs[2] - obs[1] * obs[3]  # Height of second arm
+                return -(h1 + h2)  # total height
+
+            start_height = height(data_dict['start_state'])
+        """
+        h1 = obs[:, 0]  # Height of first arm
+        h2 = obs[:, 0] * obs[:, 2] - obs[:, 1] * obs[:, 3]  # Height of second arm
+        return (h1 + h2)
+
+    def verify(self, n=2000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
+
+
+def wrap(x, m, M):
+    """
+    :param x: a scalar
+    :param m: minimum possible value in range
+    :param M: maximum possible value in range
+    Wraps ``x`` so m <= x <= M; but unlike ``bound()`` which
+    truncates, ``wrap()`` wraps x around the coordinate system defined by m,M.\n
+    For example, m = -180, M = 180 (degrees), x = 360 --> returns 0.
+    """
+    diff = M - m
+    while x > M:
+        x = x - diff
+    while x < m:
+        x = x + diff
+    return x
+
+
+def bound(x, m, M=None):
+    """
+    :param x: scalar
+    Either have m as scalar, so bound(x,m,M) which returns m <= x <= M *OR*
+    have m as length 2 vector, bound(x,m, <IGNORED>) returns m[0] <= x <= m[1].
+    """
+    if M is None:
+        M = m[1]
+        m = m[0]
+    # bound x between min (m) and Max (M)
+    return min(max(x, m), M)
+
+
+def rk4(derivs, y0, t, *args, **kwargs):
+    """
+    Integrate 1D or ND system of ODEs using 4-th order Runge-Kutta.
+    This is a toy implementation which may be useful if you find
+    yourself stranded on a system w/o scipy.  Otherwise use
+    :func:`scipy.integrate`.
+    *y0*
+        initial state vector
+    *t*
+        sample times
+    *derivs*
+        returns the derivative of the system and has the
+        signature ``dy = derivs(yi, ti)``
+    *args*
+        additional arguments passed to the derivative function
+    *kwargs*
+        additional keyword arguments passed to the derivative function
+    Example 1 ::
+        ## 2D system
+        def derivs6(x,t):
+            d1 =  x[0] + 2*x[1]
+            d2 =  -3*x[0] + 4*x[1]
+            return (d1, d2)
+        dt = 0.0005
+        t = arange(0.0, 2.0, dt)
+        y0 = (1,2)
+        yout = rk4(derivs6, y0, t)
+    Example 2::
+        ## 1D system
+        alpha = 2
+        def derivs(x,t):
+            return -alpha*x + exp(-t)
+        y0 = 1
+        yout = rk4(derivs, y0, t)
+    If you have access to scipy, you should probably be using the
+    scipy.integrate tools rather than this function.
+    """
+
+    try:
+        Ny = len(y0)
+    except TypeError:
+        yout = np.zeros((len(t),), np.float_)
+    else:
+        yout = np.zeros((len(t), Ny), np.float_)
+
+    yout[0] = y0
+    i = 0
+
+    for i in np.arange(len(t) - 1):
+
+        thist = t[i]
+        dt = t[i + 1] - thist
+        dt2 = dt / 2.0
+        y0 = yout[i]
+
+        k1 = np.asarray(derivs(y0, thist, *args, **kwargs))
+        k2 = np.asarray(derivs(y0 + dt2 * k1, thist + dt2, *args, **kwargs))
+        k3 = np.asarray(derivs(y0 + dt2 * k2, thist + dt2, *args, **kwargs))
+        k4 = np.asarray(derivs(y0 + dt * k3, thist + dt, *args, **kwargs))
+        yout[i + 1] = y0 + dt / 6.0 * (k1 + 2 * k2 + 2 * k3 + k4)
+    return yout
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/ant.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/ant.py
new file mode 100644
index 0000000..a2699aa
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/ant.py
@@ -0,0 +1,81 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class AntEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/ant.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        ob = self._get_obs()
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = old_ob[13]
+        reward_height = -3.0 * np.square(old_ob[0] - 0.57)
+        reward = reward_run + reward_ctrl + reward_height + 1.0
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            # (self.model.data.qpos.flat[:1] - self.prev_qpos[:1]) / self.dt,
+            # self.get_body_comvel("torso")[:1],
+            self.model.data.qpos.flat[2:],
+            self.model.data.qvel.flat,
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        qpos = self.init_qpos + self.np_random.uniform(size=self.model.nq, low=-.1, high=.1)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        # self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 13]
+        reward_height = -3.0 * np.square(obs[:, 0] - 0.57)
+        reward = reward_run + reward_ctrl + reward_height + 1.0
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        # reward_height = -3.0 * tf.square(next_obs[:, 0] - 0.57)
+        reward = reward_run + reward_ctrl # + reward_height
+        return -reward
+        """
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/cartpole.xml b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/cartpole.xml
new file mode 100644
index 0000000..284a58c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/cartpole.xml
@@ -0,0 +1,35 @@
+<mujoco model="cartpole">
+	<compiler inertiafromgeom="true"/>
+	<default>
+		<joint armature="0" damping="1" limited="true"/>
+		<geom contype="0" friction="1 0.1 0.1" rgba="0.7 0.7 0 1"/>
+		<tendon/>
+		<motor ctrlrange="-3 3" ctrllimited='true'/>
+	</default>
+    <asset>
+		<texture type="skybox" builtin="checker" rgb1="1 1 1" rgb2="1 1 1"
+                    width="256" height="256"/>
+        <texture name="texgeom" type="cube" builtin="flat" mark="cross" width="127" height="1278" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" markrgb="1 1 1" random="0.01" />
+		<texture name="texplane" type="2d" builtin="checker" rgb1=".5 .5 .5" rgb2=".5 .5 .5" width="100" height="100" />
+        <texture name="texplane_show" type="2d" builtin="checker" rgb1="0 0 0" rgb2="0.9 0.9 0.9" width="100" height="100" />
+        <material name='MatPlane' texture="texplane" shininess="1" texrepeat="30 30" specular="1"  reflectance="0.5" />
+        <material name='geom' texture="texgeom" texuniform="true" />
+ 	</asset>
+	<option gravity="0 0 -9.81" integrator="RK4" timestep="0.02"/>
+	<size nstack="3000"/>
+	<worldbody>
+		<geom name="rail" pos="0 0 0" quat="0.707 0 0.707 0" rgba="0.3 0.3 0.7 1" size="0.02 3" type="capsule"/>
+		<body name="cart" pos="0 0 0">
+			<joint axis="1 0 0" limited="true" name="slider" pos="0 0 0" range="-2.5 2.5" type="slide"/>
+			<geom name="cart" pos="0 0 0" quat="0.707 0 0.707 0" size="0.1 0.1" type="capsule"/>
+			<body name="pole" pos="0 0 0">
+				<joint axis="0 1 0" limited="false" name="hinge" pos="0 0 0" range="-180 180" type="hinge"/>
+				<geom fromto="0 0 0 0.001 0 -0.6" name="cpole" rgba="0 0.7 0.7 1" size="0.049 0.3" type="capsule"/>
+			</body>
+		</body>
+	</worldbody>
+	<actuator>
+		<motor gear="100" joint="slider" name="slide"/>
+	</actuator>
+</mujoco>
+
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/fixed_swimmer.xml b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/fixed_swimmer.xml
new file mode 100644
index 0000000..142f344
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/fixed_swimmer.xml
@@ -0,0 +1,43 @@
+<mujoco model="swimmer">
+  <compiler angle="degree" coordinate="local" inertiafromgeom="true"/>
+  <option collision="predefined" density="4000" integrator="RK4" timestep="0.01" viscosity="0.1"/>
+  <default>
+    <geom conaffinity="1" condim="1" contype="1" material="geom" rgba="0.8 0.6 .4 1"/>
+    <joint armature='0.1'  />
+  </default>
+  <asset>
+    <texture builtin="gradient" height="100" rgb1="1 1 1" rgb2="0 0 0" type="skybox" width="100"/>
+    <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+    <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+    <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="30 30" texture="texplane"/>
+    <material name="geom" texture="texgeom" texuniform="true"/>
+  </asset>
+
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 -0.1" rgba="0.8 0.9 0.8 1" size="40 40 0.1" type="plane"/>
+    <body name="podBody_1" pos="0 0 0">
+      <geom name='pod_1' density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule"/>
+      <joint axis="1 0 0" name="slider1" pos="0 0 0" type="slide"/>
+      <joint axis="0 1 0" name="slider2" pos="0 0 0" type="slide"/>
+      <joint axis="0 0 1" name="rot_1" pos="-1.5 0 0" type="hinge"/>
+      <site name="tip" pos="0 0 0" size="0.02 0.02"/>
+      <body name="podBody_2" pos="-1 0 0">
+        <geom name='pod_2' density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule"/>
+        <joint axis="0 0 1" limited="true" name="rot_2" pos="0 0 0" range="-100 100" type="hinge"/>
+        
+        <body name="podBody_3" pos="-1 0 0">
+          <geom name='pod_3' density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule"/>
+          <joint axis="0 0 1" limited="true" name="rot_3" pos="0 0 0" range="-100 100" type="hinge"/>
+        
+        </body>
+
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot_2"/>
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot_3"/>
+  </actuator>
+
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/half_cheetah.xml b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/half_cheetah.xml
new file mode 100644
index 0000000..40a1cb6
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/half_cheetah.xml
@@ -0,0 +1,95 @@
+<!-- Cheetah Model
+
+    The state space is populated with joints in the order that they are
+    defined in this file. The actuators also operate on joints.
+
+    State-Space (name/joint/parameter):
+        - rootx     slider      position (m)
+        - rootz     slider      position (m)
+        - rooty     hinge       angle (rad)
+        - bthigh    hinge       angle (rad)
+        - bshin     hinge       angle (rad)
+        - bfoot     hinge       angle (rad)
+        - fthigh    hinge       angle (rad)
+        - fshin     hinge       angle (rad)
+        - ffoot     hinge       angle (rad)
+        - rootx     slider      velocity (m/s)
+        - rootz     slider      velocity (m/s)
+        - rooty     hinge       angular velocity (rad/s)
+        - bthigh    hinge       angular velocity (rad/s)
+        - bshin     hinge       angular velocity (rad/s)
+        - bfoot     hinge       angular velocity (rad/s)
+        - fthigh    hinge       angular velocity (rad/s)
+        - fshin     hinge       angular velocity (rad/s)
+        - ffoot     hinge       angular velocity (rad/s)
+
+    Actuators (name/actuator/parameter):
+        - bthigh    hinge       torque (N m)
+        - bshin     hinge       torque (N m)
+        - bfoot     hinge       torque (N m)
+        - fthigh    hinge       torque (N m)
+        - fshin     hinge       torque (N m)
+        - ffoot     hinge       torque (N m)
+
+-->
+<mujoco model="cheetah">
+  <compiler angle="radian" coordinate="local" inertiafromgeom="true" settotalmass="14"/>
+  <default>
+    <joint armature=".1" damping=".01" limited="true" solimplimit="0 .8 .03" solreflimit=".02 1" stiffness="8"/>
+    <geom conaffinity="0" condim="3" contype="1" friction=".4 .1 .1" rgba="0.8 0.6 .4 1" solimp="0.0 0.8 0.01" solref="0.02 1"/>
+    <motor ctrllimited="true" ctrlrange="-1 1"/>
+  </default>
+  <size nstack="300000" nuser_geom="1"/>
+  <option gravity="0 0 -9.81" timestep="0.01"/>
+  <asset>
+    <texture builtin="gradient" height="100" rgb1="1 1 1" rgb2="0 0 0" type="skybox" width="100"/>
+    <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+    <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+    <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="150 150" texture="texplane"/>
+    <material name="geom" texture="texgeom" texuniform="true"/>
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="200 200 200" type="plane"/>
+    <body name="torso" pos="0 0 .7">
+      <joint armature="0" axis="1 0 0" damping="0" limited="false" name="rootx" pos="0 0 0" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 0 1" damping="0" limited="false" name="rootz" pos="0 0 0" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 1 0" damping="0" limited="false" name="rooty" pos="0 0 0" stiffness="0" type="hinge"/>
+      <geom fromto="-.5 0 0 .5 0 0" name="torso" size="0.046" type="capsule"/>
+      <geom axisangle="0 1 0 .87" name="head" pos=".6 0 .1" size="0.046 .15" type="capsule"/>
+      <!-- <site name='tip'  pos='.15 0 .11'/>-->
+      <body name="bthigh" pos="-.5 0 0">
+        <joint axis="0 1 0" damping="6" name="bthigh" pos="0 0 0" range="-.52 1.05" stiffness="240" type="hinge"/>
+        <geom axisangle="0 1 0 -3.8" name="bthigh" pos=".1 0 -.13" size="0.046 .145" type="capsule"/>
+        <body name="bshin" pos=".16 0 -.25">
+          <joint axis="0 1 0" damping="4.5" name="bshin" pos="0 0 0" range="-.785 .785" stiffness="180" type="hinge"/>
+          <geom axisangle="0 1 0 -2.03" name="bshin" pos="-.14 0 -.07" rgba="0.9 0.6 0.6 1" size="0.046 .15" type="capsule"/>
+          <body name="bfoot" pos="-.28 0 -.14">
+            <joint axis="0 1 0" damping="3" name="bfoot" pos="0 0 0" range="-.4 .785" stiffness="120" type="hinge"/>
+            <geom axisangle="0 1 0 -.27" name="bfoot" pos=".03 0 -.097" rgba="0.9 0.6 0.6 1" size="0.046 .094" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <body name="fthigh" pos=".5 0 0">
+        <joint axis="0 1 0" damping="4.5" name="fthigh" pos="0 0 0" range="-1 .7" stiffness="180" type="hinge"/>
+        <geom axisangle="0 1 0 .52" name="fthigh" pos="-.07 0 -.12" size="0.046 .133" type="capsule"/>
+        <body name="fshin" pos="-.14 0 -.24">
+          <joint axis="0 1 0" damping="3" name="fshin" pos="0 0 0" range="-1.2 .87" stiffness="120" type="hinge"/>
+          <geom axisangle="0 1 0 -.6" name="fshin" pos=".065 0 -.09" rgba="0.9 0.6 0.6 1" size="0.046 .106" type="capsule"/>
+          <body name="ffoot" pos=".13 0 -.18">
+            <joint axis="0 1 0" damping="1.5" name="ffoot" pos="0 0 0" range="-.5 .5" stiffness="60" type="hinge"/>
+            <geom axisangle="0 1 0 -.6" name="ffoot" pos=".045 0 -.07" rgba="0.9 0.6 0.6 1" size="0.046 .07" type="capsule"/>
+          </body>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor gear="120" joint="bthigh" name="bthigh"/>
+    <motor gear="90" joint="bshin" name="bshin"/>
+    <motor gear="60" joint="bfoot" name="bfoot"/>
+    <motor gear="120" joint="fthigh" name="fthigh"/>
+    <motor gear="60" joint="fshin" name="fshin"/>
+    <motor gear="30" joint="ffoot" name="ffoot"/>
+  </actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/pusher.xml b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/pusher.xml
new file mode 100644
index 0000000..9e81b01
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/pusher.xml
@@ -0,0 +1,101 @@
+<mujoco model="arm3d">
+  <compiler inertiafromgeom="true" angle="radian" coordinate="local"/>
+  <option timestep="0.01" gravity="0 0 0" iterations="20" integrator="Euler" />
+  
+  <default>
+    <joint armature='0.04' damping="1" limited="true"/>
+    <geom friction=".8 .1 .1" density="300" margin="0.002" condim="1" contype="0" conaffinity="0"/>
+  </default>
+  <asset>
+    <texture type="skybox" builtin="checker" rgb1="1 1 1" rgb2="1 1 1"
+             width="256" height="256"/>
+    <texture name="texgeom" type="cube" builtin="flat" mark="cross" width="127" height="1278" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" markrgb="1 1 1" random="0.01" />
+    <texture name="texplane" type="2d" builtin="checker" rgb1=".5 .5 .5" rgb2=".5 .5 .5" width="100" height="100" />
+    <texture name="texplane_show" type="2d" builtin="checker" rgb1="0 0 0" rgb2="0.9 0.9 0.9" width="100" height="100" />
+    <material name='MatPlane' texture="texplane" shininess="1" texrepeat="30 30" specular="1"  reflectance="0.5" />
+    <material name='geom' texture="texgeom" texuniform="true" />
+  </asset>
+
+  <worldbody>
+    <light diffuse=".5 .5 .5" pos="0 0 3" dir="0 0 -1"/>
+    <geom name="table" type="plane" pos="0 0.5 -0.325" size="1 1 0.1" contype="1" conaffinity="1"/>
+
+    <body name="r_shoulder_pan_link" pos="0 -0.6 0">
+      <geom name="e1" type="sphere" rgba="0.6 0.6 0.6 1" pos="-0.06 0.05 0.2" size="0.05" />
+      <geom name="e2" type="sphere" rgba="0.6 0.6 0.6 1" pos=" 0.06 0.05 0.2" size="0.05" />
+      <geom name="e1p" type="sphere" rgba="0.1 0.1 0.1 1" pos="-0.06 0.09 0.2" size="0.03" />
+      <geom name="e2p" type="sphere" rgba="0.1 0.1 0.1 1" pos=" 0.06 0.09 0.2" size="0.03" />
+      <geom name="sp" type="capsule" fromto="0 0 -0.4 0 0 0.2" size="0.1" />
+      <!--<joint name="r_shoulder_pan_joint" type="hinge" pos="0 0 0" axis="0 0 1" range="-2.2854 1.714602" damping="1.0" />-->
+      <joint name="r_shoulder_pan_joint" type="hinge" pos="0 0 0" axis="0 0 1" range="-2.2854 2.0" damping="1.0" />
+
+      <body name="r_shoulder_lift_link" pos="0.1 0 0">
+        <geom name="sl" type="capsule" fromto="0 -0.1 0 0 0.1 0" size="0.1" />
+        <joint name="r_shoulder_lift_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.5236 1.3963" damping="1.0" />
+
+        <body name="r_upper_arm_roll_link" pos="0 0 0">
+          <geom name="uar" type="capsule" fromto="-0.1 0 0 0.1 0 0" size="0.02" />
+          <joint name="r_upper_arm_roll_joint" type="hinge" pos="0 0 0" axis="1 0 0" range="-1.5 1.7" damping="0.1" />
+
+          <body name="r_upper_arm_link" pos="0 0 0">
+            <geom name="ua" type="capsule" fromto="0 0 0 0.4 0 0" size="0.06" />
+
+            <body name="r_elbow_flex_link" pos="0.4 0 0">
+              <geom name="ef" type="capsule" fromto="0 -0.02 0 0.0 0.02 0" size="0.06" />
+              <joint name="r_elbow_flex_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-2.3213 0" damping="0.1" />
+
+              <body name="r_forearm_roll_link" pos="0 0 0">
+                <geom name="fr" type="capsule" fromto="-0.1 0 0 0.1 0 0" size="0.02" />
+                <joint name="r_forearm_roll_joint" type="hinge" limited="true" pos="0 0 0" axis="1 0 0" damping=".1" range="-1.5 1.5"/>
+
+                <body name="r_forearm_link" pos="0 0 0">
+                  <geom name="fa" type="capsule" fromto="0 0 0 0.291 0 0" size="0.05" />
+
+                  <body name="r_wrist_flex_link" pos="0.321 0 0">
+                    <geom name="wf" type="capsule" fromto="0 -0.02 0 0 0.02 0" size="0.01" />
+                    <joint name="r_wrist_flex_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-1.094 0" damping=".1" />
+
+                    <body name="r_wrist_roll_link" pos="0 0 0">
+                      <joint name="r_wrist_roll_joint" type="hinge" pos="0 0 0" limited="true" axis="1 0 0" damping="0.1" range="-1.5 1.5"/>
+                      <body name="tips_arm" pos="0 0 0">
+                        <geom name="tip_arml" type="sphere" pos="0.1 -0.1 0." size="0.01" />
+                        <geom name="tip_armr" type="sphere" pos="0.1 0.1 0." size="0.01" />
+                      </body>
+                      <geom type="capsule" fromto="0 -0.1 0. 0.0 +0.1 0" size="0.02" contype="1" conaffinity="1" />
+                      <geom type="capsule" fromto="0 -0.1 0. 0.1 -0.1 0" size="0.02" contype="1" conaffinity="1" />
+                      <geom type="capsule" fromto="0 +0.1 0. 0.1 +0.1 0." size="0.02" contype="1" conaffinity="1" />
+                    </body>
+                  </body>
+                </body>
+              </body>
+            </body>
+          </body>
+        </body>
+      </body>
+    </body>
+
+    <!--<body name="object" pos="0.55 -0.3 -0.275" >-->
+    <body name="object" pos="0.45 -0.05 -0.275" >
+      <geom rgba="1 1 1 0" type="sphere" size="0.05 0.05 0.05" density="0.00001" conaffinity="0"/>
+      <geom rgba="1 1 1 1" type="cylinder" size="0.05 0.05 0.05" density="0.00001" contype="1" conaffinity="0"/>
+      <joint name="obj_slidey" type="slide" pos="0 0 0" axis="0 1 0" range="-10.3213 10.3" damping="0.5"/>
+      <joint name="obj_slidex" type="slide" pos="0 0 0" axis="1 0 0" range="-10.3213 10.3" damping="0.5"/>
+    </body>
+
+    <body name="goal" pos="0.45 -0.05 -0.3230">
+      <geom rgba="1 0 0 1" type="cylinder" size="0.08 0.001 0.1" density='0.00001' contype="0" conaffinity="0"/>
+      <joint name="goal_slidey" type="slide" pos="0 0 0" axis="0 1 0" range="-10.3213 10.3" damping="0.5"/>
+      <joint name="goal_slidex" type="slide" pos="0 0 0" axis="1 0 0" range="-10.3213 10.3" damping="0.5"/> 
+    </body>
+  </worldbody>
+
+  <actuator>
+    <motor joint="r_shoulder_pan_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_shoulder_lift_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_upper_arm_roll_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_elbow_flex_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_forearm_roll_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_wrist_flex_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_wrist_roll_joint" ctrlrange="-2.0 2.0" ctrllimited="true"/>
+  </actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/reacher3d.xml b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/reacher3d.xml
new file mode 100644
index 0000000..a51c71b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/reacher3d.xml
@@ -0,0 +1,154 @@
+<mujoco model="arm3d">
+
+    <compiler inertiafromgeom="true" angle="radian" coordinate="local" />
+    <option timestep="0.01" gravity="0 0 0" iterations="20" integrator="RK4" />
+    <default>
+        <joint armature="0.04" damping="1" limited="true" />
+        <geom friction=".5 .1 .1" margin="0.002" condim="1" contype="0" conaffinity="0" />
+    </default>
+    <asset>
+        <texture type="skybox" builtin="checker" rgb1="1 1 1" rgb2="1 1 1"
+                 width="256" height="256"/>
+        <texture name="texgeom" type="cube" builtin="flat" mark="cross" width="127" height="1278" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" markrgb="1 1 1" random="0.01" />
+        <texture name="texplane" type="2d" builtin="checker" rgb1=".5 .5 .5" rgb2=".5 .5 .5" width="100" height="100" />
+        <texture name="texplane_show" type="2d" builtin="checker" rgb1="0 0 0" rgb2="0.9 0.9 0.9" width="100" height="100" />
+        <material name='MatPlane' texture="texplane" shininess="1" texrepeat="30 30" specular="1"  reflectance="0.5" />
+        <material name='geom' texture="texgeom" texuniform="true" />
+    </asset>
+    <worldbody>
+        <light pos="0 0 5" />
+        <body name="r_shoulder_pan_link" pos="0 -0.188 0">
+            <geom name="e1" type="sphere" rgba="0.6 0.6 0.6 1" pos="-0.06 0.05 0.2" size="0.05" />
+            <geom name="e2" type="sphere" rgba="0.6 0.6 0.6 1" pos=" 0.06 0.05 0.2" size="0.05" />
+            <geom name="e1p" type="sphere" rgba="0.1 0.1 0.1 1" pos="-0.06 0.09 0.2" size="0.03" />
+            <geom name="e2p" type="sphere" rgba="0.1 0.1 0.1 1" pos=" 0.06 0.09 0.2" size="0.03" />
+            <geom name="sp" type="capsule" fromto="0 0 -0.4 0 0 0.2" size="0.1" />
+            <joint name="r_shoulder_pan_joint" type="hinge" pos="0 0 0" axis="0 0 1" range="-2.2854 1.714602" damping="10.0" />
+
+            <body name="r_shoulder_lift_link" pos="0.1 0 0">
+                <geom name="sl" type="capsule" fromto="0 -0.1 0 0 0.1 0" size="0.1" />
+                <joint name="r_shoulder_lift_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.5236 1.3963" damping="10.0" />
+
+                <body name="r_upper_arm_roll_link" pos="0 0 0">
+                    <geom name="uar" type="capsule" fromto="-0.1 0 0 0.1 0 0" size="0.02" />
+                    <joint name="r_upper_arm_roll_joint" type="hinge" pos="0 0 0" axis="1 0 0" range="-3.9 0.8" damping="0.1" />
+
+                    <body name="r_upper_arm_link" pos="0 0 0">
+                        <geom name="ua" type="capsule" fromto="0 0 0 0.4 0 0" size="0.06" />
+
+                        <body name="r_elbow_flex_link" pos="0.4 0 0">
+                            <geom name="ef" type="capsule" fromto="0 -0.02 0 0.0 0.02 0" size="0.06" />
+                            <joint name="r_elbow_flex_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-2.3213 0" damping="1.0" />
+
+                            <body name="r_forearm_roll_link" pos="0 0 0">
+                                <geom name="fr" type="capsule" fromto="-0.1 0 0 0.1 0 0" size="0.02" />
+                                <joint name="r_forearm_roll_joint" type="hinge" limited="false" pos="0 0 0" axis="1 0 0" damping=".1" />
+
+                                <body name="r_forearm_link" pos="0 0 0">
+                                    <geom name="fa" type="capsule" fromto="0 0 0 0.321 0 0" size="0.05" />
+
+                                    <body name="r_wrist_flex_link" pos="0.321 0 0">
+                                        <geom name="wf" type="capsule" fromto="0 -0.02 0 0 0.02 0" size="0.01" />
+                                        <joint name="r_wrist_flex_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-2.094 0" damping=".1" />
+
+                                        <body name="r_wrist_roll_link" pos="0 0 0">
+                                            <geom name="wr" type="capsule" fromto="-0.02 0 0 0.02 0 0" size="0.01" />
+                                            <joint name="r_wrist_roll_joint" type="hinge" pos="0 0 0" limited="false" axis="1 0 0" damping="0.1" />
+
+                                            <body name="r_gripper_palm_link" pos="0 0 0">
+                                                <geom name="pl" type="capsule" fromto="0.05 0 -0.02 0.05 0 0.02" size="0.05" />
+
+                                                <!--
+                                                <body name="r_gripper_tool_frame" pos="0.18 0 0">
+                                                    <site name="leg_bottom" pos="0 0 -0.15" size="0.01" />
+                                                    <site name="leg_top" pos="0 0 0.15" size="0.01" />
+
+                                                    <body name="ball" pos="0 0 0">
+                                                        <geom name="ball_geom" rgba="0.8 0.6 0.6 1" type="cylinder" fromto="0 0 -0.15 0 0 0.15" size="0.028" density="2000" contype="2" conaffinity="1" />
+                                                    </body>
+                                                </body>
+                                                -->
+
+                                                <body name="r_gripper_l_finger_link" pos="0.07691 0.03 0">
+                                                    <geom name="gf3" type="capsule" fromto="0 0 0 0.09137 0.00495 0" size="0.01" />
+
+                                                    <body name="r_gripper_l_finger_tip_link" pos="0.09137 0.00495 0">
+                                                        <geom name="gf4" type="capsule" fromto="0 0 0 0.09137 0.0 0" size="0.01" />
+                                                    </body>
+                                                </body>
+
+                                                <body name="r_gripper_r_finger_link" pos="0.07691 -0.03 0">
+                                                    <geom name="gf1" type="capsule" fromto="0 0 0 0.09137 -0.00495 0" size="0.01" />
+
+                                                    <body name="r_gripper_r_finger_tip_link" pos="0.09137 -0.00495 0">
+                                                        <geom name="gf2" type="capsule" fromto="0 0 0 0.09137 0.0 0" size="0.01" />
+                                                    </body>
+                                                </body>
+                                            </body>
+                                        </body>
+                                    </body>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                </body>
+            </body>
+        </body>
+
+        <!--
+        <body name="g1" pos="0.034 0.3 -0.47" axisangle="0 1 0 0.05">
+            <geom name="g1" rgba="0.2 0.2 0.2 1" type="box" size="0.003 0.01 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="g2" pos="-0.034 0.3 -0.47" axisangle="0 1 0 -0.05">
+            <geom name="g2" rgba="0.2 0.2 0.2 1" type="box" size="0.003 0.01 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="g3" pos="0.0 0.334 -0.47" axisangle="1 0 0 -0.05">
+            <geom name="g3" rgba="0.2 0.2 0.2 1" type="box" size="0.01 0.003 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="g4" pos="0.0 0.266 -0.47" axisangle="1 0 0 0.05">
+            <geom name="g4" rgba="0.2 0.2 0.2 1" type="box" size="0.01 0.003 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="fl" pos="0.0 0.3 -0.55">
+            <geom name="fl" rgba="0.2 0.2 0.2 1" type="box" size="0.2 0.2 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="w1" pos="0.216 0.3 -0.45">
+            <geom name="w1" rgba="0.2 0.2 0.2 1" type="box" size="0.183 0.3 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="w2" pos="-0.216 0.3 -0.45">
+            <geom name="w2" rgba="0.2 0.2 0.2 1" type="box" size="0.183 0.3 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="w3" pos="0.0 0.516 -0.45">
+            <geom name="w3" rgba="0.2 0.2 0.2 1" type="box" size="0.032 0.183 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="w4" pos="0.0 0.084 -0.45">
+            <geom name="w4" rgba="0.2 0.2 0.2 1" type="box" size="0.032 0.183 0.05" contype="1" conaffinity="1" />
+        </body>
+        -->
+
+        <body name="target" pos="0 0.25 0">
+            <joint armature="0" axis="1 0 0" damping="0" limited="false" name="target_x" pos="0 0 0" ref="0" stiffness="0" type="slide"/>
+            <joint armature="0" axis="0 1 0" damping="0" limited="false" name="target_y" pos="0 0 0" ref="0.25" stiffness="0" type="slide"/>
+            <joint armature="0" axis="0 0 1" damping="0" limited="false" name="target_z" pos="0 0 0" ref="0" stiffness="0" type="slide"/>
+            <geom conaffinity="0" contype="0" name="target" pos="0 0 0" rgba="0.9 0.2 0.2 1" size=".035" type="sphere"/>
+        </body>
+    </worldbody>
+
+    <actuator>
+        <motor joint="r_shoulder_pan_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+        <motor joint="r_shoulder_lift_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+        <motor joint="r_upper_arm_roll_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+        <motor joint="r_elbow_flex_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+        <motor joint="r_forearm_roll_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+        <motor joint="r_wrist_flex_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+        <motor joint="r_wrist_roll_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+    </actuator>
+
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/cartpole.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/cartpole.py
new file mode 100644
index 0000000..ddc3d41
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/cartpole.py
@@ -0,0 +1,193 @@
+"""
+Classic cart-pole system implemented by Rich Sutton et al.
+Copied from https://webdocs.cs.ualberta.ca/~sutton/book/code/pole.c
+"""
+
+import logging
+import math
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+from slbo.utils.dataset import Dataset, gen_dtype
+
+
+logger = logging.getLogger(__name__)
+
+
+class CartPoleEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 50
+    }
+
+    def __init__(self):
+        self.gravity = 9.8
+        self.masscart = 1.0
+        self.masspole = 0.1
+        self.total_mass = (self.masspole + self.masscart)
+        self.length = 0.5  # actually half the pole's length
+        self.polemass_length = (self.masspole * self.length)
+        self.force_mag = 10.0
+        self.tau = 0.02  # seconds between state updates
+
+        # Angle at which to fail the episode
+        self.theta_threshold_radians = 12 * 2 * math.pi / 360
+        self.x_threshold = 2.4
+
+        # Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds
+        high = np.array([
+            self.x_threshold * 2,
+            np.finfo(np.float32).max,
+            self.theta_threshold_radians * 2,
+            np.finfo(np.float32).max])
+
+        # self.action_space = spaces.Discrete(2)
+        self.action_space = \
+            spaces.Box(low=np.array([-1.0]), high=np.array([1.0]))
+        self.observation_space = spaces.Box(-high, high)
+
+        self._seed()
+        self.viewer = None
+        self.state = None
+
+        self.steps_beyond_done = None
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, action):
+        action = 1 if action[0] > .0 else 0
+        # assert self.action_space.contains(action), "%r (%s) invalid" % (action, type(action))
+        state = self.state
+        obs = self.state
+        reward = np.cos(obs[2]) - 0.01 * (obs[0] ** 2)
+
+        x, x_dot, theta, theta_dot = state
+        force = self.force_mag if action == 1 else -self.force_mag
+        costheta = math.cos(theta)
+        sintheta = math.sin(theta)
+        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass
+        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta * costheta / self.total_mass))
+        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass
+        x = x + self.tau * x_dot
+        x_dot = x_dot + self.tau * xacc
+        theta = theta + self.tau * theta_dot
+        theta_dot = theta_dot + self.tau * thetaacc
+        self.state = (x, x_dot, theta, theta_dot)
+        '''
+        done = x < -self.x_threshold \
+            or x > self.x_threshold \
+            or theta < -self.theta_threshold_radians \
+            or theta > self.theta_threshold_radians
+        done = bool(done)
+
+        if not done:
+            reward = 1.0
+        elif self.steps_beyond_done is None:
+            # Pole just fell!
+            self.steps_beyond_done = 0
+            reward = 1.0
+        else:
+            if self.steps_beyond_done == 0:
+                logger.warning("You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.")
+            self.steps_beyond_done += 1
+            reward = 0.0
+        '''
+        done = False
+        self.steps_beyond_done = None
+
+        return np.array(self.state), reward, done, {}
+
+    def _reset(self):
+        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))
+        self.steps_beyond_done = None
+        return np.array(self.state)
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        screen_width = 600
+        screen_height = 400
+
+        world_width = self.x_threshold * 2
+        scale = screen_width / world_width
+        carty = 100  # TOP OF CART
+        polewidth = 10.0
+        polelen = scale * 1.0
+        cartwidth = 50.0
+        cartheight = 30.0
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(screen_width, screen_height)
+            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2
+            axleoffset = cartheight / 4.0
+            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            self.carttrans = rendering.Transform()
+            cart.add_attr(self.carttrans)
+            self.viewer.add_geom(cart)
+            l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2
+            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            pole.set_color(.8, .6, .4)
+            self.poletrans = rendering.Transform(translation=(0, axleoffset))
+            pole.add_attr(self.poletrans)
+            pole.add_attr(self.carttrans)
+            self.viewer.add_geom(pole)
+            self.axle = rendering.make_circle(polewidth / 2)
+            self.axle.add_attr(self.poletrans)
+            self.axle.add_attr(self.carttrans)
+            self.axle.set_color(.5, .5, .8)
+            self.viewer.add_geom(self.axle)
+            self.track = rendering.Line((0, carty), (screen_width, carty))
+            self.track.set_color(0, 0, 0)
+            self.viewer.add_geom(self.track)
+
+        if self.state is None:
+            return None
+
+        x = self.state
+        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART
+        self.carttrans.set_translation(cartx, carty)
+        self.poletrans.set_rotation(-x[2])
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        x = obs[:, 0]
+        theta = obs[:, 2]
+        return -(np.cos(theta) - 0.01 * (x ** 2))
+
+    def verify(self, n=2000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cartpoleO001.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cartpoleO001.py
new file mode 100644
index 0000000..2407b55
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cartpoleO001.py
@@ -0,0 +1,178 @@
+"""
+Classic cart-pole system implemented by Rich Sutton et al.
+Copied from https://webdocs.cs.ualberta.ca/~sutton/book/code/pole.c
+"""
+
+import logging
+import math
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+
+
+logger = logging.getLogger(__name__)
+
+
+class CartPoleEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 50
+    }
+
+    def __init__(self):
+        self.gravity = 9.8
+        self.masscart = 1.0
+        self.masspole = 0.1
+        self.total_mass = (self.masspole + self.masscart)
+        self.length = 0.5  # actually half the pole's length
+        self.polemass_length = (self.masspole * self.length)
+        self.force_mag = 10.0
+        self.tau = 0.02  # seconds between state updates
+
+        # Angle at which to fail the episode
+        self.theta_threshold_radians = 12 * 2 * math.pi / 360
+        self.x_threshold = 2.4
+
+        # Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds
+        high = np.array([
+            self.x_threshold * 2,
+            np.finfo(np.float32).max,
+            self.theta_threshold_radians * 2,
+            np.finfo(np.float32).max])
+
+        # self.action_space = spaces.Discrete(2)
+        self.action_space = \
+            spaces.Box(low=np.array([-1.0]), high=np.array([1.0]))
+        self.observation_space = spaces.Box(-high, high)
+
+        self._seed()
+        self.viewer = None
+        self.state = None
+
+        self.steps_beyond_done = None
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, action):
+        action = 1 if action[0] > .0 else 0
+        # assert self.action_space.contains(action), "%r (%s) invalid" % (action, type(action))
+        state = self.state
+        obs = self.state
+        reward = np.cos(obs[2]) - 0.01 * (obs[0] ** 2)
+
+        x, x_dot, theta, theta_dot = state
+        force = self.force_mag if action == 1 else -self.force_mag
+        costheta = math.cos(theta)
+        sintheta = math.sin(theta)
+        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass
+        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta * costheta / self.total_mass))
+        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass
+        x = x + self.tau * x_dot
+        x_dot = x_dot + self.tau * xacc
+        theta = theta + self.tau * theta_dot
+        theta_dot = theta_dot + self.tau * thetaacc
+        self.state = (x, x_dot, theta, theta_dot)
+        '''
+        done = x < -self.x_threshold \
+            or x > self.x_threshold \
+            or theta < -self.theta_threshold_radians \
+            or theta > self.theta_threshold_radians
+        done = bool(done)
+
+        if not done:
+            reward = 1.0
+        elif self.steps_beyond_done is None:
+            # Pole just fell!
+            self.steps_beyond_done = 0
+            reward = 1.0
+        else:
+            if self.steps_beyond_done == 0:
+                logger.warning("You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.")
+            self.steps_beyond_done += 1
+            reward = 0.0
+        '''
+        done = False
+        self.steps_beyond_done = None
+
+        ob = np.array(self.state)
+        ob += np.random.uniform(low=-0.01, high=0.01, size=ob.shape)
+
+        return ob, reward, done, {}
+
+    def _reset(self):
+        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))
+        self.steps_beyond_done = None
+        return np.array(self.state)
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        screen_width = 600
+        screen_height = 400
+
+        world_width = self.x_threshold * 2
+        scale = screen_width / world_width
+        carty = 100  # TOP OF CART
+        polewidth = 10.0
+        polelen = scale * 1.0
+        cartwidth = 50.0
+        cartheight = 30.0
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(screen_width, screen_height)
+            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2
+            axleoffset = cartheight / 4.0
+            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            self.carttrans = rendering.Transform()
+            cart.add_attr(self.carttrans)
+            self.viewer.add_geom(cart)
+            l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2
+            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            pole.set_color(.8, .6, .4)
+            self.poletrans = rendering.Transform(translation=(0, axleoffset))
+            pole.add_attr(self.poletrans)
+            pole.add_attr(self.carttrans)
+            self.viewer.add_geom(pole)
+            self.axle = rendering.make_circle(polewidth / 2)
+            self.axle.add_attr(self.poletrans)
+            self.axle.add_attr(self.carttrans)
+            self.axle.set_color(.5, .5, .8)
+            self.viewer.add_geom(self.axle)
+            self.track = rendering.Line((0, carty), (screen_width, carty))
+            self.track.set_color(0, 0, 0)
+            self.viewer.add_geom(self.track)
+
+        if self.state is None:
+            return None
+
+        x = self.state
+        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART
+        self.carttrans.set_translation(cartx, carty)
+        self.poletrans.set_rotation(-x[2])
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        x = obs[:, 0]
+        theta = obs[:, 2]
+        return -(np.cos(theta) - 0.01 * (x ** 2))
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cartpoleO01.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cartpoleO01.py
new file mode 100644
index 0000000..62516c3
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cartpoleO01.py
@@ -0,0 +1,177 @@
+"""
+Classic cart-pole system implemented by Rich Sutton et al.
+Copied from https://webdocs.cs.ualberta.ca/~sutton/book/code/pole.c
+"""
+
+import logging
+import math
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+
+logger = logging.getLogger(__name__)
+
+
+class CartPoleEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 50
+    }
+
+    def __init__(self):
+        self.gravity = 9.8
+        self.masscart = 1.0
+        self.masspole = 0.1
+        self.total_mass = (self.masspole + self.masscart)
+        self.length = 0.5  # actually half the pole's length
+        self.polemass_length = (self.masspole * self.length)
+        self.force_mag = 10.0
+        self.tau = 0.02  # seconds between state updates
+
+        # Angle at which to fail the episode
+        self.theta_threshold_radians = 12 * 2 * math.pi / 360
+        self.x_threshold = 2.4
+
+        # Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds
+        high = np.array([
+            self.x_threshold * 2,
+            np.finfo(np.float32).max,
+            self.theta_threshold_radians * 2,
+            np.finfo(np.float32).max])
+
+        # self.action_space = spaces.Discrete(2)
+        self.action_space = \
+            spaces.Box(low=np.array([-1.0]), high=np.array([1.0]))
+        self.observation_space = spaces.Box(-high, high)
+
+        self._seed()
+        self.viewer = None
+        self.state = None
+
+        self.steps_beyond_done = None
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, action):
+        action = 1 if action[0] > .0 else 0
+        # assert self.action_space.contains(action), "%r (%s) invalid" % (action, type(action))
+        state = self.state
+        obs = self.state
+        reward = np.cos(obs[2]) - 0.01 * (obs[0] ** 2)
+
+        x, x_dot, theta, theta_dot = state
+        force = self.force_mag if action == 1 else -self.force_mag
+        costheta = math.cos(theta)
+        sintheta = math.sin(theta)
+        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass
+        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta * costheta / self.total_mass))
+        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass
+        x = x + self.tau * x_dot
+        x_dot = x_dot + self.tau * xacc
+        theta = theta + self.tau * theta_dot
+        theta_dot = theta_dot + self.tau * thetaacc
+        self.state = (x, x_dot, theta, theta_dot)
+        '''
+        done = x < -self.x_threshold \
+            or x > self.x_threshold \
+            or theta < -self.theta_threshold_radians \
+            or theta > self.theta_threshold_radians
+        done = bool(done)
+
+        if not done:
+            reward = 1.0
+        elif self.steps_beyond_done is None:
+            # Pole just fell!
+            self.steps_beyond_done = 0
+            reward = 1.0
+        else:
+            if self.steps_beyond_done == 0:
+                logger.warning("You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.")
+            self.steps_beyond_done += 1
+            reward = 0.0
+        '''
+        done = False
+        self.steps_beyond_done = None
+
+        ob = np.array(self.state)
+        ob += np.random.uniform(low=-0.1, high=0.1, size=ob.shape)
+
+        return ob, reward, done, {}
+
+    def _reset(self):
+        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))
+        self.steps_beyond_done = None
+        return np.array(self.state)
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        screen_width = 600
+        screen_height = 400
+
+        world_width = self.x_threshold * 2
+        scale = screen_width / world_width
+        carty = 100  # TOP OF CART
+        polewidth = 10.0
+        polelen = scale * 1.0
+        cartwidth = 50.0
+        cartheight = 30.0
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(screen_width, screen_height)
+            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2
+            axleoffset = cartheight / 4.0
+            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            self.carttrans = rendering.Transform()
+            cart.add_attr(self.carttrans)
+            self.viewer.add_geom(cart)
+            l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2
+            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            pole.set_color(.8, .6, .4)
+            self.poletrans = rendering.Transform(translation=(0, axleoffset))
+            pole.add_attr(self.poletrans)
+            pole.add_attr(self.carttrans)
+            self.viewer.add_geom(pole)
+            self.axle = rendering.make_circle(polewidth / 2)
+            self.axle.add_attr(self.poletrans)
+            self.axle.add_attr(self.carttrans)
+            self.axle.set_color(.5, .5, .8)
+            self.viewer.add_geom(self.axle)
+            self.track = rendering.Line((0, carty), (screen_width, carty))
+            self.track.set_color(0, 0, 0)
+            self.viewer.add_geom(self.track)
+
+        if self.state is None:
+            return None
+
+        x = self.state
+        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART
+        self.carttrans.set_translation(cartx, carty)
+        self.poletrans.set_rotation(-x[2])
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        x = obs[:, 0]
+        theta = obs[:, 2]
+        return -(np.cos(theta) - 0.01 * (x ** 2))
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahA003.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahA003.py
new file mode 100644
index 0000000..e759126
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahA003.py
@@ -0,0 +1,81 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from slbo.envs import BaseModelBasedEnv
+from gym.envs.mujoco import mujoco_env
+
+
+class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/half_cheetah.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        action = np.array(action)
+        action += np.random.uniform(low=-0.03, high=0.03, size=action.shape)
+        start_ob = self._get_obs()
+        reward_run = start_ob[8]
+
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        reward_ctrl = -0.1 * np.square(action).sum()
+
+        reward = reward_run + reward_ctrl
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        qpos = self.init_qpos + \
+            self.np_random.uniform(low=-.1, high=.1, size=self.model.nq)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahA01.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahA01.py
new file mode 100644
index 0000000..e496056
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahA01.py
@@ -0,0 +1,81 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/half_cheetah.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        action = np.array(action)
+        action += np.random.uniform(low=-0.1, high=0.1, size=action.shape)
+        start_ob = self._get_obs()
+        reward_run = start_ob[8]
+
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        reward_ctrl = -0.1 * np.square(action).sum()
+
+        reward = reward_run + reward_ctrl
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        qpos = self.init_qpos + \
+            self.np_random.uniform(low=-.1, high=.1, size=self.model.nq)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahO001.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahO001.py
new file mode 100644
index 0000000..252505f
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahO001.py
@@ -0,0 +1,80 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/half_cheetah.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        start_ob = self._get_obs()
+        reward_run = start_ob[8]
+
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        reward_ctrl = -0.1 * np.square(action).sum()
+
+        reward = reward_run + reward_ctrl
+        done = False
+        ob += np.random.uniform(low=-0.01, high=0.01, size=ob.shape)
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        qpos = self.init_qpos + \
+            self.np_random.uniform(low=-.1, high=.1, size=self.model.nq)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahO01.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahO01.py
new file mode 100644
index 0000000..4e6fe93
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahO01.py
@@ -0,0 +1,80 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/half_cheetah.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        start_ob = self._get_obs()
+        reward_run = start_ob[8]
+
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        reward_ctrl = -0.1 * np.square(action).sum()
+
+        reward = reward_run + reward_ctrl
+        done = False
+        ob += np.random.uniform(low=-0.1, high=0.1, size=ob.shape)
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        qpos = self.init_qpos + \
+            self.np_random.uniform(low=-.1, high=.1, size=self.model.nq)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fant.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fant.py
new file mode 100644
index 0000000..29633e2
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fant.py
@@ -0,0 +1,84 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+
+from slbo.envs import BaseModelBasedEnv
+
+
+class AntEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/ant.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        ob = self._get_obs()
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = old_ob[13]
+        reward_height = -3.0 * np.square(old_ob[0] - 0.57)
+
+        # the alive bonus
+        height = ob[0]
+        done = (height > 1.0) or (height < 0.2)
+        alive_reward = float(not done)
+
+        reward = reward_run + reward_ctrl + reward_height + alive_reward
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[2:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        qpos = self.init_qpos + \
+            self.np_random.uniform(size=self.model.nq, low=-.1, high=.1)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        # self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 13]
+        reward_height = -3.0 * np.square(obs[:, 0] - 0.57)
+
+        height = next_obs[:, 0]
+        done = np.logical_or((height > 1.0), (height < 0.2))
+        alive_reward = 1.0 - np.array(done, dtype=np.float)
+
+        reward = reward_run + reward_ctrl + reward_height + alive_reward
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+
+    def mb_step(self, states, actions, next_states):
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        height = next_states[:, 0]
+        done = np.logical_or((height > 1.0), (height < 0.2))
+        return rewards, done
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fhopper.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fhopper.py
new file mode 100644
index 0000000..84edf3d
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fhopper.py
@@ -0,0 +1,91 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HopperEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=4):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/hopper.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = old_ob[5]
+        reward_height = -3.0 * np.square(old_ob[0] - 1.3)
+        height, ang = ob[0], ob[1]
+        done = (height <= 0.7) or (abs(ang) >= 0.2)
+        alive_reward = float(not done)
+        reward = reward_run + reward_ctrl + reward_height + alive_reward
+
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-.005, high=.005, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-.005, high=.005, size=self.model.nv)
+        )
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 2
+        self.viewer.cam.distance = self.model.stat.extent * 0.75
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 5]
+        reward_height = -3.0 * np.square(obs[:, 0] - 1.3)
+        height, ang = next_obs[:, 0], next_obs[:, 1]
+        done = np.logical_or(height <= 0.7, abs(ang) >= 0.2)
+        alive_reward = 1.0 - np.array(done, dtype=np.float)
+        reward = reward_run + reward_ctrl + reward_height + alive_reward
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        # reward_height = -3.0 * tf.square(next_obs[:, 1] - 1.3)
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+        raise NotImplementedError
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        height, ang = next_states[:, 0], next_states[:, 1]
+        done = np.logical_or(height <= 0.7, abs(ang) >= 0.2)
+        return rewards, done
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fswimmer.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fswimmer.py
new file mode 100644
index 0000000..86bdc80
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fswimmer.py
@@ -0,0 +1,69 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+import os
+
+
+class fixedSwimmerEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+
+    def __init__(self):
+        dir_path = os.path.dirname(os.path.realpath(__file__))
+        mujoco_env.MujocoEnv.__init__(self, '%s/assets/fixed_swimmer.xml' % dir_path, 4)
+        utils.EzPickle.__init__(self)
+
+    def _step(self, a):
+        ctrl_cost_coeff = 0.0001
+
+        """
+        xposbefore = self.model.data.qpos[0, 0]
+        self.do_simulation(a, self.frame_skip)
+        xposafter = self.model.data.qpos[0, 0]
+        """
+
+        self.xposbefore = self.model.data.site_xpos[0][0] / self.dt
+        self.do_simulation(a, self.frame_skip)
+        self.xposafter = self.model.data.site_xpos[0][0] / self.dt
+        self.pos_diff = self.xposafter - self.xposbefore
+
+        reward_fwd = self.xposafter - self.xposbefore
+        reward_ctrl = - ctrl_cost_coeff * np.square(a).sum()
+        reward = reward_fwd + reward_ctrl
+        ob = self._get_obs()
+        return ob, reward, False, dict(reward_fwd=reward_fwd, reward_ctrl=reward_ctrl)
+
+    def _get_obs(self):
+        qpos = self.model.data.qpos
+        qvel = self.model.data.qvel
+        return np.concatenate([qpos.flat[2:], qvel.flat, self.pos_diff.flat])
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-.1, high=.1, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-.1, high=.1, size=self.model.nv)
+        )
+        return self._get_obs()
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.0001 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, -1]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+
+    def verify(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fwalker2d.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fwalker2d.py
new file mode 100644
index 0000000..25006d3
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fwalker2d.py
@@ -0,0 +1,99 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class Walker2dEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=4):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/walker2d.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = old_ob[8]
+        reward_height = -3.0 * np.square(old_ob[0] - 1.3)
+
+        height, ang = ob[0], ob[1]
+        done = (height >= 2.0) or (height <= 0.8) or (abs(ang) >= 1.0)
+        alive_reward = float(not done)
+
+        reward = reward_run + reward_ctrl + reward_height + alive_reward
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat
+        ])
+
+    def reset_model(self):
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-.005, high=.005, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-.005, high=.005, size=self.model.nv)
+        )
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 2
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward_height = -3.0 * np.square(next_obs[:, 0] - 1.3)
+        height, ang = next_obs[:, 0], next_obs[:, 1]
+        done = np.logical_or(
+            np.logical_or(height >= 2.0, height <= 0.8),
+            np.abs(ang) >= 1.0
+        )
+        alive_reward = 1.0 - np.array(done, dtype=np.float)
+        reward = reward_run + reward_ctrl + reward_height + alive_reward
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        # reward_height = -3.0 * tf.square(next_obs[:, 1] - 1.3)
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+        raise NotImplementedError
+
+    def verify(self):
+        pass
+
+    def mb_step(self, states, actions, next_states):
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        height, ang = next_states[:, 0], next_states[:, 1]
+        done = np.logical_or(
+            np.logical_or(height >= 2.0, height <= 0.8),
+            np.abs(ang) >= 1.0
+        )
+        return rewards, done
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_humanoid.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_humanoid.py
new file mode 100644
index 0000000..7d220ba
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_humanoid.py
@@ -0,0 +1,89 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+import numpy as np
+from gym.envs.mujoco import mujoco_env
+from gym import utils
+from slbo.envs import BaseModelBasedEnv
+
+
+class HumanoidEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self):
+        mujoco_env.MujocoEnv.__init__(self, 'humanoid.xml', 5)
+        utils.EzPickle.__init__(self)
+
+    def _get_obs(self):
+        data = self.model.data
+        return np.concatenate([data.qpos.flat[2:],
+                               data.qvel.flat,
+                               data.cinert.flat,
+                               data.cvel.flat,
+                               data.qfrc_actuator.flat,
+                               data.cfrc_ext.flat])
+
+    def _step(self, a):
+        data = self.model.data
+        action = a
+        if getattr(self, 'action_space', None):
+            action = np.clip(a, self.action_space.low,
+                             self.action_space.high)
+
+        # reward
+        alive_bonus = 5.0
+        lin_vel_cost = 0.25 / 0.015 * data.qvel.flat[0]
+        quad_ctrl_cost = 0.1 * np.square(action).sum()
+        quad_impact_cost = .5e-6 * np.square(data.cfrc_ext).sum()
+        quad_impact_cost = min(quad_impact_cost, 10)
+
+        self.do_simulation(action, self.frame_skip)
+        reward = lin_vel_cost - quad_ctrl_cost - quad_impact_cost + alive_bonus
+        qpos = self.model.data.qpos
+        done = bool((qpos[2] < 1.0) or (qpos[2] > 2.0))
+        return self._get_obs(), reward, done, dict(reward_linvel=lin_vel_cost, reward_quadctrl=-quad_ctrl_cost, reward_alive=alive_bonus, reward_impact=-quad_impact_cost)
+
+    def reset_model(self):
+        c = 0.01
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-c, high=c, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-c, high=c, size=self.model.nv,)
+        )
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 1
+        self.viewer.cam.distance = self.model.stat.extent * 1.0
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = 0.25 / 0.015 * obs[:, 22]
+
+        quad_impact_cost = .5e-6 * np.square(obs[:, -84:]).sum()
+        quad_impact_cost = min(quad_impact_cost, 10)
+
+        height = next_obs[:, 0]
+        done = np.logical_or((height > 2.0), (height < 1.0))
+        alive_reward = 5 * (1.0 - np.array(done, dtype=np.float))
+
+        reward = reward_run + reward_ctrl + (-quad_impact_cost) + alive_reward
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+
+        height = next_states[:, 0]
+        done = np.logical_or((height > 2.0), (height < 1.0))
+        return rewards, done
+
+    def verify(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_nostopslimhumanoid.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_nostopslimhumanoid.py
new file mode 100644
index 0000000..1c87b20
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_nostopslimhumanoid.py
@@ -0,0 +1,81 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+import numpy as np
+from gym.envs.mujoco import mujoco_env
+from gym import utils
+from slbo.envs import BaseModelBasedEnv
+
+
+class HumanoidEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self):
+        mujoco_env.MujocoEnv.__init__(self, 'humanoid.xml', 5)
+        utils.EzPickle.__init__(self)
+
+    def _get_obs(self):
+        data = self.model.data
+        return np.concatenate([data.qpos.flat[2:],
+                               data.qvel.flat])
+
+    def _step(self, a):
+        data = self.model.data
+        action = a
+        if getattr(self, 'action_space', None):
+            action = np.clip(a, self.action_space.low,
+                             self.action_space.high)
+        qpos = self.model.data.qpos
+        done = bool((qpos[2] < 1.0) or (qpos[2] > 2.0))
+
+        # reward
+        alive_bonus = 5 * (1 - float(done))
+        lin_vel_cost = 0.25 / 0.015 * data.qvel.flat[0]
+        quad_ctrl_cost = 0.1 * np.square(action).sum()
+        quad_impact_cost = 0.0
+
+        self.do_simulation(action, self.frame_skip)
+        reward = lin_vel_cost - quad_ctrl_cost - quad_impact_cost + alive_bonus
+        done = False
+        return self._get_obs(), reward, done, dict(reward_linvel=lin_vel_cost, reward_quadctrl=-quad_ctrl_cost, reward_alive=alive_bonus, reward_impact=-quad_impact_cost)
+
+    def reset_model(self):
+        c = 0.01
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-c, high=c, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-c, high=c, size=self.model.nv,)
+        )
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 1
+        self.viewer.cam.distance = self.model.stat.extent * 1.0
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = 0.25 / 0.015 * obs[:, 22]
+
+        quad_impact_cost = 0.0
+
+        height = next_obs[:, 0]
+        done = np.logical_or((height > 2.0), (height < 1.0))
+        alive_reward = 5 * (1.0 - np.array(done, dtype=np.float))
+
+        reward = reward_run + reward_ctrl + (-quad_impact_cost) + alive_reward
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_pendulumO001.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_pendulumO001.py
new file mode 100644
index 0000000..de873df
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_pendulumO001.py
@@ -0,0 +1,138 @@
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+from os import path
+
+
+class PendulumEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 30
+    }
+
+    def __init__(self):
+        self.max_speed = 8
+        self.max_torque = 2.
+        self.dt = .05
+        self.viewer = None
+
+        high = np.array([1., 1., self.max_speed])
+        self.action_space = spaces.Box(low=-self.max_torque, high=self.max_torque, shape=(1,))
+        self.observation_space = spaces.Box(low=-high, high=high)
+
+        self._seed()
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, u):
+        th, thdot = self.state  # th := theta
+        '''
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+        '''
+
+        # for the reward
+        y, x, thetadot = np.cos(th), np.sin(th), thdot
+        u = np.clip(u, -self.max_torque, self.max_torque)[0]
+        costs = y + .1 * np.abs(x) + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        reward = -costs
+
+        g = 10.
+        m = 1.
+        l = 1.
+        dt = self.dt
+
+        self.last_u = u  # for rendering
+        # costs = angle_normalize(th) ** 2 + .1 * thdot ** 2 + .001 * (u ** 2)
+
+        newthdot = thdot + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3. / (m * l ** 2) * u) * dt
+        newth = th + newthdot * dt
+        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)  # pylint: disable=E1111
+
+        self.state = np.array([newth, newthdot])
+        ob = self._get_obs()
+        ob += np.random.uniform(low=-0.01, high=0.01, size=ob.shape)
+        return ob, reward, False, {}
+
+    def _reset(self):
+        high = np.array([np.pi, 1])
+        self.state = self.np_random.uniform(low=-high, high=high)
+        self.last_u = None
+        return self._get_obs()
+
+    def _get_obs(self):
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(500, 500)
+            self.viewer.set_bounds(-2.2, 2.2, -2.2, 2.2)
+            rod = rendering.make_capsule(1, .2)
+            rod.set_color(.8, .3, .3)
+            self.pole_transform = rendering.Transform()
+            rod.add_attr(self.pole_transform)
+            self.viewer.add_geom(rod)
+            axle = rendering.make_circle(.05)
+            axle.set_color(0, 0, 0)
+            self.viewer.add_geom(axle)
+            fname = path.join(path.dirname(__file__), "assets/clockwise.png")
+            self.img = rendering.Image(fname, 1., 1.)
+            self.imgtrans = rendering.Transform()
+            self.img.add_attr(self.imgtrans)
+
+        self.viewer.add_onetime(self.img)
+        self.pole_transform.set_rotation(self.state[0] + np.pi / 2)
+        if self.last_u:
+            self.imgtrans.scale = (-self.last_u / 2, np.abs(self.last_u) / 2)
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+        dist_vec = obs[:, -3:]
+        reward_dist = - np.linalg.norm(dist_vec, axis=1)
+        reward_ctrl = - np.sum(np.square(acts), axis=1)
+        reward = reward_dist + reward_ctrl
+
+        # for the reward
+        y, x, thetadot = np.cos(th), np.sin(th), thdot
+        u = np.clip(u, -self.max_torque, self.max_torque)[0]
+        costs = y + .1 * x + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        reward = -costs
+
+        def _get_obs(self):
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+
+        """
+        y, x, thetadot = obs[:, 0], obs[:, 1], obs[:, 2]
+        u = np.clip(acts[:, 0], -self.max_torque, self.max_torque)
+        costs = y + .1 * np.abs(x) + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        return costs
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
+
+
+def angle_normalize(x):
+    return (((x + np.pi) % (2 * np.pi)) - np.pi)
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_pendulumO01.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_pendulumO01.py
new file mode 100644
index 0000000..dbd460d
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_pendulumO01.py
@@ -0,0 +1,138 @@
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+from os import path
+
+
+class PendulumEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 30
+    }
+
+    def __init__(self):
+        self.max_speed = 8
+        self.max_torque = 2.
+        self.dt = .05
+        self.viewer = None
+
+        high = np.array([1., 1., self.max_speed])
+        self.action_space = spaces.Box(low=-self.max_torque, high=self.max_torque, shape=(1,))
+        self.observation_space = spaces.Box(low=-high, high=high)
+
+        self._seed()
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, u):
+        th, thdot = self.state  # th := theta
+        '''
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+        '''
+
+        # for the reward
+        y, x, thetadot = np.cos(th), np.sin(th), thdot
+        u = np.clip(u, -self.max_torque, self.max_torque)[0]
+        costs = y + .1 * np.abs(x) + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        reward = -costs
+
+        g = 10.
+        m = 1.
+        l = 1.
+        dt = self.dt
+
+        self.last_u = u  # for rendering
+        # costs = angle_normalize(th) ** 2 + .1 * thdot ** 2 + .001 * (u ** 2)
+
+        newthdot = thdot + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3. / (m * l ** 2) * u) * dt
+        newth = th + newthdot * dt
+        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)  # pylint: disable=E1111
+
+        self.state = np.array([newth, newthdot])
+        ob = self._get_obs()
+        ob += np.random.uniform(low=-0.1, high=0.1, size=ob.shape)
+        return ob, reward, False, {}
+
+    def _reset(self):
+        high = np.array([np.pi, 1])
+        self.state = self.np_random.uniform(low=-high, high=high)
+        self.last_u = None
+        return self._get_obs()
+
+    def _get_obs(self):
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(500, 500)
+            self.viewer.set_bounds(-2.2, 2.2, -2.2, 2.2)
+            rod = rendering.make_capsule(1, .2)
+            rod.set_color(.8, .3, .3)
+            self.pole_transform = rendering.Transform()
+            rod.add_attr(self.pole_transform)
+            self.viewer.add_geom(rod)
+            axle = rendering.make_circle(.05)
+            axle.set_color(0, 0, 0)
+            self.viewer.add_geom(axle)
+            fname = path.join(path.dirname(__file__), "assets/clockwise.png")
+            self.img = rendering.Image(fname, 1., 1.)
+            self.imgtrans = rendering.Transform()
+            self.img.add_attr(self.imgtrans)
+
+        self.viewer.add_onetime(self.img)
+        self.pole_transform.set_rotation(self.state[0] + np.pi / 2)
+        if self.last_u:
+            self.imgtrans.scale = (-self.last_u / 2, np.abs(self.last_u) / 2)
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+        dist_vec = obs[:, -3:]
+        reward_dist = - np.linalg.norm(dist_vec, axis=1)
+        reward_ctrl = - np.sum(np.square(acts), axis=1)
+        reward = reward_dist + reward_ctrl
+
+        # for the reward
+        y, x, thetadot = np.cos(th), np.sin(th), thdot
+        u = np.clip(u, -self.max_torque, self.max_torque)[0]
+        costs = y + .1 * x + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        reward = -costs
+
+        def _get_obs(self):
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+
+        """
+        y, x, thetadot = obs[:, 0], obs[:, 1], obs[:, 2]
+        u = np.clip(acts[:, 0], -self.max_torque, self.max_torque)
+        costs = y + .1 * np.abs(x) + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        return costs
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
+
+
+def angle_normalize(x):
+    return (((x + np.pi) % (2 * np.pi)) - np.pi)
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_slimhumanoid.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_slimhumanoid.py
new file mode 100644
index 0000000..cfaf6ad
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_slimhumanoid.py
@@ -0,0 +1,82 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+import numpy as np
+from gym.envs.mujoco import mujoco_env
+from gym import utils
+from slbo.envs import BaseModelBasedEnv
+
+
+class HumanoidEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self):
+        mujoco_env.MujocoEnv.__init__(self, 'humanoid.xml', 5)
+        utils.EzPickle.__init__(self)
+
+    def _get_obs(self):
+        data = self.model.data
+        return np.concatenate([data.qpos.flat[2:],
+                               data.qvel.flat])
+
+    def _step(self, a):
+        data = self.model.data
+        action = a
+        if getattr(self, 'action_space', None):
+            action = np.clip(a, self.action_space.low,
+                             self.action_space.high)
+
+        # reward
+        alive_bonus = 5.0
+        lin_vel_cost = 0.25 / 0.015 * data.qvel.flat[0]
+        quad_ctrl_cost = 0.1 * np.square(action).sum()
+        quad_impact_cost = 0.0
+
+        self.do_simulation(action, self.frame_skip)
+        reward = lin_vel_cost - quad_ctrl_cost - quad_impact_cost + alive_bonus
+        qpos = self.model.data.qpos
+        done = bool((qpos[2] < 1.0) or (qpos[2] > 2.0))
+        return self._get_obs(), reward, done, dict(reward_linvel=lin_vel_cost, reward_quadctrl=-quad_ctrl_cost, reward_alive=alive_bonus, reward_impact=-quad_impact_cost)
+
+    def reset_model(self):
+        c = 0.01
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-c, high=c, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-c, high=c, size=self.model.nv,)
+        )
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 1
+        self.viewer.cam.distance = self.model.stat.extent * 1.0
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = 0.25 / 0.015 * obs[:, 22]
+
+        quad_impact_cost = 0.0
+
+        height = next_obs[:, 0]
+        done = np.logical_or((height > 2.0), (height < 1.0))
+        alive_reward = 5 * (1.0 - np.array(done, dtype=np.float))
+
+        reward = reward_run + reward_ctrl + (-quad_impact_cost) + alive_reward
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        height = next_states[:, 0]
+        done = np.logical_or((height > 2.0), (height < 1.0))
+        return rewards, done
+
+    def verify(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/half_cheetah.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/half_cheetah.py
new file mode 100644
index 0000000..97be1c5
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/half_cheetah.py
@@ -0,0 +1,76 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/half_cheetah.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        start_ob = self._get_obs()
+        reward_run = start_ob[8]
+
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        reward_ctrl = -0.1 * np.square(action).sum()
+
+        reward = reward_run + reward_ctrl
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                             self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        qpos = self.init_qpos + \
+            self.np_random.uniform(low=-.1, high=.1, size=self.model.nq)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/hopper.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/hopper.py
new file mode 100644
index 0000000..bb2f509
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/hopper.py
@@ -0,0 +1,84 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HopperEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=4):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/hopper.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = old_ob[5]
+        reward_height = -3.0 * np.square(old_ob[0] - 1.3)
+        reward = reward_run + reward_ctrl + reward_height + 1.0
+
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-.005, high=.005, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-.005, high=.005, size=self.model.nv)
+        )
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 2
+        self.viewer.cam.distance = self.model.stat.extent * 0.75
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 5]
+        reward_height = -3.0 * np.square(obs[:, 0] - 1.3)
+        reward = reward_run + reward_ctrl + reward_height + 1.0
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        # reward_height = -3.0 * tf.square(next_obs[:, 1] - 1.3)
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+        raise NotImplementedError
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/inverted_pendulum.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/inverted_pendulum.py
new file mode 100644
index 0000000..f05af7b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/inverted_pendulum.py
@@ -0,0 +1,74 @@
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.utils.dataset import Dataset, gen_dtype
+from lunzi.Logger import logger
+
+
+class InvertedPendulumEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+
+    def __init__(self):
+        utils.EzPickle.__init__(self)
+        mujoco_env.MujocoEnv.__init__(self, 'inverted_pendulum.xml', 2)
+
+    def _step(self, a):
+        # reward = 1.0
+        reward = self._get_reward()
+        self.do_simulation(a, self.frame_skip)
+        ob = self._get_obs()
+        # notdone = np.isfinite(ob).all() and (np.abs(ob[1]) <= .2)
+        # done = not notdone
+        done = False
+        return ob, reward, done, {}
+
+    def reset_model(self):
+        qpos = self.init_qpos + self.np_random.uniform(size=self.model.nq, low=-0.01, high=0.01)
+        qvel = self.init_qvel + self.np_random.uniform(size=self.model.nv, low=-0.01, high=0.01)
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def _get_reward(self):
+        old_ob = self._get_obs()
+        reward = -((old_ob[1]) ** 2)
+        return reward
+
+    def _get_obs(self):
+        return np.concatenate([self.model.data.qpos, self.model.data.qvel]).ravel()
+
+    def viewer_setup(self):
+        v = self.viewer
+        v.cam.trackbodyid = 0
+        v.cam.distance = v.model.stat.extent
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        return ((obs[:, 1]) ** 2)
+
+    def verify(self, n=2000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/mountain_car.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/mountain_car.py
new file mode 100644
index 0000000..01ed157
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/mountain_car.py
@@ -0,0 +1,199 @@
+# -*- coding: utf-8 -*-
+"""
+@author: Olivier Sigaud
+A merge between two sources:
+* Adaptation of the MountainCar Environment from the "FAReinforcement" library
+of Jose Antonio Martin H. (version 1.0), adapted by  'Tom Schaul, tom@idsia.ch'
+and then modified by Arnaud de Broissia
+* the OpenAI/gym MountainCar environment
+itself from
+https://webdocs.cs.ualberta.ca/~sutton/MountainCar/MountainCar1.cp
+"""
+
+import math
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+from slbo.utils.dataset import Dataset, gen_dtype
+from lunzi.Logger import logger
+
+
+class Continuous_MountainCarEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 30
+    }
+
+    def __init__(self):
+        self.min_action = -1.0
+        self.max_action = 1.0
+        self.min_position = -1.2
+        self.max_position = 0.6
+        self.max_speed = 0.07
+        self.goal_position = 0.45  # was 0.5 in gym, 0.45 in Arnaud de Broissia's version
+        self.power = 0.0015
+
+        self.low_state = np.array([self.min_position, -self.max_speed])
+        self.high_state = np.array([self.max_position, self.max_speed])
+
+        self.viewer = None
+
+        self.action_space = spaces.Box(self.min_action, self.max_action, shape=(1,))
+        self.observation_space = spaces.Box(self.low_state, self.high_state)
+
+        self._seed()
+        self.reset()
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, action):
+
+        position = self.state[0]
+        velocity = self.state[1]
+        force = min(max(action[0], -1.0), 1.0)
+        #reward = position
+
+        velocity += force * self.power - 0.0025 * math.cos(3 * position)
+        if (velocity > self.max_speed):
+            velocity = self.max_speed
+        if (velocity < -self.max_speed):
+            velocity = -self.max_speed
+        position += velocity
+        if (position > self.max_position):
+            position = self.max_position
+        if (position < self.min_position):
+            position = self.min_position
+        if (position == self.min_position and velocity < 0):
+            velocity = 0
+
+
+        done = bool(position >= self.goal_position)
+
+        reward = 0
+        if done:
+            reward = 100.0
+        reward -= math.pow(action[0], 2) * 0.1
+
+
+        #done = False
+        self.state = np.array([position, velocity])
+        return self.state, reward, done, {}
+
+    def _reset(self):
+        self.state = np.array([self.np_random.uniform(low=-0.6, high=-0.4), 0])
+        return np.array(self.state)
+
+#    def get_state(self):
+#        return self.state
+
+    def _height(self, xs):
+        return np.sin(3 * xs) * .45 + .55
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        screen_width = 600
+        screen_height = 400
+
+        world_width = self.max_position - self.min_position
+        scale = screen_width / world_width
+        carwidth = 40
+        carheight = 20
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(screen_width, screen_height)
+            xs = np.linspace(self.min_position, self.max_position, 100)
+            ys = self._height(xs)
+            xys = list(zip((xs - self.min_position) * scale, ys * scale))
+
+            self.track = rendering.make_polyline(xys)
+            self.track.set_linewidth(4)
+            self.viewer.add_geom(self.track)
+
+            clearance = 10
+
+            l, r, t, b = -carwidth / 2, carwidth / 2, carheight, 0
+            car = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            car.add_attr(rendering.Transform(translation=(0, clearance)))
+            self.cartrans = rendering.Transform()
+            car.add_attr(self.cartrans)
+            self.viewer.add_geom(car)
+            frontwheel = rendering.make_circle(carheight / 2.5)
+            frontwheel.set_color(.5, .5, .5)
+            frontwheel.add_attr(rendering.Transform(translation=(carwidth / 4, clearance)))
+            frontwheel.add_attr(self.cartrans)
+            self.viewer.add_geom(frontwheel)
+            backwheel = rendering.make_circle(carheight / 2.5)
+            backwheel.add_attr(rendering.Transform(translation=(-carwidth / 4, clearance)))
+            backwheel.add_attr(self.cartrans)
+            backwheel.set_color(.5, .5, .5)
+            self.viewer.add_geom(backwheel)
+            flagx = (self.goal_position - self.min_position) * scale
+            flagy1 = self._height(self.goal_position) * scale
+            flagy2 = flagy1 + 50
+            flagpole = rendering.Line((flagx, flagy1), (flagx, flagy2))
+            self.viewer.add_geom(flagpole)
+            flag = rendering.FilledPolygon([(flagx, flagy2), (flagx, flagy2 - 10), (flagx + 25, flagy2 - 5)])
+            flag.set_color(.8, .8, 0)
+            self.viewer.add_geom(flag)
+
+        pos = self.state[0]
+        self.cartrans.set_translation((pos - self.min_position) * scale, self._height(pos) * scale)
+        self.cartrans.set_rotation(math.cos(3 * pos))
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        dones = rewards > 0
+        return rewards, dones
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+        position = self.state[0]
+        velocity = self.state[1]
+        force = min(max(action[0], -1.0), 1.0)
+        reward = position
+        """
+        positions = next_obs[:,0]
+        rewards = np.zeros(len(positions))
+        for i in range(len(rewards)):
+            if positions[i] >= self.goal_position:
+                rewards[i] = 100.0
+
+        rewards = rewards - np.power(acts[:,0], 2) * 0.1
+
+        return -rewards
+
+    def verify(self, n=2000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pendulum.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pendulum.py
new file mode 100644
index 0000000..a64b39c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pendulum.py
@@ -0,0 +1,155 @@
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+from os import path
+from slbo.utils.dataset import Dataset, gen_dtype
+from lunzi.Logger import logger
+
+
+class PendulumEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 30
+    }
+
+    def __init__(self):
+        self.max_speed = 8
+        self.max_torque = 2.
+        self.dt = .05
+        self.viewer = None
+
+        high = np.array([1., 1., self.max_speed])
+        self.action_space = spaces.Box(low=-self.max_torque, high=self.max_torque, shape=(1,))
+        self.observation_space = spaces.Box(low=-high, high=high)
+
+        self._seed()
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, u):
+        th, thdot = self.state  # th := theta
+        '''
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+        '''
+
+        # for the reward
+        y, x, thetadot = np.cos(th), np.sin(th), thdot
+        u = np.clip(u, -self.max_torque, self.max_torque)[0]
+        costs = y + .1 * np.abs(x) + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        reward = -costs
+
+        g = 10.
+        m = 1.
+        l = 1.
+        dt = self.dt
+
+        self.last_u = u  # for rendering
+        # costs = angle_normalize(th) ** 2 + .1 * thdot ** 2 + .001 * (u ** 2)
+
+        newthdot = thdot + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3. / (m * l ** 2) * u) * dt
+        newth = th + newthdot * dt
+        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)  # pylint: disable=E1111
+
+        self.state = np.array([newth, newthdot])
+        return self._get_obs(), reward, False, {}
+
+    def _reset(self):
+        high = np.array([np.pi, 1])
+        self.state = self.np_random.uniform(low=-high, high=high)
+        self.last_u = None
+        return self._get_obs()
+
+    def _get_obs(self):
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(500, 500)
+            self.viewer.set_bounds(-2.2, 2.2, -2.2, 2.2)
+            rod = rendering.make_capsule(1, .2)
+            rod.set_color(.8, .3, .3)
+            self.pole_transform = rendering.Transform()
+            rod.add_attr(self.pole_transform)
+            self.viewer.add_geom(rod)
+            axle = rendering.make_circle(.05)
+            axle.set_color(0, 0, 0)
+            self.viewer.add_geom(axle)
+            fname = path.join(path.dirname(__file__), "assets/clockwise.png")
+            self.img = rendering.Image(fname, 1., 1.)
+            self.imgtrans = rendering.Transform()
+            self.img.add_attr(self.imgtrans)
+
+        self.viewer.add_onetime(self.img)
+        self.pole_transform.set_rotation(self.state[0] + np.pi / 2)
+        if self.last_u:
+            self.imgtrans.scale = (-self.last_u / 2, np.abs(self.last_u) / 2)
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+        dist_vec = obs[:, -3:]
+        reward_dist = - np.linalg.norm(dist_vec, axis=1)
+        reward_ctrl = - np.sum(np.square(acts), axis=1)
+        reward = reward_dist + reward_ctrl
+
+        # for the reward
+        y, x, thetadot = np.cos(th), np.sin(th), thdot
+        u = np.clip(u, -self.max_torque, self.max_torque)[0]
+        costs = y + .1 * x + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        reward = -costs
+
+        def _get_obs(self):
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+
+        """
+        y, x, thetadot = obs[:, 0], obs[:, 1], obs[:, 2]
+        u = np.clip(acts[:, 0], -self.max_torque, self.max_torque)
+        costs = y + .1 * np.abs(x) + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        return costs
+
+    def verify(self, n=2000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
+
+
+def angle_normalize(x):
+    return (((x + np.pi) % (2 * np.pi)) - np.pi)
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_cartpole.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_cartpole.py
new file mode 100644
index 0000000..a048535
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_cartpole.py
@@ -0,0 +1,53 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+
+
+class CartpoleEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+    PENDULUM_LENGTH = 0.6
+
+    def __init__(self):
+        utils.EzPickle.__init__(self)
+        dir_path = os.path.dirname(os.path.realpath(__file__))
+        mujoco_env.MujocoEnv.__init__(self, '%s/assets/cartpole.xml' % dir_path, 2)
+
+    def _step(self, a):
+        self.do_simulation(a, self.frame_skip)
+        ob = self._get_obs()
+
+        cost_lscale = CartpoleEnv.PENDULUM_LENGTH
+        reward = np.exp(
+            -np.sum(np.square(self._get_ee_pos(ob) - np.array([0.0, CartpoleEnv.PENDULUM_LENGTH]))) / (cost_lscale ** 2)
+        )
+        reward -= 0.01 * np.sum(np.square(a))
+
+        done = False
+        return ob, reward, done, {}
+
+    def reset_model(self):
+        qpos = self.init_qpos + np.random.normal(0, 0.1, np.shape(self.init_qpos))
+        qvel = self.init_qvel + np.random.normal(0, 0.1, np.shape(self.init_qvel))
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def _get_obs(self):
+        return np.concatenate([self.model.data.qpos, self.model.data.qvel]).ravel()
+
+    @staticmethod
+    def _get_ee_pos(x):
+        x0, theta = x[0], x[1]
+        return np.array([
+            x0 - CartpoleEnv.PENDULUM_LENGTH * np.sin(theta),
+            -CartpoleEnv.PENDULUM_LENGTH * np.cos(theta)
+        ])
+
+    def viewer_setup(self):
+        v = self.viewer
+        v.cam.trackbodyid = 0
+        v.cam.distance = v.model.stat.extent
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_cheetah.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_cheetah.py
new file mode 100644
index 0000000..1f73b66
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_cheetah.py
@@ -0,0 +1,54 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+
+
+class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+
+    def __init__(self):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.realpath(__file__))
+        mujoco_env.MujocoEnv.__init__(self, '%s/assets/half_cheetah.xml' % dir_path, 5)
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = ob[0] - 0.0 * np.square(ob[2])
+        reward = reward_run + reward_ctrl
+
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            (self.model.data.qpos.flat[:1] - self.prev_qpos[:1]) / self.dt,
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        qpos = self.init_qpos + np.random.normal(loc=0, scale=0.001, size=self.model.nq)
+        qvel = self.init_qvel + np.random.normal(loc=0, scale=0.001, size=self.model.nv)
+        self.set_state(qpos, qvel)
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.25
+        self.viewer.cam.elevation = -55
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_pusher.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_pusher.py
new file mode 100644
index 0000000..854f477
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_pusher.py
@@ -0,0 +1,85 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+
+
+class PusherEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+
+    def __init__(self):
+        dir_path = os.path.dirname(os.path.realpath(__file__))
+        mujoco_env.MujocoEnv.__init__(self, '%s/assets/pusher.xml' % dir_path, 4)
+        utils.EzPickle.__init__(self)
+        self.reset_model()
+
+    def _step(self, a):
+        obj_pos = self.get_body_com("object"),
+        vec_1 = obj_pos - self.get_body_com("tips_arm")
+        vec_2 = obj_pos - self.get_body_com("goal")
+
+        reward_near = -np.sum(np.abs(vec_1))
+        reward_dist = -np.sum(np.abs(vec_2))
+        reward_ctrl = -np.square(a).sum()
+        reward = 1.25 * reward_dist + 0.1 * reward_ctrl + 0.5 * reward_near
+
+        self.do_simulation(a, self.frame_skip)
+        ob = self._get_obs()
+        done = False
+        return ob, reward, done, {}
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = -1
+        self.viewer.cam.distance = 4.0
+
+    def reset_model(self):
+        qpos = self.init_qpos
+
+        self.goal_pos = np.asarray([0, 0])
+        self.cylinder_pos = np.array([-0.25, 0.15]) + np.random.normal(0, 0.025, [2])
+
+        qpos[-4:-2] = self.cylinder_pos
+        qpos[-2:] = self.goal_pos
+        qvel = self.init_qvel + \
+            self.np_random.uniform(low=-0.005, high=0.005, size=self.model.nv)
+        qvel[-4:] = 0
+        self.set_state(qpos, qvel)
+        self.ac_goal_pos = self.get_body_com("goal")
+
+        return self._get_obs()
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[:7],
+            self.model.data.qvel.flat[:7],
+            self.get_body_com("tips_arm"),
+            self.get_body_com("object"),
+            self.get_body_com("goal"),
+        ])
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+        to_w, og_w = 0.5, 1.25
+        tip_pos, obj_pos, goal_pos = obs[:, 14:17], obs[:, 17:20], obs[:, -3:]
+
+        tip_obj_dist = np.sum(np.abs(tip_pos - obj_pos), axis=1)
+        obj_goal_dist = np.sum(np.abs(goal_pos - obj_pos), axis=1)
+        return to_w * tip_obj_dist + og_w * obj_goal_dist
+
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward = reward_run + reward_ctrl
+        """
+        to_w, og_w = 0.5, 1.25
+        tip_pos, obj_pos, goal_pos = obs[:, 14:17], obs[:, 17:20], obs[:, -3:]
+
+        tip_obj_dist = -np.sum(np.abs(tip_pos - obj_pos), axis=1)
+        obj_goal_dist = -np.sum(np.abs(goal_pos - obj_pos), axis=1)
+        ctrl_reward = -0.1 * np.sum(np.square(acts), axis=1)
+
+        reward = to_w * tip_obj_dist + og_w * obj_goal_dist + ctrl_reward
+        return -reward
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_reacher.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_reacher.py
new file mode 100644
index 0000000..ba419ac
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_reacher.py
@@ -0,0 +1,95 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+
+
+class Reacher3DEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+
+    def __init__(self):
+        self.viewer = None
+        utils.EzPickle.__init__(self)
+        dir_path = os.path.dirname(os.path.realpath(__file__))
+        self.goal = np.zeros(3)
+        mujoco_env.MujocoEnv.__init__(self, os.path.join(dir_path, 'assets/reacher3d.xml'), 2)
+
+    def _step(self, a):
+        self.do_simulation(a, self.frame_skip)
+        ob = self._get_obs()
+        reward = -np.sum(np.square(self.get_EE_pos(ob[None]) - self.goal))
+        reward -= 0.01 * np.square(a).sum()
+        done = False
+        return ob, reward, done, dict(reward_dist=0, reward_ctrl=0)
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 1
+        self.viewer.cam.distance = 2.5
+        self.viewer.cam.elevation = -30
+        self.viewer.cam.azimuth = 270
+
+    def reset_model(self):
+        qpos, qvel = np.copy(self.init_qpos), np.copy(self.init_qvel)
+        qpos[-3:] += np.random.normal(loc=0, scale=0.1, size=[3])
+        qvel[-3:] = 0
+        self.goal = qpos[-3:]
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def _get_obs(self):
+        raw_obs = np.concatenate([
+            self.model.data.qpos.flat, self.model.data.qvel.flat[:-3],
+        ])
+
+        EE_pos = np.reshape(self.get_EE_pos(raw_obs[None]), [-1])
+
+        return np.concatenate([raw_obs, EE_pos])
+
+    def get_EE_pos(self, states):
+        theta1, theta2, theta3, theta4, theta5, theta6, theta7 = \
+            states[:, :1], states[:, 1:2], states[:, 2:3], states[:, 3:4], states[:, 4:5], states[:, 5:6], states[:, 6:]
+
+        rot_axis = np.concatenate([np.cos(theta2) * np.cos(theta1), np.cos(theta2) * np.sin(theta1), -np.sin(theta2)],
+                                  axis=1)
+        rot_perp_axis = np.concatenate([-np.sin(theta1), np.cos(theta1), np.zeros(theta1.shape)], axis=1)
+        cur_end = np.concatenate([
+            0.1 * np.cos(theta1) + 0.4 * np.cos(theta1) * np.cos(theta2),
+            0.1 * np.sin(theta1) + 0.4 * np.sin(theta1) * np.cos(theta2) - 0.188,
+            -0.4 * np.sin(theta2)
+        ], axis=1)
+
+        for length, hinge, roll in [(0.321, theta4, theta3), (0.16828, theta6, theta5)]:
+            perp_all_axis = np.cross(rot_axis, rot_perp_axis)
+            x = np.cos(hinge) * rot_axis
+            y = np.sin(hinge) * np.sin(roll) * rot_perp_axis
+            z = -np.sin(hinge) * np.cos(roll) * perp_all_axis
+            new_rot_axis = x + y + z
+            new_rot_perp_axis = np.cross(new_rot_axis, rot_axis)
+            new_rot_perp_axis[np.linalg.norm(new_rot_perp_axis, axis=1) < 1e-30] = \
+                rot_perp_axis[np.linalg.norm(new_rot_perp_axis, axis=1) < 1e-30]
+            new_rot_perp_axis /= np.linalg.norm(new_rot_perp_axis, axis=1, keepdims=True)
+            rot_axis, rot_perp_axis, cur_end = new_rot_axis, new_rot_perp_axis, cur_end + length * new_rot_axis
+
+        return cur_end
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+        def obs_cost_fn(self, obs):
+            self.ENV.goal = obs[:, 7: 10]
+            ee_pos = obs[:, -3:]
+            return np.sum(np.square(ee_pos - self.ENV.goal), axis=1)
+
+        @staticmethod
+        def ac_cost_fn(acs):
+            return 0.01 * np.sum(np.square(acs), axis=1)
+        """
+        reward_ctrl = -0.01 * np.sum(np.square(acts), axis=1)
+        goal = obs[:, 7: 10]
+        ee_pos = obs[:, -3:]
+
+        reward = -np.sum(np.square(ee_pos - goal), axis=1) + reward_ctrl
+        return -reward
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/reacher.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/reacher.py
new file mode 100644
index 0000000..636cdde
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/reacher.py
@@ -0,0 +1,66 @@
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class ReacherEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self):
+        utils.EzPickle.__init__(self)
+        mujoco_env.MujocoEnv.__init__(self, 'reacher.xml', 2)
+
+    def _step(self, a):
+        vec = self.get_body_com("fingertip") - self.get_body_com("target")
+
+        if getattr(self, 'action_space', None):
+            a = np.clip(a, self.action_space.low,
+                        self.action_space.high)
+        reward_dist = - np.linalg.norm(vec)
+        reward_ctrl = - np.square(a).sum()
+        reward = reward_dist + reward_ctrl
+        self.do_simulation(a, self.frame_skip)
+        ob = self._get_obs()
+        done = False
+        return ob, reward, done, dict(reward_dist=reward_dist, reward_ctrl=reward_ctrl)
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 0
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                             self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        qpos = self.np_random.uniform(low=-0.1, high=0.1, size=self.model.nq) + self.init_qpos
+        while True:
+            self.goal = self.np_random.uniform(low=-.2, high=.2, size=2)
+            if np.linalg.norm(self.goal) < 2:
+                break
+        qpos[-2:] = self.goal
+        qvel = self.init_qvel + self.np_random.uniform(low=-.005, high=.005, size=self.model.nv)
+        qvel[-2:] = 0
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def _get_obs(self):
+        theta = self.model.data.qpos.flat[:2]
+        return np.concatenate([
+            np.cos(theta),
+            np.sin(theta),
+            self.model.data.qpos.flat[2:],
+            self.model.data.qvel.flat[:2],
+            self.get_body_com("fingertip") - self.get_body_com("target")
+        ])
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        dist_vec = obs[:, -3:]
+        reward_dist = - np.linalg.norm(dist_vec, axis=1)
+        reward_ctrl = - np.sum(np.square(acts), axis=1)
+        reward = reward_dist + reward_ctrl
+        return -reward
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/readme.md b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/readme.md
new file mode 100644
index 0000000..28f84a0
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/readme.md
@@ -0,0 +1,5 @@
+# reference
+
+1. mbbl/env/gym_env/walker.py or mbbl/env/gym_env/reacher.py 
+
+2. https://github.com/openai/gym/blob/v0.7.4/gym/envs/mujoco/half_cheetah.py
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/swimmer.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/swimmer.py
new file mode 100644
index 0000000..33e8d79
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/swimmer.py
@@ -0,0 +1,77 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class SwimmerEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=4):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/swimmer.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        ob = self._get_obs()
+
+        reward_ctrl = -0.0001 * np.square(action).sum()
+        reward_run = old_ob[3]
+        reward = reward_run + reward_ctrl
+
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            # (self.model.data.qpos.flat[:1] - self.prev_qpos[:1]) / self.dt,
+            # self.get_body_comvel("torso")[:1],
+            self.model.data.qpos.flat[2:],
+            self.model.data.qvel.flat,
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                             self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-.1, high=.1, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-.1, high=.1, size=self.model.nv)
+        )
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.0001 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 3]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        """
+        reward_ctrl = -0.0001 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+        raise NotImplementedError
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/walker2d.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/walker2d.py
new file mode 100644
index 0000000..1031b32
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym/walker2d.py
@@ -0,0 +1,84 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class Walker2dEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=4):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/walker2d.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = old_ob[8]
+        reward_height = -3.0 * np.square(old_ob[0] - 1.3)
+        reward = reward_run + reward_ctrl + reward_height + 1.0
+
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-.005, high=.005, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-.005, high=.005, size=self.model.nv)
+        )
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 2
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward_height = -3.0 * np.square(obs[:, 0] - 1.3)
+        reward = reward_run + reward_ctrl + reward_height + 1.0
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        # reward_height = -3.0 * tf.square(next_obs[:, 1] - 1.3)
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+        raise NotImplementedError
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym_env.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym_env.py
new file mode 100644
index 0000000..32b9434
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/gym_env.py
@@ -0,0 +1,141 @@
+# import gym
+# import gym.wrappers
+# import gym.envs
+# import gym.spaces
+# import traceback
+# import logging
+#
+# try:
+#     from gym.wrappers.monitoring import logger as monitor_logger
+#
+#     monitor_logger.setLevel(logging.WARNING)
+# except Exception as e:
+#     traceback.print_exc()
+#
+# import os
+# import numpy as np
+# from rllab.misc import logger
+#
+#
+# class CappedCubicVideoSchedule(object):
+#     # Copied from gym, since this method is frequently moved around
+#     def __call__(self, count):
+#         if count < 1000:
+#             return int(round(count ** (1. / 3))) ** 3 == count
+#         else:
+#             return count % 1000 == 0
+#
+#
+# class FixedIntervalVideoSchedule(object):
+#     def __init__(self, interval):
+#         self.interval = interval
+#
+#     def __call__(self, count):
+#         return count % self.interval == 0
+#
+#
+# class NoVideoSchedule(object):
+#     def __call__(self, count):
+#         return False
+#
+#
+# class GymEnv(object):
+#     def __init__(self, env_name, record_video=True, video_schedule=None, log_dir=None, record_log=True,
+#                  force_reset=False):
+#         if log_dir is None:
+#             if logger.get_snapshot_dir() is None:
+#                 logger.log("Warning: skipping Gym environment monitoring since snapshot_dir not configured.")
+#             else:
+#                 log_dir = os.path.join(logger.get_snapshot_dir(), "gym_log")
+#
+#         env = gym.make(env_name)
+#         self.env = env
+#         self.env_id = env.spec.id
+#
+#         assert not (not record_log and record_video)
+#
+#         if log_dir is None or record_log is False:
+#             self.monitoring = False
+#         else:
+#             if not record_video:
+#                 video_schedule = NoVideoSchedule()
+#             else:
+#                 if video_schedule is None:
+#                     video_schedule = CappedCubicVideoSchedule()
+#             self.env = gym.wrappers.Monitor(self.env, log_dir, video_callable=video_schedule, force=True)
+#             self.monitoring = True
+#
+#         self._observation_space = env.observation_space
+#         self._action_space = env.action_space
+#         self._horizon = env.spec.tags['wrapper_config.TimeLimit.max_episode_steps']
+#         self._log_dir = log_dir
+#         self._force_reset = force_reset
+#
+#         self.metadata = {'render.modes': ['human', 'rgb_array']}
+#         self.reward_range = (-np.inf, np.inf)
+#         self.unwrapped = self
+#         self._configured = False
+#         self.spec = None
+#
+#     @property
+#     def inner_env(self):
+#         env = self.env
+#         while hasattr(env, "env"):
+#             env = env.env
+#         return env
+#
+#     @property
+#     def observation_space(self):
+#         return self._observation_space
+#
+#     @property
+#     def action_space(self):
+#         return self._action_space
+#
+#     @property
+#     def horizon(self):
+#         return self._horizon
+#
+#     def reset(self):
+#         if self._force_reset and hasattr(self.env, 'stats_recorder'):
+#             recorder = self.env.stats_recorder
+#             if recorder is not None:
+#                 recorder.done = True
+#
+#         return self.env.reset()
+#
+#     def step(self, action_or_predicted_result):
+#         if isinstance(action_or_predicted_result, dict):
+#             return self._step_with_predicted_dynamics(**action_or_predicted_result)
+#         else:
+#             next_obs, reward, done, info = self.env.step(action_or_predicted_result)
+#             return next_obs, reward, done, info
+#
+#     def _step_with_predicted_dynamics(self, next_obs, reward, done):
+#         qpos = self.inner_env.model.data.qpos.flatten()
+#         qvel = self.inner_env.model.data.qvel.flatten()
+#         self.env.env.set_state(qpos, qvel)
+#         return next_obs, reward, done, {}
+#
+#     def cost_np_vec(self, obs, acts, next_obs):
+#         return self.env.env.cost_np_vec(obs, acts, next_obs)
+#
+#     def cost_tf_vec(self, obs, acts, next_obs):
+#         return self.env.env.cost_tf_vec(obs, acts, next_obs)
+#
+#     def render(self, **kwargs):
+#         return self.env.render(**kwargs)
+#
+#     def terminate(self):
+#         if self.monitoring:
+#             self.env._close()
+#             if self._log_dir is not None:
+#                 print("""
+#     ***************************
+#     Training finished! You can upload results to OpenAI Gym by running the following command:
+#     python scripts/submit_gym.py %s
+#     ***************************
+#                 """ % self._log_dir)
+#
+#     def get_geom_xpos(self):
+#         return self.inner_env.data.geom_xpos
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/neural_env.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/neural_env.py
new file mode 100644
index 0000000..243aa73
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/neural_env.py
@@ -0,0 +1,112 @@
+import numpy as np
+
+
+class NeuralNetEnv:
+
+    def __init__(self, env, inner_env, dynamics):
+        self.vectorized = True
+        self.env = env
+        self.inner_env = inner_env
+        self.is_done = getattr(inner_env, 'is_done', lambda x, y: np.asarray([False] * len(x)))
+        self.dynamics = dynamics
+
+    @property
+    def observation_space(self):
+        return self.env.observation_space
+
+    @property
+    def action_space(self):
+        return self.env.action_space
+
+    @property
+    def spec(self):
+        return self.env.spec
+
+    def reset(self):
+        self.state = self.env.reset()
+        observation = np.copy(self.state)
+        return observation
+
+    def step(self, action, use_states=None):
+        action = np.clip(action, *self.action_space.bounds)
+        if use_states is not None:
+            next_observation = self.dynamics.predict([use_states], [action])[0]
+            obs_dim = self.env.observation_space.shape[0]
+            next_observation[:obs_dim] = np.clip(next_observation[:obs_dim], *self.observation_space.bounds)
+            next_observation[:obs_dim] = np.clip(next_observation[:obs_dim], -1e5, 1e5)
+        else:
+            next_observation = self.dynamics.predict([self.state], [action])[0]
+            next_observation = np.clip(next_observation, *self.observation_space.bounds)
+            next_observation = np.clip(next_observation, -1e5, 1e5)
+
+        if hasattr(self.inner_env, "env"):
+            reward = - self.inner_env.env.cost_np_vec(self.state[None], action[None], np.array([next_observation]))[0]
+        else:
+            reward = - self.inner_env.cost_np_vec(self.state[None], action[None], np.array([next_observation]))[0]
+
+        done = self.is_done(self.state[None], next_observation)[0]
+        self.state = np.reshape(next_observation, -1)
+        return self.inner_env.step({"next_obs": next_observation, "reward": reward, "done": done})
+
+    def render(self):
+        print('current state:', self.state)
+
+    def vec_env_executor(self, n_envs, max_path_length):
+        return VecSimpleEnv(env=self, inner_env=self.inner_env, n_envs=n_envs, max_path_length=max_path_length)
+
+    def terminate(self):
+        self.env.terminate()
+
+
+class VecSimpleEnv(object):
+
+    def __init__(self, env, inner_env, n_envs, max_path_length):
+        self.env = env
+        self.inner_env = inner_env
+        self.n_envs = n_envs
+        self.num_envs = n_envs
+        self.ts = np.zeros((self.n_envs,))
+        self.max_path_length = max_path_length
+        self.obs_dim = env.observation_space.shape[0]
+        self.states = np.zeros((self.n_envs, self.obs_dim))
+
+    def reset(self, dones=None):
+        if dones is None:
+            dones = np.asarray([True] * self.n_envs)
+        else:
+            dones = np.cast['bool'](dones)
+        for i, done in enumerate(dones):
+            if done:
+                self.states[i] = self.env.reset()
+        self.ts[dones] = 0
+        return self.states[dones]
+
+    def step(self, actions, use_states=None):
+        self.ts += 1
+        actions = np.clip(actions, *self.env.action_space.bounds)
+        next_observations = self.get_next_observation(actions, use_states=use_states)
+        if use_states is not None:
+            obs_dim = self.env.observation_space.shape[0]
+            next_observations[:, :obs_dim] = np.clip(next_observations[:, :obs_dim], *self.env.observation_space.bounds)
+            next_observations[:, :obs_dim] = np.clip(next_observations[:, :obs_dim], -1e5, 1e5)
+        else:
+            next_observations = np.clip(next_observations, *self.env.observation_space.bounds)
+            next_observations = np.clip(next_observations, -1e5, 1e5)
+        if hasattr(self.env.inner_env, "cost_np_vec"):
+            rewards = - self.env.inner_env.cost_np_vec(self.states, actions, next_observations)
+        else:
+            rewards = - self.env.inner_env.env.cost_np_vec(self.states, actions, next_observations)
+        self.states = next_observations
+        dones = self.env.is_done(self.states, next_observations)
+        dones[self.ts >= self.max_path_length] = True
+        if np.any(dones):
+            self.reset(dones)
+        return self.states, rewards, dones, dict()
+
+    def get_next_observation(self, actions, use_states=None):
+        if use_states is not None:
+            return self.env.dynamics.predict(use_states, actions)
+        return self.env.dynamics.predict(self.states, actions)
+
+    def terminate(self):
+        self.env.terminate()
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/proxy_env.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/proxy_env.py
new file mode 100644
index 0000000..6381ab4
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/proxy_env.py
@@ -0,0 +1,89 @@
+from gym.core import Env
+from gym.spaces import Box as GymBox
+from gym.wrappers.monitoring import Monitor
+import numpy as np
+import tensorflow as tf
+
+
+class Box:
+
+    def __init__(self, gym_box: GymBox):
+        self.gym_box = gym_box
+
+    @property
+    def flat_dim(self):
+        return np.prod(self.gym_box.shape)
+
+    @property
+    def shape(self):
+        return self.gym_box.shape
+
+    @property
+    def dtype(self):
+        return tf.float32
+
+    @property
+    def bounds(self):
+        return self.gym_box.low, self.gym_box.high
+
+    def flatten_n(self, xs):
+        xs = np.asarray(xs)
+        return xs.reshape((xs.shape[0], -1))
+
+    def sample(self):
+        return self.gym_box.sample()
+
+    def flatten(self, x):
+        return np.asarray(x).flatten()
+
+    def __repr__(self):
+        return "Box Wrapper of shape {}".format(self.shape)
+
+    def __eq__(self, other):
+        return self.gym_box.__eq__(other)
+
+
+class ProxyEnv(Env):
+
+    def __init__(self, wrapped_env: Env):
+        self._wrapped_env = wrapped_env
+        self._wrapped_observation_space = Box(wrapped_env.observation_space)
+        self._wrapped_action_space = Box(wrapped_env.action_space)
+
+    @property
+    def wrapped_env(self):
+        return self._wrapped_env
+
+    def reset(self, **kwargs):
+        return self._wrapped_env.reset(**kwargs)
+
+    @property
+    def action_space(self):
+        return self._wrapped_action_space
+
+    @property
+    def observation_space(self):
+        return self._wrapped_observation_space
+
+    def step(self, action, **kwargs):
+        return self._wrapped_env.step(action, **kwargs)
+
+    def render(self, *args, **kwargs):
+        return self._wrapped_env.render(*args, **kwargs)
+
+    def log_diagnostics(self, paths, *args, **kwargs):
+        self._wrapped_env.log_diagnostics(paths, *args, **kwargs)
+
+    @property
+    def horizon(self):
+        return self._wrapped_env.horizon
+
+    def terminate(self):
+        if isinstance(self._wrapped_env, Monitor):
+            self._wrapped_env._close()
+
+    def get_param_values(self):
+        return self._wrapped_env.get_param_values()
+
+    def set_param_values(self, params):
+        self._wrapped_env.set_param_values(params)
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/vec_env.py b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/vec_env.py
new file mode 100644
index 0000000..d0369e1
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/bm_envs/vec_env.py
@@ -0,0 +1,45 @@
+import numpy as np
+from libs.misc import tensor_utils
+
+
+class VecEnvExecutor(object):
+    def __init__(self, envs, max_path_length, **kwargs):
+        self.envs = envs
+        self._action_space = envs[0].action_space
+        self._observation_space = envs[0].observation_space
+        self.ts = np.zeros(len(self.envs), dtype='int')
+        self.max_path_length = max_path_length
+
+    def step(self, action_n, **kwargs):
+        all_results = [env.step(a) for (a, env) in zip(action_n, self.envs)]
+        obs, rewards, dones, env_infos = list(map(list, list(zip(*all_results))))
+        dones = np.asarray(dones)
+        rewards = np.asarray(rewards)
+        self.ts += 1
+        if self.max_path_length is not None:
+            dones[self.ts >= self.max_path_length] = True
+        for (i, done) in enumerate(dones):
+            if done:
+                obs[i] = self.envs[i].reset()
+                self.ts[i] = 0
+        return obs, rewards, dones, tensor_utils.stack_tensor_dict_list(env_infos)
+
+    def reset(self):
+        results = [env.reset() for env in self.envs]
+        self.ts[:] = 0
+        return results
+
+    @property
+    def num_envs(self):
+        return len(self.envs)
+
+    @property
+    def action_space(self):
+        return self._action_space
+
+    @property
+    def observation_space(self):
+        return self._observation_space
+
+    def terminate(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/mujoco/__init__.py b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco/__init__.py
new file mode 100644
index 0000000..5c7f19c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco/__init__.py
@@ -0,0 +1 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/mujoco/ant_env.py b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco/ant_env.py
new file mode 100644
index 0000000..2a300af
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco/ant_env.py
@@ -0,0 +1,50 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from rllab.envs.mujoco import ant_env
+from rllab.envs.base import Step
+from slbo.envs import BaseModelBasedEnv
+
+
+class AntEnv(ant_env.AntEnv, BaseModelBasedEnv):
+    def get_current_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat,  # 15
+            self.model.data.qvel.flat,  # 14
+            # np.clip(self.model.data.cfrc_ext, -1, 1).flat,  # 84
+            self.get_body_xmat("torso").flat,  # 9
+            self.get_body_com("torso"),  # 9
+            self.get_body_comvel("torso"),  # 3
+        ]).reshape(-1)
+
+    def step(self, action):
+        self.forward_dynamics(action)
+        comvel = self.get_body_comvel("torso")
+        forward_reward = comvel[0]
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+        ctrl_cost = 0.5 * 1e-2 * np.sum(np.square(action / scaling))
+        contact_cost = 0.
+        # contact_cost = 0.5 * 1e-3 * np.sum(
+        #     np.square(np.clip(self.model.data.cfrc_ext, -1, 1))),
+        survive_reward = 0.05
+        reward = forward_reward - ctrl_cost - contact_cost + survive_reward
+        state = self._state
+        notdone = np.isfinite(state).all() and state[2] >= 0.2 and state[2] <= 1.0
+        done = not notdone
+        ob = self.get_current_obs()
+        return Step(ob, float(reward), done)
+
+    def mb_step(self, states: np.ndarray, actions: np.ndarray, next_states: np.ndarray):
+        comvel = next_states[..., -3:]
+        forward_reward = comvel[..., 0]
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+        ctrl_cost = 0.5 * 1e-2 * np.sum(np.square(actions / scaling), axis=-1)
+        contact_cost = 0.
+        # contact_cost = 0.5 * 1e-3 * np.sum(
+        #     np.square(np.clip(self.model.data.cfrc_ext, -1, 1))),
+        survive_reward = 0.05
+        reward = forward_reward - ctrl_cost - contact_cost + survive_reward
+        notdone = np.all([next_states[..., 2] >= 0.2, next_states[..., 2] <= 1.0], axis=0)
+        return reward, 1. - notdone
+
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/mujoco/half_cheetah_env.py b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco/half_cheetah_env.py
new file mode 100644
index 0000000..29b1502
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco/half_cheetah_env.py
@@ -0,0 +1,20 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from rllab.envs.mujoco import half_cheetah_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HalfCheetahEnv(half_cheetah_env.HalfCheetahEnv, BaseModelBasedEnv):
+    def get_current_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat,  # 9
+            self.model.data.qvel.flat,  # 9
+            self.get_body_com("torso").flat,  # 3
+            self.get_body_comvel("torso").flat,  # 3
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        actions = np.clip(actions, *self.action_bounds)
+        reward_ctrl = -0.05 * np.sum(np.square(actions), axis=-1)
+        reward_fwd = next_states[..., 21]
+        return reward_ctrl + reward_fwd, np.zeros_like(reward_fwd, dtype=np.bool)
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/mujoco/hopper_env.py b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco/hopper_env.py
new file mode 100644
index 0000000..23c9c50
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco/hopper_env.py
@@ -0,0 +1,26 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from rllab.envs.mujoco import hopper_env
+from rllab.envs.base import Step
+from slbo.envs import BaseModelBasedEnv
+
+
+class HopperEnv(hopper_env.HopperEnv, BaseModelBasedEnv):
+    def get_current_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat,  # 6
+            self.model.data.qvel.flat,  # 6
+            self.get_body_com("torso").flat,  # 3
+            self.get_body_comvel("torso"),  # 3
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+        vel = next_states[:, -3]
+        reward = vel + self.alive_coeff - 0.5 * self.ctrl_cost_coeff * np.sum(np.square(actions / scaling), axis=-1)
+
+        done = ~((next_states[:, 3:12] < 100).all(axis=-1) &
+                 (next_states[:, 0] > 0.7) &
+                 (np.abs(next_states[:, 2]) < 0.2))
+        return reward, done
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/mujoco/humanoid_env.py b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco/humanoid_env.py
new file mode 100644
index 0000000..a5e55ae
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco/humanoid_env.py
@@ -0,0 +1,53 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from rllab.envs.mujoco import simple_humanoid_env
+from rllab.envs.base import Step
+import numpy as np
+from slbo.envs import BaseModelBasedEnv
+
+
+class HumanoidEnv(simple_humanoid_env.SimpleHumanoidEnv, BaseModelBasedEnv):
+    def get_current_obs(self):
+        data = self.model.data
+        return np.concatenate([
+            data.qpos.flat,  # 17
+            data.qvel.flat,  # 16
+            self.get_body_com("torso").flat,  # 3
+            self.get_body_comvel("torso").flat,  # 3
+        ])
+
+    def step(self, action):
+        self.forward_dynamics(action)
+        alive_bonus = self.alive_bonus
+        data = self.model.data
+
+        comvel = self.get_body_comvel("torso")
+        lin_vel_reward = comvel[0]
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+        ctrl_cost = .5 * self.ctrl_cost_coeff * np.sum(
+            np.square(action / scaling))
+        impact_cost = 0.
+        vel_deviation_cost = 0.5 * self.vel_deviation_cost_coeff * np.sum(
+            np.square(comvel[1:]))
+        reward = lin_vel_reward + alive_bonus - ctrl_cost - \
+            impact_cost - vel_deviation_cost
+        pos = data.qpos.flat[2]
+        done = pos < 0.8 or pos > 2.0
+
+        next_obs = self.get_current_obs()
+        return Step(next_obs, reward, done)
+
+    def mb_step(self, states, actions, next_states):
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+
+        alive_bonus = 0.2
+        lin_vel_reward = next_states[:, 36]
+        ctrl_cost = 5.e-4 * np.square(actions / scaling).sum(axis=1)
+        impact_cost = 0.
+        vel_deviation_cost = 5.e-3 * np.square(next_states[:, 37:39]).sum(axis=1)
+        reward = lin_vel_reward + alive_bonus - ctrl_cost - impact_cost - vel_deviation_cost
+
+        dones = (next_states[:, 2] < 0.8) | (next_states[:, 2] > 2.0)
+        return reward, dones
+
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/mujoco/swimmer_env.py b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco/swimmer_env.py
new file mode 100644
index 0000000..f39bee4
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco/swimmer_env.py
@@ -0,0 +1,22 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from rllab.envs.mujoco import swimmer_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class SwimmerEnv(swimmer_env.SwimmerEnv, BaseModelBasedEnv):
+    def get_current_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat,  # 5
+            self.model.data.qvel.flat,  # 5
+            self.get_body_com("torso").flat,  # 3
+            self.get_body_comvel("torso"),  # 3
+        ]).reshape(-1)
+
+    def mb_step(self, states: np.ndarray, actions: np.ndarray, next_states: np.ndarray):
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+        ctrl_cost = 0.5 * self.ctrl_cost_coeff * np.sum(np.square(actions / scaling), axis=-1)
+        forward_reward = next_states[:, -3]
+        reward = forward_reward - ctrl_cost
+        return reward, np.zeros_like(reward, dtype=np.bool)
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/mujoco/walker2d_env.py b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco/walker2d_env.py
new file mode 100644
index 0000000..8eb16bc
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco/walker2d_env.py
@@ -0,0 +1,43 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from rllab.envs.mujoco import walker2d_env
+from rllab.envs.base import Step
+from slbo.envs import BaseModelBasedEnv
+
+
+class Walker2DEnv(walker2d_env.Walker2DEnv, BaseModelBasedEnv):
+    def get_current_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat,
+            self.model.data.qvel.flat,
+            self.get_body_com("torso").flat,
+            self.get_body_comvel("torso").flat
+        ])
+
+    def step(self, action):
+        self.forward_dynamics(action)
+        forward_reward = self.get_body_comvel("torso")[0]
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+        ctrl_cost = 1e-3 * np.sum(np.square(action / scaling))
+        alive_bonus = 1.
+        reward = forward_reward - ctrl_cost + alive_bonus
+        qpos = self.model.data.qpos
+        done = not (qpos[0] > 0.8 and qpos[0] < 2.0 and qpos[2] > -1.0 and qpos[2] < 1.0)
+        next_obs = self.get_current_obs()
+        return Step(next_obs, reward, done)
+
+    def mb_step(self, states, actions, next_states):
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+
+        reward_ctrl = -0.001 * np.sum(np.square(actions / scaling), axis=-1)
+        reward_fwd = next_states[:, 21]
+        alive_bonus = 1.
+        rewards = reward_ctrl + reward_fwd + alive_bonus
+
+        dones = ~((next_states[:, 0] > 0.8) &
+                  (next_states[:, 0] < 2.0) &
+                  (next_states[:, 2] > -1.0) &
+                  (next_states[:, 2] < 1.0))
+        return rewards, dones
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/__init__.py b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/__init__.py
new file mode 100644
index 0000000..95924e3
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/__init__.py
@@ -0,0 +1,81 @@
+"""
+Mujoco Maze
+----------
+
+A maze environment using mujoco that supports custom tasks and robots.
+"""
+
+
+import gym
+
+from slbo.envs.mujoco_maze.ant import AntEnv
+from slbo.envs.mujoco_maze.maze_task import TaskRegistry
+from slbo.envs.mujoco_maze.point import PointEnv
+from slbo.envs.mujoco_maze.reacher import ReacherEnv
+from slbo.envs.mujoco_maze.swimmer import SwimmerEnv
+
+for maze_id in TaskRegistry.keys():
+    for i, task_cls in enumerate(TaskRegistry.tasks(maze_id)):
+        point_scale = task_cls.MAZE_SIZE_SCALING.point
+        if point_scale is not None:
+            # Point
+            gym.envs.register(
+                id=f"Point{maze_id}-v{i}",
+                entry_point="slbo.envs.mujoco_maze.maze_env:MazeEnv",
+                kwargs=dict(
+                    model_cls=PointEnv,
+                    maze_task=task_cls,
+                    maze_size_scaling=point_scale,
+                    inner_reward_scaling=task_cls.INNER_REWARD_SCALING,
+                ),
+                max_episode_steps=1000,
+                reward_threshold=task_cls.REWARD_THRESHOLD,
+            )
+
+        ant_scale = task_cls.MAZE_SIZE_SCALING.ant
+        if ant_scale is not None:
+            # Ant
+            gym.envs.register(
+                id=f"Ant{maze_id}-v{i}",
+                entry_point="slbo.envs.mujoco_maze.maze_env:MazeEnv",
+                kwargs=dict(
+                    model_cls=AntEnv,
+                    maze_task=task_cls,
+                    maze_size_scaling=ant_scale,
+                    inner_reward_scaling=task_cls.INNER_REWARD_SCALING,
+                ),
+                max_episode_steps=1000,
+                reward_threshold=task_cls.REWARD_THRESHOLD,
+            )
+
+        swimmer_scale = task_cls.MAZE_SIZE_SCALING.swimmer
+        if swimmer_scale is not None:
+            # Reacher
+            gym.envs.register(
+                id=f"Reacher{maze_id}-v{i}",
+                entry_point="mujoco_maze.maze_env:MazeEnv",
+                kwargs=dict(
+                    model_cls=ReacherEnv,
+                    maze_task=task_cls,
+                    maze_size_scaling=task_cls.MAZE_SIZE_SCALING.swimmer,
+                    inner_reward_scaling=task_cls.INNER_REWARD_SCALING,
+                ),
+                max_episode_steps=1000,
+                reward_threshold=task_cls.REWARD_THRESHOLD,
+            )
+            # Swimmer
+            gym.envs.register(
+                id=f"Swimmer{maze_id}-v{i}",
+                entry_point="mujoco_maze.maze_env:MazeEnv",
+                kwargs=dict(
+                    model_cls=SwimmerEnv,
+                    maze_task=task_cls,
+                    maze_size_scaling=task_cls.MAZE_SIZE_SCALING.swimmer,
+                    inner_reward_scaling=task_cls.INNER_REWARD_SCALING,
+                ),
+                max_episode_steps=1000,
+                reward_threshold=task_cls.REWARD_THRESHOLD,
+            )
+
+
+__version__ = "0.1.0"
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/agent_model.py b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/agent_model.py
new file mode 100644
index 0000000..d209d78
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/agent_model.py
@@ -0,0 +1,42 @@
+"""Common APIs for defining mujoco robot.
+"""
+from abc import ABC, abstractmethod
+from typing import Optional
+
+import numpy as np
+from gym.envs.mujoco.mujoco_env import MujocoEnv
+from gym.utils import EzPickle
+
+
+class AgentModel(ABC, MujocoEnv, EzPickle):
+    FILE: str
+    MANUAL_COLLISION: bool
+    ORI_IND: int
+    RADIUS: Optional[float] = None
+
+    def __init__(self, file_path: str, frame_skip: int) -> None:
+        MujocoEnv.__init__(self, file_path, frame_skip)
+        EzPickle.__init__(self)
+
+    def close(self):
+        if self.viewer is not None and hasattr(self.viewer, "window"):
+            import glfw
+
+            glfw.destroy_window(self.viewer.window)
+        super().close()
+
+    @abstractmethod
+    def _get_obs(self) -> np.ndarray:
+        """Returns the observation from the model.
+        """
+        pass
+
+    def get_xy(self) -> np.ndarray:
+        """Returns the coordinate of the agent.
+        """
+        pass
+
+    def set_xy(self, xy: np.ndarray) -> None:
+        """Set the coordinate of the agent.
+        """
+        pass
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/ant.py b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/ant.py
new file mode 100644
index 0000000..b6a725a
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/ant.py
@@ -0,0 +1,108 @@
+"""
+A four-legged robot as an explorer in the maze.
+Based on `models`_ and `gym`_ (both ant and ant-v3).
+
+.. _models: https://github.com/tensorflow/models/tree/master/research/efficient-hrl
+.. _gym: https://github.com/openai/gym
+"""
+
+from typing import Callable, Tuple
+
+import numpy as np
+
+from slbo.envs.mujoco_maze.agent_model import AgentModel
+
+ForwardRewardFn = Callable[[float, float], float]
+
+
+def forward_reward_vabs(xy_velocity: float) -> float:
+    return np.sum(np.abs(xy_velocity))
+
+
+def forward_reward_vnorm(xy_velocity: float) -> float:
+    return np.linalg.norm(xy_velocity)
+
+
+def q_inv(a):
+    return [a[0], -a[1], -a[2], -a[3]]
+
+
+def q_mult(a, b):  # multiply two quaternion
+    w = a[0] * b[0] - a[1] * b[1] - a[2] * b[2] - a[3] * b[3]
+    i = a[0] * b[1] + a[1] * b[0] + a[2] * b[3] - a[3] * b[2]
+    j = a[0] * b[2] - a[1] * b[3] + a[2] * b[0] + a[3] * b[1]
+    k = a[0] * b[3] + a[1] * b[2] - a[2] * b[1] + a[3] * b[0]
+    return [w, i, j, k]
+
+
+class AntEnv(AgentModel):
+    FILE: str = "ant.xml"
+    ORI_IND: int = 3
+    MANUAL_COLLISION: bool = False
+
+    def __init__(
+        self,
+        file_path: str,
+        forward_reward_weight: float = 1.0,
+        ctrl_cost_weight: float = 1e-4,
+        forward_reward_fn: ForwardRewardFn = forward_reward_vnorm,
+    ) -> None:
+        self._forward_reward_weight = forward_reward_weight
+        self._ctrl_cost_weight = ctrl_cost_weight
+        self._forward_reward_fn = forward_reward_fn
+        super().__init__(file_path, 5)
+
+    def _forward_reward(self, xy_pos_before: np.ndarray) -> Tuple[float, np.ndarray]:
+        xy_pos_after = self.sim.data.qpos[:2].copy()
+        xy_velocity = (xy_pos_after - xy_pos_before) / self.dt
+        return self._forward_reward_fn(xy_velocity)
+
+    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, dict]:
+        xy_pos_before = self.sim.data.qpos[:2].copy()
+        self.do_simulation(action, self.frame_skip)
+
+        forward_reward = self._forward_reward(xy_pos_before)
+        ctrl_cost = self._ctrl_cost_weight * np.square(action).sum()
+
+        return (
+            self._get_obs(),
+            self._forward_reward_weight * forward_reward - ctrl_cost,
+            False,
+            dict(reward_forward=forward_reward, reward_ctrl=-ctrl_cost),
+        )
+
+    def _get_obs(self):
+        # No cfrc observation
+        return np.concatenate(
+            [
+                self.sim.data.qpos.flat[:15],  # Ensures only ant obs.
+                self.sim.data.qvel.flat[:14],
+            ]
+        )
+
+    def reset_model(self):
+        qpos = self.init_qpos + self.np_random.uniform(
+            size=self.model.nq, low=-0.1, high=0.1,
+        )
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * 0.1
+
+        # Set everything other than ant to original position and 0 velocity.
+        qpos[15:] = self.init_qpos[15:]
+        qvel[14:] = 0.0
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def get_ori(self) -> np.ndarray:
+        ori = [0, 1, 0, 0]
+        rot = self.sim.data.qpos[self.ORI_IND : self.ORI_IND + 4]  # take the quaternion
+        ori = q_mult(q_mult(rot, ori), q_inv(rot))[1:3]  # project onto x-y plane
+        ori = np.arctan2(ori[1], ori[0])
+        return ori
+
+    def set_xy(self, xy: np.ndarray) -> None:
+        qpos = self.sim.data.qpos.copy()
+        qpos[:2] = xy
+        self.set_state(qpos, self.sim.data.qvel)
+
+    def get_xy(self) -> np.ndarray:
+        return np.copy(self.sim.data.qpos[:2])
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/ant.xml b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/ant.xml
new file mode 100755
index 0000000..ffe156b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/ant.xml
@@ -0,0 +1,80 @@
+<mujoco model="ant">
+  <compiler inertiafromgeom="true" angle="degree" coordinate="local" />
+  <option timestep="0.02" integrator="RK4" />
+  <custom>
+    <numeric name="init_qpos" data="0.0 0.0 0.55 1.0 0.0 0.0 0.0 0.0 1.0 0.0 -1.0 0.0 -1.0 0.0 1.0" />
+  </custom>
+  <default>
+    <joint limited="true" armature="1" damping="1" />
+    <geom condim="3" conaffinity="0" margin="0.01" friction="1 0.5 0.5" solref=".02 1" solimp=".8 .8 .01" rgba="0.8 0.6 0.4 1" density="5.0" />
+  </default>
+  <asset>
+    <texture type="skybox" builtin="gradient" width="100" height="100" rgb1="1 1 1" rgb2="0 0 0" />
+    <texture name="texgeom" type="cube" builtin="flat" mark="cross" width="127" height="1278" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" markrgb="1 1 1" random="0.01" />
+    <texture name="texplane" type="2d" builtin="checker" rgb1="0.2 0.3 0.4" rgb2="0.1 0.2 0.3" width="100" height="100" />
+    <material name='MatPlane' texture="texplane" shininess="1" texrepeat="60 60" specular="1"  reflectance="0.5" />
+    <material name='geom' texture="texgeom" texuniform="true" />
+  </asset>
+  <worldbody>
+    <light directional="true" cutoff="100" exponent="1" diffuse="1 1 1" specular=".1 .1 .1" pos="0 0 1.3" dir="-0 0 -1.3" />
+    <geom name="floor" material="MatPlane" pos="0 0 0" size="40 40 40" type="plane" conaffinity="1" rgba="0.8 0.9 0.8 1" condim="3" />
+    <body name="torso" pos="0 0 0.75">
+      <geom name="torso_geom" type="sphere" size="0.25" pos="0 0 0" />
+      <joint name="root" type="free" limited="false" pos="0 0 0" axis="0 0 1" margin="0.01" armature="0" damping="0" />
+      <body name="front_left_leg" pos="0 0 0">
+        <geom name="aux_1_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 0.2 0.2 0.0" />
+        <body name="aux_1" pos="0.2 0.2 0">
+          <joint name="hip_1" type="hinge" pos="0.0 0.0 0.0" axis="0 0 1" range="-30 30" />
+          <geom name="left_leg_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 0.2 0.2 0.0" />
+          <body pos="0.2 0.2 0">
+            <joint name="ankle_1" type="hinge" pos="0.0 0.0 0.0" axis="-1 1 0" range="30 70" />
+            <geom name="left_ankle_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 0.4 0.4 0.0" />
+          </body>
+        </body>
+      </body>
+      <body name="front_right_leg" pos="0 0 0">
+        <geom name="aux_2_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 -0.2 0.2 0.0" />
+        <body name="aux_2" pos="-0.2 0.2 0">
+          <joint name="hip_2" type="hinge" pos="0.0 0.0 0.0" axis="0 0 1" range="-30 30" />
+          <geom name="right_leg_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 -0.2 0.2 0.0" />
+          <body pos="-0.2 0.2 0">
+            <joint name="ankle_2" type="hinge" pos="0.0 0.0 0.0" axis="1 1 0" range="-70 -30" />
+            <geom name="right_ankle_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 -0.4 0.4 0.0" />
+          </body>
+        </body>
+      </body>
+      <body name="back_leg" pos="0 0 0">
+        <geom name="aux_3_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 -0.2 -0.2 0.0" />
+        <body name="aux_3" pos="-0.2 -0.2 0">
+          <joint name="hip_3" type="hinge" pos="0.0 0.0 0.0" axis="0 0 1" range="-30 30" />
+          <geom name="back_leg_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 -0.2 -0.2 0.0" />
+          <body pos="-0.2 -0.2 0">
+            <joint name="ankle_3" type="hinge" pos="0.0 0.0 0.0" axis="-1 1 0" range="-70 -30" />
+            <geom name="third_ankle_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 -0.4 -0.4 0.0" />
+          </body>
+        </body>
+      </body>
+      <body name="right_back_leg" pos="0 0 0">
+        <geom name="aux_4_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 0.2 -0.2 0.0" />
+        <body name="aux_4" pos="0.2 -0.2 0">
+          <joint name="hip_4" type="hinge" pos="0.0 0.0 0.0" axis="0 0 1" range="-30 30" />
+          <geom name="rightback_leg_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 0.2 -0.2 0.0" />
+          <body pos="0.2 -0.2 0">
+            <joint name="ankle_4" type="hinge" pos="0.0 0.0 0.0" axis="1 1 0" range="30 70" />
+            <geom name="fourth_ankle_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 0.4 -0.4 0.0" />
+          </body>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor joint="hip_4" ctrlrange="-30.0 30.0" ctrllimited="true" />
+    <motor joint="ankle_4" ctrlrange="-30.0 30.0" ctrllimited="true" />
+    <motor joint="hip_1" ctrlrange="-30.0 30.0" ctrllimited="true" />
+    <motor joint="ankle_1" ctrlrange="-30.0 30.0" ctrllimited="true" />
+    <motor joint="hip_2" ctrlrange="-30.0 30.0" ctrllimited="true" />
+    <motor joint="ankle_2" ctrlrange="-30.0 30.0" ctrllimited="true" />
+    <motor joint="hip_3" ctrlrange="-30.0 30.0" ctrllimited="true" />
+    <motor joint="ankle_3" ctrlrange="-30.0 30.0" ctrllimited="true" />
+  </actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/point.xml b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/point.xml
new file mode 100755
index 0000000..4c06cb1
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/point.xml
@@ -0,0 +1,33 @@
+<mujoco>
+    <compiler inertiafromgeom="true" angle="degree" coordinate="local" />
+    <option timestep="0.02" integrator="RK4" />
+    <default>
+        <joint limited="false" armature="0" damping="0" />
+        <geom condim="3" conaffinity="0" margin="0" friction="1.0 0.5 0.5" rgba="0.8 0.6 0.4 1" density="100" />
+    </default>
+    <asset>
+        <texture type="skybox" builtin="gradient" width="100" height="100" rgb1="1 1 1" rgb2="0 0 0" />
+        <texture name="texgeom" type="cube" builtin="flat" mark="cross" width="127" height="1278" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" markrgb="1 1 1" random="0.01" />
+        <texture name="texplane" type="2d" builtin="checker" rgb1="0.2 0.3 0.4" rgb2="0.1 0.2 0.3" width="100" height="100" />
+        <material name="MatPlane" texture="texplane" shininess="1" texrepeat="30 30" specular="1"  reflectance="0.5" />
+        <material name="geom" texture="texgeom" texuniform="true" />
+    </asset>
+    <worldbody>
+        <light directional="true" cutoff="100" exponent="1" diffuse="1 1 1" specular=".1 .1 .1" pos="0 0 1.3" dir="-0 0 -1.3" />
+        <geom name="floor" material="MatPlane" pos="0 0 0" size="40 40 40" type="plane" conaffinity="1" rgba="0.8 0.9 0.8 1" condim="3" />
+        <!--  ================= Point ================= /-->
+        <!--  Note that the solimp is modified from rllab to prevent the point from going through the wall /-->
+        <body name="torso" pos="0 0 0">
+            <geom name="pointbody" type="sphere" size="0.5" pos="0 0 0.5" rgba="0.8 0.4 0.1 1" solimp="0.9 0.99 0.001" />
+            <geom name="pointarrow" type="box" size="0.5 0.1 0.1" pos="0.6 0 0.5" rgba="0.8 0.4 0.1 1" solimp="0.9 0.99 0.001" />
+            <joint name="ballx" type="slide" axis="1 0 0" pos="0 0 0" />
+            <joint name="bally" type="slide" axis="0 1 0" pos="0 0 0" />
+            <joint name="rot" type="hinge" axis="0 0 1" pos="0 0 0" limited="false" />
+        </body>
+    </worldbody>
+    <actuator>
+        <!-- Those are just dummy actuators for providing ranges -->
+        <motor joint="ballx" ctrlrange="-1 1" ctrllimited="true" />
+        <motor joint="rot" ctrlrange="-0.25 0.25" ctrllimited="true" />
+    </actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/reacher.xml b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/reacher.xml
new file mode 100644
index 0000000..0d238c8
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/reacher.xml
@@ -0,0 +1,34 @@
+<mujoco model="swimmer">
+  <compiler angle="degree" coordinate="local" inertiafromgeom="true" />
+  <option collision="predefined" density="4000" integrator="RK4" timestep="0.01" viscosity="0.1" />
+  <default>
+    <geom conaffinity="1" condim="1" contype="1" material="geom" rgba="0.8 0.6 .4 1" />
+    <joint armature="0.1" />
+  </default>
+  <asset>
+    <texture type="skybox" builtin="gradient" width="100" height="100" rgb1="1 1 1" rgb2="0 0 0" />
+    <texture name="texgeom" type="cube" builtin="flat" mark="cross" width="127" height="1278" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" markrgb="1 1 1" random="0.01" />
+    <texture name="texplane" type="2d" builtin="checker" rgb1="0.2 0.3 0.4" rgb2="0.1 0.2 0.3" width="100" height="100" />
+    <material name='MatPlane' texture="texplane" shininess="1" texrepeat="60 60" specular="1"  reflectance="0.5" />
+    <material name='geom' texture="texgeom" texuniform="true" />
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0s 1.3" specular=".1 .1 .1" />
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 -0.1" rgba="0.8 0.9 0.8 1" size="40 40 0.1" type="plane" />
+    <!-- Reacher -->
+    <body name="torso" pos="0 0 0">
+      <camera name="track" mode="trackcom" pos="0 -3 3" xyaxes="1 0 0 0 1 1" />
+      <geom name="frontbody" density="1000" fromto="1.5 0 0 0.5 0 0" size="0.1" type="capsule" />
+      <joint axis="1 0 0" name="slider1" pos="0 0 0" type="slide" />
+      <joint axis="0 1 0" name="slider2" pos="0 0 0" type="slide" />
+      <joint axis="0 0 1" name="rot" pos="0 0 0" type="hinge" />
+      <body name="mid" pos="0.5 0 0">
+        <geom name="midbody" density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule" />
+        <joint axis="0 0 1" limited="true" name="rot2" pos="0 0 0" range="-100 100" type="hinge" />
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot2" />
+  </actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/swimmer.xml b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/swimmer.xml
new file mode 100644
index 0000000..3c6c21a
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/swimmer.xml
@@ -0,0 +1,39 @@
+<mujoco model="swimmer">
+  <compiler angle="degree" coordinate="local" inertiafromgeom="true" />
+  <option collision="predefined" density="4000" integrator="RK4" timestep="0.01" viscosity="0.1" />
+  <default>
+    <geom conaffinity="1" condim="1" contype="1" material="geom" rgba="0.8 0.6 .4 1" />
+    <joint armature="0.1" />
+  </default>
+  <asset>
+    <texture type="skybox" builtin="gradient" width="100" height="100" rgb1="1 1 1" rgb2="0 0 0" />
+    <texture name="texgeom" type="cube" builtin="flat" mark="cross" width="127" height="1278" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" markrgb="1 1 1" random="0.01" />
+    <texture name="texplane" type="2d" builtin="checker" rgb1="0.2 0.3 0.4" rgb2="0.1 0.2 0.3" width="100" height="100" />
+    <material name='MatPlane' texture="texplane" shininess="1" texrepeat="60 60" specular="1"  reflectance="0.5" />
+    <material name='geom' texture="texgeom" texuniform="true" />
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0s 1.3" specular=".1 .1 .1" />
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 -0.1" rgba="0.8 0.9 0.8 1" size="40 40 0.1" type="plane" />
+    <!--  ================= SWIMMER ================= /-->
+    <body name="torso" pos="0 0 0">
+      <camera name="track" mode="trackcom" pos="0 -3 3" xyaxes="1 0 0 0 1 1" />
+      <geom name="frontbody" density="1000" fromto="1.5 0 0 0.5 0 0" size="0.1" type="capsule" />
+      <joint axis="1 0 0" name="slider1" pos="0 0 0" type="slide" />
+      <joint axis="0 1 0" name="slider2" pos="0 0 0" type="slide" />
+      <joint axis="0 0 1" name="rot" pos="0 0 0" type="hinge" />
+      <body name="mid" pos="0.5 0 0">
+        <geom name="midbody" density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule" />
+        <joint axis="0 0 1" limited="true" name="rot2" pos="0 0 0" range="-100 100" type="hinge" />
+        <body name="back" pos="-1 0 0">
+          <geom name="backbody" density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule" />
+          <joint axis="0 0 1" limited="true" name="rot3" pos="0 0 0" range="-100 100" type="hinge" />
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot2" />
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot3" />
+  </actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_env.py b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_env.py
new file mode 100644
index 0000000..dd11988
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_env.py
@@ -0,0 +1,595 @@
+"""
+Mujoco Maze environment.
+Based on `models`_ and `rllab`_.
+
+.. _models: https://github.com/tensorflow/models/tree/master/research/efficient-hrl
+.. _rllab: https://github.com/rll/rllab
+"""
+
+import itertools as it
+import os
+import tempfile
+import xml.etree.ElementTree as ET
+from typing import List, Tuple, Type
+
+import gym
+import numpy as np
+
+from slbo.envs.mujoco_maze import maze_env_utils, maze_task
+from slbo.envs.mujoco_maze.agent_model import AgentModel
+from slbo.utils.dataset import Dataset, gen_dtype
+
+
+# Directory that contains mujoco xml files.
+MODEL_DIR = os.path.dirname(os.path.abspath(__file__)) + "/assets"
+
+
+class MazeEnv(gym.Env):
+    def __init__(
+        self,
+        model_cls: Type[AgentModel],
+        maze_task: Type[maze_task.MazeTask] = maze_task.MazeTask,
+        top_down_view: float = False,
+        maze_height: float = 0.5,
+        maze_size_scaling: float = 4.0,
+        inner_reward_scaling: float = 1.0,
+        restitution_coef: float = 0.8,
+        task_kwargs: dict = {},
+        *args,
+        **kwargs,
+    ) -> None:
+        self._task = maze_task(maze_size_scaling, **task_kwargs)
+
+        xml_path = os.path.join(MODEL_DIR, model_cls.FILE)
+        tree = ET.parse(xml_path)
+        worldbody = tree.find(".//worldbody")
+
+        self._maze_height = height = maze_height
+        self._maze_size_scaling = size_scaling = maze_size_scaling
+        self._inner_reward_scaling = inner_reward_scaling
+        self.t = 0  # time steps
+        self._observe_blocks = self._task.OBSERVE_BLOCKS
+        self._put_spin_near_agent = self._task.PUT_SPIN_NEAR_AGENT
+        # Observe other objectives
+        self._observe_balls = self._task.OBSERVE_BALLS
+        self._top_down_view = self._task.TOP_DOWN_VIEW
+        self._restitution_coef = restitution_coef
+
+        self._maze_structure = structure = self._task.create_maze()
+        # Elevate the maze to allow for falling.
+        self.elevated = any(maze_env_utils.MazeCell.CHASM in row for row in structure)
+        # Are there any movable blocks?
+        self.blocks = any(any(r.can_move() for r in row) for row in structure)
+
+        torso_x, torso_y = self._find_robot()
+        self._init_torso_x = torso_x
+        self._init_torso_y = torso_y
+        self._init_positions = [
+            (x - torso_x, y - torso_y) for x, y in self._find_all_robots()
+        ]
+
+        if model_cls.MANUAL_COLLISION:
+            if model_cls.RADIUS is None:
+                raise ValueError("Manual collision needs radius of the model")
+            self._collision = maze_env_utils.CollisionDetector(
+                structure, size_scaling, torso_x, torso_y, model_cls.RADIUS,
+            )
+            # Now all object balls have size=1.0
+            self._objball_collision = maze_env_utils.CollisionDetector(
+                structure, size_scaling, torso_x, torso_y, self._task.OBJECT_BALL_SIZE,
+            )
+        else:
+            self._collision = None
+
+        self._xy_to_rowcol = lambda x, y: (
+            2 + (y + size_scaling / 2) / size_scaling,
+            2 + (x + size_scaling / 2) / size_scaling,
+        )
+        # walls (immovable), chasms (fall), movable blocks
+        self._view = np.zeros([5, 5, 3])
+
+        height_offset = 0.0
+        if self.elevated:
+            # Increase initial z-pos of ant.
+            height_offset = height * size_scaling
+            torso = tree.find(".//body[@name='torso']")
+            torso.set("pos", f"0 0 {0.75 + height_offset:.2f}")
+        if self.blocks:
+            # If there are movable blocks, change simulation settings to perform
+            # better contact detection.
+            default = tree.find(".//default")
+            default.find(".//geom").set("solimp", ".995 .995 .01")
+
+        self.movable_blocks = []
+        self.object_balls = []
+        for i in range(len(structure)):
+            for j in range(len(structure[0])):
+                struct = structure[i][j]
+                if struct.is_robot() and self._put_spin_near_agent:
+                    struct = maze_env_utils.MazeCell.SPIN
+                x, y = j * size_scaling - torso_x, i * size_scaling - torso_y
+                h = height / 2 * size_scaling
+                size = size_scaling * 0.5
+                if self.elevated and not struct.is_chasm():
+                    # Create elevated platform.
+                    ET.SubElement(
+                        worldbody,
+                        "geom",
+                        name=f"elevated_{i}_{j}",
+                        pos=f"{x} {y} {h}",
+                        size=f"{size} {size} {h}",
+                        type="box",
+                        material="",
+                        contype="1",
+                        conaffinity="1",
+                        rgba="0.9 0.9 0.9 1",
+                    )
+                if struct.is_block():
+                    # Unmovable block.
+                    # Offset all coordinates so that robot starts at the origin.
+                    ET.SubElement(
+                        worldbody,
+                        "geom",
+                        name=f"block_{i}_{j}",
+                        pos=f"{x} {y} {h + height_offset}",
+                        size=f"{size} {size} {h}",
+                        type="box",
+                        material="",
+                        contype="1",
+                        conaffinity="1",
+                        rgba="0.4 0.4 0.4 1",
+                    )
+                elif struct.can_move():
+                    # Movable block.
+                    self.movable_blocks.append(f"movable_{i}_{j}")
+                    _add_movable_block(
+                        worldbody, struct, i, j, size_scaling, x, y, h, height_offset,
+                    )
+                elif struct.is_object_ball():
+                    # Movable Ball
+                    self.object_balls.append(f"objball_{i}_{j}")
+                    _add_object_ball(worldbody, i, j, x, y, self._task.OBJECT_BALL_SIZE)
+
+        torso = tree.find(".//body[@name='torso']")
+        geoms = torso.findall(".//geom")
+        for geom in geoms:
+            if "name" not in geom.attrib:
+                raise Exception("Every geom of the torso must have a name")
+
+        # Set goals
+        for i, goal in enumerate(self._task.goals):
+            z = goal.pos[2] if goal.dim >= 3 else 0.0
+            if goal.custom_size is None:
+                size = f"{maze_size_scaling * 0.1}"
+            else:
+                size = f"{goal.custom_size}"
+            ET.SubElement(
+                worldbody,
+                "site",
+                name=f"goal_site{i}",
+                pos=f"{goal.pos[0]} {goal.pos[1]} {z}",
+                size=f"{maze_size_scaling * 0.1}",
+                rgba=goal.rgb.rgba_str(),
+            )
+
+        _, file_path = tempfile.mkstemp(text=True, suffix=".xml")
+        tree.write(file_path)
+        self.world_tree = tree
+        self.wrapped_env = model_cls(*args, file_path=file_path, **kwargs)
+        self.observation_space = self._get_obs_space()
+
+    @property
+    def has_extended_obs(self) -> bool:
+        return self._top_down_view or self._observe_blocks or self._observe_balls
+
+    def get_ori(self) -> float:
+        return self.wrapped_env.get_ori()
+
+    def _get_obs_space(self) -> gym.spaces.Box:
+        shape = self._get_obs().shape
+        high = np.inf * np.ones(shape, dtype=np.float32)
+        low = -high
+        # Set velocity limits
+        wrapped_obs_space = self.wrapped_env.observation_space
+        high[: wrapped_obs_space.shape[0]] = wrapped_obs_space.high
+        low[: wrapped_obs_space.shape[0]] = wrapped_obs_space.low
+        # Set coordinate limits
+        low[0], high[0], low[1], high[1] = self._xy_limits()
+        # Set orientation limits
+        return gym.spaces.Box(low, high)
+
+    def _xy_limits(self) -> Tuple[float, float, float, float]:
+        xmin, ymin, xmax, ymax = 100, 100, -100, -100
+        structure = self._maze_structure
+        for i, j in it.product(range(len(structure)), range(len(structure[0]))):
+            if structure[i][j].is_block():
+                continue
+            xmin, xmax = min(xmin, j), max(xmax, j)
+            ymin, ymax = min(ymin, i), max(ymax, i)
+        x0, y0 = self._init_torso_x, self._init_torso_y
+        scaling = self._maze_size_scaling
+        xmin, xmax = (xmin - 0.5) * scaling - x0, (xmax + 0.5) * scaling - x0
+        ymin, ymax = (ymin - 0.5) * scaling - y0, (ymax + 0.5) * scaling - y0
+        return xmin, xmax, ymin, ymax
+
+    def get_top_down_view(self) -> np.ndarray:
+        self._view = np.zeros_like(self._view)
+
+        def valid(row, col):
+            return self._view.shape[0] > row >= 0 and self._view.shape[1] > col >= 0
+
+        def update_view(x, y, d, row=None, col=None):
+            if row is None or col is None:
+                x = x - self._robot_x
+                y = y - self._robot_y
+
+                row, col = self._xy_to_rowcol(x, y)
+                update_view(x, y, d, row=row, col=col)
+                return
+
+            row, row_frac, col, col_frac = int(row), row % 1, int(col), col % 1
+            if row_frac < 0:
+                row_frac += 1
+            if col_frac < 0:
+                col_frac += 1
+
+            if valid(row, col):
+                self._view[row, col, d] += (
+                    min(1.0, row_frac + 0.5) - max(0.0, row_frac - 0.5)
+                ) * (min(1.0, col_frac + 0.5) - max(0.0, col_frac - 0.5))
+            if valid(row - 1, col):
+                self._view[row - 1, col, d] += (max(0.0, 0.5 - row_frac)) * (
+                    min(1.0, col_frac + 0.5) - max(0.0, col_frac - 0.5)
+                )
+            if valid(row + 1, col):
+                self._view[row + 1, col, d] += (max(0.0, row_frac - 0.5)) * (
+                    min(1.0, col_frac + 0.5) - max(0.0, col_frac - 0.5)
+                )
+            if valid(row, col - 1):
+                self._view[row, col - 1, d] += (
+                    min(1.0, row_frac + 0.5) - max(0.0, row_frac - 0.5)
+                ) * (max(0.0, 0.5 - col_frac))
+            if valid(row, col + 1):
+                self._view[row, col + 1, d] += (
+                    min(1.0, row_frac + 0.5) - max(0.0, row_frac - 0.5)
+                ) * (max(0.0, col_frac - 0.5))
+            if valid(row - 1, col - 1):
+                self._view[row - 1, col - 1, d] += (max(0.0, 0.5 - row_frac)) * max(
+                    0.0, 0.5 - col_frac
+                )
+            if valid(row - 1, col + 1):
+                self._view[row - 1, col + 1, d] += (max(0.0, 0.5 - row_frac)) * max(
+                    0.0, col_frac - 0.5
+                )
+            if valid(row + 1, col + 1):
+                self._view[row + 1, col + 1, d] += (max(0.0, row_frac - 0.5)) * max(
+                    0.0, col_frac - 0.5
+                )
+            if valid(row + 1, col - 1):
+                self._view[row + 1, col - 1, d] += (max(0.0, row_frac - 0.5)) * max(
+                    0.0, 0.5 - col_frac
+                )
+
+        # Draw ant.
+        robot_x, robot_y = self.wrapped_env.get_body_com("torso")[:2]
+        self._robot_x = robot_x
+        self._robot_y = robot_y
+
+        structure = self._maze_structure
+        size_scaling = self._maze_size_scaling
+
+        # Draw immovable blocks and chasms.
+        for i in range(len(structure)):
+            for j in range(len(structure[0])):
+                if structure[i][j].is_block():  # Wall.
+                    update_view(
+                        j * size_scaling - self._init_torso_x,
+                        i * size_scaling - self._init_torso_y,
+                        0,
+                    )
+                if structure[i][j].is_chasm():  # Chasm.
+                    update_view(
+                        j * size_scaling - self._init_torso_x,
+                        i * size_scaling - self._init_torso_y,
+                        1,
+                    )
+
+        # Draw movable blocks.
+        for block_name in self.movable_blocks:
+            block_x, block_y = self.wrapped_env.get_body_com(block_name)[:2]
+            update_view(block_x, block_y, 2)
+
+        return self._view
+
+    def _get_obs(self) -> np.ndarray:
+        wrapped_obs = self.wrapped_env._get_obs()
+        if self._top_down_view:
+            view = [self.get_top_down_view().flat]
+        else:
+            view = []
+
+        additional_obs = []
+
+        if self._observe_balls:
+            for name in self.object_balls:
+                additional_obs.append(self.wrapped_env.get_body_com(name))
+
+        if self._observe_blocks:
+            for name in self.movable_blocks:
+                additional_obs.append(self.wrapped_env.get_body_com(name))
+
+        obs = np.concatenate([wrapped_obs[:3]] + additional_obs + [wrapped_obs[3:]])
+        return np.concatenate([obs, *view, np.array([self.t * 0.001])])
+
+    def reset(self) -> np.ndarray:
+        self.t = 0
+        self.wrapped_env.reset()
+        # Samples a new goal
+        if self._task.sample_goals():
+            self.set_marker()
+        # Samples a new start position
+        if len(self._init_positions) > 1:
+            xy = np.random.choice(self._init_positions)
+            self.wrapped_env.set_xy(xy)
+        return self._get_obs()
+
+    def set_marker(self) -> None:
+        for i, goal in enumerate(self._task.goals):
+            idx = self.model.site_name2id(f"goal{i}")
+            self.data.site_xpos[idx][: len(goal.pos)] = goal.pos
+
+    @property
+    def viewer(self):
+        return self.wrapped_env.viewer
+
+    def render(self, *args, **kwargs):
+        return self.wrapped_env.render(*args, **kwargs)
+
+    @property
+    def action_space(self):
+        return self.wrapped_env.action_space
+
+    def _find_robot(self) -> Tuple[float, float]:
+        structure = self._maze_structure
+        size_scaling = self._maze_size_scaling
+        for i, j in it.product(range(len(structure)), range(len(structure[0]))):
+            if structure[i][j].is_robot():
+                return j * size_scaling, i * size_scaling
+        raise ValueError("No robot in maze specification.")
+
+    def _find_all_robots(self) -> List[Tuple[float, float]]:
+        structure = self._maze_structure
+        size_scaling = self._maze_size_scaling
+        coords = []
+        for i, j in it.product(range(len(structure)), range(len(structure[0]))):
+            if structure[i][j].is_robot():
+                coords.append((j * size_scaling, i * size_scaling))
+        return coords
+
+    def _objball_positions(self) -> None:
+        return [
+            self.wrapped_env.get_body_com(name)[:2].copy() for name in self.object_balls
+        ]
+
+    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, dict]:
+        action = np.clip(action, self.action_space.low, self.action_space.high)
+        self.t += 1
+        if self.wrapped_env.MANUAL_COLLISION:
+            old_pos = self.wrapped_env.get_xy()
+            old_objballs = self._objball_positions()
+            inner_next_obs, inner_reward, _, info = self.wrapped_env.step(action)
+            new_pos = self.wrapped_env.get_xy()
+            new_objballs = self._objball_positions()
+            # Checks that the new_position is in the wall
+            collision = self._collision.detect(old_pos, new_pos)
+            if collision is not None:
+                pos = collision.point + self._restitution_coef * collision.rest()
+                if self._collision.detect(old_pos, pos) is not None:
+                    # If pos is also not in the wall, we give up computing the position
+                    self.wrapped_env.set_xy(old_pos)
+                else:
+                    self.wrapped_env.set_xy(pos)
+            # Do the same check for object balls
+            for name, old, new in zip(self.object_balls, old_objballs, new_objballs):
+                collision = self._objball_collision.detect(old, new)
+                if collision is not None:
+                    pos = collision.point + self._restitution_coef * collision.rest()
+                    if self._objball_collision.detect(old, pos) is not None:
+                        pos = old
+                    idx = self.wrapped_env.model.body_name2id(name)
+                    self.wrapped_env.data.xipos[idx][:2] = pos
+        else:
+            inner_next_obs, inner_reward, _, info = self.wrapped_env.step(action)
+        next_obs = self._get_obs()
+        inner_reward = self._inner_reward_scaling * inner_reward
+        outer_reward = self._task.reward(next_obs)
+        done = self._task.termination(next_obs)
+        info["position"] = self.wrapped_env.get_xy()
+        return next_obs, inner_reward + outer_reward, done, info
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        rewards = []
+        dones = []
+        for i in range(len(next_states)):
+            rewards.append(self._task.reward(next_states[i]))
+            dones.append(self._task.termination(next_states[i]))
+        inner_rewards = np.linalg.norm((states[:,:2] - next_states[:,:2])/self.wrapped_env.dt,axis=-1)
+        reward_ctrl = self.wrapped_env._ctrl_cost_weight * np.sum(np.square(actions), axis=-1)
+        #print(inner_.rewards)
+        rewards = np.array(rewards)
+        assert rewards.shape == inner_rewards.shape
+        assert inner_rewards.shape == reward_ctrl.shape
+        rewards = rewards + self._inner_reward_scaling * (inner_rewards - reward_ctrl)
+        #print(rewards)
+        return rewards, np.array(dones, dtype=np.bool)
+
+
+    def verify(self, n=2000, eps=1e-4):
+        print(self._inner_reward_scaling)
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        print(dataset.reward)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        print('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
+
+
+    def close(self) -> None:
+        self.wrapped_env.close()
+
+
+def _add_object_ball(
+    worldbody: ET.Element, i: str, j: str, x: float, y: float, size: float
+) -> None:
+    body = ET.SubElement(worldbody, "body", name=f"objball_{i}_{j}", pos=f"{x} {y} 0")
+    mass = 0.0001 * (size ** 3)
+    ET.SubElement(
+        body,
+        "geom",
+        type="sphere",
+        name=f"objball_{i}_{j}_geom",
+        size=f"{size}",  # Radius
+        pos=f"0.0 0.0 {size}",  # Z = size so that this ball can move!!
+        rgba=maze_task.BLUE.rgba_str(),
+        contype="1",
+        conaffinity="1",
+        solimp="0.9 0.99 0.001",
+        mass=f"{mass}",
+    )
+    ET.SubElement(
+        body,
+        "joint",
+        name=f"objball_{i}_{j}_x",
+        axis="1 0 0",
+        pos="0 0 0.0",
+        type="slide",
+    )
+    ET.SubElement(
+        body,
+        "joint",
+        name=f"objball_{i}_{j}_y",
+        axis="0 1 0",
+        pos="0 0 0",
+        type="slide",
+    )
+    ET.SubElement(
+        body,
+        "joint",
+        name=f"objball_{i}_{j}_rot",
+        axis="0 0 1",
+        pos="0 0 0",
+        type="hinge",
+        limited="false",
+    )
+
+
+def _add_movable_block(
+    worldbody: ET.Element,
+    struct: maze_env_utils.MazeCell,
+    i: str,
+    j: str,
+    size_scaling: float,
+    x: float,
+    y: float,
+    h: float,
+    height_offset: float,
+) -> None:
+    falling = struct.can_move_z()
+    if struct.can_spin():
+        h *= 0.1
+        x += size_scaling * 0.25
+        shrink = 0.1
+    elif falling:
+        # The "falling" blocks are shrunk slightly and increased in mass to
+        # ensure it can fall easily through a gap in the platform blocks.
+        shrink = 0.99
+    elif struct.is_half_block():
+        shrink = 0.5
+    else:
+        shrink = 1.0
+    size = size_scaling * 0.5 * shrink
+    movable_body = ET.SubElement(
+        worldbody, "body", name=f"movable_{i}_{j}", pos=f"{x} {y} {h}",
+    )
+    ET.SubElement(
+        movable_body,
+        "geom",
+        name=f"block_{i}_{j}",
+        pos="0 0 0",
+        size=f"{size} {size} {h}",
+        type="box",
+        material="",
+        mass="0.001" if falling else "0.0002",
+        contype="1",
+        conaffinity="1",
+        rgba="0.9 0.1 0.1 1",
+    )
+    if struct.can_move_x():
+        ET.SubElement(
+            movable_body,
+            "joint",
+            axis="1 0 0",
+            name=f"movable_x_{i}_{j}",
+            armature="0",
+            damping="0.0",
+            limited="true" if falling else "false",
+            range=f"{-size_scaling} {size_scaling}",
+            margin="0.01",
+            pos="0 0 0",
+            type="slide",
+        )
+    if struct.can_move_y():
+        ET.SubElement(
+            movable_body,
+            "joint",
+            armature="0",
+            axis="0 1 0",
+            damping="0.0",
+            limited="true" if falling else "false",
+            range=f"{-size_scaling} {size_scaling}",
+            margin="0.01",
+            name=f"movable_y_{i}_{j}",
+            pos="0 0 0",
+            type="slide",
+        )
+    if struct.can_move_z():
+        ET.SubElement(
+            movable_body,
+            "joint",
+            armature="0",
+            axis="0 0 1",
+            damping="0.0",
+            limited="true",
+            range=f"{-height_offset} 0",
+            margin="0.01",
+            name=f"movable_z_{i}_{j}",
+            pos="0 0 0",
+            type="slide",
+        )
+    if struct.can_spin():
+        ET.SubElement(
+            movable_body,
+            "joint",
+            armature="0",
+            axis="0 0 1",
+            damping="0.0",
+            limited="false",
+            name=f"spinable_{i}_{j}",
+            pos="0 0 0",
+            type="ball",
+        )
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_env_utils.py b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_env_utils.py
new file mode 100644
index 0000000..348b88c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_env_utils.py
@@ -0,0 +1,205 @@
+"""
+Utilities for creating maze.
+Based on `models`_ and `rllab`_.
+
+.. _models: https://github.com/tensorflow/models/tree/master/research/efficient-hrl
+.. _rllab: https://github.com/rll/rllab
+"""
+
+import itertools as it
+from enum import Enum
+from typing import Any, List, Optional, Sequence, Tuple, Union
+
+import numpy as np
+
+Self = Any
+Point = np.complex
+
+
+class MazeCell(Enum):
+    # Robot: Start position
+    ROBOT = -1
+    # Blocks
+    EMPTY = 0
+    BLOCK = 1
+    CHASM = 2
+    OBJECT_BALL = 3
+    # Moves
+    XY_BLOCK = 14
+    XZ_BLOCK = 15
+    YZ_BLOCK = 16
+    XYZ_BLOCK = 17
+    XY_HALF_BLOCK = 18
+    SPIN = 19
+
+    def is_block(self) -> bool:
+        return self == self.BLOCK
+
+    def is_chasm(self) -> bool:
+        return self == self.CHASM
+
+    def is_object_ball(self) -> bool:
+        return self == self.OBJECT_BALL
+
+    def is_empty(self) -> bool:
+        return self == self.ROBOT or self == self.EMPTY
+
+    def is_robot(self) -> bool:
+        return self == self.ROBOT
+
+    def is_wall_or_chasm(self) -> bool:
+        return self in [self.BLOCK, self.CHASM]
+
+    def can_move_x(self) -> bool:
+        return self in [
+            self.XY_BLOCK,
+            self.XY_HALF_BLOCK,
+            self.XZ_BLOCK,
+            self.XYZ_BLOCK,
+            self.SPIN,
+        ]
+
+    def can_move_y(self) -> bool:
+        return self in [
+            self.XY_BLOCK,
+            self.XY_HALF_BLOCK,
+            self.YZ_BLOCK,
+            self.XYZ_BLOCK,
+            self.SPIN,
+        ]
+
+    def can_move_z(self) -> bool:
+        return self in [self.XZ_BLOCK, self.YZ_BLOCK, self.XYZ_BLOCK]
+
+    def can_spin(self) -> bool:
+        return self == self.SPIN
+
+    def can_move(self) -> bool:
+        return self.can_move_x() or self.can_move_y() or self.can_move_z()
+
+    def is_half_block(self) -> bool:
+        return self in [self.XY_HALF_BLOCK]
+
+
+class Line:
+    def __init__(
+        self, p1: Union[Sequence[float], Point], p2: Union[Sequence[float], Point],
+    ) -> None:
+        self.p1 = p1 if isinstance(p1, Point) else np.complex(*p1)
+        self.p2 = p2 if isinstance(p2, Point) else np.complex(*p2)
+        self.v1 = self.p2 - self.p1
+        self.conj_v1 = np.conjugate(self.v1)
+        self.norm = np.absolute(self.v1)
+
+    def _intersect(self, other: Self) -> bool:
+        v2 = other.p1 - self.p1
+        v3 = other.p2 - self.p1
+        return (self.conj_v1 * v2).imag * (self.conj_v1 * v3).imag <= 0.0
+
+    def _projection(self, p: Point) -> Point:
+        nv1 = -self.v1
+        nv1_norm = np.absolute(nv1) ** 2
+        scale = np.real(np.conjugate(p - self.p1) * nv1) / nv1_norm
+        return self.p1 + nv1 * scale
+
+    def reflection(self, p: Point) -> Point:
+        return p + 2.0 * (self._projection(p) - p)
+
+    def distance(self, p: Point) -> float:
+        return np.absolute(p - self._projection(p))
+
+    def intersect(self, other: Self) -> Point:
+        if self._intersect(other) and other._intersect(self):
+            return self._cross_point(other)
+        else:
+            return None
+
+    def _cross_point(self, other: Self) -> Optional[Point]:
+        v2 = other.p2 - other.p1
+        v3 = self.p2 - other.p1
+        a, b = (self.conj_v1 * v2).imag, (self.conj_v1 * v3).imag
+        return other.p1 + b / a * v2
+
+    def __repr__(self) -> str:
+        x1, y1 = self.p1.real, self.p1.imag
+        x2, y2 = self.p2.real, self.p2.imag
+        return f"Line(({x1}, {y1}) -> ({x2}, {y2}))"
+
+
+class Collision:
+    def __init__(self, point: Point, reflection: Point) -> None:
+        self._point = point
+        self._reflection = reflection
+
+    @property
+    def point(self) -> np.ndarray:
+        return np.array([self._point.real, self._point.imag])
+
+    def rest(self) -> np.ndarray:
+        p = self._reflection - self._point
+        return np.array([p.real, p.imag])
+
+
+class CollisionDetector:
+    """For manual collision detection.
+    """
+
+    EPS: float = 0.05
+    NEIGHBORS: List[Tuple[int, int]] = [[0, -1], [-1, 0], [0, 1], [1, 0]]
+
+    def __init__(
+        self,
+        structure: list,
+        size_scaling: float,
+        torso_x: float,
+        torso_y: float,
+        radius: float,
+    ) -> None:
+        h, w = len(structure), len(structure[0])
+        self.lines = []
+
+        def is_empty(i, j) -> bool:
+            if 0 <= i < h and 0 <= j < w:
+                return structure[i][j].is_empty()
+            else:
+                return False
+
+        for i, j in it.product(range(len(structure)), range(len(structure[0]))):
+            if not structure[i][j].is_block():
+                continue
+            y_base = i * size_scaling - torso_y
+            x_base = j * size_scaling - torso_x
+            offset = size_scaling * 0.5 + radius
+            min_y, max_y = y_base - offset, y_base + offset
+            min_x, max_x = x_base - offset, x_base + offset
+            for dx, dy in self.NEIGHBORS:
+                if not is_empty(i + dy, j + dx):
+                    continue
+                self.lines.append(
+                    Line(
+                        (max_x if dx == 1 else min_x, max_y if dy == 1 else min_y),
+                        (min_x if dx == -1 else max_x, min_y if dy == -1 else max_y),
+                    )
+                )
+
+    def detect(self, old_pos: np.ndarray, new_pos: np.ndarray) -> Optional[Collision]:
+        move = Line(old_pos, new_pos)
+        # First, checks that it actually moved
+        if move.norm <= 1e-8:
+            return None
+        # Next, checks that the trajectory cross the wall or not
+        collisions = []
+        for line in self.lines:
+            intersection = line.intersect(move)
+            if intersection is not None:
+                reflection = line.reflection(move.p2)
+                collisions.append(Collision(intersection, reflection))
+        if len(collisions) == 0:
+            return None
+        col = collisions[0]
+        dist = np.absolute(col._point - move.p1)
+        for collision in collisions[1:]:
+            new_dist = np.absolute(collision._point - move.p1)
+            if new_dist < dist:
+                col, dist = collision, new_dist
+        return col
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_task.py b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_task.py
new file mode 100644
index 0000000..77fca6b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_task.py
@@ -0,0 +1,477 @@
+"""Maze tasks that are defined by their map, termination condition, and goals.
+"""
+
+from abc import ABC, abstractmethod
+from typing import Dict, List, NamedTuple, Optional, Tuple, Type
+
+import numpy as np
+
+from slbo.envs.mujoco_maze.maze_env_utils import MazeCell
+
+
+class Rgb(NamedTuple):
+    red: float
+    green: float
+    blue: float
+
+    def rgba_str(self) -> str:
+        return f"{self.red} {self.green} {self.blue} 1"
+
+
+RED = Rgb(0.7, 0.1, 0.1)
+GREEN = Rgb(0.1, 0.7, 0.1)
+BLUE = Rgb(0.1, 0.1, 0.7)
+
+
+class MazeGoal:
+    def __init__(
+        self,
+        pos: np.ndarray,
+        reward_scale: float = 1.0,
+        rgb: Rgb = RED,
+        threshold: float = 0.6,
+        custom_size: Optional[float] = None,
+    ) -> None:
+        assert 0.0 <= reward_scale <= 1.0
+        self.pos = pos
+        self.dim = pos.shape[0]
+        self.reward_scale = reward_scale
+        self.rgb = rgb
+        self.threshold = threshold
+        self.custom_size = custom_size
+
+    def neighbor(self, obs: np.ndarray) -> float:
+        return np.linalg.norm(obs[: self.dim] - self.pos) <= self.threshold
+
+    def euc_dist(self, obs: np.ndarray) -> float:
+        return np.sum(np.square(obs[: self.dim] - self.pos)) ** 0.5
+
+
+class Scaling(NamedTuple):
+    ant: Optional[float]
+    point: Optional[float]
+    swimmer: Optional[float]
+
+
+class MazeTask(ABC):
+    REWARD_THRESHOLD: float
+    PENALTY: Optional[float] = None
+    MAZE_SIZE_SCALING: Scaling = Scaling(8.0, 4.0, 4.0)
+    INNER_REWARD_SCALING: float = 0.01
+    # For Fall/Push/BlockMaze
+    OBSERVE_BLOCKS: bool = False
+    # For Billiard
+    OBSERVE_BALLS: bool = False
+    OBJECT_BALL_SIZE: float = 1.0
+    # Unused now
+    PUT_SPIN_NEAR_AGENT: bool = False
+    TOP_DOWN_VIEW: bool = False
+
+    def __init__(self, scale: float) -> None:
+        self.goals = []
+        self.scale = scale
+
+    def sample_goals(self) -> bool:
+        return False
+
+    def termination(self, obs: np.ndarray) -> bool:
+        for goal in self.goals:
+            if goal.neighbor(obs):
+                return True
+        return False
+
+    @abstractmethod
+    def reward(self, obs: np.ndarray) -> float:
+        pass
+
+    @staticmethod
+    @abstractmethod
+    def create_maze() -> List[List[MazeCell]]:
+        pass
+
+
+class DistRewardMixIn:
+    REWARD_THRESHOLD: float = -1000.0
+    goals: List[MazeGoal]
+    scale: float
+
+    def reward(self, obs: np.ndarray) -> float:
+        return -self.goals[0].euc_dist(obs) / self.scale
+
+
+class GoalRewardUMaze(MazeTask):
+    REWARD_THRESHOLD: float = 0.9
+    PENALTY: float = -0.0001
+
+    def __init__(self, scale: float) -> None:
+        super().__init__(scale)
+        self.goals = [MazeGoal(np.array([0.0, 2.0 * scale]))]
+
+    def reward(self, obs: np.ndarray) -> float:
+        return 100.0 if self.termination(obs) else self.PENALTY
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B, R = MazeCell.EMPTY, MazeCell.BLOCK, MazeCell.ROBOT
+        return [
+            [B, B, B, B, B],
+            [B, R, E, E, B],
+            [B, B, B, E, B],
+            [B, E, E, E, B],
+            [B, B, B, B, B],
+        ]
+
+
+class DistRewardUMaze(GoalRewardUMaze, DistRewardMixIn):
+    pass
+
+
+class GoalRewardSimpleRoom(GoalRewardUMaze):
+    def __init__(self, scale: float) -> None:
+        super().__init__(scale)
+        self.goals = [MazeGoal(np.array([2.0 * scale, 0.0]))]
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B, R = MazeCell.EMPTY, MazeCell.BLOCK, MazeCell.ROBOT
+        return [
+            [B, B, B, B, B],
+            [B, R, E, E, B],
+            [B, B, B, B, B],
+        ]
+
+
+class DistRewardSimpleRoom(GoalRewardSimpleRoom, DistRewardMixIn):
+    pass
+
+
+class GoalRewardPush(GoalRewardUMaze):
+    OBSERVE_BLOCKS: bool = True
+
+    def __init__(self, scale: float) -> None:
+        super().__init__(scale)
+        self.goals = [MazeGoal(np.array([0.0, 2.375 * scale]))]
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B, R, M = MazeCell.EMPTY, MazeCell.BLOCK, MazeCell.ROBOT, MazeCell.XY_BLOCK
+        return [
+            [B, B, B, B, B],
+            [B, E, R, B, B],
+            [B, E, M, E, B],
+            [B, B, E, B, B],
+            [B, B, B, B, B],
+        ]
+
+
+class DistRewardPush(GoalRewardPush, DistRewardMixIn):
+    pass
+
+
+class GoalRewardFall(GoalRewardUMaze):
+    OBSERVE_BLOCKS: bool = True
+
+    def __init__(self, scale: float) -> None:
+        super().__init__(scale)
+        self.goals = [MazeGoal(np.array([0.0, 3.375 * scale, 4.5]))]
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B, C, R = MazeCell.EMPTY, MazeCell.BLOCK, MazeCell.CHASM, MazeCell.ROBOT
+        M = MazeCell.YZ_BLOCK
+        return [
+            [B, B, B, B],
+            [B, R, E, B],
+            [B, E, M, B],
+            [B, C, C, B],
+            [B, E, E, B],
+            [B, B, B, B],
+        ]
+
+
+class DistRewardFall(GoalRewardFall, DistRewardMixIn):
+    pass
+
+
+class GoalReward2Rooms(MazeTask):
+    REWARD_THRESHOLD: float = 0.9
+    PENALTY: float = -0.0001
+    MAZE_SIZE_SCALING: Scaling = Scaling(4.0, 4.0, 4.0)
+
+    def __init__(self, scale: float, goal: Tuple[int, int] = (4.0, -2.0)) -> None:
+        super().__init__(scale)
+        self.goals = [MazeGoal(np.array(goal) * scale)]
+
+    def reward(self, obs: np.ndarray) -> float:
+        for goal in self.goals:
+            if goal.neighbor(obs):
+                return goal.reward_scale
+        return self.PENALTY
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B, R = MazeCell.EMPTY, MazeCell.BLOCK, MazeCell.ROBOT
+        return [
+            [B, B, B, B, B, B, B, B],
+            [B, E, E, E, B, E, E, B],
+            [B, E, E, E, B, E, E, B],
+            [B, E, R, E, B, E, E, B],
+            [B, E, E, E, B, E, E, B],
+            [B, E, E, E, E, E, E, B],
+            [B, B, B, B, B, B, B, B],
+        ]
+
+
+class DistReward2Rooms(GoalReward2Rooms, DistRewardMixIn):
+    pass
+
+
+class SubGoal2Rooms(GoalReward2Rooms):
+    def __init__(
+        self,
+        scale: float,
+        primary_goal: Tuple[float, float] = (4.0, -2.0),
+        subgoals: List[Tuple[float, float]] = [(1.0, -2.0), (-1.0, 2.0)],
+    ) -> None:
+        super().__init__(scale, primary_goal)
+        for subgoal in subgoals:
+            self.goals.append(
+                MazeGoal(np.array(subgoal) * scale, reward_scale=0.5, rgb=GREEN)
+            )
+
+
+class GoalReward4Rooms(MazeTask):
+    REWARD_THRESHOLD: float = 0.9
+    PENALTY: float = -0.0001
+    MAZE_SIZE_SCALING: Scaling = Scaling(4.0, 4.0, 4.0)
+
+    def __init__(self, scale: float) -> None:
+        super().__init__(scale)
+        self.goals = [MazeGoal(np.array([6.0 * scale, -6.0 * scale]))]
+
+    def reward(self, obs: np.ndarray) -> float:
+        for goal in self.goals:
+            if goal.neighbor(obs):
+                return goal.reward_scale
+        return self.PENALTY
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B, R = MazeCell.EMPTY, MazeCell.BLOCK, MazeCell.ROBOT
+        return [
+            [B, B, B, B, B, B, B, B, B],
+            [B, E, E, E, B, E, E, E, B],
+            [B, E, E, E, E, E, E, E, B],
+            [B, E, E, E, B, E, E, E, B],
+            [B, B, E, B, B, B, E, B, B],
+            [B, E, E, E, B, E, E, E, B],
+            [B, E, E, E, E, E, E, E, B],
+            [B, R, E, E, B, E, E, E, B],
+            [B, B, B, B, B, B, B, B, B],
+        ]
+
+
+class DistReward4Rooms(GoalReward4Rooms, DistRewardMixIn):
+    pass
+
+
+class SubGoal4Rooms(GoalReward4Rooms):
+    def __init__(self, scale: float) -> None:
+        super().__init__(scale)
+        self.goals += [
+            MazeGoal(np.array([0.0 * scale, -6.0 * scale]), 0.5, GREEN),
+            MazeGoal(np.array([6.0 * scale, 0.0 * scale]), 0.5, GREEN),
+        ]
+
+
+class GoalRewardTRoom(MazeTask):
+    REWARD_THRESHOLD: float = 0.9
+    PENALTY: float = -0.0001
+    MAZE_SIZE_SCALING: Scaling = Scaling(4.0, 4.0, 4.0)
+
+    def __init__(self, scale: float, goal: Tuple[float, float] = (2.0, -3.0)) -> None:
+        super().__init__(scale)
+        self.goals = [MazeGoal(np.array(goal) * scale)]
+
+    def reward(self, obs: np.ndarray) -> float:
+        for goal in self.goals:
+            if goal.neighbor(obs):
+                return goal.reward_scale
+        return self.PENALTY
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B, R = MazeCell.EMPTY, MazeCell.BLOCK, MazeCell.ROBOT
+        return [
+            [B, B, B, B, B, B, B],
+            [B, E, E, B, E, E, B],
+            [B, E, E, B, E, E, B],
+            [B, E, B, B, B, E, B],
+            [B, E, E, R, E, E, B],
+            [B, B, B, B, B, B, B],
+        ]
+
+
+class DistRewardTRoom(GoalRewardTRoom, DistRewardMixIn):
+    pass
+
+
+class SubGoalTRoom(GoalRewardTRoom):
+    def __init__(
+        self,
+        scale: float,
+        primary_goal: Tuple[float, float] = (2.0, -3.0),
+        subgoal: Tuple[float, float] = (-2.0, -3.0),
+    ) -> None:
+        super().__init__(scale, primary_goal)
+        self.goals.append(
+            MazeGoal(np.array(subgoal) * scale, reward_scale=0.5, rgb=GREEN)
+        )
+
+
+class GoalRewardBlockMaze(GoalRewardUMaze):
+    MAZE_SIZE_SCALING: Scaling = Scaling(8.0, 4.0, None)
+    OBSERVE_BLOCKS: bool = True
+
+    def __init__(self, scale: float) -> None:
+        super().__init__(scale)
+        self.goals = [MazeGoal(np.array([0.0, 3.0 * scale]))]
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B, R = MazeCell.EMPTY, MazeCell.BLOCK, MazeCell.ROBOT
+        M = MazeCell.XY_BLOCK
+        return [
+            [B, B, B, B, B],
+            [B, R, E, E, B],
+            [B, B, B, M, B],
+            [B, E, E, E, B],
+            [B, E, E, E, B],
+            [B, B, B, B, B],
+        ]
+
+
+class DistRewardBlockMaze(GoalRewardBlockMaze, DistRewardMixIn):
+    pass
+
+
+class GoalRewardBilliard(MazeTask):
+    REWARD_THRESHOLD: float = 0.9
+    PENALTY: float = -0.0001
+    MAZE_SIZE_SCALING: Scaling = Scaling(None, 3.0, None)
+    OBSERVE_BALLS: bool = True
+    GOAL_SIZE: float = 0.3
+
+    def __init__(self, scale: float, goal: Tuple[float, float] = (2.0, -3.0)) -> None:
+        super().__init__(scale)
+        goal = np.array(goal) * scale
+        self.goals.append(
+            MazeGoal(goal, threshold=self._threshold(), custom_size=self.GOAL_SIZE)
+        )
+
+    def _threshold(self) -> float:
+        return self.OBJECT_BALL_SIZE + self.GOAL_SIZE
+
+    def reward(self, obs: np.ndarray) -> float:
+        object_pos = obs[3:6]
+        for goal in self.goals:
+            if goal.neighbor(object_pos):
+                return goal.reward_scale
+        return self.PENALTY
+
+    def termination(self, obs: np.ndarray) -> bool:
+        object_pos = obs[3:6]
+        for goal in self.goals:
+            if goal.neighbor(object_pos):
+                return True
+        return False
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B = MazeCell.EMPTY, MazeCell.BLOCK
+        R, M = MazeCell.ROBOT, MazeCell.OBJECT_BALL
+        return [
+            [B, B, B, B, B, B, B],
+            [B, E, E, E, E, E, B],
+            [B, E, E, E, E, E, B],
+            [B, E, E, M, E, E, B],
+            [B, E, E, R, E, E, B],
+            [B, E, E, E, E, E, B],
+            [B, B, B, B, B, B, B],
+        ]
+
+
+class DistRewardBilliard(GoalRewardBilliard):
+    def reward(self, obs: np.ndarray) -> float:
+        return -self.goals[0].euc_dist(obs[3:6]) / self.scale
+
+
+class SubGoalBilliard(GoalRewardBilliard):
+    def __init__(
+        self,
+        scale: float,
+        primary_goal: Tuple[float, float] = (2.0, -3.0),
+        subgoals: List[Tuple[float, float]] = [(-2.0, -3.0), (-2.0, 1.0), (2.0, 1.0)],
+    ) -> None:
+        super().__init__(scale, primary_goal)
+        for subgoal in subgoals:
+            self.goals.append(
+                MazeGoal(
+                    np.array(subgoal) * scale,
+                    reward_scale=0.5,
+                    rgb=GREEN,
+                    threshold=self._threshold(),
+                    custom_size=self.GOAL_SIZE,
+                )
+            )
+
+
+class BanditBilliard(SubGoalBilliard):
+    def __init__(
+        self,
+        scale: float,
+        primary_goal: Tuple[float, float] = (4.0, -2.0),
+        subgoals: List[Tuple[float, float]] = [(4.0, 2.0)],
+    ) -> None:
+        super().__init__(scale, primary_goal, subgoals)
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B = MazeCell.EMPTY, MazeCell.BLOCK
+        R, M = MazeCell.ROBOT, MazeCell.OBJECT_BALL
+        return [
+            [B, B, B, B, B, B, B],
+            [B, E, E, B, B, E, B],
+            [B, E, E, E, E, E, B],
+            [B, R, M, E, B, B, B],
+            [B, E, E, E, E, E, B],
+            [B, E, E, E, E, E, B],
+            [B, B, B, B, B, B, B],
+        ]
+
+
+class TaskRegistry:
+    REGISTRY: Dict[str, List[Type[MazeTask]]] = {
+        "SimpleRoom": [DistRewardSimpleRoom, GoalRewardSimpleRoom],
+        "UMaze": [DistRewardUMaze, GoalRewardUMaze],
+        "Push": [DistRewardPush, GoalRewardPush],
+        "Fall": [DistRewardFall, GoalRewardFall],
+        "2Rooms": [DistReward2Rooms, GoalReward2Rooms, SubGoal2Rooms],
+        "4Rooms": [DistReward4Rooms, GoalReward4Rooms, SubGoal4Rooms],
+        "TRoom": [DistRewardTRoom, GoalRewardTRoom, SubGoalTRoom],
+        "BlockMaze": [DistRewardBlockMaze, GoalRewardBlockMaze],
+        "Billiard": [
+            DistRewardBilliard,
+            GoalRewardBilliard,
+            SubGoalBilliard,
+            BanditBilliard,
+        ],
+    }
+
+    @staticmethod
+    def keys() -> List[str]:
+        return list(TaskRegistry.REGISTRY.keys())
+
+    @staticmethod
+    def tasks(key: str) -> List[Type[MazeTask]]:
+        return TaskRegistry.REGISTRY[key]
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/point.py b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/point.py
new file mode 100644
index 0000000..4602746
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/point.py
@@ -0,0 +1,81 @@
+"""
+A ball-like robot as an explorer in the maze.
+Based on `models`_ and `rllab`_.
+
+.. _models: https://github.com/tensorflow/models/tree/master/research/efficient-hrl
+.. _rllab: https://github.com/rll/rllab
+"""
+
+from typing import Optional, Tuple
+
+import gym
+import numpy as np
+
+from slbo.envs.mujoco_maze.agent_model import AgentModel
+
+
+class PointEnv(AgentModel):
+    FILE: str = "point.xml"
+    ORI_IND: int = 2
+    MANUAL_COLLISION: bool = True
+    RADIUS: float = 0.4
+
+    VELOCITY_LIMITS: float = 10.0
+
+    def __init__(self, file_path: Optional[str] = None):
+        super().__init__(file_path, 1)
+        high = np.inf * np.ones(6, dtype=np.float32)
+        high[3:] = self.VELOCITY_LIMITS * 1.2
+        high[self.ORI_IND] = np.pi
+        low = -high
+        self.observation_space = gym.spaces.Box(low, high)
+
+    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, dict]:
+        qpos = self.sim.data.qpos.copy()
+        qpos[2] += action[1]
+        # Clip orientation
+        if qpos[2] < -np.pi:
+            qpos[2] += np.pi * 2
+        elif np.pi < qpos[2]:
+            qpos[2] -= np.pi * 2
+        ori = qpos[2]
+        # Compute increment in each direction
+        qpos[0] += np.cos(ori) * action[0]
+        qpos[1] += np.sin(ori) * action[0]
+        qvel = np.clip(self.sim.data.qvel, -self.VELOCITY_LIMITS, self.VELOCITY_LIMITS)
+        self.set_state(qpos, qvel)
+        for _ in range(0, self.frame_skip):
+            self.sim.step()
+        next_obs = self._get_obs()
+        return next_obs, 0.0, False, {}
+
+    def _get_obs(self):
+        return np.concatenate(
+            [
+                self.sim.data.qpos.flat[:3],  # Only point-relevant coords.
+                self.sim.data.qvel.flat[:3],
+            ]
+        )
+
+    def reset_model(self):
+        qpos = self.init_qpos + self.np_random.uniform(
+            size=self.sim.model.nq, low=-0.1, high=0.1
+        )
+        qvel = self.init_qvel + self.np_random.randn(self.sim.model.nv) * 0.1
+
+        # Set everything other than point to original position and 0 velocity.
+        qpos[3:] = self.init_qpos[3:]
+        qvel[3:] = 0.0
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def get_xy(self):
+        return self.sim.data.qpos[:2].copy()
+
+    def set_xy(self, xy: np.ndarray) -> None:
+        qpos = self.sim.data.qpos.copy()
+        qpos[:2] = xy
+        self.set_state(qpos, self.sim.data.qvel)
+
+    def get_ori(self):
+        return self.sim.data.qpos[self.ORI_IND]
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/reacher.py b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/reacher.py
new file mode 100644
index 0000000..d2db6aa
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/reacher.py
@@ -0,0 +1,72 @@
+"""
+Based on the reacher in `dm_control`_.
+
+.. _gym: https://github.com/openai/gym
+"""
+
+from typing import Tuple
+
+import numpy as np
+
+from slbo.envs.mujoco_maze.agent_model import AgentModel
+from slbo.envs.mujoco_maze.ant import ForwardRewardFn, forward_reward_vnorm
+
+
+class ReacherEnv(AgentModel):
+    FILE: str = "reacher.xml"
+    MANUAL_COLLISION: bool = False
+
+    def __init__(
+        self,
+        file_path: str = None,
+        forward_reward_weight: float = 1.0,
+        ctrl_cost_weight: float = 1e-4,
+        forward_reward_fn: ForwardRewardFn = forward_reward_vnorm,
+    ) -> None:
+        self._forward_reward_weight = forward_reward_weight
+        self._ctrl_cost_weight = ctrl_cost_weight
+        self._forward_reward_fn = forward_reward_fn
+        super().__init__(file_path, 4)
+
+    def _forward_reward(self, xy_pos_before: np.ndarray) -> Tuple[float, np.ndarray]:
+        xy_pos_after = self.sim.data.qpos[:2].copy()
+        xy_velocity = (xy_pos_after - xy_pos_before) / self.dt
+        return self._forward_reward_fn(xy_velocity)
+
+    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, dict]:
+        xy_pos_before = self.sim.data.qpos[:2].copy()
+        self.do_simulation(action, self.frame_skip)
+
+        forward_reward = self._forward_reward(xy_pos_before)
+        ctrl_cost = self._ctrl_cost_weight * np.sum(np.square(action))
+        return (
+            self._get_obs(),
+            self._forward_reward_weight * forward_reward - ctrl_cost,
+            False,
+            dict(reward_forward=forward_reward, reward_ctrl=-ctrl_cost),
+        )
+
+    def _get_obs(self) -> np.ndarray:
+        position = self.sim.data.qpos.flat.copy()
+        velocity = self.sim.data.qvel.flat.copy()
+        observation = np.concatenate([position, velocity]).ravel()
+        return observation
+
+    def reset_model(self) -> np.ndarray:
+        qpos = self.init_qpos + self.np_random.uniform(
+            low=-0.1, high=0.1, size=self.model.nq,
+        )
+        qvel = self.init_qvel + self.np_random.uniform(
+            low=-0.1, high=0.1, size=self.model.nv,
+        )
+
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def set_xy(self, xy: np.ndarray) -> None:
+        qpos = self.sim.data.qpos.copy()
+        qpos[:2] = xy
+        self.set_state(qpos, self.sim.data.qvel)
+
+    def get_xy(self) -> np.ndarray:
+        return np.copy(self.sim.data.qpos[:2])
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/swimmer.py b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/swimmer.py
new file mode 100644
index 0000000..cd825df
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/mujoco_maze/swimmer.py
@@ -0,0 +1,73 @@
+"""
+Swimmer robot as an explorer in the maze.
+Based on `gym`_ (swimmer-v3).
+
+.. _gym: https://github.com/openai/gym
+"""
+
+from typing import Tuple
+
+import numpy as np
+
+from slbo.envs.mujoco_maze.agent_model import AgentModel
+from slbo.envs.mujoco_maze.ant import ForwardRewardFn, forward_reward_vnorm
+
+
+class SwimmerEnv(AgentModel):
+    FILE: str = "swimmer.xml"
+    MANUAL_COLLISION: bool = False
+
+    def __init__(
+        self,
+        file_path: str = None,
+        forward_reward_weight: float = 1.0,
+        ctrl_cost_weight: float = 1e-4,
+        forward_reward_fn: ForwardRewardFn = forward_reward_vnorm,
+    ) -> None:
+        self._forward_reward_weight = forward_reward_weight
+        self._ctrl_cost_weight = ctrl_cost_weight
+        self._forward_reward_fn = forward_reward_fn
+        super().__init__(file_path, 4)
+
+    def _forward_reward(self, xy_pos_before: np.ndarray) -> Tuple[float, np.ndarray]:
+        xy_pos_after = self.sim.data.qpos[:2].copy()
+        xy_velocity = (xy_pos_after - xy_pos_before) / self.dt
+        return self._forward_reward_fn(xy_velocity)
+
+    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, dict]:
+        xy_pos_before = self.sim.data.qpos[:2].copy()
+        self.do_simulation(action, self.frame_skip)
+
+        forward_reward = self._forward_reward(xy_pos_before)
+        ctrl_cost = self._ctrl_cost_weight * np.sum(np.square(action))
+        return (
+            self._get_obs(),
+            self._forward_reward_weight * forward_reward - ctrl_cost,
+            False,
+            dict(reward_forward=forward_reward, reward_ctrl=-ctrl_cost),
+        )
+
+    def _get_obs(self) -> np.ndarray:
+        position = self.sim.data.qpos.flat.copy()
+        velocity = self.sim.data.qvel.flat.copy()
+        observation = np.concatenate([position, velocity]).ravel()
+        return observation
+
+    def reset_model(self) -> np.ndarray:
+        qpos = self.init_qpos + self.np_random.uniform(
+            low=-0.1, high=0.1, size=self.model.nq,
+        )
+        qvel = self.init_qvel + self.np_random.uniform(
+            low=-0.1, high=0.1, size=self.model.nv,
+        )
+
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def set_xy(self, xy: np.ndarray) -> None:
+        qpos = self.sim.data.qpos.copy()
+        qpos[:2] = xy
+        self.set_state(qpos, self.sim.data.qvel)
+
+    def get_xy(self) -> np.ndarray:
+        return np.copy(self.sim.data.qpos[:2])
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/LICENSE.md b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/LICENSE.md
new file mode 100644
index 0000000..22ce901
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/LICENSE.md
@@ -0,0 +1,222 @@
+# Fetch Robotics
+The model of the [Fetch](http://fetchrobotics.com/platforms-research-development/) is based on [models provided by Fetch](https://github.com/fetchrobotics/fetch_ros/tree/indigo-devel/fetch_description). It was adapted and refined by OpenAI.
+
+# ShadowHand
+The model of the [ShadowHand](https://www.shadowrobot.com/products/dexterous-hand/) is based on [models provided by ShadowRobot](https://github.com/shadow-robot/sr_common/tree/kinetic-devel/sr_description/hand/model), and on code used under the following license:
+
+(C) Vikash Kumar, CSE, UW. Licensed under Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
+
+	                                 Apache License
+	                           Version 2.0, January 2004
+	                        http://www.apache.org/licenses/
+
+	   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+	   1. Definitions.
+
+	      "License" shall mean the terms and conditions for use, reproduction,
+	      and distribution as defined by Sections 1 through 9 of this document.
+
+	      "Licensor" shall mean the copyright owner or entity authorized by
+	      the copyright owner that is granting the License.
+
+	      "Legal Entity" shall mean the union of the acting entity and all
+	      other entities that control, are controlled by, or are under common
+	      control with that entity. For the purposes of this definition,
+	      "control" means (i) the power, direct or indirect, to cause the
+	      direction or management of such entity, whether by contract or
+	      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+	      outstanding shares, or (iii) beneficial ownership of such entity.
+
+	      "You" (or "Your") shall mean an individual or Legal Entity
+	      exercising permissions granted by this License.
+
+	      "Source" form shall mean the preferred form for making modifications,
+	      including but not limited to software source code, documentation
+	      source, and configuration files.
+
+	      "Object" form shall mean any form resulting from mechanical
+	      transformation or translation of a Source form, including but
+	      not limited to compiled object code, generated documentation,
+	      and conversions to other media types.
+
+	      "Work" shall mean the work of authorship, whether in Source or
+	      Object form, made available under the License, as indicated by a
+	      copyright notice that is included in or attached to the work
+	      (an example is provided in the Appendix below).
+
+	      "Derivative Works" shall mean any work, whether in Source or Object
+	      form, that is based on (or derived from) the Work and for which the
+	      editorial revisions, annotations, elaborations, or other modifications
+	      represent, as a whole, an original work of authorship. For the purposes
+	      of this License, Derivative Works shall not include works that remain
+	      separable from, or merely link (or bind by name) to the interfaces of,
+	      the Work and Derivative Works thereof.
+
+	      "Contribution" shall mean any work of authorship, including
+	      the original version of the Work and any modifications or additions
+	      to that Work or Derivative Works thereof, that is intentionally
+	      submitted to Licensor for inclusion in the Work by the copyright owner
+	      or by an individual or Legal Entity authorized to submit on behalf of
+	      the copyright owner. For the purposes of this definition, "submitted"
+	      means any form of electronic, verbal, or written communication sent
+	      to the Licensor or its representatives, including but not limited to
+	      communication on electronic mailing lists, source code control systems,
+	      and issue tracking systems that are managed by, or on behalf of, the
+	      Licensor for the purpose of discussing and improving the Work, but
+	      excluding communication that is conspicuously marked or otherwise
+	      designated in writing by the copyright owner as "Not a Contribution."
+
+	      "Contributor" shall mean Licensor and any individual or Legal Entity
+	      on behalf of whom a Contribution has been received by Licensor and
+	      subsequently incorporated within the Work.
+
+	   2. Grant of Copyright License. Subject to the terms and conditions of
+	      this License, each Contributor hereby grants to You a perpetual,
+	      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+	      copyright license to reproduce, prepare Derivative Works of,
+	      publicly display, publicly perform, sublicense, and distribute the
+	      Work and such Derivative Works in Source or Object form.
+
+	   3. Grant of Patent License. Subject to the terms and conditions of
+	      this License, each Contributor hereby grants to You a perpetual,
+	      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+	      (except as stated in this section) patent license to make, have made,
+	      use, offer to sell, sell, import, and otherwise transfer the Work,
+	      where such license applies only to those patent claims licensable
+	      by such Contributor that are necessarily infringed by their
+	      Contribution(s) alone or by combination of their Contribution(s)
+	      with the Work to which such Contribution(s) was submitted. If You
+	      institute patent litigation against any entity (including a
+	      cross-claim or counterclaim in a lawsuit) alleging that the Work
+	      or a Contribution incorporated within the Work constitutes direct
+	      or contributory patent infringement, then any patent licenses
+	      granted to You under this License for that Work shall terminate
+	      as of the date such litigation is filed.
+
+	   4. Redistribution. You may reproduce and distribute copies of the
+	      Work or Derivative Works thereof in any medium, with or without
+	      modifications, and in Source or Object form, provided that You
+	      meet the following conditions:
+
+	      (a) You must give any other recipients of the Work or
+	          Derivative Works a copy of this License; and
+
+	      (b) You must cause any modified files to carry prominent notices
+	          stating that You changed the files; and
+
+	      (c) You must retain, in the Source form of any Derivative Works
+	          that You distribute, all copyright, patent, trademark, and
+	          attribution notices from the Source form of the Work,
+	          excluding those notices that do not pertain to any part of
+	          the Derivative Works; and
+
+	      (d) If the Work includes a "NOTICE" text file as part of its
+	          distribution, then any Derivative Works that You distribute must
+	          include a readable copy of the attribution notices contained
+	          within such NOTICE file, excluding those notices that do not
+	          pertain to any part of the Derivative Works, in at least one
+	          of the following places: within a NOTICE text file distributed
+	          as part of the Derivative Works; within the Source form or
+	          documentation, if provided along with the Derivative Works; or,
+	          within a display generated by the Derivative Works, if and
+	          wherever such third-party notices normally appear. The contents
+	          of the NOTICE file are for informational purposes only and
+	          do not modify the License. You may add Your own attribution
+	          notices within Derivative Works that You distribute, alongside
+	          or as an addendum to the NOTICE text from the Work, provided
+	          that such additional attribution notices cannot be construed
+	          as modifying the License.
+
+	      You may add Your own copyright statement to Your modifications and
+	      may provide additional or different license terms and conditions
+	      for use, reproduction, or distribution of Your modifications, or
+	      for any such Derivative Works as a whole, provided Your use,
+	      reproduction, and distribution of the Work otherwise complies with
+	      the conditions stated in this License.
+
+	   5. Submission of Contributions. Unless You explicitly state otherwise,
+	      any Contribution intentionally submitted for inclusion in the Work
+	      by You to the Licensor shall be under the terms and conditions of
+	      this License, without any additional terms or conditions.
+	      Notwithstanding the above, nothing herein shall supersede or modify
+	      the terms of any separate license agreement you may have executed
+	      with Licensor regarding such Contributions.
+
+	   6. Trademarks. This License does not grant permission to use the trade
+	      names, trademarks, service marks, or product names of the Licensor,
+	      except as required for reasonable and customary use in describing the
+	      origin of the Work and reproducing the content of the NOTICE file.
+
+	   7. Disclaimer of Warranty. Unless required by applicable law or
+	      agreed to in writing, Licensor provides the Work (and each
+	      Contributor provides its Contributions) on an "AS IS" BASIS,
+	      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+	      implied, including, without limitation, any warranties or conditions
+	      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+	      PARTICULAR PURPOSE. You are solely responsible for determining the
+	      appropriateness of using or redistributing the Work and assume any
+	      risks associated with Your exercise of permissions under this License.
+
+	   8. Limitation of Liability. In no event and under no legal theory,
+	      whether in tort (including negligence), contract, or otherwise,
+	      unless required by applicable law (such as deliberate and grossly
+	      negligent acts) or agreed to in writing, shall any Contributor be
+	      liable to You for damages, including any direct, indirect, special,
+	      incidental, or consequential damages of any character arising as a
+	      result of this License or out of the use or inability to use the
+	      Work (including but not limited to damages for loss of goodwill,
+	      work stoppage, computer failure or malfunction, or any and all
+	      other commercial damages or losses), even if such Contributor
+	      has been advised of the possibility of such damages.
+
+	   9. Accepting Warranty or Additional Liability. While redistributing
+	      the Work or Derivative Works thereof, You may choose to offer,
+	      and charge a fee for, acceptance of support, warranty, indemnity,
+	      or other liability obligations and/or rights consistent with this
+	      License. However, in accepting such obligations, You may act only
+	      on Your own behalf and on Your sole responsibility, not on behalf
+	      of any other Contributor, and only if You agree to indemnify,
+	      defend, and hold each Contributor harmless for any liability
+	      incurred by, or claims asserted against, such Contributor by reason
+	      of your accepting any such warranty or additional liability.
+
+	   END OF TERMS AND CONDITIONS
+
+	   APPENDIX: How to apply the Apache License to your work.
+
+	      To apply the Apache License to your work, attach the following
+	      boilerplate notice, with the fields enclosed by brackets "[]"
+	      replaced with your own identifying information. (Don't include
+	      the brackets!)  The text should be enclosed in the appropriate
+	      comment syntax for the file format. We also recommend that a
+	      file or class name and description of purpose be included on the
+	      same "printed page" as the copyright notice for easier
+	      identification within third-party archives.
+
+	   Copyright [yyyy] [name of copyright owner]
+
+	   Licensed under the Apache License, Version 2.0 (the "License");
+	   you may not use this file except in compliance with the License.
+	   You may obtain a copy of the License at
+
+	       http://www.apache.org/licenses/LICENSE-2.0
+
+	   Unless required by applicable law or agreed to in writing, software
+	   distributed under the License is distributed on an "AS IS" BASIS,
+	   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+	   See the License for the specific language governing permissions and
+	   limitations under the License.
+
+Additional license notices:
+
+	Sources		: 1) Manipulator and Manipulation in High Dimensional Spaces. Vikash Kumar, Ph.D. Thesis, CSE, Univ. of Washington. 2016.
+
+	Mujoco		:: Advanced physics simulation engine
+		Source		: www.roboti.us
+		Version		: 1.40
+		Released 	: 17Jan'17
+
+	Author		:: Vikash Kumar
+		Contacts 	: vikash@openai.com
+		Last edits 	: 3Apr'17
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/pick_and_place.xml b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/pick_and_place.xml
new file mode 100644
index 0000000..337032a
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/pick_and_place.xml
@@ -0,0 +1,35 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+	<compiler angle="radian" coordinate="local" meshdir="../stls/fetch" texturedir="../textures"></compiler>
+	<option timestep="0.002">
+		<flag warmstart="enable"></flag>
+	</option>
+
+	<include file="shared.xml"></include>
+	
+	<worldbody>
+		<geom name="floor0" pos="0.8 0.75 0" size="0.85 0.7 1" type="plane" condim="3" material="floor_mat"></geom>
+		<body name="floor0" pos="0.8 0.75 0">
+			<site name="target0" pos="0 0 0.5" size="0.02 0.02 0.02" rgba="1 0 0 1" type="sphere"></site>
+		</body>
+
+		<include file="robot.xml"></include>
+		
+		<body pos="1.3 0.75 0.2" name="table0">
+			<geom size="0.25 0.35 0.2" type="box" mass="2000" material="table_mat"></geom>
+		</body>
+		
+		<body name="object0" pos="0.025 0.025 0.025">
+			<joint name="object0:joint" type="free" damping="0.01"></joint>
+			<geom size="0.025 0.025 0.025" type="box" condim="3" name="object0" material="block_mat" mass="2"></geom>
+			<site name="object0" pos="0 0 0" size="0.02 0.02 0.02" rgba="1 0 0 1" type="sphere"></site>
+		</body>
+
+		<light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 0 4" dir="0 0 -1" name="light0"></light>
+	</worldbody>
+
+	<actuator>
+		<position ctrllimited="true" ctrlrange="0 0.2" joint="robot0:l_gripper_finger_joint" kp="30000" name="robot0:l_gripper_finger_joint" user="1"></position>
+		<position ctrllimited="true" ctrlrange="0 0.2" joint="robot0:r_gripper_finger_joint" kp="30000" name="robot0:r_gripper_finger_joint" user="1"></position>
+	</actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/push.xml b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/push.xml
new file mode 100644
index 0000000..8e12db2
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/push.xml
@@ -0,0 +1,32 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+	<compiler angle="radian" coordinate="local" meshdir="../stls/fetch" texturedir="../textures"></compiler>
+	<option timestep="0.002">
+		<flag warmstart="enable"></flag>
+	</option>
+
+	<include file="shared.xml"></include>
+	
+	<worldbody>
+		<geom name="floor0" pos="0.8 0.75 0" size="0.85 0.70 1" type="plane" condim="3" material="floor_mat"></geom>
+		<body name="floor0" pos="0.8 0.75 0">
+			<site name="target0" pos="0 0 0.5" size="0.02 0.02 0.02" rgba="1 0 0 1" type="sphere"></site>
+		</body>
+
+		<include file="robot.xml"></include>
+
+		<body pos="1.3 0.75 0.2" name="table0">
+			<geom size="0.25 0.35 0.2" type="box" mass="2000" material="table_mat"></geom>
+		</body>
+		
+		<body name="object0" pos="0.025 0.025 0.025">
+			<joint name="object0:joint" type="free" damping="0.01"></joint>
+			<geom size="0.025 0.025 0.025" type="box" condim="3" name="object0" material="block_mat" mass="2"></geom>
+			<site name="object0" pos="0 0 0" size="0.02 0.02 0.02" rgba="1 0 0 1" type="sphere"></site>
+		</body>
+
+		<light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 0 4" dir="0 0 -1" name="light0"></light>
+	</worldbody>
+	
+	<actuator></actuator>
+</mujoco>
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/reach.xml b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/reach.xml
new file mode 100644
index 0000000..c73d624
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/reach.xml
@@ -0,0 +1,26 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+	<compiler angle="radian" coordinate="local" meshdir="../stls/fetch" texturedir="../textures"></compiler>
+	<option timestep="0.002">
+		<flag warmstart="enable"></flag>
+	</option>
+
+	<include file="shared.xml"></include>
+	
+	<worldbody>
+		<geom name="floor0" pos="0.8 0.75 0" size="0.85 0.7 1" type="plane" condim="3" material="floor_mat"></geom>
+		<body name="floor0" pos="0.8 0.75 0">
+			<site name="target0" pos="0 0 0.5" size="0.02 0.02 0.02" rgba="1 0 0 1" type="sphere"></site>
+		</body>
+
+		<include file="robot.xml"></include>
+		
+		<body pos="1.3 0.75 0.2" name="table0">
+			<geom size="0.25 0.35 0.2" type="box" mass="2000" material="table_mat"></geom>
+		</body>
+
+		<light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 0 4" dir="0 0 -1" name="light0"></light>
+	</worldbody>
+	
+	<actuator></actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/robot.xml b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/robot.xml
new file mode 100644
index 0000000..b627d49
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/robot.xml
@@ -0,0 +1,123 @@
+<mujoco>
+	<body mocap="true" name="robot0:mocap" pos="0 0 0">
+		<geom conaffinity="0" contype="0" pos="0 0 0" rgba="0 0.5 0 0.7" size="0.005 0.005 0.005" type="box"></geom>
+		<geom conaffinity="0" contype="0" pos="0 0 0" rgba="0 0.5 0 0.1" size="1 0.005 0.005" type="box"></geom>
+		<geom conaffinity="0" contype="0" pos="0 0 0" rgba="0 0.5 0 0.1" size="0.005 1 0.001" type="box"></geom>
+		<geom conaffinity="0" contype="0" pos="0 0 0" rgba="0 0.5 0 0.1" size="0.005 0.005 1" type="box"></geom>
+	</body>
+	<body childclass="robot0:fetch" name="robot0:base_link" pos="0.2869 0.2641 0">
+		<joint armature="0.0001" axis="1 0 0" damping="1e+11" name="robot0:slide0" pos="0 0 0" type="slide"></joint>
+		<joint armature="0.0001" axis="0 1 0" damping="1e+11" name="robot0:slide1" pos="0 0 0" type="slide"></joint>
+		<joint armature="0.0001" axis="0 0 1" damping="1e+11" name="robot0:slide2" pos="0 0 0" type="slide"></joint>
+		<inertial diaginertia="1.2869 1.2236 0.9868" mass="70.1294" pos="-0.0036 0 0.0014" quat="0.7605 -0.0133 -0.0061 0.6491"></inertial>
+		<geom mesh="robot0:base_link" name="robot0:base_link" material="robot0:base_mat" class="robot0:grey"></geom>
+		<body name="robot0:torso_lift_link" pos="-0.0869 0 0.3774">
+			<inertial diaginertia="0.3365 0.3354 0.0943" mass="10.7796" pos="-0.0013 -0.0009 0.2935" quat="0.9993 -0.0006 0.0336 0.0185"></inertial>
+			<joint axis="0 0 1" damping="1e+07" name="robot0:torso_lift_joint" range="0.0386 0.3861" type="slide"></joint>
+			<geom mesh="robot0:torso_lift_link" name="robot0:torso_lift_link" material="robot0:torso_mat"></geom>
+			<body name="robot0:head_pan_link" pos="0.0531 0 0.603">
+				<inertial diaginertia="0.0185 0.0128 0.0095" mass="2.2556" pos="0.0321 0.0161 0.039" quat="0.5148 0.5451 -0.453 0.4823"></inertial>
+				<joint axis="0 0 1" name="robot0:head_pan_joint" range="-1.57 1.57"></joint>
+				<geom mesh="robot0:head_pan_link" name="robot0:head_pan_link" material="robot0:head_mat" class="robot0:grey"></geom>
+				<body name="robot0:head_tilt_link" pos="0.1425 0 0.058">
+					<inertial diaginertia="0.0063 0.0059 0.0014" mass="0.9087" pos="0.0081 0.0025 0.0113" quat="0.6458 0.66 -0.274 0.2689"></inertial>
+					<joint axis="0 1 0" damping="1000" name="robot0:head_tilt_joint" range="-0.76 1.45" ref="0.06"></joint>
+					<geom mesh="robot0:head_tilt_link" name="robot0:head_tilt_link" material="robot0:head_mat" class="robot0:blue"></geom>
+					<body name="robot0:head_camera_link" pos="0.055 0 0.0225">
+						<inertial diaginertia="0 0 0" mass="0" pos="0.055 0 0.0225"></inertial>
+						<body name="robot0:head_camera_rgb_frame" pos="0 0.02 0">
+							<inertial diaginertia="0 0 0" mass="0" pos="0 0.02 0"></inertial>
+							<body name="robot0:head_camera_rgb_optical_frame" pos="0 0 0" quat="0.5 -0.5 0.5 -0.5">
+								<inertial diaginertia="0 0 0" mass="0" pos="0 0 0" quat="0.5 -0.5 0.5 -0.5"></inertial>
+								<camera euler="3.1415 0 0" fovy="50" name="head_camera_rgb" pos="0 0 0"></camera>
+							</body>
+						</body>
+						<body name="robot0:head_camera_depth_frame" pos="0 0.045 0">
+							<inertial diaginertia="0 0 0" mass="0" pos="0 0.045 0"></inertial>
+							<body name="robot0:head_camera_depth_optical_frame" pos="0 0 0" quat="0.5 -0.5 0.5 -0.5">
+								<inertial diaginertia="0 0 0" mass="0" pos="0 0 0" quat="0.5 -0.5 0.5 -0.5"></inertial>
+							</body>
+						</body>
+					</body>
+				</body>
+			</body>
+			<body name="robot0:shoulder_pan_link" pos="0.1195 0 0.3486">
+				<inertial diaginertia="0.009 0.0086 0.0041" mass="2.5587" pos="0.0927 -0.0056 0.0564" quat="-0.1364 0.7624 -0.1562 0.613"></inertial>
+				<joint axis="0 0 1" name="robot0:shoulder_pan_joint" range="-1.6056 1.6056"></joint>
+				<geom mesh="robot0:shoulder_pan_link" name="robot0:shoulder_pan_link" material="robot0:arm_mat"></geom>
+				<body name="robot0:shoulder_lift_link" pos="0.117 0 0.06">
+					<inertial diaginertia="0.0116 0.0112 0.0023" mass="2.6615" pos="0.1432 0.0072 -0.0001" quat="0.4382 0.4382 0.555 0.555"></inertial>
+					<joint axis="0 1 0" name="robot0:shoulder_lift_joint" range="-1.221 1.518"></joint>
+					<geom mesh="robot0:shoulder_lift_link" name="robot0:shoulder_lift_link" material="robot0:arm_mat" class="robot0:blue"></geom>
+					<body name="robot0:upperarm_roll_link" pos="0.219 0 0">
+						<inertial diaginertia="0.0047 0.0045 0.0019" mass="2.3311" pos="0.1165 0.0014 0" quat="-0.0136 0.707 0.0136 0.707"></inertial>
+						<joint axis="1 0 0" limited="false" name="robot0:upperarm_roll_joint"></joint>
+						<geom mesh="robot0:upperarm_roll_link" name="robot0:upperarm_roll_link" material="robot0:arm_mat"></geom>
+						<body name="robot0:elbow_flex_link" pos="0.133 0 0">
+							<inertial diaginertia="0.0086 0.0084 0.002" mass="2.1299" pos="0.1279 0.0073 0" quat="0.4332 0.4332 0.5589 0.5589"></inertial>
+							<joint axis="0 1 0" name="robot0:elbow_flex_joint" range="-2.251 2.251"></joint>
+							<geom mesh="robot0:elbow_flex_link" name="robot0:elbow_flex_link" material="robot0:arm_mat" class="robot0:blue"></geom>
+							<body name="robot0:forearm_roll_link" pos="0.197 0 0">
+								<inertial diaginertia="0.0035 0.0031 0.0015" mass="1.6563" pos="0.1097 -0.0266 0" quat="-0.0715 0.7035 0.0715 0.7035"></inertial>
+								<joint armature="2.7538" axis="1 0 0" damping="3.5247" frictionloss="0" limited="false" name="robot0:forearm_roll_joint" stiffness="10"></joint>
+								<geom mesh="robot0:forearm_roll_link" name="robot0:forearm_roll_link" material="robot0:arm_mat"></geom>
+								<body name="robot0:wrist_flex_link" pos="0.1245 0 0">
+									<inertial diaginertia="0.0042 0.0042 0.0018" mass="1.725" pos="0.0882 0.0009 -0.0001" quat="0.4895 0.4895 0.5103 0.5103"></inertial>
+									<joint axis="0 1 0" name="robot0:wrist_flex_joint" range="-2.16 2.16"></joint>
+									<geom mesh="robot0:wrist_flex_link" name="robot0:wrist_flex_link" material="robot0:arm_mat" class="robot0:blue"></geom>
+									<body name="robot0:wrist_roll_link" pos="0.1385 0 0">
+										<inertial diaginertia="0.0001 0.0001 0.0001" mass="0.1354" pos="0.0095 0.0004 -0.0002"></inertial>
+										<joint axis="1 0 0" limited="false" name="robot0:wrist_roll_joint"></joint>
+										<geom mesh="robot0:wrist_roll_link" name="robot0:wrist_roll_link" material="robot0:arm_mat"></geom>
+										<body euler="0 0 0" name="robot0:gripper_link" pos="0.1664 0 0">
+											<inertial diaginertia="0.0024 0.0019 0.0013" mass="1.5175" pos="-0.09 -0.0001 -0.0017" quat="0 0.7071 0 0.7071"></inertial>
+											<geom mesh="robot0:gripper_link" name="robot0:gripper_link" material="robot0:gripper_mat"></geom>
+											<body name="robot0:gripper_camera_link" pos="0.055 0 0.0225">
+												<body name="robot0:gripper_camera_rgb_frame" pos="0 0.02 0">
+													<body name="robot0:gripper_camera_rgb_optical_frame" pos="0 0 0" quat="0.5 -0.5 0.5 -0.5">
+														<camera euler="3.1415 0 0" fovy="50" name="gripper_camera_rgb" pos="0 0 0"></camera>
+													</body>
+												</body>
+												<body name="robot0:gripper_camera_depth_frame" pos="0 0.045 0">
+													<body name="robot0:gripper_camera_depth_optical_frame" pos="0 0 0" quat="0.5 -0.5 0.5 -0.5"></body>
+												</body>
+											</body>
+
+											<body childclass="robot0:fetchGripper" name="robot0:r_gripper_finger_link" pos="0 0.0159 0">
+												<inertial diaginertia="0.1 0.1 0.1" mass="4" pos="-0.01 0 0"></inertial>
+												<joint axis="0 1 0" name="robot0:r_gripper_finger_joint" range="0 0.05"></joint>
+												<geom pos="0 -0.008 0" size="0.0385 0.007 0.0135" type="box" name="robot0:r_gripper_finger_link" material="robot0:gripper_finger_mat" condim="4" friction="1 0.05 0.01"></geom>
+											</body>
+											<body childclass="robot0:fetchGripper" name="robot0:l_gripper_finger_link" pos="0 -0.0159 0">
+												<inertial diaginertia="0.1 0.1 0.1" mass="4" pos="-0.01 0 0"></inertial>
+												<joint axis="0 -1 0" name="robot0:l_gripper_finger_joint" range="0 0.05"></joint>
+												<geom pos="0 0.008 0" size="0.0385 0.007 0.0135" type="box" name="robot0:l_gripper_finger_link" material="robot0:gripper_finger_mat" condim="4" friction="1 0.05 0.01"></geom>
+											</body>
+											<site name="robot0:grip" pos="0.02 0 0" rgba="0 0 0 0" size="0.02 0.02 0.02"></site>
+										</body>
+									</body>
+								</body>
+							</body>
+						</body>
+					</body>
+				</body>
+			</body>
+		</body>
+		<body name="robot0:estop_link" pos="-0.1246 0.2389 0.3113" quat="0.7071 0.7071 0 0">
+			<inertial diaginertia="0 0 0" mass="0.002" pos="0.0024 -0.0033 0.0067" quat="0.3774 -0.1814 0.1375 0.8977"></inertial>
+			<geom mesh="robot0:estop_link" rgba="0.8 0 0 1" name="robot0:estop_link"></geom>
+		</body>
+		<body name="robot0:laser_link" pos="0.235 0 0.2878" quat="0 1 0 0">
+			<inertial diaginertia="0 0 0" mass="0.0083" pos="-0.0306 0.0007 0.0552" quat="0.5878 0.5378 -0.4578 0.3945"></inertial>
+			<geom mesh="robot0:laser_link" rgba="0.7922 0.8196 0.9333 1" name="robot0:laser_link"></geom>
+			<camera euler="1.55 -1.55 3.14" fovy="25" name="lidar" pos="0 0 0.02"></camera>
+		</body>
+		<body name="robot0:torso_fixed_link" pos="-0.0869 0 0.3774">
+			<inertial diaginertia="0.3865 0.3394 0.1009" mass="13.2775" pos="-0.0722 0.0057 0.2656" quat="0.9995 0.0249 0.0177 0.011"></inertial>
+			<geom mesh="robot0:torso_fixed_link" name="robot0:torso_fixed_link" class="robot0:blue"></geom>
+		</body>
+		<body name="robot0:external_camera_body_0" pos="0 0 0">
+			<camera euler="0 0.75 1.57" fovy="43.3" name="external_camera_0" pos="1.3 0 1.2"></camera>
+		</body>
+	</body>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/shared.xml b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/shared.xml
new file mode 100644
index 0000000..5d61fef
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/shared.xml
@@ -0,0 +1,66 @@
+<mujoco>
+    <asset>
+        <texture type="skybox" builtin="gradient" rgb1="0.44 0.85 0.56" rgb2="0.46 0.87 0.58" width="32" height="32"></texture>
+        <texture name="texture_block" file="block.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+
+        <material name="floor_mat" specular="0" shininess="0.5" reflectance="0" rgba="0.2 0.2 0.2 1"></material>
+        <material name="table_mat" specular="0" shininess="0.5" reflectance="0" rgba="0.93 0.93 0.93 1"></material>
+        <material name="block_mat" specular="0" shininess="0.5" reflectance="0" rgba="0.2 0.2 0.2 1"></material>
+        <material name="puck_mat" specular="0" shininess="0.5" reflectance="0" rgba="0.2 0.2 0.2 1"></material>
+        <material name="robot0:geomMat" shininess="0.03" specular="0.4"></material>
+        <material name="robot0:gripper_finger_mat" shininess="0.03" specular="0.4" reflectance="0"></material>
+        <material name="robot0:gripper_mat" shininess="0.03" specular="0.4" reflectance="0"></material>
+        <material name="robot0:arm_mat" shininess="0.03" specular="0.4" reflectance="0"></material>
+        <material name="robot0:head_mat" shininess="0.03" specular="0.4" reflectance="0"></material>
+        <material name="robot0:torso_mat" shininess="0.03" specular="0.4" reflectance="0"></material>
+        <material name="robot0:base_mat" shininess="0.03" specular="0.4" reflectance="0"></material>
+        
+        <mesh file="base_link_collision.stl" name="robot0:base_link"></mesh>
+        <mesh file="bellows_link_collision.stl" name="robot0:bellows_link"></mesh>
+        <mesh file="elbow_flex_link_collision.stl" name="robot0:elbow_flex_link"></mesh>
+        <mesh file="estop_link.stl" name="robot0:estop_link"></mesh>
+        <mesh file="forearm_roll_link_collision.stl" name="robot0:forearm_roll_link"></mesh>
+        <mesh file="gripper_link.stl" name="robot0:gripper_link"></mesh>
+        <mesh file="head_pan_link_collision.stl" name="robot0:head_pan_link"></mesh>
+        <mesh file="head_tilt_link_collision.stl" name="robot0:head_tilt_link"></mesh>
+        <mesh file="l_wheel_link_collision.stl" name="robot0:l_wheel_link"></mesh>
+        <mesh file="laser_link.stl" name="robot0:laser_link"></mesh>
+        <mesh file="r_wheel_link_collision.stl" name="robot0:r_wheel_link"></mesh>
+        <mesh file="torso_lift_link_collision.stl" name="robot0:torso_lift_link"></mesh>
+        <mesh file="shoulder_pan_link_collision.stl" name="robot0:shoulder_pan_link"></mesh>
+        <mesh file="shoulder_lift_link_collision.stl" name="robot0:shoulder_lift_link"></mesh>
+        <mesh file="upperarm_roll_link_collision.stl" name="robot0:upperarm_roll_link"></mesh>
+        <mesh file="wrist_flex_link_collision.stl" name="robot0:wrist_flex_link"></mesh>
+        <mesh file="wrist_roll_link_collision.stl" name="robot0:wrist_roll_link"></mesh>
+        <mesh file="torso_fixed_link.stl" name="robot0:torso_fixed_link"></mesh>
+    </asset>
+
+    <equality>
+        <weld body1="robot0:mocap" body2="robot0:gripper_link" solimp="0.9 0.95 0.001" solref="0.02 1"></weld>
+    </equality>
+    
+    <contact>
+        <exclude body1="robot0:r_gripper_finger_link" body2="robot0:l_gripper_finger_link"></exclude>
+        <exclude body1="robot0:torso_lift_link" body2="robot0:torso_fixed_link"></exclude>
+        <exclude body1="robot0:torso_lift_link" body2="robot0:shoulder_pan_link"></exclude>
+    </contact>
+    
+    <default>
+        <default class="robot0:fetch">
+            <geom margin="0.001" material="robot0:geomMat" rgba="1 1 1 1" solimp="0.99 0.99 0.01" solref="0.01 1" type="mesh" user="0"></geom>
+            <joint armature="1" damping="50" frictionloss="0" stiffness="0"></joint>
+            
+            <default class="robot0:fetchGripper">
+                <geom condim="4" margin="0.001" type="box" user="0" rgba="0.356 0.361 0.376 1.0"></geom>
+                <joint armature="100" damping="1000" limited="true" solimplimit="0.99 0.999 0.01" solreflimit="0.01 1" type="slide"></joint>
+            </default>
+
+            <default class="robot0:grey">
+                <geom rgba="0.356 0.361 0.376 1.0"></geom>
+            </default>
+            <default class="robot0:blue">
+                <geom rgba="0.086 0.506 0.767 1.0"></geom>
+            </default>
+        </default>
+    </default>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/slide.xml b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/slide.xml
new file mode 100644
index 0000000..efbfb51
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/slide.xml
@@ -0,0 +1,32 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+	<compiler angle="radian" coordinate="local" meshdir="../stls/fetch" texturedir="../textures"></compiler>
+	<option timestep="0.002">
+		<flag warmstart="enable"></flag>
+	</option>
+
+	<include file="shared.xml"></include>
+	
+	<worldbody>
+		<geom name="floor0" pos="1 0.75 0" size="1.05 0.7 1" type="plane" condim="3" material="floor_mat"></geom>
+		<body name="floor0" pos="1 0.75 0">
+			<site name="target0" pos="0 0 0.5" size="0.02 0.02 0.02" rgba="1 0 0 1" type="sphere"></site>
+		</body>
+
+		<include file="robot.xml"></include>
+		
+		<body name="table0" pos="1.32441906 0.75018422 0.2">
+			<geom size="0.625 0.45 0.2" type="box" condim="3" name="table0" material="table_mat" mass="2000" friction="0.1 0.005 0.0001"></geom>
+		</body>
+
+		<body name="object0" pos="0.025 0.025 0.02">
+			<joint name="object0:joint" type="free" damping="0.01"></joint>
+			<geom size="0.025 0.02" type="cylinder" condim="3" name="object0" material="puck_mat" friction="0.1 0.005 0.0001" mass="2"></geom>
+			<site name="object0" pos="0 0 0" size="0.02 0.02 0.02" rgba="1 0 0 1" type="sphere"></site>
+		</body>
+
+		<light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 0 4" dir="0 0 -1" name="light0"></light>
+	</worldbody>
+
+	<actuator></actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_block.xml b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_block.xml
new file mode 100644
index 0000000..83a6517
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_block.xml
@@ -0,0 +1,41 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+    <compiler angle="radian" coordinate="local" meshdir="../stls/hand" texturedir="../textures"></compiler>
+    <option timestep="0.002" iterations="20" apirate="200">
+        <flag warmstart="enable"></flag>
+    </option>
+
+    <include file="shared.xml"></include>
+
+    <asset>
+        <include file="shared_asset.xml"></include>
+
+        <texture name="texture:object" file="block.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+        <texture name="texture:hidden" file="block_hidden.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+
+        <material name="material:object" texture="texture:object" specular="1" shininess="0.3" reflectance="0"></material>
+        <material name="material:hidden" texture="texture:hidden" specular="1" shininess="0.3" reflectance="0"></material>
+        <material name="material:target" texture="texture:object" specular="1" shininess="0.3" reflectance="0" rgba="1 1 1 0.5"></material>
+    </asset>
+
+    <worldbody>
+        <geom name="floor0" pos="1 1 0" size="1 1 1" type="plane" condim="3" material="floor_mat"></geom>
+        <body name="floor0" pos="1 1 0"></body>
+
+        <include file="robot.xml"></include>
+
+        <body name="object" pos="1 0.87 0.2">
+            <geom name="object" type="box" size="0.025 0.025 0.025" material="material:object" condim="4" density="567"></geom>
+            <geom name="object_hidden" type="box" size="0.024 0.024 0.024" material="material:hidden" condim="4" contype="0" conaffinity="0" mass="0"></geom>
+            <site name="object:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <joint name="object:joint" type="free" damping="0.01"></joint>
+        </body>
+        <body name="target" pos="1 0.87 0.2">
+            <geom name="target" type="box" size="0.025 0.025 0.025" material="material:target" condim="4" group="2" contype="0" conaffinity="0"></geom>
+            <site name="target:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <joint name="target:joint" type="free" damping="0.01"></joint>
+        </body>
+        
+        <light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 1 4" dir="0 0 -1" name="light0"></light>
+    </worldbody>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_block_touch_sensors.xml b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_block_touch_sensors.xml
new file mode 100644
index 0000000..b649f10
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_block_touch_sensors.xml
@@ -0,0 +1,42 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+    <compiler angle="radian" coordinate="local" meshdir="../stls/hand" texturedir="../textures"></compiler>
+    <option timestep="0.002" iterations="20" apirate="200">
+        <flag warmstart="enable"></flag>
+    </option>
+
+    <include file="shared.xml"></include>
+    <include file="shared_touch_sensors_92.xml"></include>
+
+    <asset>
+        <include file="shared_asset.xml"></include>
+
+        <texture name="texture:object" file="block.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+        <texture name="texture:hidden" file="block_hidden.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+
+        <material name="material:object" texture="texture:object" specular="1" shininess="0.3" reflectance="0"></material>
+        <material name="material:hidden" texture="texture:hidden" specular="1" shininess="0.3" reflectance="0"></material>
+        <material name="material:target" texture="texture:object" specular="1" shininess="0.3" reflectance="0" rgba="1 1 1 0.5"></material>
+    </asset>
+
+    <worldbody>
+        <geom name="floor0" pos="1 1 0" size="1 1 1" type="plane" condim="3" material="floor_mat"></geom>
+        <body name="floor0" pos="1 1 0"></body>
+
+        <include file="robot_touch_sensors_92.xml"></include>
+
+        <body name="object" pos="1 0.87 0.2">
+            <geom name="object" type="box" size="0.025 0.025 0.025" material="material:object" condim="4" density="567"></geom>
+            <geom name="object_hidden" type="box" size="0.024 0.024 0.024" material="material:hidden" condim="4" contype="0" conaffinity="0" mass="0"></geom>
+            <site name="object:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <joint name="object:joint" type="free" damping="0.01"></joint>
+        </body>
+        <body name="target" pos="1 0.87 0.2">
+            <geom name="target" type="box" size="0.025 0.025 0.025" material="material:target" condim="4" group="2" contype="0" conaffinity="0"></geom>
+            <site name="target:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <joint name="target:joint" type="free" damping="0.01"></joint>
+        </body>
+
+        <light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 1 4" dir="0 0 -1" name="light0"></light>
+    </worldbody>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_egg.xml b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_egg.xml
new file mode 100644
index 0000000..d60217f
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_egg.xml
@@ -0,0 +1,41 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+    <compiler angle="radian" coordinate="local" meshdir="../stls/hand" texturedir="../textures"></compiler>
+    <option timestep="0.002" iterations="20" apirate="200">
+        <flag warmstart="enable"></flag>
+    </option>
+
+    <include file="shared.xml"></include>
+
+    <asset>
+        <include file="shared_asset.xml"></include>
+
+        <texture name="texture:object" file="block.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+        <texture name="texture:hidden" file="block_hidden.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+
+        <material name="material:object" texture="texture:object" specular="1" shininess="0.3" reflectance="0"></material>
+        <material name="material:hidden" texture="texture:hidden" specular="1" shininess="0.3" reflectance="0"></material>
+        <material name="material:target" texture="texture:object" specular="1" shininess="0.3" reflectance="0" rgba="1 1 1 0.5"></material>
+    </asset>
+
+    <worldbody>
+        <geom name="floor0" pos="1 1 0" size="1 1 1" type="plane" condim="3" material="floor_mat"></geom>
+        <body name="floor0" pos="1 1 0"></body>
+
+        <include file="robot.xml"></include>
+
+        <body name="object" pos="1 0.87 0.2">
+            <geom name="object" type="ellipsoid" size="0.03 0.03 0.04" material="material:object" condim="4"></geom>
+            <geom name="object_hidden" type="ellipsoid" size="0.029 0.029 0.03" material="material:hidden" condim="4" contype="0" conaffinity="0" mass="0"></geom>
+            <site name="object:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <joint name="object:joint" type="free" damping="0.01"></joint>
+        </body>
+        <body name="target" pos="1 0.87 0.2">
+            <geom name="target" type="ellipsoid" size="0.03 0.03 0.04" material="material:target" condim="4" group="2" contype="0" conaffinity="0"></geom>
+            <site name="target:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <joint name="target:joint" type="free" damping="0.01"></joint>
+        </body>
+        
+        <light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 1 4" dir="0 0 -1" name="light0"></light>
+    </worldbody>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_egg_touch_sensors.xml b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_egg_touch_sensors.xml
new file mode 100644
index 0000000..73af83c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_egg_touch_sensors.xml
@@ -0,0 +1,42 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+    <compiler angle="radian" coordinate="local" meshdir="../stls/hand" texturedir="../textures"></compiler>
+    <option timestep="0.002" iterations="20" apirate="200">
+        <flag warmstart="enable"></flag>
+    </option>
+
+    <include file="shared.xml"></include>
+    <include file="shared_touch_sensors_92.xml"></include>
+
+    <asset>
+        <include file="shared_asset.xml"></include>
+
+        <texture name="texture:object" file="block.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+        <texture name="texture:hidden" file="block_hidden.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+
+        <material name="material:object" texture="texture:object" specular="1" shininess="0.3" reflectance="0"></material>
+        <material name="material:hidden" texture="texture:hidden" specular="1" shininess="0.3" reflectance="0"></material>
+        <material name="material:target" texture="texture:object" specular="1" shininess="0.3" reflectance="0" rgba="1 1 1 0.5"></material>
+    </asset>
+
+    <worldbody>
+        <geom name="floor0" pos="1 1 0" size="1 1 1" type="plane" condim="3" material="floor_mat"></geom>
+        <body name="floor0" pos="1 1 0"></body>
+
+        <include file="robot_touch_sensors_92.xml"></include>
+
+        <body name="object" pos="1 0.87 0.2">
+            <geom name="object" type="ellipsoid" size="0.03 0.03 0.04" material="material:object" condim="4"></geom>
+            <geom name="object_hidden" type="ellipsoid" size="0.029 0.029 0.03" material="material:hidden" condim="4" contype="0" conaffinity="0" mass="0"></geom>
+            <site name="object:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <joint name="object:joint" type="free" damping="0.01"></joint>
+        </body>
+        <body name="target" pos="1 0.87 0.2">
+            <geom name="target" type="ellipsoid" size="0.03 0.03 0.04" material="material:target" condim="4" group="2" contype="0" conaffinity="0"></geom>
+            <site name="target:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <joint name="target:joint" type="free" damping="0.01"></joint>
+        </body>
+
+        <light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 1 4" dir="0 0 -1" name="light0"></light>
+    </worldbody>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_pen.xml b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_pen.xml
new file mode 100644
index 0000000..20a6fb5
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_pen.xml
@@ -0,0 +1,40 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+    <compiler angle="radian" coordinate="local" meshdir="../stls/hand" texturedir="../textures"></compiler>
+    <option timestep="0.002" iterations="20" apirate="200">
+        <flag warmstart="enable"></flag>
+    </option>
+
+    <include file="shared.xml"></include>
+
+    <asset>
+        <include file="shared_asset.xml"></include>
+
+        <material name="material:object" specular="0" shininess="0.5" reflectance="0.0" rgba="0.46 0.81 0.88 1.0"></material>
+        <material name="material:target" specular="0" shininess="0.5" reflectance="0.0" rgba="0.46 0.81 0.88 0.5"></material>
+    </asset>
+
+    <worldbody>
+        <geom name="floor0" pos="1 1 -0.2" size="1 1 1" type="plane" condim="3" material="floor_mat"></geom>
+        <body name="floor0" pos="1 1 0"></body>
+
+        <include file="robot.xml"></include>
+
+        <body name="object" pos="1 0.87 0.2" euler="-1 1 0">
+            <geom name="object" type="capsule" size="0.008 0.1" material="material:object" condim="4"></geom>
+            <site name="object:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <site name="object:top" pos="0 0 0.1" rgba="1 0 0 1" size="0.0081"></site>
+            <site name="object:bottom" pos="0 0 -0.1" rgba="0 1 0 1" size="0.0081"></site>
+            <joint name="object:joint" type="free" damping="0.01"></joint>
+        </body>
+        <body name="target" pos="1 0.87 0.2" euler="-1 1 0">
+            <geom name="target" type="capsule" size="0.008 0.1" material="material:target" condim="4" group="2" contype="0" conaffinity="0"></geom>
+            <site name="target:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <site name="target:top" pos="0 0 0.1" rgba="1 0 0 0.5" size="0.0081"></site>
+            <site name="target:bottom" pos="0 0 -0.1" rgba="0 1 0 0.5" size="0.0081"></site>
+            <joint name="target:joint" type="free" damping="0.01"></joint>
+        </body>
+        
+        <light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 1 4" dir="0 0 -1" name="light0"></light>
+    </worldbody>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_pen_touch_sensors.xml b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_pen_touch_sensors.xml
new file mode 100644
index 0000000..758839b
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_pen_touch_sensors.xml
@@ -0,0 +1,41 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+    <compiler angle="radian" coordinate="local" meshdir="../stls/hand" texturedir="../textures"></compiler>
+    <option timestep="0.002" iterations="20" apirate="200">
+        <flag warmstart="enable"></flag>
+    </option>
+
+    <include file="shared.xml"></include>
+    <include file="shared_touch_sensors_92.xml"></include>
+
+    <asset>
+        <include file="shared_asset.xml"></include>
+
+        <material name="material:object" specular="0" shininess="0.5" reflectance="0.0" rgba="0.46 0.81 0.88 1.0"></material>
+        <material name="material:target" specular="0" shininess="0.5" reflectance="0.0" rgba="0.46 0.81 0.88 0.5"></material>
+    </asset>
+
+    <worldbody>
+        <geom name="floor0" pos="1 1 -0.2" size="1 1 1" type="plane" condim="3" material="floor_mat"></geom>
+        <body name="floor0" pos="1 1 0"></body>
+
+        <include file="robot_touch_sensors_92.xml"></include>
+
+        <body name="object" pos="1 0.87 0.2" euler="-1 1 0">
+            <geom name="object" type="capsule" size="0.008 0.1" material="material:object" condim="4"></geom>
+            <site name="object:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <site name="object:top" pos="0 0 0.1" rgba="1 0 0 1" size="0.0081"></site>
+            <site name="object:bottom" pos="0 0 -0.1" rgba="0 1 0 1" size="0.0081"></site>
+            <joint name="object:joint" type="free" damping="0.01"></joint>
+        </body>
+        <body name="target" pos="1 0.87 0.2" euler="-1 1 0">
+            <geom name="target" type="capsule" size="0.008 0.1" material="material:target" condim="4" group="2" contype="0" conaffinity="0"></geom>
+            <site name="target:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <site name="target:top" pos="0 0 0.1" rgba="1 0 0 0.5" size="0.0081"></site>
+            <site name="target:bottom" pos="0 0 -0.1" rgba="0 1 0 0.5" size="0.0081"></site>
+            <joint name="target:joint" type="free" damping="0.01"></joint>
+        </body>
+
+        <light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 1 4" dir="0 0 -1" name="light0"></light>
+    </worldbody>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/reach.xml b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/reach.xml
new file mode 100644
index 0000000..71f6dfe
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/reach.xml
@@ -0,0 +1,34 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+    <compiler angle="radian" coordinate="local" meshdir="../stls/hand" texturedir="../textures"></compiler>
+    <option timestep="0.002" iterations="20" apirate="200">
+        <flag warmstart="enable"></flag>
+    </option>
+
+    <include file="shared.xml"></include>
+
+    <asset>
+        <include file="shared_asset.xml"></include>
+    </asset>
+
+    <worldbody>
+        <geom name="floor0" pos="1 1 0" size="1 1 1" type="plane" condim="3" material="floor_mat"></geom>
+        <body name="floor0" pos="1 1 0">
+            <site name="target0" pos="0 0 0" size="0.005" rgba="1 0 0 1" type="sphere"></site>
+            <site name="target1" pos="0 0 0" size="0.005" rgba="0 1 0 1" type="sphere"></site>
+            <site name="target2" pos="0 0 0" size="0.005" rgba="0 0 1 1" type="sphere"></site>
+            <site name="target3" pos="0 0 0" size="0.005" rgba="1 1 0 1" type="sphere"></site>
+            <site name="target4" pos="0 0 0" size="0.005" rgba="1 0 1 1" type="sphere"></site>
+
+            <site name="finger0" pos="0 0 0" size="0.01" rgba="1 0 0 0.2" type="sphere"></site>
+            <site name="finger1" pos="0 0 0" size="0.01" rgba="0 1 0 0.2" type="sphere"></site>
+            <site name="finger2" pos="0 0 0" size="0.01" rgba="0 0 1 0.2" type="sphere"></site>
+            <site name="finger3" pos="0 0 0" size="0.01" rgba="1 1 0 0.2" type="sphere"></site>
+            <site name="finger4" pos="0 0 0" size="0.01" rgba="1 0 1 0.2" type="sphere"></site>
+        </body>
+
+        <include file="robot.xml"></include>
+        
+        <light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 0 4" dir="0 0 -1" name="light0"></light>
+    </worldbody>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/robot.xml b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/robot.xml
new file mode 100644
index 0000000..dbb9e43
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/robot.xml
@@ -0,0 +1,160 @@
+<!-- See LICENSE.md for legal notices. LICENSE.md must be kept together with this file. -->
+<mujoco>
+    <body name="robot0:hand mount" pos="1 1.25 0.15" euler="1.5708 0 3.14159">
+        <inertial mass="0.1" pos="0 0 0" diaginertia="0.001 0.001 0.001"></inertial>
+        <body childclass="robot0:asset_class" name="robot0:forearm" pos="0 0.01 0" euler="0 0 0">
+            <inertial pos="0.001 -0.002 0.29" quat="0.982 -0.016 0 -0.188" mass="4" diaginertia="0.01 0.01 0.0075"></inertial>
+            <geom class="robot0:D_Vizual" pos="0 0.01 0.04" name="robot0:V_forearm" mesh="robot0:forearm" euler="0 0 1.57"></geom>
+            <geom class="robot0:DC_Hand" name="robot0:C_forearm" type="mesh" mesh="robot0:forearm_cvx" pos="0 0.01 0.04" euler="0 0 1.57" rgba="0.4 0.5 0.6 0.7"></geom>
+            <body name="robot0:wrist" pos="0 0 0.256">
+                <inertial pos="0.003 0 0.016" quat="0.504 0.496 0.495 0.504" mass="0.3" diaginertia="0.001 0.001 0.001"></inertial>
+                <joint name="robot0:WRJ1" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.489 0.14" damping="0.5" armature="0.005" user="1123"></joint>
+                <geom class="robot0:D_Vizual" name="robot0:V_wrist" mesh="robot0:wrist"></geom>
+                <geom class="robot0:DC_Hand" name="robot0:C_wrist" type="capsule" pos="0 0 0" quat="0.707 0.707 0 0" size="0.015 0.01" rgba="0.4 0.5 0.6 0.1"></geom>
+                <body name="robot0:palm" pos="0 0 0.034">
+                    <inertial pos="0.006 0 0.036" quat="0.716 0.044 0.075 0.693" mass="0.3" diaginertia="0.001 0.001 0.001"></inertial>
+                    <joint name="robot0:WRJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="-0.698 0.489" damping="0.5" armature="0.005" user="1122"></joint>
+                    <geom class="robot0:D_Vizual" name="robot0:V_palm" mesh="robot0:palm"></geom>
+                    <geom class="robot0:DC_Hand" name="robot0:C_palm0" type="box" pos="0.011 0 0.038" size="0.032 0.0111 0.049" rgba="0.4 0.5 0.6 0.1"></geom>
+                    <geom class="robot0:DC_Hand" name="robot0:C_palm1" type="box" pos="-0.032 0 0.014" size="0.011 0.0111 0.025" rgba="0.4 0.5 0.6 0.1"></geom>
+                    <body name="robot0:ffknuckle" pos="0.033 0 0.095">
+                        <inertial pos="0 0 0" quat="0.52 0.854 0.006 -0.003" mass="0.008" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:FFJ3" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.349 0.349" user="1103"></joint>
+                        <geom class="robot0:D_Vizual" name="robot0:V_ffknuckle" mesh="robot0:knuckle"></geom>
+                        <body name="robot0:ffproximal" pos="0 0 0">
+                            <inertial pos="0 0 0.023" quat="0.707 -0.004 0.004 0.707" mass="0.014" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:FFJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1102"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_ffproximal" mesh="robot0:F3"></geom>
+                            <geom class="robot0:DC_Hand" name="robot0:C_ffproximal" type="capsule" pos="0 0 0.0225" size="0.01 0.0225"></geom>
+                            <body name="robot0:ffmiddle" pos="0 0 0.045">
+                                <inertial pos="0 0 0.011" quat="0.707 0 0 0.707" mass="0.012" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:FFJ1" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1101"></joint>
+                                <geom class="robot0:D_Vizual" name="robot0:V_ffmiddle" mesh="robot0:F2"></geom>
+                                <geom class="robot0:DC_Hand" name="robot0:C_ffmiddle" type="capsule" pos="0 0 0.0125" size="0.00805 0.0125"></geom>
+                                <body name="robot0:ffdistal" pos="0 0 0.025">
+                                    <inertial pos="0 0 0.015" quat="0.707 -0.003 0.003 0.707" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:FFJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1100"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_ffdistal" pos="0 0 0.001" mesh="robot0:F1"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_ffdistal" type="capsule" pos="0 0 0.012" size="0.00705 0.012" condim="4"></geom>
+                                    <site name="robot0:S_fftip" pos="0 0 0.026" group="3"></site>
+                                    <site class="robot0:D_Touch" name="robot0:Tch_fftip"></site>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                    <body name="robot0:mfknuckle" pos="0.011 0 0.099">
+                        <inertial pos="0 0 0" quat="0.52 0.854 0.006 -0.003" mass="0.008" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:MFJ3" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.349 0.349" user="1107"></joint>
+                        <geom class="robot0:D_Vizual" name="robot0:V_mfknuckle" mesh="robot0:knuckle"></geom>
+                        <body name="robot0:mfproximal" pos="0 0 0">
+                            <inertial pos="0 0 0.023" quat="0.707 -0.004 0.004 0.707" mass="0.014" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:MFJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1106"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_mfproximal" mesh="robot0:F3"></geom>
+                            <geom class="robot0:DC_Hand" name="robot0:C_mfproximal" type="capsule" pos="0 0 0.0225" size="0.01 0.0225"></geom>
+                            <body name="robot0:mfmiddle" pos="0 0 0.045">
+                                <inertial pos="0 0 0.012" quat="0.707 0 0 0.707" mass="0.012" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:MFJ1" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1105"></joint>
+                                <geom class="robot0:D_Vizual" name="robot0:V_mfmiddle" mesh="robot0:F2"></geom>
+                                <geom class="robot0:DC_Hand" name="robot0:C_mfmiddle" type="capsule" pos="0 0 0.0125" size="0.00805 0.0125"></geom>
+                                <body name="robot0:mfdistal" pos="0 0 0.025">
+                                    <inertial pos="0 0 0.015" quat="0.707 -0.003 0.003 0.707" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:MFJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1104"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_mfdistal" mesh="robot0:F1"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_mfdistal" type="capsule" pos="0 0 0.012" size="0.00705 0.012" condim="4"></geom>
+                                    <site name="robot0:S_mftip" pos="0 0 0.026" group="3"></site>
+                                    <site class="robot0:D_Touch" name="robot0:Tch_mftip"></site>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                    <body name="robot0:rfknuckle" pos="-0.011 0 0.095">
+                        <inertial pos="0 0 0" quat="0.52 0.854 0.006 -0.003" mass="0.008" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:RFJ3" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.349 0.349" user="1111"></joint>
+                        <geom class="robot0:D_Vizual" name="robot0:V_rfknuckle" mesh="robot0:knuckle"></geom>
+                        <body name="robot0:rfproximal" pos="0 0 0">
+                            <inertial pos="0 0 0.023" quat="0.707 -0.004 0.004 0.707" mass="0.014" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:RFJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1110"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_rfproximal" mesh="robot0:F3"></geom>
+                            <geom class="robot0:DC_Hand" name="robot0:C_rfproximal" type="capsule" pos="0 0 0.0225" size="0.01 0.0225"></geom>
+                            <body name="robot0:rfmiddle" pos="0 0 0.045">
+                                <inertial pos="0 0 0.012" quat="0.707 0 0 0.707" mass="0.012" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:RFJ1" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1109"></joint>
+                                <geom class="robot0:D_Vizual" name="robot0:V_rfmiddle" mesh="robot0:F2"></geom>
+                                <geom class="robot0:DC_Hand" name="robot0:C_rfmiddle" type="capsule" pos="0 0 0.0125" size="0.00805 0.0125"></geom>
+                                <body name="robot0:rfdistal" pos="0 0 0.025">
+                                    <inertial pos="0 0 0.015" quat="0.707 -0.003 0.003 0.707" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:RFJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1108"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_rfdistal" mesh="robot0:F1" pos="0 0 0.001"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_rfdistal" type="capsule" pos="0 0 0.012" size="0.00705 0.012" condim="4"></geom>
+                                    <site name="robot0:S_rftip" pos="0 0 0.026" group="3"></site>
+                                    <site class="robot0:D_Touch" name="robot0:Tch_rftip"></site>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                    <body name="robot0:lfmetacarpal" pos="-0.017 0 0.044">
+                        <inertial pos="-0.014 0.001 0.014" quat="0.709 -0.092 -0.063 0.696" mass="0.075" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:LFJ4" type="hinge" pos="0 0 0" axis="0.571 0 0.821" range="0 0.785" user="1116"></joint>
+                        <geom class="robot0:D_Vizual" name="robot0:V_lfmetacarpal" pos="-0.016 0 -0.023" mesh="robot0:lfmetacarpal"></geom>
+                        <geom class="robot0:DC_Hand" name="robot0:C_lfmetacarpal" type="box" pos="-0.0165 0 0.01" size="0.0095 0.0111 0.025" rgba="0.4 0.5 0.6 0.2"></geom>
+                        <body name="robot0:lfknuckle" pos="-0.017 0 0.044">
+                            <inertial pos="0 0 0" quat="0.52 0.854 0.006 -0.003" mass="0.008" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:LFJ3" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.349 0.349" user="1115"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_lfknuckle" mesh="robot0:knuckle"></geom>
+                            <body name="robot0:lfproximal" pos="0 0 0">
+                                <inertial pos="0 0 0.023" quat="0.707 -0.004 0.004 0.707" mass="0.014" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:LFJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1114"></joint>
+                                <geom class="robot0:D_Vizual" name="robot0:V_lfproximal" mesh="robot0:F3"></geom>
+                                <geom class="robot0:DC_Hand" name="robot0:C_lfproximal" type="capsule" pos="0 0 0.0225" size="0.01 0.0225"></geom>
+                                <body name="robot0:lfmiddle" pos="0 0 0.045">
+                                    <inertial pos="0 0 0.012" quat="0.707 0 0 0.707" mass="0.012" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:LFJ1" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1113"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_lfmiddle" mesh="robot0:F2"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_lfmiddle" type="capsule" pos="0 0 0.0125" size="0.00805 0.0125"></geom>
+                                    <body name="robot0:lfdistal" pos="0 0 0.025">
+                                        <inertial pos="0 0 0.015" quat="0.707 -0.003 0.003 0.707" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                        <joint name="robot0:LFJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1112"></joint>
+                                        <geom class="robot0:D_Vizual" name="robot0:V_lfdistal" mesh="robot0:F1" pos="0 0 0.001"></geom>
+                                        <geom class="robot0:DC_Hand" name="robot0:C_lfdistal" type="capsule" pos="0 0 0.012" size="0.00705 0.012" condim="4"></geom>
+                                        <site name="robot0:S_lftip" pos="0 0 0.026" group="3"></site>
+                                        <site class="robot0:D_Touch" name="robot0:Tch_lftip"></site>
+                                    </body>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                    <body name="robot0:thbase" pos="0.034 -0.009 0.029" axisangle="0 1 0 0.785">
+                        <inertial pos="0 0 0" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:THJ4" type="hinge" pos="0 0 0" axis="0 0 -1" range="-1.047 1.047" user="1121"></joint>
+                        <geom name="robot0:V_thbase" type="box" group="1" pos="0 0 0" size="0.001 0.001 0.001"></geom>
+                        <body name="robot0:thproximal" pos="0 0 0">
+                            <inertial pos="0 0 0.017" quat="0.982 0 0.001 0.191" mass="0.016" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:THJ3" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.222" user="1120"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_thproximal" mesh="robot0:TH3_z"></geom>
+                            <geom class="robot0:DC_Hand" name="robot0:C_thproximal" type="capsule" pos="0 0 0.019" size="0.013 0.019" rgba="0.4 0.5 0.6 0.1"></geom>
+                            <body name="robot0:thhub" pos="0 0 0.038">
+                                <inertial pos="0 0 0" mass="0.002" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:THJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="-0.209 0.209" user="1119"></joint>
+                                <geom name="robot0:V_thhub" type="box" group="1" pos="0 0 0" size="0.001 0.001 0.001"></geom>
+                                <body name="robot0:thmiddle" pos="0 0 0">
+                                    <inertial pos="0 0 0.016" quat="1 -0.001 -0.007 0.003" mass="0.016" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:THJ1" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.524 0.524" user="1118"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_thmiddle" mesh="robot0:TH2_z"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_thmiddle" type="capsule" pos="0 0 0.016" size="0.011 0.016"></geom>
+                                    <body name="robot0:thdistal" pos="0 0 0.032">
+                                        <inertial pos="0 0 0.016" quat="0.999 -0.005 -0.047 0.005" mass="0.016" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                        <joint name="robot0:THJ0" type="hinge" pos="0 0 0" axis="0 1 0" range="-1.571 0" user="1117"></joint>
+                                        <geom class="robot0:D_Vizual" name="robot0:V_thdistal" mesh="robot0:TH1_z"></geom>
+                                        <geom class="robot0:DC_Hand" name="robot0:C_thdistal" type="capsule" pos="0 0 0.013" size="0.00918 0.013" condim="4"></geom>
+                                        <site name="robot0:S_thtip" pos="0 0 0.0275" group="3"></site>
+                                        <site class="robot0:D_Touch" name="robot0:Tch_thtip" size="0.005 0.011 0.016" pos="-0.005 0 0.02"></site>
+                                    </body>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                </body>
+            </body>
+        </body>
+    </body>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/robot_touch_sensors_92.xml b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/robot_touch_sensors_92.xml
new file mode 100644
index 0000000..fa6d41c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/robot_touch_sensors_92.xml
@@ -0,0 +1,252 @@
+<!-- See LICENSE.md for legal notices. LICENSE.md must be kept together with this file. -->
+<mujoco>
+    <body name="robot0:hand mount" pos="1 1.25 0.15" euler="1.5708 0 3.14159">
+        <inertial mass="0.1" pos="0 0 0" diaginertia="0.001 0.001 0.001"></inertial>
+        <body childclass="robot0:asset_class" name="robot0:forearm" pos="0 0.01 0" euler="0 0 0">
+            <inertial pos="0.001 -0.002 0.29" quat="0.982 -0.016 0 -0.188" mass="4" diaginertia="0.01 0.01 0.0075"></inertial>
+            <geom class="robot0:D_Vizual" pos="0 0.01 0.04" name="robot0:V_forearm" mesh="robot0:forearm" euler="0 0 1.57"></geom>
+            <geom class="robot0:DC_Hand" name="robot0:C_forearm" type="mesh" mesh="robot0:forearm_cvx" pos="0 0.01 0.04" euler="0 0 1.57" rgba="0.4 0.5 0.6 0.7"></geom>
+            <body name="robot0:wrist" pos="0 0 0.256">
+                <inertial pos="0.003 0 0.016" quat="0.504 0.496 0.495 0.504" mass="0.3" diaginertia="0.001 0.001 0.001"></inertial>
+                <joint name="robot0:WRJ1" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.489 0.14" damping="0.5" armature="0.005" user="1123"></joint>
+                <geom class="robot0:D_Vizual" name="robot0:V_wrist" mesh="robot0:wrist"></geom>
+                <geom class="robot0:DC_Hand" name="robot0:C_wrist" type="capsule" pos="0 0 0" quat="0.707 0.707 0 0" size="0.015 0.01" rgba="0.4 0.5 0.6 0.1"></geom>
+                <body name="robot0:palm" pos="0 0 0.034">
+                    <inertial pos="0.006 0 0.036" quat="0.716 0.044 0.075 0.693" mass="0.3" diaginertia="0.001 0.001 0.001"></inertial>
+                    <joint name="robot0:WRJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="-0.698 0.489" damping="0.5" armature="0.005" user="1122"></joint>
+                    <geom class="robot0:D_Vizual" name="robot0:V_palm" mesh="robot0:palm"></geom>
+                    <geom class="robot0:DC_Hand" name="robot0:C_palm0" type="box" pos="0.011 0 0.038" size="0.032 0.0111 0.049" rgba="0.4 0.5 0.6 0.1"></geom>
+                    <site name="robot0:T_palm_b0" type="box" pos="0.011 -0.005 0.006"  size="0.033 0.007 0.019" rgba="0 0 0 0.33"/>
+                    <site name="robot0:T_palm_bl" type="box" pos="-0.011 -0.005 0.041" size="0.011 0.007 0.016" rgba="1 0 0 0.33"/>
+                    <site name="robot0:T_palm_bm" type="box" pos="0.011 -0.005 0.041"  size="0.011 0.007 0.016" rgba="0 0 0 0.33"/>
+                    <site name="robot0:T_palm_br" type="box" pos="0.033 -0.005 0.041"  size="0.011 0.007 0.016" rgba="1 0 0 0.33"/>
+                    <site name="robot0:T_palm_fl" type="box" pos="-0.011 -0.005 0.073" size="0.011 0.007 0.016" rgba="0 0 0 0.33"/>
+                    <site name="robot0:T_palm_fm" type="box" pos="0.011 -0.005 0.073"  size="0.011 0.007 0.016" rgba="1 0 0 0.33"/>
+                    <site name="robot0:T_palm_fr" type="box" pos="0.033 -0.005 0.073"  size="0.011 0.007 0.016" rgba="0 0 0 0.33"/>
+                    <geom class="robot0:DC_Hand" name="robot0:C_palm1" type="box" pos="-0.032 0 0.014" size="0.011 0.0111 0.025" rgba="0.4 0.5 0.6 0.1"></geom>
+                    <site name="robot0:T_palm_b1" type="box" pos="-0.0325 -0.005 0.014" size="0.012 0.007 0.027" rgba="0 0 0 0.33"/>
+                    <body name="robot0:ffknuckle" pos="0.033 0 0.095">
+                        <inertial pos="0 0 0" quat="0.52 0.854 0.006 -0.003" mass="0.008" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:FFJ3" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.349 0.349" user="1103"></joint>
+                        <geom class="robot0:D_Vizual" name="robot0:V_ffknuckle" mesh="robot0:knuckle"></geom>
+                        <body name="robot0:ffproximal" pos="0 0 0">
+                            <inertial pos="0 0 0.023" quat="0.707 -0.004 0.004 0.707" mass="0.014" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:FFJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1102"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_ffproximal" mesh="robot0:F3"></geom>
+                            <geom class="robot0:DC_Hand" name="robot0:C_ffproximal" type="capsule" pos="0 0 0.0225" size="0.01 0.0225"></geom>
+                            <site name="robot0:T_ffproximal_front_left_bottom"  type="box"    pos="-0.005 -0.005 0.00625" size="0.005 0.005 0.01625" rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_ffproximal_front_right_bottom" type="box"    pos="0.005 -0.005 0.00625"  size="0.005 0.005 0.01625" rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_ffproximal_front_left_top"     type="box"    pos="-0.005 -0.005 0.03875" size="0.005 0.005 0.01625" rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_ffproximal_front_right_top"    type="box"    pos="0.005 -0.005 0.03875"  size="0.005 0.005 0.01625" rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_ffproximal_back_left"          type="box"    pos="-0.005 0.005 0.0225"   size="0.005 0.005 0.0325"  rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_ffproximal_back_right"         type="box"    pos="0.005 0.005 0.0225"    size="0.005 0.005 0.0325"  rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_ffproximal_tip"                type="sphere" pos="0 0 0.045"             size="0.011"               rgba="1 1 0 0.33"/>
+                            <body name="robot0:ffmiddle" pos="0 0 0.045">
+                                <inertial pos="0 0 0.011" quat="0.707 0 0 0.707" mass="0.012" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:FFJ1" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1101"></joint>
+                                <geom class="robot0:D_Vizual" name="robot0:V_ffmiddle" mesh="robot0:F2"></geom>
+                                <geom class="robot0:DC_Hand" name="robot0:C_ffmiddle" type="capsule" pos="0 0 0.0125" size="0.00805 0.0125"></geom>
+                                <site name="robot0:T_ffmiddle_front_left"  type="box"     pos="-0.00451 -0.00451 0.0125" size="0.00451 0.00451 0.02055" rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_ffmiddle_front_right" type="box"     pos="0.00451 -0.00451 0.0125"  size="0.00451 0.00451 0.02055" rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_ffmiddle_back_left"   type="box"     pos="-0.00451 0.00451 0.0125"  size="0.00451 0.00451 0.02055" rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_ffmiddle_back_right"  type="box"     pos="0.00451 0.00451 0.0125"   size="0.00451 0.00451 0.02055" rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_ffmiddle_tip"         type="sphere"  pos="0 0 0.025"                size="0.009"                   rgba="1 1 0 0.33"/>
+                                <body name="robot0:ffdistal" pos="0 0 0.025">
+                                    <inertial pos="0 0 0.015" quat="0.707 -0.003 0.003 0.707" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:FFJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1100"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_ffdistal" pos="0 0 0.001" mesh="robot0:F1"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_ffdistal" type="capsule" pos="0 0 0.012" size="0.00705 0.012" condim="4"></geom>
+                                    <site name="robot0:S_fftip" pos="0 0 0.026" group="3"></site>
+                                    <site class="robot0:D_Touch" name="robot0:Tch_fftip"></site>
+                                    <site name="robot0:T_fftip_front_left"  type="box"      pos="-0.00451 -0.00451 0.012" size="0.00451 0.00451 0.01905" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_fftip_front_right" type="box"      pos="0.00451 -0.00451 0.012"  size="0.00451 0.00451 0.01905" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_fftip_back_left"   type="box"      pos="-0.00451 0.00451 0.012"  size="0.00451 0.00451 0.01905" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_fftip_back_right"  type="box"      pos="0.00451 0.00451 0.012"   size="0.00451 0.00451 0.01905" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_fftip_tip"         type="sphere"   pos="0 0 0.024"               size="0.008"                   rgba="1 1 0 0.33"/>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                    <body name="robot0:mfknuckle" pos="0.011 0 0.099">
+                        <inertial pos="0 0 0" quat="0.52 0.854 0.006 -0.003" mass="0.008" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:MFJ3" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.349 0.349" user="1107"></joint>
+                        <geom class="robot0:D_Vizual" name="robot0:V_mfknuckle" mesh="robot0:knuckle"></geom>
+                        <body name="robot0:mfproximal" pos="0 0 0">
+                            <inertial pos="0 0 0.023" quat="0.707 -0.004 0.004 0.707" mass="0.014" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:MFJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1106"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_mfproximal" mesh="robot0:F3"></geom>
+                            <geom class="robot0:DC_Hand" name="robot0:C_mfproximal" type="capsule" pos="0 0 0.0225" size="0.01 0.0225"></geom>
+                            <site name="robot0:T_mfproximal_front_left_bottom"  type="box"    pos="-0.005 -0.005 0.00625" size="0.005 0.005 0.01625" rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_mfproximal_front_right_bottom" type="box"    pos="0.005 -0.005 0.00625"  size="0.005 0.005 0.01625" rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_mfproximal_front_left_top"     type="box"    pos="-0.005 -0.005 0.03875" size="0.005 0.005 0.01625" rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_mfproximal_front_right_top"    type="box"    pos="0.005 -0.005 0.03875"  size="0.005 0.005 0.01625" rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_mfproximal_back_left"          type="box"    pos="-0.005 0.005 0.0225"   size="0.005 0.005 0.0325"  rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_mfproximal_back_right"         type="box"    pos="0.005 0.005 0.0225"    size="0.005 0.005 0.0325"  rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_mfproximal_tip"                type="sphere" pos="0 0 0.045"             size="0.011"               rgba="1 1 0 0.33"/>
+                            <body name="robot0:mfmiddle" pos="0 0 0.045">
+                                <inertial pos="0 0 0.012" quat="0.707 0 0 0.707" mass="0.012" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:MFJ1" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1105"></joint>
+                                <geom class="robot0:D_Vizual" name="robot0:V_mfmiddle" mesh="robot0:F2"></geom>
+                                <geom class="robot0:DC_Hand" name="robot0:C_mfmiddle" type="capsule" pos="0 0 0.0125" size="0.00805 0.0125"></geom>
+                                <site name="robot0:T_mfmiddle_front_left"  type="box"    pos="-0.00451 -0.00451 0.0125" size="0.00451 0.00451 0.02055" rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_mfmiddle_front_right" type="box"    pos="0.00451 -0.00451 0.0125"  size="0.00451 0.00451 0.02055" rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_mfmiddle_back_left"   type="box"    pos="-0.00451 0.00451 0.0125"  size="0.00451 0.00451 0.02055" rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_mfmiddle_back_right"  type="box"    pos="0.00451 0.00451 0.0125"   size="0.00451 0.00451 0.02055" rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_mfmiddle_tip"         type="sphere" pos="0 0 0.025"                size="0.009"                   rgba="1 1 0 0.33"/>
+                                <body name="robot0:mfdistal" pos="0 0 0.025">
+                                    <inertial pos="0 0 0.015" quat="0.707 -0.003 0.003 0.707" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:MFJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1104"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_mfdistal" mesh="robot0:F1"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_mfdistal" type="capsule" pos="0 0 0.012" size="0.00705 0.012" condim="4"></geom>
+                                    <site name="robot0:S_mftip" pos="0 0 0.026" group="3"></site>
+                                    <site class="robot0:D_Touch" name="robot0:Tch_mftip"></site>
+                                    <site name="robot0:T_mftip_front_left"  type="box"    pos="-0.00451 -0.00451 0.012" size="0.00451 0.00451 0.01905" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_mftip_front_right" type="box"    pos="0.00451 -0.00451 0.012"  size="0.00451 0.00451 0.01905" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_mftip_back_left"   type="box"    pos="-0.00451 0.00451 0.012"  size="0.00451 0.00451 0.01905" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_mftip_back_right"  type="box"    pos="0.00451 0.00451 0.012"   size="0.00451 0.00451 0.01905" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_mftip_tip"         type="sphere" pos="0 0 0.024"               size="0.008"                   rgba="1 1 0 0.33"/>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                    <body name="robot0:rfknuckle" pos="-0.011 0 0.095">
+                        <inertial pos="0 0 0" quat="0.52 0.854 0.006 -0.003" mass="0.008" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:RFJ3" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.349 0.349" user="1111"></joint>
+                        <geom class="robot0:D_Vizual" name="robot0:V_rfknuckle" mesh="robot0:knuckle"></geom>
+                        <body name="robot0:rfproximal" pos="0 0 0">
+                            <inertial pos="0 0 0.023" quat="0.707 -0.004 0.004 0.707" mass="0.014" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:RFJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1110"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_rfproximal" mesh="robot0:F3"></geom>
+                            <geom class="robot0:DC_Hand" name="robot0:C_rfproximal" type="capsule" pos="0 0 0.0225" size="0.01 0.0225"></geom>
+                            <site name="robot0:T_rfproximal_front_left_bottom"  type="box"    pos="-0.005 -0.005 0.00625" size="0.005 0.005 0.01625" rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_rfproximal_front_right_bottom" type="box"    pos="0.005 -0.005 0.00625"  size="0.005 0.005 0.01625" rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_rfproximal_front_left_top"     type="box"    pos="-0.005 -0.005 0.03875" size="0.005 0.005 0.01625" rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_rfproximal_front_right_top"    type="box"    pos="0.005 -0.005 0.03875"  size="0.005 0.005 0.01625" rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_rfproximal_back_left"          type="box"    pos="-0.005 0.005 0.0225"   size="0.005 0.005 0.0325"  rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_rfproximal_back_right"         type="box"    pos="0.005 0.005 0.0225"    size="0.005 0.005 0.0325"  rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_rfproximal_tip"                type="sphere" pos="0 0 0.045"             size="0.011"               rgba="1 1 0 0.33"/>
+                            <body name="robot0:rfmiddle" pos="0 0 0.045">
+                                <inertial pos="0 0 0.012" quat="0.707 0 0 0.707" mass="0.012" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:RFJ1" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1109"></joint>
+                                <geom class="robot0:D_Vizual" name="robot0:V_rfmiddle" mesh="robot0:F2"></geom>
+                                <geom class="robot0:DC_Hand" name="robot0:C_rfmiddle" type="capsule" pos="0 0 0.0125" size="0.00805 0.0125"></geom>
+                                <site name="robot0:T_rfmiddle_front_left"  type="box"    pos="-0.00451 -0.00451 0.0125" size="0.00451 0.00451 0.02055" rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_rfmiddle_front_right" type="box"    pos="0.00451 -0.00451 0.0125"  size="0.00451 0.00451 0.02055" rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_rfmiddle_back_left"   type="box"    pos="-0.00451 0.00451 0.0125"  size="0.00451 0.00451 0.02055" rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_rfmiddle_back_right"  type="box"    pos="0.00451 0.00451 0.0125"   size="0.00451 0.00451 0.02055" rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_rfmiddle_tip"         type="sphere" pos="0 0 0.025"                size="0.009"                   rgba="1 1 0 0.33"/>
+                                <body name="robot0:rfdistal" pos="0 0 0.025">
+                                    <inertial pos="0 0 0.015" quat="0.707 -0.003 0.003 0.707" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:RFJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1108"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_rfdistal" mesh="robot0:F1" pos="0 0 0.001"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_rfdistal" type="capsule" pos="0 0 0.012" size="0.00705 0.012" condim="4"></geom>
+                                    <site name="robot0:S_rftip" pos="0 0 0.026" group="3"></site>
+                                    <site class="robot0:D_Touch" name="robot0:Tch_rftip"></site>
+                                    <site name="robot0:T_rftip_front_left"  type="box"    pos="-0.00451 -0.00451 0.012" size="0.00451 0.00451 0.01905" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_rftip_front_right" type="box"    pos="0.00451 -0.00451 0.012"  size="0.00451 0.00451 0.01905" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_rftip_back_left"   type="box"    pos="-0.00451 0.00451 0.012"  size="0.00451 0.00451 0.01905" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_rftip_back_right"  type="box"    pos="0.00451 0.00451 0.012"   size="0.00451 0.00451 0.01905" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_rftip_tip"         type="sphere" pos="0 0 0.024"               size="0.008"                   rgba="1 1 0 0.33"/>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                    <body name="robot0:lfmetacarpal" pos="-0.017 0 0.044">
+                        <inertial pos="-0.014 0.001 0.014" quat="0.709 -0.092 -0.063 0.696" mass="0.075" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:LFJ4" type="hinge" pos="0 0 0" axis="0.571 0 0.821" range="0 0.785" user="1116"></joint>
+                        <geom class="robot0:D_Vizual" name="robot0:V_lfmetacarpal" pos="-0.016 0 -0.023" mesh="robot0:lfmetacarpal"></geom>
+                        <geom class="robot0:DC_Hand" name="robot0:C_lfmetacarpal" type="box" pos="-0.0165 0 0.01" size="0.0095 0.0111 0.025" rgba="0.4 0.5 0.6 0.2"></geom>
+                        <site name="robot0:T_lfmetacarpal_front" type="box" pos="-0.0165 -0.005 0.00975" size="0.01 0.007 0.026"  rgba="1 0 0 0.33"/>
+                        <body name="robot0:lfknuckle" pos="-0.017 0 0.044">
+                            <inertial pos="0 0 0" quat="0.52 0.854 0.006 -0.003" mass="0.008" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:LFJ3" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.349 0.349" user="1115"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_lfknuckle" mesh="robot0:knuckle"></geom>
+                            <body name="robot0:lfproximal" pos="0 0 0">
+                                <inertial pos="0 0 0.023" quat="0.707 -0.004 0.004 0.707" mass="0.014" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:LFJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1114"></joint>
+                                <geom class="robot0:D_Vizual" name="robot0:V_lfproximal" mesh="robot0:F3"></geom>
+                                <geom class="robot0:DC_Hand" name="robot0:C_lfproximal" type="capsule" pos="0 0 0.0225" size="0.01 0.0225"></geom>
+                                <site name="robot0:T_lfproximal_front_left_bottom"  type="box"    pos="-0.005 -0.005 0.00625" size="0.005 0.005 0.01625" rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_lfproximal_front_right_bottom" type="box"    pos="0.005 -0.005 0.00625"  size="0.005 0.005 0.01625" rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_lfproximal_front_left_top"     type="box"    pos="-0.005 -0.005 0.03875" size="0.005 0.005 0.01625" rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_lfproximal_front_right_top"    type="box"    pos="0.005 -0.005 0.03875"  size="0.005 0.005 0.01625" rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_lfproximal_back_left"          type="box"    pos="-0.005 0.005 0.0225"   size="0.005 0.005 0.0325"  rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_lfproximal_back_right"         type="box"    pos="0.005 0.005 0.0225"    size="0.005 0.005 0.0325"  rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_lfproximal_tip"                type="sphere" pos="0 0 0.045"             size="0.011"               rgba="1 1 0 0.33"/>
+                                <body name="robot0:lfmiddle" pos="0 0 0.045">
+                                    <inertial pos="0 0 0.012" quat="0.707 0 0 0.707" mass="0.012" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:LFJ1" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1113"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_lfmiddle" mesh="robot0:F2"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_lfmiddle" type="capsule" pos="0 0 0.0125" size="0.00805 0.0125"></geom>
+                                    <site name="robot0:T_lfmiddle_front_left"  type="box"    pos="-0.00451 -0.00451 0.0125" size="0.00451 0.00451 0.02055" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_lfmiddle_front_right" type="box"    pos="0.00451 -0.00451 0.0125"  size="0.00451 0.00451 0.02055" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_lfmiddle_back_left"   type="box"    pos="-0.00451 0.00451 0.0125"  size="0.00451 0.00451 0.02055" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_lfmiddle_back_right"  type="box"    pos="0.00451 0.00451 0.0125"   size="0.00451 0.00451 0.02055" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_lfmiddle_tip"         type="sphere" pos="0 0 0.025"                size="0.009"                   rgba="1 1 0 0.33"/>
+                                    <body name="robot0:lfdistal" pos="0 0 0.025">
+                                        <inertial pos="0 0 0.015" quat="0.707 -0.003 0.003 0.707" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                        <joint name="robot0:LFJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1112"></joint>
+                                        <geom class="robot0:D_Vizual" name="robot0:V_lfdistal" mesh="robot0:F1" pos="0 0 0.001"></geom>
+                                        <geom class="robot0:DC_Hand" name="robot0:C_lfdistal" type="capsule" pos="0 0 0.012" size="0.00705 0.012" condim="4"></geom>
+                                        <site name="robot0:S_lftip" pos="0 0 0.026" group="3"></site>
+                                        <site class="robot0:D_Touch" name="robot0:Tch_lftip"></site>
+                                        <site name="robot0:T_lftip_front_left"  type="box"    pos="-0.00451 -0.00451 0.012" size="0.00451 0.00451 0.01905" rgba="1 0 0 0.33"/>
+                                        <site name="robot0:T_lftip_front_right" type="box"    pos="0.00451 -0.00451 0.012"  size="0.00451 0.00451 0.01905" rgba="0 0 0 0.33"/>
+                                        <site name="robot0:T_lftip_back_left"   type="box"    pos="-0.00451 0.00451 0.012"  size="0.00451 0.00451 0.01905" rgba="0 0 0 0.33"/>
+                                        <site name="robot0:T_lftip_back_right"  type="box"    pos="0.00451 0.00451 0.012"   size="0.00451 0.00451 0.01905" rgba="1 0 0 0.33"/>
+                                        <site name="robot0:T_lftip_tip"         type="sphere" pos="0 0 0.024"               size="0.008"                   rgba="1 1 0 0.33"/>
+                                    </body>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                    <body name="robot0:thbase" pos="0.034 -0.009 0.029" axisangle="0 1 0 0.785">
+                        <inertial pos="0 0 0" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:THJ4" type="hinge" pos="0 0 0" axis="0 0 -1" range="-1.047 1.047" user="1121"></joint>
+                        <geom name="robot0:V_thbase" type="box" group="1" pos="0 0 0" size="0.001 0.001 0.001"></geom>
+                        <body name="robot0:thproximal" pos="0 0 0">
+                            <inertial pos="0 0 0.017" quat="0.982 0 0.001 0.191" mass="0.016" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:THJ3" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.222" user="1120"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_thproximal" mesh="robot0:TH3_z"></geom>
+                            <geom class="robot0:DC_Hand" name="robot0:C_thproximal" type="capsule" pos="0 0 0.019" size="0.013 0.019" rgba="0.4 0.5 0.6 0.1"></geom>
+                            <site name="robot0:T_thproximal_front_left"  type="box"    pos="-0.007 -0.007 0.019" size="0.007 0.007 0.032" rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_thproximal_front_right" type="box"    pos="0.007 -0.007 0.019"  size="0.007 0.007 0.032" rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_thproximal_back_left"   type="box"    pos="-0.007 0.007 0.019"  size="0.007 0.007 0.032" rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_thproximal_back_right"  type="box"    pos="0.007 0.007 0.019"   size="0.007 0.007 0.032" rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_thproximal_tip"         type="sphere" pos="0 0 0.038"           size="0.014"             rgba="1 1 0 0.33"/>
+                            <body name="robot0:thhub" pos="0 0 0.038">
+                                <inertial pos="0 0 0" mass="0.002" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:THJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="-0.209 0.209" user="1119"></joint>
+                                <geom name="robot0:V_thhub" type="box" group="1" pos="0 0 0" size="0.001 0.001 0.001"></geom>
+                                <body name="robot0:thmiddle" pos="0 0 0">
+                                    <inertial pos="0 0 0.016" quat="1 -0.001 -0.007 0.003" mass="0.016" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:THJ1" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.524 0.524" user="1118"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_thmiddle" mesh="robot0:TH2_z"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_thmiddle" type="capsule" pos="0 0 0.016" size="0.011 0.016"></geom>
+                                    <site name="robot0:T_thmiddle_front_left"  type="box"    pos="-0.006 -0.006 0.016" size="0.006 0.006 0.027" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_thmiddle_front_right" type="box"    pos="0.006 -0.006 0.016"  size="0.006 0.006 0.027" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_thmiddle_back_left"   type="box"    pos="-0.006 0.006 0.016"  size="0.006 0.006 0.027" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_thmiddle_back_right"  type="box"    pos="0.006 0.006 0.016"   size="0.006 0.006 0.027" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_thmiddle_tip"         type="sphere" pos="0 0 0.032"           size="0.012"             rgba="1 1 0 0.33"/>
+                                    <body name="robot0:thdistal" pos="0 0 0.032">
+                                        <inertial pos="0 0 0.016" quat="0.999 -0.005 -0.047 0.005" mass="0.016" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                        <joint name="robot0:THJ0" type="hinge" pos="0 0 0" axis="0 1 0" range="-1.571 0" user="1117"></joint>
+                                        <geom class="robot0:D_Vizual" name="robot0:V_thdistal" mesh="robot0:TH1_z"></geom>
+                                        <geom class="robot0:DC_Hand" name="robot0:C_thdistal" type="capsule" pos="0 0 0.013" size="0.00918 0.013" condim="4"></geom>
+                                        <site name="robot0:S_thtip" pos="0 0 0.0275" group="3"></site>
+                                        <site class="robot0:D_Touch" name="robot0:Tch_thtip" size="0.005 0.011 0.016" pos="-0.005 0 0.02"></site>
+                                        <site name="robot0:T_thtip_front_left"  type="box"    pos="-0.0056 -0.0056 0.013" size="0.0056 0.0056 0.02218" rgba="0 0 0 0.33"/>
+                                        <site name="robot0:T_thtip_front_right" type="box"    pos="0.0056 -0.0056 0.013"  size="0.0056 0.0056 0.02218" rgba="1 0 0 0.33"/>
+                                        <site name="robot0:T_thtip_back_left"   type="box"    pos="-0.0056 0.0056 0.013"  size="0.0056 0.0056 0.02218" rgba="1 0 0 0.33"/>
+                                        <site name="robot0:T_thtip_back_right"  type="box"    pos="0.0056 0.0056 0.013"   size="0.0056 0.0056 0.02218" rgba="0 0 0 0.33"/>
+                                        <site name="robot0:T_thtip_tip"         type="sphere" pos="0 0 0.026"             size="0.01"                  rgba="1 1 0 0.33"/>
+                                    </body>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                </body>
+            </body>
+        </body>
+    </body>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared.xml b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared.xml
new file mode 100644
index 0000000..f27f265
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared.xml
@@ -0,0 +1,254 @@
+<!-- See LICENSE.md for legal notices. LICENSE.md must be kept together with this file. -->
+<mujoco>
+    <size njmax="500" nconmax="100" nuser_jnt="1" nuser_site="1" nuser_tendon="1" nuser_sensor="1" nuser_actuator="16" nstack="600000"></size>
+
+    <visual>
+        <map fogstart="3" fogend="5" force="0.1"></map>
+        <quality shadowsize="4096"></quality>
+    </visual>
+
+    <default>
+        <default class="robot0:asset_class">
+            <geom friction="1 0.005 0.001" condim="3" margin="0.0005" contype="1" conaffinity="1"></geom>
+            <joint limited="true" damping="0.1" armature="0.001" margin="0.01" frictionloss="0.001"></joint>
+            <site size="0.005" rgba="0.4 0.9 0.4 1"></site>
+            <general ctrllimited="true" forcelimited="true"></general>
+        </default>
+        <default class="robot0:D_Touch">
+            <site type="box" size="0.009 0.004 0.013" pos="0 -0.004 0.018" rgba="0.8 0.8 0.8 0.15" group="4"></site>
+        </default>
+        <default class="robot0:DC_Hand">
+            <geom material="robot0:MatColl" contype="1" conaffinity="0" group="4"></geom>
+        </default>
+        <default class="robot0:D_Vizual">
+            <geom material="robot0:MatViz" contype="0" conaffinity="0" group="1" type="mesh"></geom>
+        </default>
+        <default class="robot0:free">
+            <joint type="free" damping="0" armature="0" limited="false"></joint>
+        </default>
+    </default>
+
+    <contact>
+        <pair geom1="robot0:C_ffdistal" geom2="robot0:C_thdistal" condim="1"></pair>
+        <pair geom1="robot0:C_ffmiddle" geom2="robot0:C_thdistal" condim="1"></pair>
+        <pair geom1="robot0:C_ffproximal" geom2="robot0:C_thdistal" condim="1"></pair>
+        <pair geom1="robot0:C_mfproximal" geom2="robot0:C_thdistal" condim="1"></pair>
+        <pair geom1="robot0:C_mfdistal" geom2="robot0:C_thdistal" condim="1"></pair>
+        <pair geom1="robot0:C_rfdistal" geom2="robot0:C_thdistal" condim="1"></pair>
+        <pair geom1="robot0:C_lfdistal" geom2="robot0:C_thdistal" condim="1"></pair>
+        <pair geom1="robot0:C_palm0" geom2="robot0:C_thdistal" condim="1"></pair>
+        <pair geom1="robot0:C_mfdistal" geom2="robot0:C_ffdistal" condim="1"></pair>
+        <pair geom1="robot0:C_rfdistal" geom2="robot0:C_mfdistal" condim="1"></pair>
+        <pair geom1="robot0:C_lfdistal" geom2="robot0:C_rfdistal" condim="1"></pair>
+        <pair geom1="robot0:C_mfproximal" geom2="robot0:C_ffproximal" condim="1"></pair>
+        <pair geom1="robot0:C_rfproximal" geom2="robot0:C_mfproximal" condim="1"></pair>
+        <pair geom1="robot0:C_lfproximal" geom2="robot0:C_rfproximal" condim="1"></pair>
+        <pair geom1="robot0:C_lfdistal" geom2="robot0:C_rfdistal" condim="1"></pair>
+        <pair geom1="robot0:C_lfdistal" geom2="robot0:C_mfdistal" condim="1"></pair>
+        <pair geom1="robot0:C_lfdistal" geom2="robot0:C_rfmiddle" condim="1"></pair>
+        <pair geom1="robot0:C_lfmiddle" geom2="robot0:C_rfdistal" condim="1"></pair>
+        <pair geom1="robot0:C_lfmiddle" geom2="robot0:C_rfmiddle" condim="1"></pair>
+    </contact>
+
+    <tendon>
+        <fixed name="robot0:T_WRJ1r" limited="true" range="-0.032 0.032" user="1236">
+            <joint joint="robot0:WRJ1" coef="0.0325"></joint>
+        </fixed>
+        <fixed name="robot0:T_WRJ1l" limited="true" range="-0.032 0.032" user="1237">
+            <joint joint="robot0:WRJ1" coef="-0.0325"></joint>
+        </fixed>
+        <fixed name="robot0:T_WRJ0u" limited="true" range="-0.032 0.032" user="1236">
+            <joint joint="robot0:WRJ0" coef="0.0175"></joint>
+        </fixed>
+        <fixed name="robot0:T_WRJ0d" limited="true" range="-0.032 0.032" user="1237">
+            <joint joint="robot0:WRJ0" coef="-0.0175"></joint>
+        </fixed>
+        <fixed name="robot0:T_FFJ3r" limited="true" range="-0.018 0.018" user="1204">
+            <joint joint="robot0:FFJ3" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_FFJ3l" limited="true" range="-0.018 0.018" user="1205">
+            <joint joint="robot0:FFJ3" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_FFJ2u" limited="true" range="-0.007 0.03" user="1202">
+            <joint joint="robot0:FFJ2" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_FFJ2d" limited="true" range="-0.03 0.007" user="1203">
+            <joint joint="robot0:FFJ2" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_FFJ1c" limited="true" range="-0.001 0.001">
+            <joint joint="robot0:FFJ0" coef="0.00705"></joint>
+            <joint joint="robot0:FFJ1" coef="-0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_FFJ1u" limited="true" range="-0.007 0.03" user="1200">
+            <joint joint="robot0:FFJ0" coef="0.00705"></joint>
+            <joint joint="robot0:FFJ1" coef="0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_FFJ1d" limited="true" range="-0.03 0.007" user="1201">
+            <joint joint="robot0:FFJ0" coef="-0.00705"></joint>
+            <joint joint="robot0:FFJ1" coef="-0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_MFJ3r" limited="true" range="-0.018 0.018" user="1210">
+            <joint joint="robot0:MFJ3" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_MFJ3l" limited="true" range="-0.018 0.018" user="1211">
+            <joint joint="robot0:MFJ3" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_MFJ2u" limited="true" range="-0.007 0.03" user="1208">
+            <joint joint="robot0:MFJ2" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_MFJ2d" limited="true" range="-0.03 0.007" user="1209">
+            <joint joint="robot0:MFJ2" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_MFJ1c" limited="true" range="-0.001 0.001">
+            <joint joint="robot0:MFJ0" coef="0.00705"></joint>
+            <joint joint="robot0:MFJ1" coef="-0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_MFJ1u" limited="true" range="-0.007 0.03" user="1206">
+            <joint joint="robot0:MFJ0" coef="0.00705"></joint>
+            <joint joint="robot0:MFJ1" coef="0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_MFJ1d" limited="true" range="-0.03 0.007" user="1207">
+            <joint joint="robot0:MFJ0" coef="-0.00705"></joint>
+            <joint joint="robot0:MFJ1" coef="-0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_RFJ3r" limited="true" range="-0.018 0.018" user="1216">
+            <joint joint="robot0:RFJ3" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_RFJ3l" limited="true" range="-0.018 0.018" user="1217">
+            <joint joint="robot0:RFJ3" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_RFJ2u" limited="true" range="-0.007 0.03" user="1214">
+            <joint joint="robot0:RFJ2" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_RFJ2d" limited="true" range="-0.03 0.007" user="1215">
+            <joint joint="robot0:RFJ2" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_RFJ1c" limited="true" range="-0.001 0.001">
+            <joint joint="robot0:RFJ0" coef="0.00705"></joint>
+            <joint joint="robot0:RFJ1" coef="-0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_RFJ1u" limited="true" range="-0.007 0.03" user="1212">
+            <joint joint="robot0:RFJ0" coef="0.00705"></joint>
+            <joint joint="robot0:RFJ1" coef="0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_RFJ1d" limited="true" range="-0.03 0.007" user="1213">
+            <joint joint="robot0:RFJ0" coef="-0.00705"></joint>
+            <joint joint="robot0:RFJ1" coef="-0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ4u" limited="true" range="-0.007 0.03" user="1224">
+            <joint joint="robot0:LFJ4" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ4d" limited="true" range="-0.03 0.007" user="1225">
+            <joint joint="robot0:LFJ4" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ3r" limited="true" range="-0.018 0.018" user="1222">
+            <joint joint="robot0:LFJ3" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ3l" limited="true" range="-0.018 0.018" user="1223">
+            <joint joint="robot0:LFJ3" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ2u" limited="true" range="-0.007 0.03" user="1220">
+            <joint joint="robot0:LFJ2" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ2d" limited="true" range="-0.03 0.007" user="1221">
+            <joint joint="robot0:LFJ2" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ1c" limited="true" range="-0.001 0.001">
+            <joint joint="robot0:LFJ0" coef="0.00705"></joint>
+            <joint joint="robot0:LFJ1" coef="-0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ1u" limited="true" range="-0.007 0.03" user="1218">
+            <joint joint="robot0:LFJ0" coef="0.00705"></joint>
+            <joint joint="robot0:LFJ1" coef="0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ1d" limited="true" range="-0.03 0.007" user="1219">
+            <joint joint="robot0:LFJ0" coef="-0.00705"></joint>
+            <joint joint="robot0:LFJ1" coef="-0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ4a" limited="true" range="-0.018 0.018" user="1234">
+            <joint joint="robot0:THJ4" coef="0.01636"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ4c" limited="true" range="-0.018 0.018" user="1235">
+            <joint joint="robot0:THJ4" coef="-0.01636"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ3u" limited="true" range="-0.007 0.03" user="1232">
+            <joint joint="robot0:THJ3" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ3d" limited="true" range="-0.03 0.007" user="1233">
+            <joint joint="robot0:THJ3" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ2u" limited="true" range="-0.018 0.018" user="1230">
+            <joint joint="robot0:THJ2" coef="0.011"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ2d" limited="true" range="-0.018 0.018" user="1231">
+            <joint joint="robot0:THJ2" coef="-0.011"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ1r" limited="true" range="-0.018 0.018" user="1228">
+            <joint joint="robot0:THJ1" coef="0.011"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ1l" limited="true" range="-0.018 0.018" user="1229">
+            <joint joint="robot0:THJ1" coef="-0.011"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ0r" limited="true" range="-0.03 0.007" user="1226">
+            <joint joint="robot0:THJ0" coef="0.009"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ0l" limited="true" range="-0.007 0.03" user="1227">
+            <joint joint="robot0:THJ0" coef="-0.009"></joint>
+        </fixed>
+    </tendon>
+
+    <sensor>
+        <jointpos name="robot0:Sjp_WRJ1" joint="robot0:WRJ1"></jointpos>
+        <jointpos name="robot0:Sjp_WRJ0" joint="robot0:WRJ0"></jointpos>
+        <jointpos name="robot0:Sjp_FFJ3" joint="robot0:FFJ3"></jointpos>
+        <jointpos name="robot0:Sjp_FFJ2" joint="robot0:FFJ2"></jointpos>
+        <jointpos name="robot0:Sjp_FFJ1" joint="robot0:FFJ1"></jointpos>
+        <jointpos name="robot0:Sjp_FFJ0" joint="robot0:FFJ0"></jointpos>
+        <jointpos name="robot0:Sjp_MFJ3" joint="robot0:MFJ3"></jointpos>
+        <jointpos name="robot0:Sjp_MFJ2" joint="robot0:MFJ2"></jointpos>
+        <jointpos name="robot0:Sjp_MFJ1" joint="robot0:MFJ1"></jointpos>
+        <jointpos name="robot0:Sjp_MFJ0" joint="robot0:MFJ0"></jointpos>
+        <jointpos name="robot0:Sjp_RFJ3" joint="robot0:RFJ3"></jointpos>
+        <jointpos name="robot0:Sjp_RFJ2" joint="robot0:RFJ2"></jointpos>
+        <jointpos name="robot0:Sjp_RFJ1" joint="robot0:RFJ1"></jointpos>
+        <jointpos name="robot0:Sjp_RFJ0" joint="robot0:RFJ0"></jointpos>
+        <jointpos name="robot0:Sjp_LFJ4" joint="robot0:LFJ4"></jointpos>
+        <jointpos name="robot0:Sjp_LFJ3" joint="robot0:LFJ3"></jointpos>
+        <jointpos name="robot0:Sjp_LFJ2" joint="robot0:LFJ2"></jointpos>
+        <jointpos name="robot0:Sjp_LFJ1" joint="robot0:LFJ1"></jointpos>
+        <jointpos name="robot0:Sjp_LFJ0" joint="robot0:LFJ0"></jointpos>
+        <jointpos name="robot0:Sjp_THJ4" joint="robot0:THJ4"></jointpos>
+        <jointpos name="robot0:Sjp_THJ3" joint="robot0:THJ3"></jointpos>
+        <jointpos name="robot0:Sjp_THJ2" joint="robot0:THJ2"></jointpos>
+        <jointpos name="robot0:Sjp_THJ1" joint="robot0:THJ1"></jointpos>
+        <jointpos name="robot0:Sjp_THJ0" joint="robot0:THJ0"></jointpos>
+        <touch name="robot0:ST_Tch_fftip" site="robot0:Tch_fftip"></touch>
+        <touch name="robot0:ST_Tch_mftip" site="robot0:Tch_mftip"></touch>
+        <touch name="robot0:ST_Tch_rftip" site="robot0:Tch_rftip"></touch>
+        <touch name="robot0:ST_Tch_lftip" site="robot0:Tch_lftip"></touch>
+        <touch name="robot0:ST_Tch_thtip" site="robot0:Tch_thtip"></touch>
+    </sensor>
+
+    <actuator>
+        <position name="robot0:A_WRJ1" class="robot0:asset_class" user="2038" joint="robot0:WRJ1" ctrlrange="-0.489 0.14" kp="5" forcerange="-4.785 4.785"></position>
+        <position name="robot0:A_WRJ0" class="robot0:asset_class" user="2036" joint="robot0:WRJ0" ctrlrange="-0.698 0.489" kp="5" forcerange="-2.175 2.175"></position>
+        <position name="robot0:A_FFJ3" class="robot0:asset_class" user="2004" joint="robot0:FFJ3" ctrlrange="-0.349 0.349" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_FFJ2" class="robot0:asset_class" user="2002" joint="robot0:FFJ2" ctrlrange="0 1.571" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_FFJ1" class="robot0:asset_class" user="2000" joint="robot0:FFJ1" ctrlrange="0 1.571" kp="1" forcerange="-0.7245 0.7245"></position>
+        <position name="robot0:A_MFJ3" class="robot0:asset_class" user="2010" joint="robot0:MFJ3" ctrlrange="-0.349 0.349" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_MFJ2" class="robot0:asset_class" user="2008" joint="robot0:MFJ2" ctrlrange="0 1.571" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_MFJ1" class="robot0:asset_class" user="2006" joint="robot0:MFJ1" ctrlrange="0 1.571" kp="1" forcerange="-0.7245 0.7245"></position>
+        <position name="robot0:A_RFJ3" class="robot0:asset_class" user="2016" joint="robot0:RFJ3" ctrlrange="-0.349 0.349" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_RFJ2" class="robot0:asset_class" user="2014" joint="robot0:RFJ2" ctrlrange="0 1.571" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_RFJ1" class="robot0:asset_class" user="2012" joint="robot0:RFJ1" ctrlrange="0 1.571" kp="1" forcerange="-0.7245 0.7245"></position>
+        <position name="robot0:A_LFJ4" class="robot0:asset_class" user="2024" joint="robot0:LFJ4" ctrlrange="0 0.785" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_LFJ3" class="robot0:asset_class" user="2022" joint="robot0:LFJ3" ctrlrange="-0.349 0.349" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_LFJ2" class="robot0:asset_class" user="2020" joint="robot0:LFJ2" ctrlrange="0 1.571" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_LFJ1" class="robot0:asset_class" user="2018" joint="robot0:LFJ1" ctrlrange="0 1.571" kp="1" forcerange="-0.7245 0.7245"></position>
+        <position name="robot0:A_THJ4" class="robot0:asset_class" user="2034" joint="robot0:THJ4" ctrlrange="-1.047 1.047" kp="1" forcerange="-2.3722 2.3722"></position>
+        <position name="robot0:A_THJ3" class="robot0:asset_class" user="2032" joint="robot0:THJ3" ctrlrange="0 1.222" kp="1" forcerange="-1.45 1.45"></position>
+        <position name="robot0:A_THJ2" class="robot0:asset_class" user="2030" joint="robot0:THJ2" ctrlrange="-0.209 0.209" kp="1" forcerange="-0.99 0.99"></position>
+        <position name="robot0:A_THJ1" class="robot0:asset_class" user="2028" joint="robot0:THJ1" ctrlrange="-0.524 0.524" kp="1" forcerange="-0.99 0.99"></position>
+        <position name="robot0:A_THJ0" class="robot0:asset_class" user="2026" joint="robot0:THJ0" ctrlrange="-1.571 0" kp="1" forcerange="-0.81 0.81"></position>
+    </actuator>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared_asset.xml b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared_asset.xml
new file mode 100644
index 0000000..ec9a0b0
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared_asset.xml
@@ -0,0 +1,26 @@
+<!-- See LICENSE.md for legal notices. LICENSE.md must be kept together with this file. -->
+<mujoco>
+    <texture type="skybox" builtin="gradient" rgb1="0.44 0.85 0.56" rgb2="0.46 0.87 0.58" width="32" height="32"></texture>
+
+    <texture name="robot0:texplane" type="2d" builtin="checker" rgb1="0.2 0.3 0.4" rgb2="0.1 0.15 0.2" width="512" height="512"></texture>
+    <texture name="robot0:texgeom" type="cube" builtin="flat" mark="cross" width="127" height="127" rgb1="0.3 0.6 0.5" rgb2="0.3 0.6 0.5" markrgb="0 0 0" random="0.01"></texture>
+
+    <material name="robot0:MatGnd" reflectance="0.5" texture="robot0:texplane" texrepeat="1 1" texuniform="true"></material>
+    <material name="robot0:MatColl" specular="1" shininess="0.3" reflectance="0.5" rgba="0.4 0.5 0.6 1"></material>
+    <material name="robot0:MatViz" specular="0.75" shininess="0.1" reflectance="0.5" rgba="0.93 0.93 0.93 1"></material>
+    <material name="robot0:object" texture="robot0:texgeom" texuniform="false"></material>
+    <material name="floor_mat" specular="0" shininess="0.5" reflectance="0" rgba="0.2 0.2 0.2 0"></material>
+
+    <mesh name="robot0:forearm" file="forearm_electric.stl"></mesh>
+    <mesh name="robot0:forearm_cvx" file="forearm_electric_cvx.stl"></mesh>
+    <mesh name="robot0:wrist" scale="0.001 0.001 0.001" file="wrist.stl"></mesh>
+    <mesh name="robot0:palm" scale="0.001 0.001 0.001" file="palm.stl"></mesh>
+    <mesh name="robot0:knuckle" scale="0.001 0.001 0.001" file="knuckle.stl"></mesh>
+    <mesh name="robot0:F3" scale="0.001 0.001 0.001" file="F3.stl"></mesh>
+    <mesh name="robot0:F2" scale="0.001 0.001 0.001" file="F2.stl"></mesh>
+    <mesh name="robot0:F1" scale="0.001 0.001 0.001" file="F1.stl"></mesh>
+    <mesh name="robot0:lfmetacarpal" scale="0.001 0.001 0.001" file="lfmetacarpal.stl"></mesh>
+    <mesh name="robot0:TH3_z" scale="0.001 0.001 0.001" file="TH3_z.stl"></mesh>
+    <mesh name="robot0:TH2_z" scale="0.001 0.001 0.001" file="TH2_z.stl"></mesh>
+    <mesh name="robot0:TH1_z" scale="0.001 0.001 0.001" file="TH1_z.stl"></mesh>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared_touch_sensors_92.xml b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared_touch_sensors_92.xml
new file mode 100644
index 0000000..472c84c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared_touch_sensors_92.xml
@@ -0,0 +1,120 @@
+<mujoco>
+    <sensor>
+
+        <!--PALM-->
+        <touch name="robot0:TS_palm_b0" site="robot0:T_palm_b0"></touch>
+        <touch name="robot0:TS_palm_bl" site="robot0:T_palm_bl"></touch>
+        <touch name="robot0:TS_palm_bm" site="robot0:T_palm_bm"></touch>
+        <touch name="robot0:TS_palm_br" site="robot0:T_palm_br"></touch>
+        <touch name="robot0:TS_palm_fl" site="robot0:T_palm_fl"></touch>
+        <touch name="robot0:TS_palm_fm" site="robot0:T_palm_fm"></touch>
+        <touch name="robot0:TS_palm_fr" site="robot0:T_palm_fr"></touch>
+        <touch name="robot0:TS_palm_b1" site="robot0:T_palm_b1"></touch>
+
+         <!--FOREFINGER-->
+        <touch name="robot0:TS_ffproximal_front_left_bottom"  site="robot0:T_ffproximal_front_left_bottom"></touch>
+        <touch name="robot0:TS_ffproximal_front_right_bottom" site="robot0:T_ffproximal_front_right_bottom"></touch>
+        <touch name="robot0:TS_ffproximal_front_left_top"     site="robot0:T_ffproximal_front_left_top"></touch>
+        <touch name="robot0:TS_ffproximal_front_right_top"    site="robot0:T_ffproximal_front_right_top"></touch>
+        <touch name="robot0:TS_ffproximal_back_left"          site="robot0:T_ffproximal_back_left"></touch>
+        <touch name="robot0:TS_ffproximal_back_right"         site="robot0:T_ffproximal_back_right"></touch>
+        <touch name="robot0:TS_ffproximal_tip"                site="robot0:T_ffproximal_tip"></touch>
+
+        <touch name="robot0:TS_ffmiddle_front_left"  site="robot0:T_ffmiddle_front_left"></touch>
+        <touch name="robot0:TS_ffmiddle_front_right" site="robot0:T_ffmiddle_front_right"></touch>
+        <touch name="robot0:TS_ffmiddle_back_left"   site="robot0:T_ffmiddle_back_left"></touch>
+        <touch name="robot0:TS_ffmiddle_back_right"  site="robot0:T_ffmiddle_back_right"></touch>
+        <touch name="robot0:TS_ffmiddle_tip"         site="robot0:T_ffmiddle_tip"></touch>
+
+        <touch name="robot0:TS_fftip_front_left"  site="robot0:T_fftip_front_left"></touch>
+        <touch name="robot0:TS_fftip_front_right" site="robot0:T_fftip_front_right"></touch>
+        <touch name="robot0:TS_fftip_back_left"   site="robot0:T_fftip_back_left"></touch>
+        <touch name="robot0:TS_fftip_back_right"  site="robot0:T_fftip_back_right"></touch>
+        <touch name="robot0:TS_fftip_tip"         site="robot0:T_fftip_tip"></touch>
+
+        <!-- MIDDLE FINGER -->
+        <touch name="robot0:TS_mfproximal_front_left_bottom"  site="robot0:T_mfproximal_front_left_bottom"></touch>
+        <touch name="robot0:TS_mfproximal_front_right_bottom" site="robot0:T_mfproximal_front_right_bottom"></touch>
+        <touch name="robot0:TS_mfproximal_front_left_top"     site="robot0:T_mfproximal_front_left_top"></touch>
+        <touch name="robot0:TS_mfproximal_front_right_top"    site="robot0:T_mfproximal_front_right_top"></touch>
+        <touch name="robot0:TS_mfproximal_back_left"          site="robot0:T_mfproximal_back_left"></touch>
+        <touch name="robot0:TS_mfproximal_back_right"         site="robot0:T_mfproximal_back_right"></touch>
+        <touch name="robot0:TS_mfproximal_tip"                site="robot0:T_mfproximal_tip"></touch>
+
+        <touch name="robot0:TS_mfmiddle_front_left"  site="robot0:T_mfmiddle_front_left"></touch>
+        <touch name="robot0:TS_mfmiddle_front_right" site="robot0:T_mfmiddle_front_right"></touch>
+        <touch name="robot0:TS_mfmiddle_back_left"   site="robot0:T_mfmiddle_back_left"></touch>
+        <touch name="robot0:TS_mfmiddle_back_right"  site="robot0:T_mfmiddle_back_right"></touch>
+        <touch name="robot0:TS_mfmiddle_tip"         site="robot0:T_mfmiddle_tip"></touch>
+
+        <touch name="robot0:TS_mftip_front_left"  site="robot0:T_mftip_front_left"></touch>
+        <touch name="robot0:TS_mftip_front_right" site="robot0:T_mftip_front_right"></touch>
+        <touch name="robot0:TS_mftip_back_left"   site="robot0:T_mftip_back_left"></touch>
+        <touch name="robot0:TS_mftip_back_right"  site="robot0:T_mftip_back_right"></touch>
+        <touch name="robot0:TS_mftip_tip"         site="robot0:T_mftip_tip"></touch>
+
+        <!-- RING FINGER -->
+        <touch name="robot0:TS_rfproximal_front_left_bottom"  site="robot0:T_rfproximal_front_left_bottom"></touch>
+        <touch name="robot0:TS_rfproximal_front_right_bottom" site="robot0:T_rfproximal_front_right_bottom"></touch>
+        <touch name="robot0:TS_rfproximal_front_left_top"     site="robot0:T_rfproximal_front_left_top"></touch>
+        <touch name="robot0:TS_rfproximal_front_right_top"    site="robot0:T_rfproximal_front_right_top"></touch>
+        <touch name="robot0:TS_rfproximal_back_left"          site="robot0:T_rfproximal_back_left"></touch>
+        <touch name="robot0:TS_rfproximal_back_right"         site="robot0:T_rfproximal_back_right"></touch>
+        <touch name="robot0:TS_rfproximal_tip"                site="robot0:T_rfproximal_tip"></touch>
+
+        <touch name="robot0:TS_rfmiddle_front_left"  site="robot0:T_rfmiddle_front_left"></touch>
+        <touch name="robot0:TS_rfmiddle_front_right" site="robot0:T_rfmiddle_front_right"></touch>
+        <touch name="robot0:TS_rfmiddle_back_left"   site="robot0:T_rfmiddle_back_left"></touch>
+        <touch name="robot0:TS_rfmiddle_back_right"  site="robot0:T_rfmiddle_back_right"></touch>
+        <touch name="robot0:TS_rfmiddle_tip"         site="robot0:T_rfmiddle_tip"></touch>
+
+        <touch name="robot0:TS_rftip_front_left"  site="robot0:T_rftip_front_left"></touch>
+        <touch name="robot0:TS_rftip_front_right" site="robot0:T_rftip_front_right"></touch>
+        <touch name="robot0:TS_rftip_back_left"   site="robot0:T_rftip_back_left"></touch>
+        <touch name="robot0:TS_rftip_back_right"  site="robot0:T_rftip_back_right"></touch>
+        <touch name="robot0:TS_rftip_tip"         site="robot0:T_rftip_tip"></touch>
+
+        <!-- LITTLE FINGER -->
+        <touch name="robot0:TS_lfmetacarpal_front" site="robot0:T_lfmetacarpal_front"></touch>
+
+        <touch name="robot0:TS_lfproximal_front_left_bottom"  site="robot0:T_lfproximal_front_left_bottom"></touch>
+        <touch name="robot0:TS_lfproximal_front_right_bottom" site="robot0:T_lfproximal_front_right_bottom"></touch>
+        <touch name="robot0:TS_lfproximal_front_left_top"     site="robot0:T_lfproximal_front_left_top"></touch>
+        <touch name="robot0:TS_lfproximal_front_right_top"    site="robot0:T_lfproximal_front_right_top"></touch>
+        <touch name="robot0:TS_lfproximal_back_left"          site="robot0:T_lfproximal_back_left"></touch>
+        <touch name="robot0:TS_lfproximal_back_right"         site="robot0:T_lfproximal_back_right"></touch>
+        <touch name="robot0:TS_lfproximal_tip"                site="robot0:T_lfproximal_tip"></touch>
+
+        <touch name="robot0:TS_lfmiddle_front_left"  site="robot0:T_lfmiddle_front_left"></touch>
+        <touch name="robot0:TS_lfmiddle_front_right" site="robot0:T_lfmiddle_front_right"></touch>
+        <touch name="robot0:TS_lfmiddle_back_left"   site="robot0:T_lfmiddle_back_left"></touch>
+        <touch name="robot0:TS_lfmiddle_back_right"  site="robot0:T_lfmiddle_back_right"></touch>
+        <touch name="robot0:TS_lfmiddle_tip"         site="robot0:T_lfmiddle_tip"></touch>
+
+        <touch name="robot0:TS_lftip_front_left"  site="robot0:T_lftip_front_left"></touch>
+        <touch name="robot0:TS_lftip_front_right" site="robot0:T_lftip_front_right"></touch>
+        <touch name="robot0:TS_lftip_back_left"   site="robot0:T_lftip_back_left"></touch>
+        <touch name="robot0:TS_lftip_back_right"  site="robot0:T_lftip_back_right"></touch>
+        <touch name="robot0:TS_lftip_tip"         site="robot0:T_lftip_tip"></touch>
+
+        <!--THUMB-->
+        <touch name="robot0:TS_thproximal_front_left"  site="robot0:T_thproximal_front_left"></touch>
+        <touch name="robot0:TS_thproximal_front_right" site="robot0:T_thproximal_front_right"></touch>
+        <touch name="robot0:TS_thproximal_back_left"   site="robot0:T_thproximal_back_left"></touch>
+        <touch name="robot0:TS_thproximal_back_right"  site="robot0:T_thproximal_back_right"></touch>
+        <touch name="robot0:TS_thproximal_tip"         site="robot0:T_thproximal_tip"></touch>
+
+        <touch name="robot0:TS_thmiddle_front_left"  site="robot0:T_thmiddle_front_left"></touch>
+        <touch name="robot0:TS_thmiddle_front_right" site="robot0:T_thmiddle_front_right"></touch>
+        <touch name="robot0:TS_thmiddle_back_left"   site="robot0:T_thmiddle_back_left"></touch>
+        <touch name="robot0:TS_thmiddle_back_right"  site="robot0:T_thmiddle_back_right"></touch>
+        <touch name="robot0:TS_thmiddle_tip"         site="robot0:T_thmiddle_tip"></touch>
+
+        <touch name="robot0:TS_thtip_front_left"  site="robot0:T_thtip_front_left"></touch>
+        <touch name="robot0:TS_thtip_front_right" site="robot0:T_thtip_front_right"></touch>
+        <touch name="robot0:TS_thtip_back_left"   site="robot0:T_thtip_back_left"></touch>
+        <touch name="robot0:TS_thtip_back_right"  site="robot0:T_thtip_back_right"></touch>
+        <touch name="robot0:TS_thtip_tip"         site="robot0:T_thtip_tip"></touch>
+
+    </sensor>
+</mujoco>
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/base_link_collision.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/base_link_collision.stl
new file mode 100644
index 0000000..1ef459f
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/base_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/bellows_link_collision.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/bellows_link_collision.stl
new file mode 100644
index 0000000..a7e5ab7
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/bellows_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/elbow_flex_link_collision.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/elbow_flex_link_collision.stl
new file mode 100644
index 0000000..b0eea07
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/elbow_flex_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/estop_link.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/estop_link.stl
new file mode 100644
index 0000000..f6d1c72
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/estop_link.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/forearm_roll_link_collision.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/forearm_roll_link_collision.stl
new file mode 100644
index 0000000..fe468c5
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/forearm_roll_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/gripper_link.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/gripper_link.stl
new file mode 100644
index 0000000..8a14874
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/gripper_link.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/head_pan_link_collision.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/head_pan_link_collision.stl
new file mode 100644
index 0000000..c77b5b1
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/head_pan_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/head_tilt_link_collision.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/head_tilt_link_collision.stl
new file mode 100644
index 0000000..53c2ddc
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/head_tilt_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/l_wheel_link_collision.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/l_wheel_link_collision.stl
new file mode 100644
index 0000000..5c17524
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/l_wheel_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/laser_link.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/laser_link.stl
new file mode 100644
index 0000000..fa4882f
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/laser_link.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/r_wheel_link_collision.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/r_wheel_link_collision.stl
new file mode 100644
index 0000000..3742b24
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/r_wheel_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/shoulder_lift_link_collision.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/shoulder_lift_link_collision.stl
new file mode 100644
index 0000000..c9aff0d
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/shoulder_lift_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/shoulder_pan_link_collision.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/shoulder_pan_link_collision.stl
new file mode 100644
index 0000000..ac17a94
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/shoulder_pan_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/torso_fixed_link.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/torso_fixed_link.stl
new file mode 100644
index 0000000..7cf7fc1
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/torso_fixed_link.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/torso_lift_link_collision.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/torso_lift_link_collision.stl
new file mode 100644
index 0000000..4ce5fcf
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/torso_lift_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/upperarm_roll_link_collision.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/upperarm_roll_link_collision.stl
new file mode 100644
index 0000000..1207932
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/upperarm_roll_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/wrist_flex_link_collision.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/wrist_flex_link_collision.stl
new file mode 100644
index 0000000..3215d2e
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/wrist_flex_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/wrist_roll_link_collision.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/wrist_roll_link_collision.stl
new file mode 100644
index 0000000..742bdd9
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/wrist_roll_link_collision.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F1.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F1.stl
new file mode 100644
index 0000000..515d3c9
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F1.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F2.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F2.stl
new file mode 100644
index 0000000..7bc5e20
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F2.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F3.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F3.stl
new file mode 100644
index 0000000..223f06f
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F3.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH1_z.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH1_z.stl
new file mode 100644
index 0000000..400ee2d
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH1_z.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH2_z.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH2_z.stl
new file mode 100644
index 0000000..5ace838
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH2_z.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH3_z.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH3_z.stl
new file mode 100644
index 0000000..23485ab
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH3_z.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/forearm_electric.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/forearm_electric.stl
new file mode 100644
index 0000000..80f6f3d
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/forearm_electric.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/forearm_electric_cvx.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/forearm_electric_cvx.stl
new file mode 100644
index 0000000..3c30f57
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/forearm_electric_cvx.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/knuckle.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/knuckle.stl
new file mode 100644
index 0000000..4faedd7
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/knuckle.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/lfmetacarpal.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/lfmetacarpal.stl
new file mode 100644
index 0000000..535cf4d
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/lfmetacarpal.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/palm.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/palm.stl
new file mode 100644
index 0000000..65e47eb
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/palm.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/wrist.stl b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/wrist.stl
new file mode 100644
index 0000000..420d5f9
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/wrist.stl differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/textures/block.png b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/textures/block.png
new file mode 100644
index 0000000..0243b8f
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/textures/block.png differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/textures/block_hidden.png b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/textures/block_hidden.png
new file mode 100644
index 0000000..e08b861
Binary files /dev/null and b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/assets/textures/block_hidden.png differ
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/fetch/__init__.py b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/fetch/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/fetch/pick_and_place.py b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/fetch/pick_and_place.py
new file mode 100644
index 0000000..c6c5e7e
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/fetch/pick_and_place.py
@@ -0,0 +1,23 @@
+import os
+from gym import utils
+from gym.envs.robotics import fetch_env
+
+
+# Ensure we get the path separator correct on windows
+MODEL_XML_PATH = os.path.join('fetch', 'pick_and_place.xml')
+
+
+class FetchPickAndPlaceEnv(fetch_env.FetchEnv, utils.EzPickle):
+    def __init__(self, reward_type='sparse'):
+        initial_qpos = {
+            'robot0:slide0': 0.405,
+            'robot0:slide1': 0.48,
+            'robot0:slide2': 0.0,
+            'object0:joint': [1.25, 0.53, 0.4, 1., 0., 0., 0.],
+        }
+        fetch_env.FetchEnv.__init__(
+            self, MODEL_XML_PATH, has_object=True, block_gripper=False, n_substeps=20,
+            gripper_extra_height=0.2, target_in_the_air=True, target_offset=0.0,
+            obj_range=0.15, target_range=0.15, distance_threshold=0.05,
+            initial_qpos=initial_qpos, reward_type=reward_type)
+        utils.EzPickle.__init__(self)
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/fetch/push.py b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/fetch/push.py
new file mode 100644
index 0000000..043d101
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/fetch/push.py
@@ -0,0 +1,23 @@
+import os
+from gym import utils
+from slbo.envs.robotics import fetch_env
+
+
+# Ensure we get the path separator correct on windows
+MODEL_XML_PATH = os.path.join('fetch', 'push.xml')
+
+
+class FetchPushEnv(fetch_env.FetchEnv, utils.EzPickle):
+    def __init__(self, reward_type='sparse'):
+        initial_qpos = {
+            'robot0:slide0': 0.405,
+            'robot0:slide1': 0.48,
+            'robot0:slide2': 0.0,
+            'object0:joint': [1.25, 0.53, 0.4, 1., 0., 0., 0.],
+        }
+        fetch_env.FetchEnv.__init__(
+            self, MODEL_XML_PATH, has_object=True, block_gripper=True, n_substeps=20,
+            gripper_extra_height=0.0, target_in_the_air=False, target_offset=0.0,
+            obj_range=0.15, target_range=0.15, distance_threshold=0.05,
+            initial_qpos=initial_qpos, reward_type=reward_type)
+        utils.EzPickle.__init__(self)
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/fetch/reach.py b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/fetch/reach.py
new file mode 100644
index 0000000..cc3fc46
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/fetch/reach.py
@@ -0,0 +1,22 @@
+import os
+from gym import utils
+from gym.envs.robotics import fetch_env
+
+
+# Ensure we get the path separator correct on windows
+MODEL_XML_PATH = os.path.join('fetch', 'reach.xml')
+
+
+class FetchReachEnv(fetch_env.FetchEnv, utils.EzPickle):
+    def __init__(self, reward_type='sparse'):
+        initial_qpos = {
+            'robot0:slide0': 0.4049,
+            'robot0:slide1': 0.48,
+            'robot0:slide2': 0.0,
+        }
+        fetch_env.FetchEnv.__init__(
+            self, MODEL_XML_PATH, has_object=False, block_gripper=True, n_substeps=20,
+            gripper_extra_height=0.2, target_in_the_air=True, target_offset=0.0,
+            obj_range=0.15, target_range=0.15, distance_threshold=0.05,
+            initial_qpos=initial_qpos, reward_type=reward_type)
+        utils.EzPickle.__init__(self)
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/fetch/slide.py b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/fetch/slide.py
new file mode 100644
index 0000000..63234db
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/fetch/slide.py
@@ -0,0 +1,25 @@
+import os
+import numpy as np
+
+from gym import utils
+from gym.envs.robotics import fetch_env
+
+
+# Ensure we get the path separator correct on windows
+MODEL_XML_PATH = os.path.join('fetch', 'slide.xml')
+
+
+class FetchSlideEnv(fetch_env.FetchEnv, utils.EzPickle):
+    def __init__(self, reward_type='sparse'):
+        initial_qpos = {
+            'robot0:slide0': 0.05,
+            'robot0:slide1': 0.48,
+            'robot0:slide2': 0.0,
+            'object0:joint': [1.7, 1.1, 0.41, 1., 0., 0., 0.],
+        }
+        fetch_env.FetchEnv.__init__(
+            self, MODEL_XML_PATH, has_object=True, block_gripper=True, n_substeps=20,
+            gripper_extra_height=-0.02, target_in_the_air=False, target_offset=np.array([0.4, 0.0, 0.0]),
+            obj_range=0.1, target_range=0.3, distance_threshold=0.05,
+            initial_qpos=initial_qpos, reward_type=reward_type)
+        utils.EzPickle.__init__(self)
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/fetch_env.py b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/fetch_env.py
new file mode 100644
index 0000000..8e19c42
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/fetch_env.py
@@ -0,0 +1,197 @@
+import numpy as np
+
+from gym.envs.robotics import rotations, robot_env, utils
+
+
+def goal_distance(goal_a, goal_b):
+    assert goal_a.shape == goal_b.shape
+    return np.linalg.norm(goal_a - goal_b, axis=-1)
+
+
+class FetchEnv(robot_env.RobotEnv):
+    """Superclass for all Fetch environments.
+    """
+
+    def __init__(
+        self, model_path, n_substeps, gripper_extra_height, block_gripper,
+        has_object, target_in_the_air, target_offset, obj_range, target_range,
+        distance_threshold, initial_qpos, reward_type,
+    ):
+        """Initializes a new Fetch environment.
+
+        Args:
+            model_path (string): path to the environments XML file
+            n_substeps (int): number of substeps the simulation runs on every call to step
+            gripper_extra_height (float): additional height above the table when positioning the gripper
+            block_gripper (boolean): whether or not the gripper is blocked (i.e. not movable) or not
+            has_object (boolean): whether or not the environment has an object
+            target_in_the_air (boolean): whether or not the target should be in the air above the table or on the table surface
+            target_offset (float or array with 3 elements): offset of the target
+            obj_range (float): range of a uniform distribution for sampling initial object positions
+            target_range (float): range of a uniform distribution for sampling a target
+            distance_threshold (float): the threshold after which a goal is considered achieved
+            initial_qpos (dict): a dictionary of joint names and values that define the initial configuration
+            reward_type ('sparse' or 'dense'): the reward type, i.e. sparse or dense
+        """
+        self.gripper_extra_height = gripper_extra_height
+        self.block_gripper = block_gripper
+        self.has_object = has_object
+        self.target_in_the_air = target_in_the_air
+        self.target_offset = target_offset
+        self.obj_range = obj_range
+        self.target_range = target_range
+        self.distance_threshold = distance_threshold
+        self.reward_type = reward_type
+
+        super(FetchEnv, self).__init__(
+            model_path=model_path, n_substeps=n_substeps, n_actions=4,
+            initial_qpos=initial_qpos)
+
+    # GoalEnv methods
+    # ----------------------------
+
+    def compute_reward(self, achieved_goal, goal, info):
+        # Compute distance between goal and the achieved goal.
+        d = goal_distance(achieved_goal, goal)
+        return -(d > self.distance_threshold).astype(np.float32)
+
+    # RobotEnv methods
+    # ----------------------------
+
+    def _step_callback(self):
+        if self.block_gripper:
+            self.sim.data.set_joint_qpos('robot0:l_gripper_finger_joint', 0.)
+            self.sim.data.set_joint_qpos('robot0:r_gripper_finger_joint', 0.)
+            self.sim.forward()
+
+    def _set_action(self, action):
+        assert action.shape == (4,)
+        action = action.copy()  # ensure that we don't change the action outside of this scope
+        pos_ctrl, gripper_ctrl = action[:3], action[3]
+
+        pos_ctrl *= 0.05  # limit maximum change in position
+        rot_ctrl = [1., 0., 1., 0.]  # fixed rotation of the end effector, expressed as a quaternion
+        gripper_ctrl = np.array([gripper_ctrl, gripper_ctrl])
+        assert gripper_ctrl.shape == (2,)
+        if self.block_gripper:
+            gripper_ctrl = np.zeros_like(gripper_ctrl)
+        action = np.concatenate([pos_ctrl, rot_ctrl, gripper_ctrl])
+
+        # Apply action to simulation.
+        utils.ctrl_set_action(self.sim, action)
+        utils.mocap_set_action(self.sim, action)
+
+    def _get_obs(self):
+        # positions
+        grip_pos = self.sim.data.get_site_xpos('robot0:grip')
+        dt = self.sim.nsubsteps * self.sim.model.opt.timestep
+        grip_velp = self.sim.data.get_site_xvelp('robot0:grip') * dt
+        robot_qpos, robot_qvel = utils.robot_get_obs(self.sim)
+        if self.has_object:
+            object_pos = self.sim.data.get_site_xpos('object0')
+            # rotations
+            object_rot = rotations.mat2euler(self.sim.data.get_site_xmat('object0'))
+            # velocities
+            object_velp = self.sim.data.get_site_xvelp('object0') * dt
+            object_velr = self.sim.data.get_site_xvelr('object0') * dt
+            # gripper state
+            object_rel_pos = object_pos - grip_pos
+            object_velp -= grip_velp
+        else:
+            object_pos = object_rot = object_velp = object_velr = object_rel_pos = np.zeros(0)
+        gripper_state = robot_qpos[-2:]
+        gripper_vel = robot_qvel[-2:] * dt  # change to a scalar if the gripper is made symmetric
+
+        if not self.has_object:
+            achieved_goal = grip_pos.copy()
+        else:
+            achieved_goal = np.squeeze(object_pos.copy())
+        obs = np.concatenate([
+            grip_pos, object_pos.ravel(), object_rel_pos.ravel(), gripper_state, object_rot.ravel(),
+            object_velp.ravel(), object_velr.ravel(), grip_velp, gripper_vel,
+        ])
+
+        return {
+            'observation': obs.copy(),
+            'achieved_goal': achieved_goal.copy(),
+            'desired_goal': self.goal.copy(),
+        }
+
+    def mb_step(self, states, actions, next_states):
+        a = next_states[:,:3]
+        b = next_states[:,3:6]
+        d = np.linalg.norm(a - b, axis=-1)
+        rewards = -(d <= self.distance_threshold).astype(np.float32) 
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
+
+    def _viewer_setup(self):
+        body_id = self.sim.model.body_name2id('robot0:gripper_link')
+        lookat = self.sim.data.body_xpos[body_id]
+        for idx, value in enumerate(lookat):
+            self.viewer.cam.lookat[idx] = value
+        self.viewer.cam.distance = 2.5
+        self.viewer.cam.azimuth = 132.
+        self.viewer.cam.elevation = -14.
+
+    def _render_callback(self):
+        # Visualize target.
+        sites_offset = (self.sim.data.site_xpos - self.sim.model.site_pos).copy()
+        site_id = self.sim.model.site_name2id('target0')
+        self.sim.model.site_pos[site_id] = self.goal - sites_offset[0]
+        self.sim.forward()
+
+    def _reset_sim(self):
+        self.sim.set_state(self.initial_state)
+
+        # Randomize start position of object.
+        if self.has_object:
+            object_xpos = self.initial_gripper_xpos[:2]
+            while np.linalg.norm(object_xpos - self.initial_gripper_xpos[:2]) < 0.1:
+                object_xpos = self.initial_gripper_xpos[:2] + self.np_random.uniform(-self.obj_range, self.obj_range, size=2)
+            object_qpos = self.sim.data.get_joint_qpos('object0:joint')
+            assert object_qpos.shape == (7,)
+            object_qpos[:2] = object_xpos
+            self.sim.data.set_joint_qpos('object0:joint', object_qpos)
+
+        self.sim.forward()
+        return True
+
+    def _sample_goal(self):
+        if self.has_object:
+            goal = self.initial_gripper_xpos[:3] + self.np_random.uniform(-self.target_range, self.target_range, size=3)
+            goal += self.target_offset
+            goal[2] = self.height_offset
+            if self.target_in_the_air and self.np_random.uniform() < 0.5:
+                goal[2] += self.np_random.uniform(0, 0.45)
+        else:
+            goal = self.initial_gripper_xpos[:3] + self.np_random.uniform(-self.target_range, self.target_range, size=3)
+        return goal.copy()
+
+    def _is_success(self, achieved_goal, desired_goal):
+        d = goal_distance(achieved_goal, desired_goal)
+        return (d < self.distance_threshold).astype(np.float32)
+
+    def _env_setup(self, initial_qpos):
+        for name, value in initial_qpos.items():
+            self.sim.data.set_joint_qpos(name, value)
+        utils.reset_mocap_welds(self.sim)
+        self.sim.forward()
+
+        # Move end effector into position.
+        gripper_target = np.array([-0.498, 0.005, -0.431 + self.gripper_extra_height]) + self.sim.data.get_site_xpos('robot0:grip')
+        gripper_rotation = np.array([1., 0., 1., 0.])
+        self.sim.data.set_mocap_pos('robot0:mocap', gripper_target)
+        self.sim.data.set_mocap_quat('robot0:mocap', gripper_rotation)
+        for _ in range(10):
+            self.sim.step()
+
+        # Extract information for sampling goals.
+        self.initial_gripper_xpos = self.sim.data.get_site_xpos('robot0:grip').copy()
+        if self.has_object:
+            self.height_offset = self.sim.data.get_site_xpos('object0')[2]
+
+    def render(self, mode='human', width=500, height=500):
+        return super(FetchEnv, self).render(mode, width, height)
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/hand/__init__.py b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/hand/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/hand/manipulate.py b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/hand/manipulate.py
new file mode 100644
index 0000000..85a7dd0
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/hand/manipulate.py
@@ -0,0 +1,344 @@
+import os
+import numpy as np
+
+from gym import utils, error
+from gym.envs.robotics import rotations, hand_env
+from gym.envs.robotics.utils import robot_get_obs
+from gym.wrappers import FlattenObservation
+
+
+from slbo.utils.dataset import Dataset, gen_dtype
+
+try:
+    import mujoco_py
+except ImportError as e:
+    raise error.DependencyNotInstalled("{}. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)".format(e))
+
+
+def quat_from_angle_and_axis(angle, axis):
+    assert axis.shape == (3,)
+    axis /= np.linalg.norm(axis)
+    quat = np.concatenate([[np.cos(angle / 2.)], np.sin(angle / 2.) * axis])
+    quat /= np.linalg.norm(quat)
+    return quat
+
+
+# Ensure we get the path separator correct on windows
+MANIPULATE_BLOCK_XML = os.path.join('hand', 'manipulate_block.xml')
+MANIPULATE_EGG_XML = os.path.join('hand', 'manipulate_egg.xml')
+MANIPULATE_PEN_XML = os.path.join('hand', 'manipulate_pen.xml')
+
+
+class ManipulateEnv(hand_env.HandEnv):
+    def __init__(
+        self, model_path, target_position, target_rotation,
+        target_position_range, reward_type, initial_qpos=None,
+        randomize_initial_position=True, randomize_initial_rotation=True,
+        distance_threshold=0.01, rotation_threshold=0.1, n_substeps=20, relative_control=False,
+        ignore_z_target_rotation=False,
+    ):
+        """Initializes a new Hand manipulation environment.
+
+        Args:
+            model_path (string): path to the environments XML file
+            target_position (string): the type of target position:
+                - ignore: target position is fully ignored, i.e. the object can be positioned arbitrarily
+                - fixed: target position is set to the initial position of the object
+                - random: target position is fully randomized according to target_position_range
+            target_rotation (string): the type of target rotation:
+                - ignore: target rotation is fully ignored, i.e. the object can be rotated arbitrarily
+                - fixed: target rotation is set to the initial rotation of the object
+                - xyz: fully randomized target rotation around the X, Y and Z axis
+                - z: fully randomized target rotation around the Z axis
+                - parallel: fully randomized target rotation around Z and axis-aligned rotation around X, Y
+            ignore_z_target_rotation (boolean): whether or not the Z axis of the target rotation is ignored
+            target_position_range (np.array of shape (3, 2)): range of the target_position randomization
+            reward_type ('sparse' or 'dense'): the reward type, i.e. sparse or dense
+            initial_qpos (dict): a dictionary of joint names and values that define the initial configuration
+            randomize_initial_position (boolean): whether or not to randomize the initial position of the object
+            randomize_initial_rotation (boolean): whether or not to randomize the initial rotation of the object
+            distance_threshold (float, in meters): the threshold after which the position of a goal is considered achieved
+            rotation_threshold (float, in radians): the threshold after which the rotation of a goal is considered achieved
+            n_substeps (int): number of substeps the simulation runs on every call to step
+            relative_control (boolean): whether or not the hand is actuated in absolute joint positions or relative to the current state
+        """
+        self.target_position = target_position
+        self.target_rotation = target_rotation
+        self.target_position_range = target_position_range
+        self.parallel_quats = [rotations.euler2quat(r) for r in rotations.get_parallel_rotations()]
+        self.randomize_initial_rotation = randomize_initial_rotation
+        self.randomize_initial_position = randomize_initial_position
+        self.distance_threshold = distance_threshold
+        self.rotation_threshold = rotation_threshold
+        self.reward_type = reward_type
+        self.ignore_z_target_rotation = ignore_z_target_rotation
+
+        assert self.target_position in ['ignore', 'fixed', 'random']
+        assert self.target_rotation in ['ignore', 'fixed', 'xyz', 'z', 'parallel']
+        initial_qpos = initial_qpos or {}
+
+        hand_env.HandEnv.__init__(
+            self, model_path, n_substeps=n_substeps, initial_qpos=initial_qpos,
+            relative_control=relative_control)
+
+
+    def _get_achieved_goal(self):
+        # Object position and rotation.
+        object_qpos = self.sim.data.get_joint_qpos('object:joint')
+        assert object_qpos.shape == (7,)
+        return object_qpos
+
+    def _goal_distance(self, goal_a, goal_b):
+        assert goal_a.shape == goal_b.shape
+        assert goal_a.shape[-1] == 7
+
+        d_pos = np.zeros_like(goal_a[..., 0])
+        d_rot = np.zeros_like(goal_b[..., 0])
+        if self.target_position != 'ignore':
+            delta_pos = goal_a[..., :3] - goal_b[..., :3]
+            d_pos = np.linalg.norm(delta_pos, axis=-1)
+
+        if self.target_rotation != 'ignore':
+            quat_a, quat_b = goal_a[..., 3:], goal_b[..., 3:]
+
+            if self.ignore_z_target_rotation:
+                # Special case: We want to ignore the Z component of the rotation.
+                # This code here assumes Euler angles with xyz convention. We first transform
+                # to euler, then set the Z component to be equal between the two, and finally
+                # transform back into quaternions.
+                euler_a = rotations.quat2euler(quat_a)
+                euler_b = rotations.quat2euler(quat_b)
+                euler_a[2] = euler_b[2]
+                quat_a = rotations.euler2quat(euler_a)
+
+            # Subtract quaternions and extract angle between them.
+            quat_diff = rotations.quat_mul(quat_a, rotations.quat_conjugate(quat_b))
+            angle_diff = 2 * np.arccos(np.clip(quat_diff[..., 0], -1., 1.))
+            d_rot = angle_diff
+        assert d_pos.shape == d_rot.shape
+        return d_pos, d_rot
+
+    # GoalEnv methods
+    # ----------------------------
+
+    def compute_reward(self, achieved_goal, goal, info):
+        if self.reward_type == 'sparse':
+            success = self._is_success(achieved_goal, goal).astype(np.float32)
+            return success 
+        else:
+            d_pos, d_rot = self._goal_distance(achieved_goal, goal)
+            # We weigh the difference in position to avoid that `d_pos` (in meters) is completely
+            # dominated by `d_rot` (in radians).
+            return -(10. * d_pos + d_rot)
+
+    # RobotEnv methods
+    # ----------------------------
+
+    def _is_success(self, achieved_goal, desired_goal):
+        d_pos, d_rot = self._goal_distance(achieved_goal, desired_goal)
+        achieved_pos = (d_pos < self.distance_threshold).astype(np.float32)
+        achieved_rot = (d_rot < self.rotation_threshold).astype(np.float32)
+        achieved_both = achieved_pos * achieved_rot
+        return achieved_both
+
+    def _env_setup(self, initial_qpos):
+        for name, value in initial_qpos.items():
+            self.sim.data.set_joint_qpos(name, value)
+        self.sim.forward()
+
+    def _reset_sim(self):
+        self.sim.set_state(self.initial_state)
+        self.sim.forward()
+
+        initial_qpos = self.sim.data.get_joint_qpos('object:joint').copy()
+        initial_pos, initial_quat = initial_qpos[:3], initial_qpos[3:]
+        assert initial_qpos.shape == (7,)
+        assert initial_pos.shape == (3,)
+        assert initial_quat.shape == (4,)
+        initial_qpos = None
+
+        # Randomization initial rotation.
+        if self.randomize_initial_rotation:
+            if self.target_rotation == 'z':
+                angle = self.np_random.uniform(-np.pi, np.pi)
+                axis = np.array([0., 0., 1.])
+                offset_quat = quat_from_angle_and_axis(angle, axis)
+                initial_quat = rotations.quat_mul(initial_quat, offset_quat)
+            elif self.target_rotation == 'parallel':
+                angle = self.np_random.uniform(-np.pi, np.pi)
+                axis = np.array([0., 0., 1.])
+                z_quat = quat_from_angle_and_axis(angle, axis)
+                parallel_quat = self.parallel_quats[self.np_random.randint(len(self.parallel_quats))]
+                offset_quat = rotations.quat_mul(z_quat, parallel_quat)
+                initial_quat = rotations.quat_mul(initial_quat, offset_quat)
+            elif self.target_rotation in ['xyz', 'ignore']:
+                angle = self.np_random.uniform(-np.pi, np.pi)
+                axis = self.np_random.uniform(-1., 1., size=3)
+                offset_quat = quat_from_angle_and_axis(angle, axis)
+                initial_quat = rotations.quat_mul(initial_quat, offset_quat)
+            elif self.target_rotation == 'fixed':
+                pass
+            else:
+                raise error.Error('Unknown target_rotation option "{}".'.format(self.target_rotation))
+
+        # Randomize initial position.
+        if self.randomize_initial_position:
+            if self.target_position != 'fixed':
+                initial_pos += self.np_random.normal(size=3, scale=0.005)
+
+        initial_quat /= np.linalg.norm(initial_quat)
+        initial_qpos = np.concatenate([initial_pos, initial_quat])
+        self.sim.data.set_joint_qpos('object:joint', initial_qpos)
+
+        def is_on_palm():
+            self.sim.forward()
+            cube_middle_idx = self.sim.model.site_name2id('object:center')
+            cube_middle_pos = self.sim.data.site_xpos[cube_middle_idx]
+            is_on_palm = (cube_middle_pos[2] > 0.04)
+            return is_on_palm
+
+        # Run the simulation for a bunch of timesteps to let everything settle in.
+        for _ in range(10):
+            self._set_action(np.zeros(20))
+            try:
+                self.sim.step()
+            except mujoco_py.MujocoException:
+                return False
+        return is_on_palm()
+
+    def _sample_goal(self):
+        # Select a goal for the object position.
+        target_pos = None
+        if self.target_position == 'random':
+            assert self.target_position_range.shape == (3, 2)
+            offset = self.np_random.uniform(self.target_position_range[:, 0], self.target_position_range[:, 1])
+            assert offset.shape == (3,)
+            target_pos = self.sim.data.get_joint_qpos('object:joint')[:3] + offset
+        elif self.target_position in ['ignore', 'fixed']:
+            target_pos = self.sim.data.get_joint_qpos('object:joint')[:3]
+        else:
+            raise error.Error('Unknown target_position option "{}".'.format(self.target_position))
+        assert target_pos is not None
+        assert target_pos.shape == (3,)
+
+        # Select a goal for the object rotation.
+        target_quat = None
+        if self.target_rotation == 'z':
+            angle = self.np_random.uniform(-np.pi, np.pi)
+            axis = np.array([0., 0., 1.])
+            target_quat = quat_from_angle_and_axis(angle, axis)
+        elif self.target_rotation == 'parallel':
+            angle = self.np_random.uniform(-np.pi, np.pi)
+            axis = np.array([0., 0., 1.])
+            target_quat = quat_from_angle_and_axis(angle, axis)
+            parallel_quat = self.parallel_quats[self.np_random.randint(len(self.parallel_quats))]
+            target_quat = rotations.quat_mul(target_quat, parallel_quat)
+        elif self.target_rotation == 'xyz':
+            angle = self.np_random.uniform(-np.pi, np.pi)
+            axis = self.np_random.uniform(-1., 1., size=3)
+            target_quat = quat_from_angle_and_axis(angle, axis)
+        elif self.target_rotation in ['ignore', 'fixed']:
+            target_quat = self.sim.data.get_joint_qpos('object:joint')[3:]
+        else:
+            raise error.Error('Unknown target_rotation option "{}".'.format(self.target_rotation))
+        assert target_quat is not None
+        assert target_quat.shape == (4,)
+
+        target_quat /= np.linalg.norm(target_quat)  # normalized quaternion
+        goal = np.concatenate([target_pos, target_quat])
+        return goal
+
+    def _render_callback(self):
+        # Assign current state to target object but offset a bit so that the actual object
+        # is not obscured.
+        goal = self.goal.copy()
+        assert goal.shape == (7,)
+        if self.target_position == 'ignore':
+            # Move the object to the side since we do not care about it's position.
+            goal[0] += 0.15
+        self.sim.data.set_joint_qpos('target:joint', goal)
+        self.sim.data.set_joint_qvel('target:joint', np.zeros(6))
+
+        if 'object_hidden' in self.sim.model.geom_names:
+            hidden_id = self.sim.model.geom_name2id('object_hidden')
+            self.sim.model.geom_rgba[hidden_id, 3] = 1.
+        self.sim.forward()
+
+    def _get_obs(self):
+        robot_qpos, robot_qvel = robot_get_obs(self.sim)
+        object_qvel = self.sim.data.get_joint_qvel('object:joint')
+        achieved_goal = self._get_achieved_goal().ravel()  # this contains the object position + rotation
+        observation = np.concatenate([robot_qpos, robot_qvel, object_qvel, achieved_goal])
+        return {
+            'observation': observation.copy(),
+            'achieved_goal': achieved_goal.copy(),
+            'desired_goal': self.goal.ravel().copy(),
+        }
+
+
+class HandBlockEnv(ManipulateEnv, utils.EzPickle):
+    def __init__(self, target_position='random', target_rotation='xyz', reward_type='sparse'):
+        utils.EzPickle.__init__(self, target_position, target_rotation, reward_type)
+        ManipulateEnv.__init__(self,
+            model_path=MANIPULATE_BLOCK_XML, target_position=target_position,
+            target_rotation=target_rotation,
+            target_position_range=np.array([(-0.04, 0.04), (-0.06, 0.02), (0.0, 0.06)]),
+            reward_type=reward_type)
+
+
+class HandEggEnv(ManipulateEnv, utils.EzPickle):
+    def __init__(self, target_position='random', target_rotation='fixed', reward_type='sparse'):
+        utils.EzPickle.__init__(self, target_position, target_rotation, reward_type)
+        ManipulateEnv.__init__(self,
+            model_path=MANIPULATE_EGG_XML, target_position=target_position,
+            target_rotation=target_rotation,
+            target_position_range=np.array([(-0.04, 0.04), (-0.06, 0.02), (0.0, 0.06)]),
+            reward_type=reward_type)
+
+
+    def verify(self, n=2000, eps=1e-4):
+        dummy = FlattenObservation(self)
+
+        dataset = Dataset(gen_dtype(dummy, 'state action next_state reward done'), n)
+        state = dummy.reset()
+        #print(state)
+        for _ in range(n):
+            action = dummy.action_space.sample()
+            next_state, reward, done, _ = dummy.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = dummy.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        # logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
+
+    def mb_step(self, states, actions, next_states):
+        a = next_states[:,:7]
+        b = next_states[:,7:14]
+        if self.reward_type == 'sparse':
+            success = self._is_success(a, b).astype(np.float32)
+            rewards = (success * 10 - 1.)
+        else:
+            d_pos, d_rot = self._goal_distance(a, b)
+            # We weigh the difference in position to avoid that `d_pos` (in meters) is completely
+            # dominated by `d_rot` (in radians).
+            rewards = -(10. * d_pos + d_rot)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+
+class HandPenEnv(ManipulateEnv, utils.EzPickle):
+    def __init__(self, target_position='random', target_rotation='xyz', reward_type='sparse'):
+        utils.EzPickle.__init__(self, target_position, target_rotation, reward_type)
+        ManipulateEnv.__init__(self,
+            model_path=MANIPULATE_PEN_XML, target_position=target_position,
+            target_rotation=target_rotation,
+            target_position_range=np.array([(-0.04, 0.04), (-0.06, 0.02), (0.0, 0.06)]),
+            randomize_initial_rotation=False, reward_type=reward_type,
+            ignore_z_target_rotation=True, distance_threshold=0.05)
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/hand/manipulate_touch_sensors.py b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/hand/manipulate_touch_sensors.py
new file mode 100644
index 0000000..c364868
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/hand/manipulate_touch_sensors.py
@@ -0,0 +1,131 @@
+import os
+import numpy as np
+
+from gym import utils, error, spaces
+from gym.envs.robotics.hand import manipulate
+
+# Ensure we get the path separator correct on windows
+MANIPULATE_BLOCK_XML = os.path.join('hand', 'manipulate_block_touch_sensors.xml')
+MANIPULATE_EGG_XML = os.path.join('hand', 'manipulate_egg_touch_sensors.xml')
+MANIPULATE_PEN_XML = os.path.join('hand', 'manipulate_pen_touch_sensors.xml')
+
+
+class ManipulateTouchSensorsEnv(manipulate.ManipulateEnv):
+    def __init__(
+        self, model_path, target_position, target_rotation,
+        target_position_range, reward_type, initial_qpos={},
+        randomize_initial_position=True, randomize_initial_rotation=True,
+        distance_threshold=0.01, rotation_threshold=0.1, n_substeps=20, relative_control=False,
+        ignore_z_target_rotation=False, touch_visualisation="on_touch", touch_get_obs="sensordata",
+    ):
+        """Initializes a new Hand manipulation environment with touch sensors.
+
+        Args:
+            touch_visualisation (string): how touch sensor sites are visualised
+                - "on_touch": shows touch sensor sites only when touch values > 0
+                - "always": always shows touch sensor sites
+                - "off" or else: does not show touch sensor sites
+            touch_get_obs (string): touch sensor readings
+                - "boolean": returns 1 if touch sensor reading != 0.0 else 0
+                - "sensordata": returns original touch sensor readings from self.sim.data.sensordata[id]
+                - "log": returns log(x+1) touch sensor readings from self.sim.data.sensordata[id]
+                - "off" or else: does not add touch sensor readings to the observation
+
+        """
+        self.touch_visualisation = touch_visualisation
+        self.touch_get_obs = touch_get_obs
+        self._touch_sensor_id_site_id = []
+        self._touch_sensor_id = []
+        self.touch_color = [1, 0, 0, 0.5]
+        self.notouch_color = [0, 0.5, 0, 0.2]
+
+        manipulate.ManipulateEnv.__init__(
+            self, model_path, target_position, target_rotation,
+            target_position_range, reward_type, initial_qpos=initial_qpos,
+            randomize_initial_position=randomize_initial_position, randomize_initial_rotation=randomize_initial_rotation,
+            distance_threshold=distance_threshold, rotation_threshold=rotation_threshold, n_substeps=n_substeps, relative_control=relative_control,
+            ignore_z_target_rotation=ignore_z_target_rotation,
+        )
+
+        for k, v in self.sim.model._sensor_name2id.items():  # get touch sensor site names and their ids
+            if 'robot0:TS_' in k:
+                self._touch_sensor_id_site_id.append((v, self.sim.model._site_name2id[k.replace('robot0:TS_', 'robot0:T_')]))
+                self._touch_sensor_id.append(v)
+
+        if self.touch_visualisation == 'off':  # set touch sensors rgba values
+            for _, site_id in self._touch_sensor_id_site_id:
+                self.sim.model.site_rgba[site_id][3] = 0.0
+        elif self.touch_visualisation == 'always':
+            pass
+
+        obs = self._get_obs()
+        self.observation_space = spaces.Dict(dict(
+            desired_goal=spaces.Box(-np.inf, np.inf, shape=obs['achieved_goal'].shape, dtype='float32'),
+            achieved_goal=spaces.Box(-np.inf, np.inf, shape=obs['achieved_goal'].shape, dtype='float32'),
+            observation=spaces.Box(-np.inf, np.inf, shape=obs['observation'].shape, dtype='float32'),
+        ))
+
+    def _render_callback(self):
+        super(ManipulateTouchSensorsEnv, self)._render_callback()
+        if self.touch_visualisation == 'on_touch':
+            for touch_sensor_id, site_id in self._touch_sensor_id_site_id:
+                if self.sim.data.sensordata[touch_sensor_id] != 0.0:
+                    self.sim.model.site_rgba[site_id] = self.touch_color
+                else:
+                    self.sim.model.site_rgba[site_id] = self.notouch_color
+
+    def _get_obs(self):
+        robot_qpos, robot_qvel = manipulate.robot_get_obs(self.sim)
+        object_qvel = self.sim.data.get_joint_qvel('object:joint')
+        achieved_goal = self._get_achieved_goal().ravel()  # this contains the object position + rotation
+        touch_values = []  # get touch sensor readings. if there is one, set value to 1
+        if self.touch_get_obs == 'sensordata':
+            touch_values = self.sim.data.sensordata[self._touch_sensor_id]
+        elif self.touch_get_obs == 'boolean':
+            touch_values = self.sim.data.sensordata[self._touch_sensor_id] > 0.0
+        elif self.touch_get_obs == 'log':
+            touch_values = np.log(self.sim.data.sensordata[self._touch_sensor_id] + 1.0)
+        observation = np.concatenate([robot_qpos, robot_qvel, object_qvel, touch_values, achieved_goal])
+
+        return {
+            'observation': observation.copy(),
+            'achieved_goal': achieved_goal.copy(),
+            'desired_goal': self.goal.ravel().copy(),
+        }
+
+
+class HandBlockTouchSensorsEnv(ManipulateTouchSensorsEnv, utils.EzPickle):
+    def __init__(self, target_position='random', target_rotation='xyz', touch_get_obs='sensordata', reward_type='sparse'):
+        utils.EzPickle.__init__(self, target_position, target_rotation, touch_get_obs, reward_type)
+        ManipulateTouchSensorsEnv.__init__(self,
+            model_path=MANIPULATE_BLOCK_XML,
+            touch_get_obs=touch_get_obs,
+            target_rotation=target_rotation,
+            target_position=target_position,
+            target_position_range=np.array([(-0.04, 0.04), (-0.06, 0.02), (0.0, 0.06)]),
+            reward_type=reward_type)
+
+
+class HandEggTouchSensorsEnv(ManipulateTouchSensorsEnv, utils.EzPickle):
+    def __init__(self, target_position='random', target_rotation='xyz', touch_get_obs='sensordata', reward_type='sparse'):
+        utils.EzPickle.__init__(self, target_position, target_rotation, touch_get_obs, reward_type)
+        ManipulateTouchSensorsEnv.__init__(self,
+            model_path=MANIPULATE_EGG_XML,
+            touch_get_obs=touch_get_obs,
+            target_rotation=target_rotation,
+            target_position=target_position,
+            target_position_range=np.array([(-0.04, 0.04), (-0.06, 0.02), (0.0, 0.06)]),
+            reward_type=reward_type)
+
+
+class HandPenTouchSensorsEnv(ManipulateTouchSensorsEnv, utils.EzPickle):
+    def __init__(self, target_position='random', target_rotation='xyz', touch_get_obs='sensordata', reward_type='sparse'):
+        utils.EzPickle.__init__(self, target_position, target_rotation, touch_get_obs, reward_type)
+        ManipulateTouchSensorsEnv.__init__(self,
+            model_path=MANIPULATE_PEN_XML,
+            touch_get_obs=touch_get_obs,
+            target_rotation=target_rotation,
+            target_position=target_position,
+            target_position_range=np.array([(-0.04, 0.04), (-0.06, 0.02), (0.0, 0.06)]),
+            randomize_initial_rotation=False, reward_type=reward_type,
+            ignore_z_target_rotation=True, distance_threshold=0.05)
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/hand/reach.py b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/hand/reach.py
new file mode 100644
index 0000000..1459e5a
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/hand/reach.py
@@ -0,0 +1,159 @@
+import os
+import numpy as np
+
+from gym import utils
+from slbo.envs.robotics import hand_env
+from gym.envs.robotics.utils import robot_get_obs
+
+
+FINGERTIP_SITE_NAMES = [
+    'robot0:S_fftip',
+    'robot0:S_mftip',
+    'robot0:S_rftip',
+    'robot0:S_lftip',
+    'robot0:S_thtip',
+]
+
+
+DEFAULT_INITIAL_QPOS = {
+    'robot0:WRJ1': -0.16514339750464327,
+    'robot0:WRJ0': -0.31973286565062153,
+    'robot0:FFJ3': 0.14340512546557435,
+    'robot0:FFJ2': 0.32028208333591573,
+    'robot0:FFJ1': 0.7126053607727917,
+    'robot0:FFJ0': 0.6705281001412586,
+    'robot0:MFJ3': 0.000246444303701037,
+    'robot0:MFJ2': 0.3152655251085491,
+    'robot0:MFJ1': 0.7659800313729842,
+    'robot0:MFJ0': 0.7323156897425923,
+    'robot0:RFJ3': 0.00038520700007378114,
+    'robot0:RFJ2': 0.36743546201985233,
+    'robot0:RFJ1': 0.7119514095008576,
+    'robot0:RFJ0': 0.6699446327514138,
+    'robot0:LFJ4': 0.0525442258033891,
+    'robot0:LFJ3': -0.13615534724474673,
+    'robot0:LFJ2': 0.39872030433433003,
+    'robot0:LFJ1': 0.7415570009679252,
+    'robot0:LFJ0': 0.704096378652974,
+    'robot0:THJ4': 0.003673823825070126,
+    'robot0:THJ3': 0.5506291436028695,
+    'robot0:THJ2': -0.014515151997119306,
+    'robot0:THJ1': -0.0015229223564485414,
+    'robot0:THJ0': -0.7894883021600622,
+}
+
+
+# Ensure we get the path separator correct on windows
+MODEL_XML_PATH = os.path.join('hand', 'reach.xml')
+
+
+def goal_distance(goal_a, goal_b):
+    assert goal_a.shape == goal_b.shape
+    return np.linalg.norm(goal_a - goal_b, axis=-1)
+
+
+class HandReachEnv(hand_env.HandEnv, utils.EzPickle):
+    def __init__(
+        self, distance_threshold=0.01, n_substeps=20, relative_control=False,
+        initial_qpos=DEFAULT_INITIAL_QPOS, reward_type='sparse',
+    ):
+        utils.EzPickle.__init__(**locals())
+        self.distance_threshold = distance_threshold
+        self.reward_type = 'sparse'
+
+        hand_env.HandEnv.__init__(
+            self, MODEL_XML_PATH, n_substeps=n_substeps, initial_qpos=initial_qpos,
+            relative_control=relative_control)
+
+    def _get_achieved_goal(self):
+        goal = [self.sim.data.get_site_xpos(name) for name in FINGERTIP_SITE_NAMES]
+        return np.array(goal).flatten()
+
+    # GoalEnv methods
+    # ----------------------------
+
+    def compute_reward(self, achieved_goal, goal, info):
+        d = goal_distance(achieved_goal, goal)
+        if self.reward_type == 'sparse':
+            return -(d > self.distance_threshold).astype(np.float32)
+        else:
+            return -d
+
+    # RobotEnv methods
+    # ----------------------------
+
+    def mb_step(self, states, actions, next_states):
+        a = next_states[:,:15]
+        b = next_states[:,15:30]
+        d = goal_distance(achieved_goal, goal)
+        if self.reward_type == 'sparse':
+            rewards =  -(d > self.distance_threshold).astype(np.float32)
+        else:
+            rewards =  -d
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def _env_setup(self, initial_qpos):
+        for name, value in initial_qpos.items():
+            self.sim.data.set_joint_qpos(name, value)
+        self.sim.forward()
+
+        self.initial_goal = self._get_achieved_goal().copy()
+        self.palm_xpos = self.sim.data.body_xpos[self.sim.model.body_name2id('robot0:palm')].copy()
+
+    def _get_obs(self):
+        robot_qpos, robot_qvel = robot_get_obs(self.sim)
+        achieved_goal = self._get_achieved_goal().ravel()
+        observation = np.concatenate([robot_qpos, robot_qvel, achieved_goal])
+        return {
+            'observation': observation.copy(),
+            'achieved_goal': achieved_goal.copy(),
+            'desired_goal': self.goal.copy(),
+        }
+
+    def _sample_goal(self):
+        thumb_name = 'robot0:S_thtip'
+        finger_names = [name for name in FINGERTIP_SITE_NAMES if name != thumb_name]
+        finger_name = self.np_random.choice(finger_names)
+
+        thumb_idx = FINGERTIP_SITE_NAMES.index(thumb_name)
+        finger_idx = FINGERTIP_SITE_NAMES.index(finger_name)
+        assert thumb_idx != finger_idx
+
+        # Pick a meeting point above the hand.
+        meeting_pos = self.palm_xpos + np.array([0.0, -0.09, 0.05])
+        meeting_pos += self.np_random.normal(scale=0.005, size=meeting_pos.shape)
+
+        # Slightly move meeting goal towards the respective finger to avoid that they
+        # overlap.
+        goal = self.initial_goal.copy().reshape(-1, 3)
+        for idx in [thumb_idx, finger_idx]:
+            offset_direction = (meeting_pos - goal[idx])
+            offset_direction /= np.linalg.norm(offset_direction)
+            goal[idx] = meeting_pos - 0.005 * offset_direction
+
+        if self.np_random.uniform() < 0.1:
+            # With some probability, ask all fingers to move back to the origin.
+            # This avoids that the thumb constantly stays near the goal position already.
+            goal = self.initial_goal.copy()
+        return goal.flatten()
+
+    def _is_success(self, achieved_goal, desired_goal):
+        d = goal_distance(achieved_goal, desired_goal)
+        return (d < self.distance_threshold).astype(np.float32)
+
+    def _render_callback(self):
+        # Visualize targets.
+        sites_offset = (self.sim.data.site_xpos - self.sim.model.site_pos).copy()
+        goal = self.goal.reshape(5, 3)
+        for finger_idx in range(5):
+            site_name = 'target{}'.format(finger_idx)
+            site_id = self.sim.model.site_name2id(site_name)
+            self.sim.model.site_pos[site_id] = goal[finger_idx] - sites_offset[site_id]
+
+        # Visualize finger positions.
+        achieved_goal = self._get_achieved_goal().reshape(5, 3)
+        for finger_idx in range(5):
+            site_name = 'finger{}'.format(finger_idx)
+            site_id = self.sim.model.site_name2id(site_name)
+            self.sim.model.site_pos[site_id] = achieved_goal[finger_idx] - sites_offset[site_id]
+        self.sim.forward()
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/hand_env.py b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/hand_env.py
new file mode 100644
index 0000000..a66e2cd
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/hand_env.py
@@ -0,0 +1,55 @@
+import os
+import copy
+import numpy as np
+
+import gym
+from gym import error, spaces
+from gym.utils import seeding
+from gym.envs.robotics import robot_env
+
+
+class HandEnv(robot_env.RobotEnv):
+    def __init__(self, model_path, n_substeps, initial_qpos, relative_control):
+        self.relative_control = relative_control
+
+        super(HandEnv, self).__init__(
+            model_path=model_path, n_substeps=n_substeps, n_actions=20,
+            initial_qpos=initial_qpos)
+
+    # RobotEnv methods
+    # ----------------------------
+
+    def _set_action(self, action):
+        assert action.shape == (20,)
+
+        ctrlrange = self.sim.model.actuator_ctrlrange
+        actuation_range = (ctrlrange[:, 1] - ctrlrange[:, 0]) / 2.
+        if self.relative_control:
+            actuation_center = np.zeros_like(action)
+            for i in range(self.sim.data.ctrl.shape[0]):
+                actuation_center[i] = self.sim.data.get_joint_qpos(
+                    self.sim.model.actuator_names[i].replace(':A_', ':'))
+            for joint_name in ['FF', 'MF', 'RF', 'LF']:
+                act_idx = self.sim.model.actuator_name2id(
+                    'robot0:A_{}J1'.format(joint_name))
+                actuation_center[act_idx] += self.sim.data.get_joint_qpos(
+                    'robot0:{}J0'.format(joint_name))
+        else:
+            actuation_center = (ctrlrange[:, 1] + ctrlrange[:, 0]) / 2.
+        self.sim.data.ctrl[:] = actuation_center + action * actuation_range
+        self.sim.data.ctrl[:] = np.clip(self.sim.data.ctrl, ctrlrange[:, 0], ctrlrange[:, 1])
+
+    def _viewer_setup(self):
+        body_id = self.sim.model.body_name2id('robot0:palm')
+        lookat = self.sim.data.body_xpos[body_id]
+        for idx, value in enumerate(lookat):
+            self.viewer.cam.lookat[idx] = value
+        self.viewer.cam.distance = 0.5
+        self.viewer.cam.azimuth = 55.
+        self.viewer.cam.elevation = -25.
+
+    def render(self, mode='human', width=500, height=500):
+        return super(HandEnv, self).render(mode, width, height)
+
+    def verify(self):
+        pass
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/robot_env.py b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/robot_env.py
new file mode 100644
index 0000000..584ab1d
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/robot_env.py
@@ -0,0 +1,170 @@
+import os
+import copy
+import numpy as np
+
+import gym
+from gym import error, spaces
+from gym.utils import seeding
+
+try:
+    import mujoco_py
+except ImportError as e:
+    raise error.DependencyNotInstalled("{}. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)".format(e))
+
+DEFAULT_SIZE = 500
+
+class RobotEnv(gym.GoalEnv):
+    def __init__(self, model_path, initial_qpos, n_actions, n_substeps):
+        if model_path.startswith('/'):
+            fullpath = model_path
+        else:
+            fullpath = os.path.join(os.path.dirname(__file__), 'assets', model_path)
+        if not os.path.exists(fullpath):
+            raise IOError('File {} does not exist'.format(fullpath))
+
+        model = mujoco_py.load_model_from_path(fullpath)
+        self.sim = mujoco_py.MjSim(model, nsubsteps=n_substeps)
+        self.viewer = None
+        self._viewers = {}
+
+        self.metadata = {
+            'render.modes': ['human', 'rgb_array'],
+            'video.frames_per_second': int(np.round(1.0 / self.dt))
+        }
+
+        self.seed()
+        self._env_setup(initial_qpos=initial_qpos)
+        self.initial_state = copy.deepcopy(self.sim.get_state())
+
+        self.goal = self._sample_goal()
+        obs = self._get_obs()
+        self.action_space = spaces.Box(-1., 1., shape=(n_actions,), dtype='float32')
+        self.observation_space = spaces.Dict(dict(
+            desired_goal=spaces.Box(-np.inf, np.inf, shape=obs['achieved_goal'].shape, dtype='float32'),
+            achieved_goal=spaces.Box(-np.inf, np.inf, shape=obs['achieved_goal'].shape, dtype='float32'),
+            observation=spaces.Box(-np.inf, np.inf, shape=obs['observation'].shape, dtype='float32'),
+        ))
+
+    @property
+    def dt(self):
+        return self.sim.model.opt.timestep * self.sim.nsubsteps
+
+    # Env methods
+    # ----------------------------
+
+    def seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def step(self, action):
+        action = np.clip(action, self.action_space.low, self.action_space.high)
+        self._set_action(action)
+        self.sim.step()
+        self._step_callback()
+        obs = self._get_obs()
+
+        done = False
+        info = {
+            'is_success': self._is_success(obs['achieved_goal'], self.goal),
+        }
+        reward = self.compute_reward(obs['achieved_goal'], self.goal, info)
+        return obs, reward, done, info
+
+    def reset(self):
+        # Attempt to reset the simulator. Since we randomize initial conditions, it
+        # is possible to get into a state with numerical issues (e.g. due to penetration or
+        # Gimbel lock) or we may not achieve an initial condition (e.g. an object is within the hand).
+        # In this case, we just keep randomizing until we eventually achieve a valid initial
+        # configuration.
+        super(RobotEnv, self).reset()
+        did_reset_sim = False
+        while not did_reset_sim:
+            did_reset_sim = self._reset_sim()
+        self.goal = self._sample_goal().copy()
+        obs = self._get_obs()
+        return obs
+
+    def close(self):
+        if self.viewer is not None:
+            # self.viewer.finish()
+            self.viewer = None
+            self._viewers = {}
+
+    def render(self, mode='human', width=DEFAULT_SIZE, height=DEFAULT_SIZE):
+        self._render_callback()
+        if mode == 'rgb_array':
+            self._get_viewer(mode).render(width, height)
+            # window size used for old mujoco-py:
+            data = self._get_viewer(mode).read_pixels(width, height, depth=False)
+            # original image is upside-down, so flip it
+            return data[::-1, :, :]
+        elif mode == 'human':
+            self._get_viewer(mode).render()
+
+    def _get_viewer(self, mode):
+        self.viewer = self._viewers.get(mode)
+        if self.viewer is None:
+            if mode == 'human':
+                self.viewer = mujoco_py.MjViewer(self.sim)
+            elif mode == 'rgb_array':
+                self.viewer = mujoco_py.MjRenderContextOffscreen(self.sim, device_id=-1)
+            self._viewer_setup()
+            self._viewers[mode] = self.viewer
+        return self.viewer
+
+    # Extension methods
+    # ----------------------------
+
+    def _reset_sim(self):
+        """Resets a simulation and indicates whether or not it was successful.
+        If a reset was unsuccessful (e.g. if a randomized state caused an error in the
+        simulation), this method should indicate such a failure by returning False.
+        In such a case, this method will be called again to attempt a the reset again.
+        """
+        self.sim.set_state(self.initial_state)
+        self.sim.forward()
+        return True
+
+    def _get_obs(self):
+        """Returns the observation.
+        """
+        raise NotImplementedError()
+
+    def _set_action(self, action):
+        """Applies the given action to the simulation.
+        """
+        raise NotImplementedError()
+
+    def _is_success(self, achieved_goal, desired_goal):
+        """Indicates whether or not the achieved goal successfully achieved the desired goal.
+        """
+        raise NotImplementedError()
+
+    def _sample_goal(self):
+        """Samples a new goal and returns it.
+        """
+        raise NotImplementedError()
+
+    def _env_setup(self, initial_qpos):
+        """Initial configuration of the environment. Can be used to configure initial state
+        and extract information from the simulation.
+        """
+        pass
+
+    def _viewer_setup(self):
+        """Initial configuration of the viewer. Can be used to set the camera position,
+        for example.
+        """
+        pass
+
+    def _render_callback(self):
+        """A custom callback that is called before rendering. Can be used
+        to implement custom visualizations.
+        """
+        pass
+
+    def _step_callback(self):
+        """A custom callback that is called after stepping the simulation. Can be used
+        to enforce additional constraints on the simulation state.
+        """
+        pass
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/rotations.py b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/rotations.py
new file mode 100644
index 0000000..4aafb64
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/rotations.py
@@ -0,0 +1,369 @@
+# Copyright (c) 2009-2017, Matthew Brett and Christoph Gohlke
+#    All rights reserved.
+#
+#    Redistribution and use in source and binary forms, with or without
+#    modification, are permitted provided that the following conditions are
+#    met:
+#
+#    1. Redistributions of source code must retain the above copyright notice,
+#    this list of conditions and the following disclaimer.
+#
+#    2. Redistributions in binary form must reproduce the above copyright
+#    notice, this list of conditions and the following disclaimer in the
+#    documentation and/or other materials provided with the distribution.
+#
+#    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
+#    IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
+#    THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+#    PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR
+#    CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+#    EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+#    PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#    PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#    LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#    NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#    SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+# Many methods borrow heavily or entirely from transforms3d:
+# https://github.com/matthew-brett/transforms3d
+# They have mostly been modified to support batched operations.
+
+import numpy as np
+import itertools
+
+'''
+Rotations
+=========
+
+Note: these have caused many subtle bugs in the past.
+Be careful while updating these methods and while using them in clever ways.
+
+See MuJoCo documentation here: http://mujoco.org/book/modeling.html#COrientation
+
+Conventions
+-----------
+    - All functions accept batches as well as individual rotations
+    - All rotation conventions match respective MuJoCo defaults
+    - All angles are in radians
+    - Matricies follow LR convention
+    - Euler Angles are all relative with 'xyz' axes ordering
+    - See specific representation for more information
+
+Representations
+---------------
+
+Euler
+    There are many euler angle frames -- here we will strive to use the default
+        in MuJoCo, which is eulerseq='xyz'.
+    This frame is a relative rotating frame, about x, y, and z axes in order.
+        Relative rotating means that after we rotate about x, then we use the
+        new (rotated) y, and the same for z.
+
+Quaternions
+    These are defined in terms of rotation (angle) about a unit vector (x, y, z)
+    We use the following <q0, q1, q2, q3> convention:
+            q0 = cos(angle / 2)
+            q1 = sin(angle / 2) * x
+            q2 = sin(angle / 2) * y
+            q3 = sin(angle / 2) * z
+        This is also sometimes called qw, qx, qy, qz.
+    Note that quaternions are ambiguous, because we can represent a rotation by
+        angle about vector <x, y, z> and -angle about vector <-x, -y, -z>.
+        To choose between these, we pick "first nonzero positive", where we
+        make the first nonzero element of the quaternion positive.
+    This can result in mismatches if you're converting an quaternion that is not
+        "first nonzero positive" to a different representation and back.
+
+Axis Angle
+    (Not currently implemented)
+    These are very straightforward.  Rotation is angle about a unit vector.
+
+XY Axes
+    (Not currently implemented)
+    We are given x axis and y axis, and z axis is cross product of x and y.
+
+Z Axis
+    This is NOT RECOMMENDED.  Defines a unit vector for the Z axis,
+        but rotation about this axis is not well defined.
+    Instead pick a fixed reference direction for another axis (e.g. X)
+        and calculate the other (e.g. Y = Z cross-product X),
+        then use XY Axes rotation instead.
+
+SO3
+    (Not currently implemented)
+    While not supported by MuJoCo, this representation has a lot of nice features.
+    We expect to add support for these in the future.
+
+TODO / Missing
+--------------
+    - Rotation integration or derivatives (e.g. velocity conversions)
+    - More representations (SO3, etc)
+    - Random sampling (e.g. sample uniform random rotation)
+    - Performance benchmarks/measurements
+    - (Maybe) define everything as to/from matricies, for simplicity
+'''
+
+# For testing whether a number is close to zero
+_FLOAT_EPS = np.finfo(np.float64).eps
+_EPS4 = _FLOAT_EPS * 4.0
+
+
+def euler2mat(euler):
+    """ Convert Euler Angles to Rotation Matrix.  See rotation.py for notes """
+    euler = np.asarray(euler, dtype=np.float64)
+    assert euler.shape[-1] == 3, "Invalid shaped euler {}".format(euler)
+
+    ai, aj, ak = -euler[..., 2], -euler[..., 1], -euler[..., 0]
+    si, sj, sk = np.sin(ai), np.sin(aj), np.sin(ak)
+    ci, cj, ck = np.cos(ai), np.cos(aj), np.cos(ak)
+    cc, cs = ci * ck, ci * sk
+    sc, ss = si * ck, si * sk
+
+    mat = np.empty(euler.shape[:-1] + (3, 3), dtype=np.float64)
+    mat[..., 2, 2] = cj * ck
+    mat[..., 2, 1] = sj * sc - cs
+    mat[..., 2, 0] = sj * cc + ss
+    mat[..., 1, 2] = cj * sk
+    mat[..., 1, 1] = sj * ss + cc
+    mat[..., 1, 0] = sj * cs - sc
+    mat[..., 0, 2] = -sj
+    mat[..., 0, 1] = cj * si
+    mat[..., 0, 0] = cj * ci
+    return mat
+
+
+def euler2quat(euler):
+    """ Convert Euler Angles to Quaternions.  See rotation.py for notes """
+    euler = np.asarray(euler, dtype=np.float64)
+    assert euler.shape[-1] == 3, "Invalid shape euler {}".format(euler)
+
+    ai, aj, ak = euler[..., 2] / 2, -euler[..., 1] / 2, euler[..., 0] / 2
+    si, sj, sk = np.sin(ai), np.sin(aj), np.sin(ak)
+    ci, cj, ck = np.cos(ai), np.cos(aj), np.cos(ak)
+    cc, cs = ci * ck, ci * sk
+    sc, ss = si * ck, si * sk
+
+    quat = np.empty(euler.shape[:-1] + (4,), dtype=np.float64)
+    quat[..., 0] = cj * cc + sj * ss
+    quat[..., 3] = cj * sc - sj * cs
+    quat[..., 2] = -(cj * ss + sj * cc)
+    quat[..., 1] = cj * cs - sj * sc
+    return quat
+
+
+def mat2euler(mat):
+    """ Convert Rotation Matrix to Euler Angles.  See rotation.py for notes """
+    mat = np.asarray(mat, dtype=np.float64)
+    assert mat.shape[-2:] == (3, 3), "Invalid shape matrix {}".format(mat)
+
+    cy = np.sqrt(mat[..., 2, 2] * mat[..., 2, 2] + mat[..., 1, 2] * mat[..., 1, 2])
+    condition = cy > _EPS4
+    euler = np.empty(mat.shape[:-1], dtype=np.float64)
+    euler[..., 2] = np.where(condition,
+                             -np.arctan2(mat[..., 0, 1], mat[..., 0, 0]),
+                             -np.arctan2(-mat[..., 1, 0], mat[..., 1, 1]))
+    euler[..., 1] = np.where(condition,
+                             -np.arctan2(-mat[..., 0, 2], cy),
+                             -np.arctan2(-mat[..., 0, 2], cy))
+    euler[..., 0] = np.where(condition,
+                             -np.arctan2(mat[..., 1, 2], mat[..., 2, 2]),
+                             0.0)
+    return euler
+
+
+def mat2quat(mat):
+    """ Convert Rotation Matrix to Quaternion.  See rotation.py for notes """
+    mat = np.asarray(mat, dtype=np.float64)
+    assert mat.shape[-2:] == (3, 3), "Invalid shape matrix {}".format(mat)
+
+    Qxx, Qyx, Qzx = mat[..., 0, 0], mat[..., 0, 1], mat[..., 0, 2]
+    Qxy, Qyy, Qzy = mat[..., 1, 0], mat[..., 1, 1], mat[..., 1, 2]
+    Qxz, Qyz, Qzz = mat[..., 2, 0], mat[..., 2, 1], mat[..., 2, 2]
+    # Fill only lower half of symmetric matrix
+    K = np.zeros(mat.shape[:-2] + (4, 4), dtype=np.float64)
+    K[..., 0, 0] = Qxx - Qyy - Qzz
+    K[..., 1, 0] = Qyx + Qxy
+    K[..., 1, 1] = Qyy - Qxx - Qzz
+    K[..., 2, 0] = Qzx + Qxz
+    K[..., 2, 1] = Qzy + Qyz
+    K[..., 2, 2] = Qzz - Qxx - Qyy
+    K[..., 3, 0] = Qyz - Qzy
+    K[..., 3, 1] = Qzx - Qxz
+    K[..., 3, 2] = Qxy - Qyx
+    K[..., 3, 3] = Qxx + Qyy + Qzz
+    K /= 3.0
+    # TODO: vectorize this -- probably could be made faster
+    q = np.empty(K.shape[:-2] + (4,))
+    it = np.nditer(q[..., 0], flags=['multi_index'])
+    while not it.finished:
+        # Use Hermitian eigenvectors, values for speed
+        vals, vecs = np.linalg.eigh(K[it.multi_index])
+        # Select largest eigenvector, reorder to w,x,y,z quaternion
+        q[it.multi_index] = vecs[[3, 0, 1, 2], np.argmax(vals)]
+        # Prefer quaternion with positive w
+        # (q * -1 corresponds to same rotation as q)
+        if q[it.multi_index][0] < 0:
+            q[it.multi_index] *= -1
+        it.iternext()
+    return q
+
+
+def quat2euler(quat):
+    """ Convert Quaternion to Euler Angles.  See rotation.py for notes """
+    return mat2euler(quat2mat(quat))
+
+
+def subtract_euler(e1, e2):
+    assert e1.shape == e2.shape
+    assert e1.shape[-1] == 3
+    q1 = euler2quat(e1)
+    q2 = euler2quat(e2)
+    q_diff = quat_mul(q1, quat_conjugate(q2))
+    return quat2euler(q_diff)
+
+
+def quat2mat(quat):
+    """ Convert Quaternion to Euler Angles.  See rotation.py for notes """
+    quat = np.asarray(quat, dtype=np.float64)
+    assert quat.shape[-1] == 4, "Invalid shape quat {}".format(quat)
+
+    w, x, y, z = quat[..., 0], quat[..., 1], quat[..., 2], quat[..., 3]
+    Nq = np.sum(quat * quat, axis=-1)
+    s = 2.0 / Nq
+    X, Y, Z = x * s, y * s, z * s
+    wX, wY, wZ = w * X, w * Y, w * Z
+    xX, xY, xZ = x * X, x * Y, x * Z
+    yY, yZ, zZ = y * Y, y * Z, z * Z
+
+    mat = np.empty(quat.shape[:-1] + (3, 3), dtype=np.float64)
+    mat[..., 0, 0] = 1.0 - (yY + zZ)
+    mat[..., 0, 1] = xY - wZ
+    mat[..., 0, 2] = xZ + wY
+    mat[..., 1, 0] = xY + wZ
+    mat[..., 1, 1] = 1.0 - (xX + zZ)
+    mat[..., 1, 2] = yZ - wX
+    mat[..., 2, 0] = xZ - wY
+    mat[..., 2, 1] = yZ + wX
+    mat[..., 2, 2] = 1.0 - (xX + yY)
+    return np.where((Nq > _FLOAT_EPS)[..., np.newaxis, np.newaxis], mat, np.eye(3))
+
+def quat_conjugate(q):
+    inv_q = -q
+    inv_q[..., 0] *= -1
+    return inv_q
+
+def quat_mul(q0, q1):
+    assert q0.shape == q1.shape
+    assert q0.shape[-1] == 4
+    assert q1.shape[-1] == 4
+
+    w0 = q0[..., 0]
+    x0 = q0[..., 1]
+    y0 = q0[..., 2]
+    z0 = q0[..., 3]
+
+    w1 = q1[..., 0]
+    x1 = q1[..., 1]
+    y1 = q1[..., 2]
+    z1 = q1[..., 3]
+
+    w = w0 * w1 - x0 * x1 - y0 * y1 - z0 * z1
+    x = w0 * x1 + x0 * w1 + y0 * z1 - z0 * y1
+    y = w0 * y1 + y0 * w1 + z0 * x1 - x0 * z1
+    z = w0 * z1 + z0 * w1 + x0 * y1 - y0 * x1
+    q = np.array([w, x, y, z])
+    if q.ndim == 2:
+        q = q.swapaxes(0, 1)
+    assert q.shape == q0.shape
+    return q
+
+def quat_rot_vec(q, v0):
+    q_v0 = np.array([0, v0[0], v0[1], v0[2]])
+    q_v = quat_mul(q, quat_mul(q_v0, quat_conjugate(q)))
+    v = q_v[1:]
+    return v
+
+def quat_identity():
+    return np.array([1, 0, 0, 0])
+
+def quat2axisangle(quat):
+    theta = 0;
+    axis = np.array([0, 0, 1]);
+    sin_theta = np.linalg.norm(quat[1:])
+
+    if (sin_theta > 0.0001):
+        theta = 2 * np.arcsin(sin_theta)
+        theta *= 1 if quat[0] >= 0 else -1
+        axis = quat[1:] / sin_theta
+
+    return axis, theta
+
+def euler2point_euler(euler):
+    _euler = euler.copy()
+    if len(_euler.shape) < 2:
+        _euler = np.expand_dims(_euler,0)
+    assert(_euler.shape[1] == 3)
+    _euler_sin = np.sin(_euler)
+    _euler_cos = np.cos(_euler)
+    return np.concatenate([_euler_sin, _euler_cos], axis=-1)
+
+def point_euler2euler(euler):
+    _euler = euler.copy()
+    if len(_euler.shape) < 2:
+        _euler = np.expand_dims(_euler,0)
+    assert(_euler.shape[1] == 6)
+    angle = np.arctan(_euler[..., :3] / _euler[..., 3:])
+    angle[_euler[..., 3:] < 0] += np.pi
+    return angle
+
+def quat2point_quat(quat):
+    # Should be in qw, qx, qy, qz
+    _quat = quat.copy()
+    if len(_quat.shape) < 2:
+        _quat = np.expand_dims(_quat, 0)
+    assert(_quat.shape[1] == 4)
+    angle = np.arccos(_quat[:,[0]]) * 2
+    xyz = _quat[:, 1:]
+    xyz[np.squeeze(np.abs(np.sin(angle/2))) >= 1e-5] = (xyz / np.sin(angle / 2))[np.squeeze(np.abs(np.sin(angle/2))) >= 1e-5]
+    return np.concatenate([np.sin(angle),np.cos(angle), xyz], axis=-1)
+
+def point_quat2quat(quat):
+    _quat = quat.copy()
+    if len(_quat.shape) < 2:
+        _quat = np.expand_dims(_quat, 0)
+    assert(_quat.shape[1] == 5)
+    angle = np.arctan(_quat[:,[0]] / _quat[:,[1]])
+    qw = np.cos(angle / 2)
+
+    qxyz = _quat[:, 2:]
+    qxyz[np.squeeze(np.abs(np.sin(angle/2))) >= 1e-5] = (qxyz * np.sin(angle/2))[np.squeeze(np.abs(np.sin(angle/2))) >= 1e-5]
+    return np.concatenate([qw, qxyz], axis=-1)
+
+def normalize_angles(angles):
+    '''Puts angles in [-pi, pi] range.'''
+    angles = angles.copy()
+    if angles.size > 0:
+        angles = (angles + np.pi) % (2 * np.pi) - np.pi
+        assert -np.pi-1e-6 <= angles.min() and angles.max() <= np.pi+1e-6
+    return angles
+
+def round_to_straight_angles(angles):
+    '''Returns closest angle modulo 90 degrees '''
+    angles = np.round(angles / (np.pi / 2)) * (np.pi / 2)
+    return normalize_angles(angles)
+
+def get_parallel_rotations():
+    mult90 = [0, np.pi/2, -np.pi/2, np.pi]
+    parallel_rotations = []
+    for euler in itertools.product(mult90, repeat=3):
+        canonical = mat2euler(euler2mat(euler))
+        canonical = np.round(canonical / (np.pi / 2))
+        if canonical[0] == -2:
+            canonical[0] = 2
+        if canonical[2] == -2:
+            canonical[2] = 2
+        canonical *= np.pi / 2
+        if all([(canonical != rot).any() for rot in parallel_rotations]):
+            parallel_rotations += [canonical]
+    assert len(parallel_rotations) == 24
+    return parallel_rotations
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/robotics/utils.py b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/utils.py
new file mode 100644
index 0000000..a73e5f6
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/robotics/utils.py
@@ -0,0 +1,96 @@
+import numpy as np
+
+from gym import error
+try:
+    import mujoco_py
+except ImportError as e:
+    raise error.DependencyNotInstalled("{}. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)".format(e))
+
+
+def robot_get_obs(sim):
+    """Returns all joint positions and velocities associated with
+    a robot.
+    """
+    if sim.data.qpos is not None and sim.model.joint_names:
+        names = [n for n in sim.model.joint_names if n.startswith('robot')]
+        return (
+            np.array([sim.data.get_joint_qpos(name) for name in names]),
+            np.array([sim.data.get_joint_qvel(name) for name in names]),
+        )
+    return np.zeros(0), np.zeros(0)
+
+
+def ctrl_set_action(sim, action):
+    """For torque actuators it copies the action into mujoco ctrl field.
+    For position actuators it sets the target relative to the current qpos.
+    """
+    if sim.model.nmocap > 0:
+        _, action = np.split(action, (sim.model.nmocap * 7, ))
+    if sim.data.ctrl is not None:
+        for i in range(action.shape[0]):
+            if sim.model.actuator_biastype[i] == 0:
+                sim.data.ctrl[i] = action[i]
+            else:
+                idx = sim.model.jnt_qposadr[sim.model.actuator_trnid[i, 0]]
+                sim.data.ctrl[i] = sim.data.qpos[idx] + action[i]
+
+
+def mocap_set_action(sim, action):
+    """The action controls the robot using mocaps. Specifically, bodies
+    on the robot (for example the gripper wrist) is controlled with
+    mocap bodies. In this case the action is the desired difference
+    in position and orientation (quaternion), in world coordinates,
+    of the of the target body. The mocap is positioned relative to
+    the target body according to the delta, and the MuJoCo equality
+    constraint optimizer tries to center the welded body on the mocap.
+    """
+    if sim.model.nmocap > 0:
+        action, _ = np.split(action, (sim.model.nmocap * 7, ))
+        action = action.reshape(sim.model.nmocap, 7)
+
+        pos_delta = action[:, :3]
+        quat_delta = action[:, 3:]
+
+        reset_mocap2body_xpos(sim)
+        sim.data.mocap_pos[:] = sim.data.mocap_pos + pos_delta
+        sim.data.mocap_quat[:] = sim.data.mocap_quat + quat_delta
+
+
+def reset_mocap_welds(sim):
+    """Resets the mocap welds that we use for actuation.
+    """
+    if sim.model.nmocap > 0 and sim.model.eq_data is not None:
+        for i in range(sim.model.eq_data.shape[0]):
+            if sim.model.eq_type[i] == mujoco_py.const.EQ_WELD:
+                sim.model.eq_data[i, :] = np.array(
+                    [0., 0., 0., 1., 0., 0., 0.])
+    sim.forward()
+
+
+def reset_mocap2body_xpos(sim):
+    """Resets the position and orientation of the mocap bodies to the same
+    values as the bodies they're welded to.
+    """
+
+    if (sim.model.eq_type is None or
+        sim.model.eq_obj1id is None or
+        sim.model.eq_obj2id is None):
+        return
+    for eq_type, obj1_id, obj2_id in zip(sim.model.eq_type,
+                                         sim.model.eq_obj1id,
+                                         sim.model.eq_obj2id):
+        if eq_type != mujoco_py.const.EQ_WELD:
+            continue
+
+        mocap_id = sim.model.body_mocapid[obj1_id]
+        if mocap_id != -1:
+            # obj1 is the mocap, obj2 is the welded body
+            body_idx = obj2_id
+        else:
+            # obj2 is the mocap, obj1 is the welded body
+            mocap_id = sim.model.body_mocapid[obj2_id]
+            body_idx = obj1_id
+
+        assert (mocap_id != -1)
+        sim.data.mocap_pos[mocap_id][:] = sim.data.body_xpos[body_idx]
+        sim.data.mocap_quat[mocap_id][:] = sim.data.body_xquat[body_idx]
diff --git a/experiments02/ant_umaze_1234/src/slbo/envs/virtual_env.py b/experiments02/ant_umaze_1234/src/slbo/envs/virtual_env.py
new file mode 100644
index 0000000..2d0cc76
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/envs/virtual_env.py
@@ -0,0 +1,94 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from gym.spaces import Box
+from slbo.dynamics_model import DynamicsModel
+from slbo.random_net import RandomNet
+from slbo.envs import BaseBatchedEnv, BaseModelBasedEnv
+from slbo.utils.pc_utils import compute_cov_pi
+
+
+class VirtualEnv(BaseBatchedEnv):
+    _states: np.ndarray
+
+    def __init__(self, model: DynamicsModel, env: BaseModelBasedEnv, random_net:RandomNet,  n_envs: int, 
+                    feature_size: int, bonus_scale: float, lamb: float, opt_model=False):
+        super().__init__()
+        self.n_envs = n_envs
+        self.observation_space = env.observation_space  # ???
+
+        dim_state = env.observation_space.shape[0]
+        dim_action = env.action_space.shape[0]
+        if opt_model:
+            self.action_space = Box(low=np.r_[env.action_space.low, np.zeros(dim_state) - 1.],
+                                    high=np.r_[env.action_space.high, np.zeros(dim_state) + 1.],
+                                    dtype=np.float32)
+        else:
+            self.action_space = env.action_space
+
+        self._opt_model = opt_model
+        self._model = model
+        self._env = env
+        self._random_net = random_net
+
+        self._states = np.zeros((self.n_envs, dim_state), dtype=np.float32)
+
+        self.feature_size = feature_size
+        self.cov_pis = None
+        self.inv_cov = None
+        self.bonus_scale = bonus_scale
+        self.lamb = lamb
+        self.pre = True
+
+    def _scale_action(self, actions):
+        lo, hi = self.action_space.low, self.action_space.high
+        return lo + (actions + 1.) * 0.5 * (hi - lo)
+
+    def step(self, actions):
+        if self._opt_model:
+            actions = actions[..., :self._env.action_space.shape[0]]
+
+        next_states = self._model.eval('next_states', states=self._states, actions=actions)
+        features = self._random_net.eval('features', states=self._states, actions=actions)
+        #print(features.shape)
+        rewards, dones = self._env.mb_step(self._states, self._scale_action(actions), next_states)
+
+        if not self.pre:
+            bonus = self.compute_bonus(features)
+            rewards = rewards + self.bonus_scale * bonus
+
+        self._states = next_states
+        return self._states.copy(), rewards, dones, [{} for _ in range(self.n_envs)]
+
+    def reset(self):
+        return self.partial_reset(range(self.n_envs))
+
+    def partial_reset(self, indices):
+        initial_states = np.array([self._env.reset() for _ in indices])
+
+        self._states = self._states.copy()
+        self._states[indices] = initial_states
+
+        return initial_states.copy()
+
+    def set_state(self, states):
+        self._states = states.copy()
+
+    def render(self, mode='human'):
+        pass
+
+    def update_cov(self, states, actions):
+        features = self._random_net.eval('features', states=states, actions=actions)
+
+        if self.pre:
+            self.cov_pis = compute_cov_pi(features)
+            self.pre = False
+        else:
+            self.cov_pis = self.cov_pis + compute_cov_pi(features)
+        
+        cur_cov = self.lamb * np.identity(self.feature_size) + self.cov_pis
+        self.inv_cov = np.linalg.inv(cur_cov)
+
+
+    def compute_bonus(self,features):
+        bonus = np.sqrt(np.sum(np.dot(features, self.inv_cov)*features,1))
+        return bonus
diff --git a/experiments02/ant_umaze_1234/src/slbo/loss/__init__.py b/experiments02/ant_umaze_1234/src/slbo/loss/__init__.py
new file mode 100644
index 0000000..5c7f19c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/loss/__init__.py
@@ -0,0 +1 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
diff --git a/experiments02/ant_umaze_1234/src/slbo/loss/multi_step_loss.py b/experiments02/ant_umaze_1234/src/slbo/loss/multi_step_loss.py
new file mode 100644
index 0000000..a2aadb7
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/loss/multi_step_loss.py
@@ -0,0 +1,65 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi import Tensor
+from slbo.utils.normalizer import Normalizers
+
+
+class MultiStepLoss(nn.Module):
+    op_train: Tensor
+    op_grad_norm: Tensor
+    _step: int
+    _criterion: nn.Module
+    _normalizers: Normalizers
+    _model: nn.Module
+
+    def __init__(self, model: nn.Module, normalizers: Normalizers, dim_state: int, dim_action: int,
+                 criterion: nn.Module, step=4):
+        super().__init__()
+        self._step = step
+        self._criterion = criterion
+        self._model = model
+        self._normalizers = normalizers
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, shape=[step, None, dim_state])
+            self.op_actions = tf.placeholder(tf.float32, shape=[step, None, dim_action])
+            self.op_masks = tf.placeholder(tf.float32, shape=[step, None])
+            self.op_next_states_ = tf.placeholder(tf.float32, shape=[step, None, dim_state])
+
+        self.op_loss = self(self.op_states, self.op_actions, self.op_next_states_, self.op_masks)
+
+    def forward(self, states: Tensor, actions: Tensor, next_states_: Tensor, masks: Tensor):
+        """
+            All inputs have shape [num_steps, batch_size, xxx]
+        """
+
+        cur_states = states[0]
+        loss = []
+        for i in range(self._step):
+            next_states = self._model(cur_states, actions[i])
+            diffs = next_states - cur_states - next_states_[i] + states[i]
+            weighted_diffs = diffs / self._normalizers.diff.op_std.maximum(1e-6)
+            loss.append(self._criterion(weighted_diffs, 0, cur_states))
+
+            if i < self._step - 1:
+                cur_states = states[i + 1] + masks[i].expand_dims(-1) * (next_states - states[i + 1])
+
+        return tf.add_n(loss) / self._step
+
+    @nn.make_method(fetch='loss')
+    def get_loss(self, states, next_states_, actions, masks): pass
+
+    def build_backward(self, lr: float, weight_decay: float, max_grad_norm=2.):
+        loss = self.op_loss.reduce_mean(name='Loss')
+
+        optimizer = tf.train.AdamOptimizer(lr)
+        params = self._model.parameters()
+        regularization = weight_decay * tf.add_n([tf.nn.l2_loss(t) for t in params], name='regularization')
+
+        grads_and_vars = optimizer.compute_gradients(loss + regularization, var_list=params)
+        print([var.name for grad, var in grads_and_vars])
+        clip_grads, op_grad_norm = tf.clip_by_global_norm([grad for grad, _ in grads_and_vars], max_grad_norm)
+        clip_grads_and_vars = [(grad, var) for grad, (_, var) in zip(clip_grads, grads_and_vars)]
+        self.op_train = optimizer.apply_gradients(clip_grads_and_vars)
+        self.op_grad_norm = op_grad_norm
diff --git a/experiments02/ant_umaze_1234/src/slbo/partial_envs.py b/experiments02/ant_umaze_1234/src/slbo/partial_envs.py
new file mode 100644
index 0000000..b9aee20
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/partial_envs.py
@@ -0,0 +1,95 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+
+import gym
+import slbo.envs.mujoco_maze
+
+from gym.wrappers import FlattenObservation
+
+
+from slbo.envs.bm_envs.gym.half_cheetah import HalfCheetahEnv
+from slbo.envs.bm_envs.gym.walker2d import Walker2dEnv
+# from slbo.envs.mujoco.humanoid_env import HumanoidEnv
+from slbo.envs.bm_envs.gym.ant import AntEnv
+from slbo.envs.bm_envs.gym.hopper import HopperEnv
+from slbo.envs.bm_envs.gym.swimmer import SwimmerEnv
+from slbo.envs.bm_envs.gym.reacher import ReacherEnv
+from slbo.envs.bm_envs.gym.pendulum import PendulumEnv
+from slbo.envs.bm_envs.gym.inverted_pendulum import InvertedPendulumEnv
+from slbo.envs.bm_envs.gym.acrobot import AcrobotEnv
+from slbo.envs.bm_envs.gym.cartpole import CartPoleEnv
+from slbo.envs.bm_envs.gym.mountain_car import Continuous_MountainCarEnv
+
+from slbo.envs.bm_envs.gym import gym_fswimmer
+from slbo.envs.bm_envs.gym import gym_fwalker2d
+from slbo.envs.bm_envs.gym import gym_fhopper
+from slbo.envs.bm_envs.gym import gym_fant
+
+from slbo.envs.bm_envs.gym import gym_cheetahA01
+from slbo.envs.bm_envs.gym import gym_cheetahA003
+from slbo.envs.bm_envs.gym import gym_cheetahO01
+from slbo.envs.bm_envs.gym import gym_cheetahO001
+from slbo.envs.bm_envs.gym import gym_pendulumO01
+from slbo.envs.bm_envs.gym import gym_pendulumO001
+from slbo.envs.bm_envs.gym import gym_cartpoleO01
+from slbo.envs.bm_envs.gym import gym_cartpoleO001
+
+from slbo.envs.bm_envs.gym import gym_humanoid
+from slbo.envs.bm_envs.gym import gym_nostopslimhumanoid
+from slbo.envs.bm_envs.gym import gym_slimhumanoid
+from slbo.envs.robotics.fetch.push import FetchPushEnv
+from slbo.envs.robotics.hand.reach import HandReachEnv
+from slbo.envs.robotics.hand.manipulate import HandEggEnv
+
+
+def make_env(id: str):
+    if "Point" in id or "Ant" in id:
+        env = gym.make(id)
+        env.seed(np.random.randint(2**60))
+    else:
+        envs = {
+            'HandEgg': HandEggEnv,
+            'FetchPush': FetchPushEnv,
+            'HandReach': HandReachEnv,
+            'HalfCheetah': HalfCheetahEnv,
+            'Walker2D': Walker2dEnv,
+            'Ant': AntEnv,
+            'Hopper': HopperEnv,
+            'Swimmer': SwimmerEnv,
+            'FixedSwimmer': gym_fswimmer.fixedSwimmerEnv,
+            'FixedWalker': gym_fwalker2d.Walker2dEnv,
+            'FixedHopper': gym_fhopper.HopperEnv,
+            'FixedAnt': gym_fant.AntEnv,
+            'Reacher': ReacherEnv,
+            'Pendulum': PendulumEnv,
+            'InvertedPendulum': InvertedPendulumEnv,
+            'Acrobot': AcrobotEnv,
+            'CartPole': CartPoleEnv,
+            'MountainCar': Continuous_MountainCarEnv,
+
+            'HalfCheetahO01': gym_cheetahO01.HalfCheetahEnv,
+            'HalfCheetahO001': gym_cheetahO001.HalfCheetahEnv,
+            'HalfCheetahA01': gym_cheetahA01.HalfCheetahEnv,
+            'HalfCheetahA003': gym_cheetahA003.HalfCheetahEnv,
+
+            'PendulumO01': gym_pendulumO01.PendulumEnv,
+            'PendulumO001': gym_pendulumO001.PendulumEnv,
+
+            'CartPoleO01': gym_cartpoleO01.CartPoleEnv,
+            'CartPoleO001': gym_cartpoleO001.CartPoleEnv,
+
+            'gym_humanoid': gym_humanoid.HumanoidEnv,
+            'gym_slimhumanoid': gym_slimhumanoid.HumanoidEnv,
+            'gym_nostopslimhumanoid': gym_nostopslimhumanoid.HumanoidEnv,
+        }
+        
+        env = envs[id]()
+        if not hasattr(env, 'reward_range'):
+            env.reward_range = (-np.inf, np.inf)
+        if not hasattr(env, 'metadata'):
+            env.metadata = {}
+        env.seed(np.random.randint(2**60))
+        if 'Fetch' in id or 'Hand' in id: 
+            env = FlattenObservation(env)
+
+    return env
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/slbo/policies/__init__.py b/experiments02/ant_umaze_1234/src/slbo/policies/__init__.py
new file mode 100644
index 0000000..3ffa40d
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/policies/__init__.py
@@ -0,0 +1,13 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import abc
+from typing import Union
+import lunzi.nn as nn
+
+
+class BasePolicy(abc.ABC):
+    @abc.abstractmethod
+    def get_actions(self, states):
+        pass
+
+
+BaseNNPolicy = Union[BasePolicy, nn.Module]  # should be Intersection, see PEP544
diff --git a/experiments02/ant_umaze_1234/src/slbo/policies/gaussian_mlp_policy.py b/experiments02/ant_umaze_1234/src/slbo/policies/gaussian_mlp_policy.py
new file mode 100644
index 0000000..c6bcb27
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/policies/gaussian_mlp_policy.py
@@ -0,0 +1,69 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List
+import tensorflow as tf
+import numpy as np
+from lunzi import Tensor
+from lunzi import nn
+from baselines.common.tf_util import normc_initializer
+from slbo.utils.truncated_normal import LimitedEntNormal
+from . import BasePolicy
+from slbo.utils.normalizer import GaussianNormalizer
+
+
+class GaussianMLPPolicy(nn.Module, BasePolicy):
+    op_states: Tensor
+
+    def __init__(self, dim_state: int, dim_action: int, hidden_sizes: List[int], normalizer: GaussianNormalizer,
+                 init_std=1.):
+        super().__init__()
+        self.dim_state = dim_state
+        self.dim_action = dim_action
+        self.hidden_sizes = hidden_sizes
+        self.init_std = init_std
+        self.normalizer = normalizer
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, shape=[None, dim_state], name='states')
+            self.op_actions_ = tf.placeholder(tf.float32, shape=[None, dim_action], name='actions')
+
+            layers = []
+            # note that the placeholder has size 105.
+            all_sizes = [dim_state, *self.hidden_sizes]
+            for i, (in_features, out_features) in enumerate(zip(all_sizes[:-1], all_sizes[1:])):
+                layers.append(nn.Linear(in_features, out_features, weight_initializer=normc_initializer(1)))
+                layers.append(nn.Tanh())
+            layers.append(nn.Linear(all_sizes[-1], dim_action, weight_initializer=normc_initializer(0.01)))
+            self.net = nn.Sequential(*layers)
+
+            self.op_log_std = nn.Parameter(
+                tf.constant(np.log(self.init_std), shape=[self.dim_action], dtype=tf.float32), name='log_std')
+
+        self.distribution = self(self.op_states)
+        self.op_actions = self.distribution.sample()
+        self.op_actions_mean = self.distribution.mean()
+        self.op_actions_std = self.distribution.stddev()
+        self.op_nlls_ = -self.distribution.log_prob(self.op_actions_).reduce_sum(axis=1)
+
+        self.register_callable('[states] => [actions]', self.fast)
+
+    def forward(self, states):
+        states = self.normalizer(states)
+        actions_mean = self.net(states)
+        distribution = LimitedEntNormal(actions_mean, self.op_log_std.exp())
+
+        return distribution
+
+    @nn.make_method(fetch='actions')
+    def get_actions(self, states): pass
+
+    def fast(self, states, use_log_prob=False):
+        states = self.normalizer.fast(states)
+        actions_mean = self.net.fast(states)
+        noise = np.random.randn(*actions_mean.shape)
+        actions = actions_mean + noise * np.exp(self.op_log_std.numpy())
+        if use_log_prob:
+            log_prob = -noise**2 / 2 - np.log(2 * np.pi) / 2 - self.op_log_std.numpy()
+            return actions, log_prob.sum(axis=1)
+        return actions
+
+    def clone(self):
+        return GaussianMLPPolicy(self.dim_state, self.dim_action, self.hidden_sizes, self.normalizer, self.init_std)
diff --git a/experiments02/ant_umaze_1234/src/slbo/policies/uniform_policy.py b/experiments02/ant_umaze_1234/src/slbo/policies/uniform_policy.py
new file mode 100644
index 0000000..ca9f821
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/policies/uniform_policy.py
@@ -0,0 +1,11 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from . import BasePolicy
+
+
+class UniformPolicy(BasePolicy):
+    def __init__(self, dim_action):
+        self.dim_action = dim_action
+
+    def get_actions(self, states):
+        return np.random.uniform(-1., 1., states.shape[:-1] + (self.dim_action,))
diff --git a/experiments02/ant_umaze_1234/src/slbo/q_function/__init__.py b/experiments02/ant_umaze_1234/src/slbo/q_function/__init__.py
new file mode 100644
index 0000000..e8dfcba
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/q_function/__init__.py
@@ -0,0 +1,13 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import abc
+from typing import Union
+import lunzi.nn as nn
+
+
+class BaseQFunction(abc.ABC):
+    @abc.abstractmethod
+    def get_q(self, states, values):
+        pass
+
+
+BaseNNQFunction = Union[BaseQFunction, nn.Module]
diff --git a/experiments02/ant_umaze_1234/src/slbo/q_function/mlp_q_function.py b/experiments02/ant_umaze_1234/src/slbo/q_function/mlp_q_function.py
new file mode 100644
index 0000000..c8eed83
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/q_function/mlp_q_function.py
@@ -0,0 +1,22 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List
+import tensorflow as tf
+from . import BaseQFunction
+import lunzi.nn as nn
+from slbo.utils.multi_layer_perceptron import MultiLayerPerceptron
+
+
+class MLPQFunction(MultiLayerPerceptron, BaseQFunction):
+    def __init__(self, dim_state: int, dim_action: int, hidden_states: List[int]):
+        super().__init__((dim_state + dim_action, *hidden_states, 1), squeeze=True)
+        self._dim_state = dim_state
+        self._dim_action = dim_action
+
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, [None, dim_state])
+            self.op_actions = tf.placeholder(tf.float32, [None, dim_action])
+
+        self.op_Q = self.forward(self.op_states, self.op_actions)
+
+    @nn.make_method(fetch='Q')
+    def get_q(self, states, actions): pass
diff --git a/experiments02/ant_umaze_1234/src/slbo/random_net.py b/experiments02/ant_umaze_1234/src/slbo/random_net.py
new file mode 100644
index 0000000..c9a94e9
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/random_net.py
@@ -0,0 +1,41 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List
+import tensorflow as tf
+from lunzi import Tensor
+import lunzi.nn as nn
+from slbo.utils.normalizer import Normalizers
+from slbo.utils.multi_layer_perceptron import MultiLayerPerceptron
+
+
+class RandomNet(MultiLayerPerceptron):
+    op_loss: Tensor
+    op_train: Tensor
+    op_grad_norm: Tensor
+
+    def __init__(self, dim_state: int, dim_action: int, normalizers: Normalizers, hidden_sizes: List[int]):
+        initializer = tf.truncated_normal_initializer(mean=0.0, stddev=1e-5)
+
+        self.dim_state = dim_state
+        self.dim_action = dim_action
+        self.hidden_sizes = hidden_sizes
+        self.op_states = tf.placeholder(tf.float32, shape=[None, self.dim_state], name='states')
+        self.op_actions = tf.placeholder(tf.float32, shape=[None, self.dim_action], name='actions')
+        super().__init__([dim_state + dim_action, *hidden_sizes[:-1]],
+                         activation=nn.ReLU,
+                         weight_initializer=initializer, build=False)
+
+        self.normalizers = normalizers
+        self.build()
+
+    def build(self):
+        self.op_features = self.forward(self.op_states, self.op_actions)
+
+    def forward(self, states, actions):
+        assert actions.shape[-1] == self.dim_action
+        inputs = tf.concat([self.normalizers.state(states), actions.clip_by_value(-1., 1.)], axis=1)
+
+        features = super().forward(inputs)
+        return features
+
+    def clone(self):
+        return RandomNet(self.dim_state, self.dim_action, self.normalizers, self.hidden_sizes)
diff --git a/experiments02/ant_umaze_1234/src/slbo/utils/OU_noise.py b/experiments02/ant_umaze_1234/src/slbo/utils/OU_noise.py
new file mode 100644
index 0000000..eae7158
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/utils/OU_noise.py
@@ -0,0 +1,36 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from slbo.policies import BasePolicy
+
+
+class OUNoise(object):
+    _policy: BasePolicy
+
+    def __init__(self, action_space, mu=0.0, theta=0.15, sigma=0.3, shape=None):
+        self.mu = mu
+        self.theta = theta
+        self.sigma = sigma
+        self.action_space = action_space
+        self._state = None
+        if shape:
+            self.shape = shape
+        else:
+            self.shape = action_space.shape
+
+        self.reset()
+
+    def reset(self):
+        self._state = np.ones(self.shape) * self.mu
+
+    def next(self):
+        delta = self.theta * (self.mu - self._state) + self.sigma * np.random.randn(*self._state.shape)
+        self._state = self._state + delta
+        return self._state
+
+    def get_actions(self, states):
+        return self._policy.get_actions(states) + self.next()
+
+    def make(self, policy: BasePolicy):
+        self._policy = policy
+        return self
+
diff --git a/experiments02/ant_umaze_1234/src/slbo/utils/__init__.py b/experiments02/ant_umaze_1234/src/slbo/utils/__init__.py
new file mode 100644
index 0000000..5c7f19c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/utils/__init__.py
@@ -0,0 +1 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
diff --git a/experiments02/ant_umaze_1234/src/slbo/utils/average_meter.py b/experiments02/ant_umaze_1234/src/slbo/utils/average_meter.py
new file mode 100644
index 0000000..b3e285c
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/utils/average_meter.py
@@ -0,0 +1,20 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+class AverageMeter(object):
+    sum: float
+    count: float
+
+    def __init__(self, discount=1.):
+        self.discount = discount
+        self.reset()
+
+    def update(self, value, count=1):
+        self.sum = self.sum * self.discount + value * count
+        self.count = self.count * self.discount + count
+        return self.get()
+
+    def get(self):
+        return self.sum / (self.count + 1.e-8)
+
+    def reset(self):
+        self.sum = 0.
+        self.count = 0.
diff --git a/experiments02/ant_umaze_1234/src/slbo/utils/dataset.py b/experiments02/ant_umaze_1234/src/slbo/utils/dataset.py
new file mode 100644
index 0000000..c0046d2
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/utils/dataset.py
@@ -0,0 +1,28 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+import lunzi.dataset as dataset
+import gym
+
+
+def gen_dtype(env: gym.Env, fields: str):
+    dtypes = {
+        'state': ('state', 'f8', env.observation_space.shape),
+        'action': ('action', 'f8', env.action_space.shape),
+        'next_state': ('next_state', 'f8', env.observation_space.shape),
+        'reward': ('reward', 'f8'),
+        'done': ('done', 'bool'),
+        'timeout': ('timeout', 'bool'),
+        'return_': ('return_', 'f8'),
+        'advantage': ('advantage', 'f8'),
+    }
+    return [dtypes[field] for field in fields.split(' ')]
+
+
+class Dataset(dataset.Dataset):
+    def sample_multi_step(self, size: int, n_env: int, n_step=1):
+        starts = np.random.randint(0, self._len, size=size)
+        batch = []
+        for step in range(n_step):
+            batch.append(self[(starts + step * n_env) % self._len])
+        return np.concatenate(batch).reshape(n_step, size).view(np.recarray)
+
diff --git a/experiments02/ant_umaze_1234/src/slbo/utils/flags.py b/experiments02/ant_umaze_1234/src/slbo/utils/flags.py
new file mode 100644
index 0000000..8c7bb40
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/utils/flags.py
@@ -0,0 +1,163 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import time
+import os
+import yaml
+from subprocess import check_output, CalledProcessError
+from lunzi.config import BaseFLAGS, expand, parse
+from lunzi.Logger import logger, FileSink
+
+
+class FLAGS(BaseFLAGS):
+    _initialized = False
+
+    use_prev = True
+    seed = 100
+    log_dir: str = None
+    run_id: str = None
+    algorithm = 'OLBO'  # possible options: OLBO, baseline, MF
+
+    class pc(BaseFLAGS):
+        bonus_scale = 1
+        lamb = 0.01
+        bonus_stop_time= 30
+
+    class slbo(BaseFLAGS):
+        n_iters = 50
+        n_policy_iters = 10
+        n_model_iters = 100
+        n_stages = 100
+        n_evaluate_iters = 5
+        opt_model = False
+        start = 'reset'  # possibly 'buffer'
+
+    class plan(BaseFLAGS):
+        max_steps = 1000
+        n_envs = None
+        n_trpo_samples = 4000
+
+        @classmethod
+        def finalize(cls):
+            if cls.n_envs is None:
+                cls.n_envs = cls.n_trpo_samples // cls.max_steps
+            assert cls.n_envs * cls.max_steps == cls.n_trpo_samples
+
+    class env(BaseFLAGS):
+        id = 'HalfCheetah-v2'
+
+    class rollout(BaseFLAGS):
+        normalizer = 'policy'
+        max_buf_size = 100000
+        n_train_samples = 4000
+        n_dev_samples = 0
+        n_test_samples = 10000
+
+        @classmethod
+        def finalize(cls):
+            cls.n_dev_samples = cls.n_dev_samples or cls.n_train_samples
+
+    class ckpt(BaseFLAGS):
+        n_save_stages = 10
+        model_load = None
+        policy_load = None
+        buf_load = None
+        buf_load_index = 0
+        base = '/tmp/mbrl/logs'
+        warm_up = None
+
+        @classmethod
+        def finalize(cls):
+            for key, value in cls.as_dict().items():
+                if isinstance(value, str):
+                    setattr(cls, key, expand(value))
+
+    class OUNoise(BaseFLAGS):
+        theta = 0.15
+        sigma = 0.3
+
+    class model(BaseFLAGS):
+        hidden_sizes = [500, 500]
+        loss = 'L2'  # possibly L1, L2, MSE, G
+        G_coef = 0.5
+        multi_step = 1
+        lr = 1e-3
+        weight_decay = 1e-5
+        validation_freq = 1
+        optimizer = 'Adam'
+        train_batch_size = 256
+        dev_batch_size = 1024
+
+    class policy(BaseFLAGS):
+        hidden_sizes = [32, 32]
+        init_std = 1.
+
+    class PPO(BaseFLAGS):
+        n_minibatches = 32
+        n_opt_epochs = 10
+        ent_coef = 0.005
+        lr = 3e-4
+        clip_range = 0.2
+
+    class TRPO(BaseFLAGS):
+        cg_damping = 0.1
+        n_cg_iters = 10
+        max_kl = 0.01
+        vf_lr = 1e-3
+        n_vf_iters = 5
+        ent_coef = 0.0
+
+    class runner(BaseFLAGS):
+        lambda_ = 0.95
+        gamma = 0.99
+        max_steps = 500
+
+    @classmethod
+    def set_seed(cls):
+        if cls.seed == 0:  # auto seed
+            cls.seed = int.from_bytes(os.urandom(3), 'little') + 1  # never use seed 0 for RNG, 0 is for `urandom`
+        logger.warning("Setting random seed to %s", cls.seed)
+
+        import numpy as np
+        import tensorflow as tf
+        import random
+        np.random.seed(cls.seed)
+        tf.set_random_seed(np.random.randint(2**30))
+        random.seed(np.random.randint(2**30))
+
+    @classmethod
+    def finalize(cls):
+        log_dir = cls.log_dir
+        if log_dir is None:
+            run_id = cls.run_id
+            if run_id is None:
+                run_id = time.strftime('%Y-%m-%d_%H-%M-%S')
+
+            log_dir = os.path.join(cls.ckpt.base, run_id)
+            cls.log_dir = log_dir
+
+        if not os.path.exists(log_dir):
+            os.makedirs(log_dir)
+
+        for t in range(60):
+            try:
+                cls.commit = check_output(['git', 'rev-parse', 'HEAD']).decode('utf-8').strip()
+                check_output(['git', 'add', '.'])
+                check_output(['git', 'checkout-index', '-a', '-f', f'--prefix={log_dir}/src/'])
+                break
+            except CalledProcessError:
+                pass
+            time.sleep(1)
+        else:
+            raise RuntimeError('Failed after 60 trials.')
+
+        yaml.dump(cls.as_dict(), open(os.path.join(log_dir, 'config.yml'), 'w'), default_flow_style=False)
+        open(os.path.join(log_dir, 'diff.patch'), 'w').write(
+            check_output(['git', '--no-pager', 'diff', 'HEAD']).decode('utf-8'))
+
+        logger.add_sink(FileSink(os.path.join(log_dir, 'log.json')))
+        logger.info("log_dir = %s", log_dir)
+
+        cls.set_frozen()
+
+
+parse(FLAGS)
+
diff --git a/experiments02/ant_umaze_1234/src/slbo/utils/multi_layer_perceptron.py b/experiments02/ant_umaze_1234/src/slbo/utils/multi_layer_perceptron.py
new file mode 100644
index 0000000..0fe5dfe
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/utils/multi_layer_perceptron.py
@@ -0,0 +1,51 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+
+
+class MultiLayerPerceptron(nn.Module):
+    def __init__(self, blocks, activation=nn.ReLU, squeeze=False, weight_initializer=None, build=True):
+        super().__init__()
+
+        self._blocks = blocks
+        if build:
+            self.op_inputs = tf.placeholder(tf.float32, [None, self._blocks[0]])
+
+        with self.scope:
+            kwargs = {}
+            if weight_initializer is not None:
+                kwargs['weight_initializer'] = weight_initializer
+            layers = []
+            for in_features, out_features in zip(blocks[:-1], blocks[1:]):
+                if layers:
+                    layers.append(activation())
+                layers.append(nn.Linear(in_features, out_features, **kwargs))
+            if squeeze:
+                layers.append(nn.Squeeze(axis=1))
+            self.net = nn.Sequential(*layers)
+
+        self._squeeze = squeeze
+        self._activation = activation
+
+        if build:
+            self.build()
+
+    def build(self):
+        self.op_outputs = self.forward(self.op_inputs)
+
+    def forward(self, *inputs):
+        if len(inputs) > 1:
+            inputs = tf.concat(inputs, axis=-1)
+        else:
+            inputs = inputs[0]
+        return self.net(inputs)
+
+    def fast(self, *inputs):
+        return self.net.fast(np.concatenate(inputs, axis=-1))
+
+    def clone(self):
+        return MultiLayerPerceptron(self._blocks, self._activation, self._squeeze)
+
+    def extra_repr(self):
+        return f'activation = {self._activation}, blocks = {self._blocks}, squeeze = {self._squeeze}'
diff --git a/experiments02/ant_umaze_1234/src/slbo/utils/normalizer.py b/experiments02/ant_umaze_1234/src/slbo/utils/normalizer.py
new file mode 100644
index 0000000..3bb3685
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/utils/normalizer.py
@@ -0,0 +1,66 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List
+import numpy as np
+import tensorflow as tf
+import lunzi.nn as nn
+from lunzi.Logger import logger
+from lunzi import Tensor
+from slbo.utils.np_utils import gaussian_kl
+
+
+class GaussianNormalizer(nn.Module):
+    def __init__(self, name: str, shape: List[int], eps=1e-8, verbose=False):  # batch_size x ...
+        super().__init__()
+
+        self.name = name
+        self.shape = shape
+        self.eps = eps
+        self._verbose = verbose
+
+        with self.scope:
+            self.op_mean = nn.Parameter(tf.zeros(shape, dtype=tf.float32), name='mean', trainable=False)
+            self.op_std = nn.Parameter(tf.ones(shape, dtype=tf.float32), name='std', trainable=False)
+            self.op_n = nn.Parameter(tf.zeros([], dtype=tf.int64), name='n', trainable=False)
+
+    def extra_repr(self):
+        return f'shape={self.shape}'
+
+    def forward(self, x: Tensor, inverse=False):
+        if inverse:
+            return x * self.op_std + self.op_mean
+        return (x - self.op_mean).div(self.op_std.maximum(self.eps))
+
+    def update(self, samples: np.ndarray):
+        old_mean, old_std, old_n = self.op_mean.numpy(), self.op_std.numpy(), self.op_n.numpy()
+        samples = samples - old_mean
+
+        m = samples.shape[0]
+        delta = samples.mean(axis=0)
+        new_n = old_n + m
+        new_mean = old_mean + delta * m / new_n
+        new_std = np.sqrt((old_std**2 * old_n + samples.var(axis=0) * m + delta**2 * old_n * m / new_n) / new_n)
+
+        kl_old_new = gaussian_kl(new_mean, new_std, old_mean, old_std).sum()
+        self.load_state_dict({'op_mean': new_mean, 'op_std': new_std, 'op_n': new_n})
+
+        if self._verbose:
+            logger.info("updating Normalizer<%s>, KL divergence = %.6f", self.name, kl_old_new)
+
+    def fast(self, samples: np.ndarray, inverse=False) -> np.ndarray:
+        mean, std = self.op_mean.numpy(), self.op_std.numpy()
+        if inverse:
+            return samples * std + mean
+        return (samples - mean) / np.maximum(std, self.eps)
+
+
+class Normalizers(nn.Module):
+    def __init__(self, dim_action: int, dim_state: int):
+        super().__init__()
+        self.action = GaussianNormalizer('action', [dim_action])
+        self.state = GaussianNormalizer('state', [dim_state])
+        self.diff = GaussianNormalizer('diff', [dim_state])
+
+    def forward(self):
+        pass
+
+
diff --git a/experiments02/ant_umaze_1234/src/slbo/utils/np_utils.py b/experiments02/ant_umaze_1234/src/slbo/utils/np_utils.py
new file mode 100644
index 0000000..d170ed4
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/utils/np_utils.py
@@ -0,0 +1,9 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+
+
+def gaussian_kl(mean_1: np.ndarray, std_1: np.ndarray, mean_2: np.ndarray, std_2: np.ndarray) -> np.ndarray:
+    eps = 1e-20
+    std_1 = np.maximum(std_1, eps)
+    std_2 = np.maximum(std_2, eps)
+    return np.log(std_2 / std_1) + (std_1**2 + (mean_1 - mean_2)**2) / std_2**2 / 2. - 0.5
diff --git a/experiments02/ant_umaze_1234/src/slbo/utils/pc_utils.py b/experiments02/ant_umaze_1234/src/slbo/utils/pc_utils.py
new file mode 100644
index 0000000..d488ef2
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/utils/pc_utils.py
@@ -0,0 +1,58 @@
+import numpy as np
+import copy
+import os
+import scipy.spatial
+import scipy.signal
+
+
+def median_trick(X, args):
+    #median trick for computing the bandwith for kernel regression.
+    N = X.shape[0]
+    #print(X.shape)
+    perm = np.random.choice(N, np.min([N,args.update_size * args.buffer_width]), replace=False)
+    dsample = X[perm]
+    pd = scipy.spatial.distance.pdist(dsample)
+    sigma = np.median(pd)
+    return sigma
+
+def compute_cov_pi(phi):
+    #cov = np.zeros((phi.shape[1],phi.shape[1]))
+
+    #for i in range(len(phi)):
+    #    cov += np.outer(phi[i],phi[i])
+    cov = np.dot(phi.T,phi)
+    cov /= phi.shape[0]
+
+    #print(cov)
+
+    return cov
+
+def discount_cumsum(x, discount):
+    """
+    magic from rllab for computing discounted cumulative sums of vectors.
+    input:
+        vector x,
+        [x0,
+         x1,
+         x2]
+    output:
+        [x0 + discount * x1 + discount^2 * x2,
+         x1 + discount * x2,
+         x2]
+    """
+    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]
+
+
+
+
+def soft_update_from_to(source, target, tau):
+    for target_param, param in zip(target.parameters(), source.parameters()):
+        target_param.data.copy_(
+            target_param.data * (1.0 - tau) + param.data * tau
+        )
+
+def copy_model_params_from_to(source, target):
+    for target_param, param in zip(target.parameters(), source.parameters()):
+        target_param.data.copy_(param.data)
+
+
diff --git a/experiments02/ant_umaze_1234/src/slbo/utils/runner.py b/experiments02/ant_umaze_1234/src/slbo/utils/runner.py
new file mode 100644
index 0000000..774665d
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/utils/runner.py
@@ -0,0 +1,98 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from lunzi.dataset import Dataset
+from slbo.envs import BaseBatchedEnv
+from slbo.policies import BasePolicy
+from slbo.utils.dataset import Dataset, gen_dtype
+from slbo.v_function import BaseVFunction
+
+
+class Runner(object):
+    _states: np.ndarray  # [np.float]
+    _n_steps: np.ndarray
+    _returns: np.ndarray
+
+    def __init__(self, env: BaseBatchedEnv, max_steps: int, gamma=0.99, lambda_=0.95, rescale_action=False):
+        self.env = env
+        self.n_envs = env.n_envs
+        self.gamma = gamma
+        self.lambda_ = lambda_
+        self.max_steps = max_steps
+        self.rescale_action = rescale_action
+        self._dtype = gen_dtype(env, 'state action next_state reward done timeout')
+
+        self.reset()
+
+    def reset(self):
+        self.set_state(self.env.reset(), set_env_state=False)
+
+    def set_state(self, states: np.ndarray, set_env_state=True):
+        self._states = states.copy()
+        if set_env_state:
+            self.env.set_state(states)
+        self._n_steps = np.zeros(self.n_envs, 'i4')
+        self._returns = np.zeros(self.n_envs, 'f8')
+
+    def get_state(self):
+        return self._states.copy()
+
+    def run(self, policy: BasePolicy, n_samples: int, render=False):
+        ep_infos = []
+        n_steps = n_samples // self.n_envs
+        assert n_steps * self.n_envs == n_samples
+        dataset = Dataset(self._dtype, n_samples)
+
+        for T in range(n_steps):
+            unscaled_actions = policy.get_actions(self._states)
+            if self.rescale_action:
+                lo, hi = self.env.action_space.low, self.env.action_space.high
+                actions = (lo + (unscaled_actions + 1.) * 0.5 * (hi - lo))
+            else:
+                actions = unscaled_actions
+
+            next_states, rewards, dones, infos = self.env.step(actions)
+            if render:
+                #print(actions)
+                self.env.render()
+            dones = dones.astype(bool)
+            self._returns += rewards
+            self._n_steps += 1
+            timeouts = self._n_steps == self.max_steps
+
+            steps = [self._states.copy(), unscaled_actions, next_states.copy(), rewards, dones, timeouts]
+            dataset.extend(np.rec.fromarrays(steps, dtype=self._dtype))
+
+            indices = np.where(dones | timeouts)[0]
+            if len(indices) > 0:
+                next_states = next_states.copy()
+                next_states[indices] = self.env.partial_reset(indices)
+                for index in indices:
+                    infos[index]['episode'] = {'return': self._returns[index], 'success': (self._returns[index] > 50)}
+                self._n_steps[indices] = 0
+                self._returns[indices] = 0.
+
+            self._states = next_states.copy()
+            ep_infos.extend([info['episode'] for info in infos if 'episode' in info])
+
+        if render:
+            self.env.close()
+
+        return dataset, ep_infos
+
+    def compute_advantage(self, vfn: BaseVFunction, samples: Dataset):
+        n_steps = len(samples) // self.n_envs
+        samples = samples.reshape((n_steps, self.n_envs))
+        use_next_vf = ~samples.done
+        use_next_adv = ~(samples.done | samples.timeout)
+
+        next_values = vfn.get_values(samples[-1].next_state)
+        values = vfn.get_values(samples.reshape(-1).state).reshape(n_steps, self.n_envs)
+        advantages = np.zeros((n_steps, self.n_envs), dtype=np.float32)
+        last_gae_lambda = 0
+
+        for t in reversed(range(n_steps)):
+            delta = samples[t].reward + self.gamma * next_values * use_next_vf[t] - values[t]
+            advantages[t] = last_gae_lambda = delta + self.gamma * self.lambda_ * last_gae_lambda * use_next_adv[t]
+            next_values = values[t]
+        return advantages.reshape(-1), values.reshape(-1)
+
diff --git a/experiments02/ant_umaze_1234/src/slbo/utils/tf_utils.py b/experiments02/ant_umaze_1234/src/slbo/utils/tf_utils.py
new file mode 100644
index 0000000..154c60a
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/utils/tf_utils.py
@@ -0,0 +1,20 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+
+
+def get_tf_config():
+    gpu_frac = 1
+
+    gpu_options = tf.GPUOptions(
+        per_process_gpu_memory_fraction=gpu_frac,
+        allow_growth=True,
+    )
+    config = tf.ConfigProto(
+        gpu_options=gpu_options,
+        log_device_placement=False,
+        allow_soft_placement=True,
+        inter_op_parallelism_threads=1,
+        intra_op_parallelism_threads=1,
+    )
+
+    return config
diff --git a/experiments02/ant_umaze_1234/src/slbo/utils/truncated_normal.py b/experiments02/ant_umaze_1234/src/slbo/utils/truncated_normal.py
new file mode 100644
index 0000000..04a0edc
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/utils/truncated_normal.py
@@ -0,0 +1,12 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+import numpy as np
+
+
+class LimitedEntNormal(tf.distributions.Normal):
+    def _entropy(self):
+        limit = 2.
+        lo, hi = (-limit - self._loc) / self._scale / np.sqrt(2), (limit - self._loc) / self._scale / np.sqrt(2)
+        return 0.5 * (self._scale.log() + np.log(2 * np.pi) / 2) * (hi.erf() - lo.erf()) + 0.5 * \
+            (tf.exp(-hi * hi) * hi - tf.exp(-lo * lo) * lo)
+
diff --git a/experiments02/ant_umaze_1234/src/slbo/v_function/__init__.py b/experiments02/ant_umaze_1234/src/slbo/v_function/__init__.py
new file mode 100644
index 0000000..7794a84
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/v_function/__init__.py
@@ -0,0 +1,13 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import abc
+from typing import Union
+import lunzi.nn as nn
+
+
+class BaseVFunction(abc.ABC):
+    @abc.abstractmethod
+    def get_values(self, states):
+        pass
+
+
+BaseNNVFunction = Union[BaseVFunction, nn.Module]  # in fact it should be Intersection
\ No newline at end of file
diff --git a/experiments02/ant_umaze_1234/src/slbo/v_function/mlp_v_function.py b/experiments02/ant_umaze_1234/src/slbo/v_function/mlp_v_function.py
new file mode 100644
index 0000000..d9749e9
--- /dev/null
+++ b/experiments02/ant_umaze_1234/src/slbo/v_function/mlp_v_function.py
@@ -0,0 +1,24 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+from baselines.common.tf_util import normc_initializer
+from slbo.utils.multi_layer_perceptron import MultiLayerPerceptron
+import lunzi.nn as nn
+from . import BaseVFunction
+
+
+class MLPVFunction(BaseVFunction, nn.Module):
+    def __init__(self, dim_state, hidden_sizes, normalizer=None):
+        super().__init__()
+        self.mlp = MultiLayerPerceptron((dim_state, *hidden_sizes, 1), activation=nn.Tanh, squeeze=True,
+                                        weight_initializer=normc_initializer(1.), build=False)
+        self.normalizer = normalizer
+        self.op_states = tf.placeholder(tf.float32, shape=[None, dim_state])
+        self.op_values = self.forward(self.op_states)
+
+    def forward(self, states):
+        states = self.normalizer(states)
+        return self.mlp(states)
+
+    @nn.make_method(fetch='values')
+    def get_values(self, states): pass
+
diff --git a/experiments02/ant_umaze_1234/stage-0.npy b/experiments02/ant_umaze_1234/stage-0.npy
new file mode 100644
index 0000000..53a5017
Binary files /dev/null and b/experiments02/ant_umaze_1234/stage-0.npy differ
diff --git a/experiments02/ant_umaze_1234/stage-10.npy b/experiments02/ant_umaze_1234/stage-10.npy
new file mode 100644
index 0000000..c3f5766
Binary files /dev/null and b/experiments02/ant_umaze_1234/stage-10.npy differ
diff --git a/experiments02/ant_umaze_1234/stage-20.npy b/experiments02/ant_umaze_1234/stage-20.npy
new file mode 100644
index 0000000..f49a404
Binary files /dev/null and b/experiments02/ant_umaze_1234/stage-20.npy differ
diff --git a/experiments02/ant_umaze_1234/stage-30.npy b/experiments02/ant_umaze_1234/stage-30.npy
new file mode 100644
index 0000000..0918a5f
Binary files /dev/null and b/experiments02/ant_umaze_1234/stage-30.npy differ
diff --git a/experiments02/ant_umaze_1234/stage-40.npy b/experiments02/ant_umaze_1234/stage-40.npy
new file mode 100644
index 0000000..bea1822
Binary files /dev/null and b/experiments02/ant_umaze_1234/stage-40.npy differ
diff --git a/experiments02/ant_umaze_1234/timesteps.npy b/experiments02/ant_umaze_1234/timesteps.npy
new file mode 100644
index 0000000..f387f40
Binary files /dev/null and b/experiments02/ant_umaze_1234/timesteps.npy differ
diff --git a/experiments05/ant_umaze_1234/config.yml b/experiments05/ant_umaze_1234/config.yml
new file mode 100644
index 0000000..cad5fe6
--- /dev/null
+++ b/experiments05/ant_umaze_1234/config.yml
@@ -0,0 +1,76 @@
+OUNoise:
+  sigma: 0.3
+  theta: 0.15
+PPO:
+  clip_range: 0.2
+  ent_coef: 0.005
+  lr: 0.0003
+  n_minibatches: 32
+  n_opt_epochs: 10
+TRPO:
+  cg_damping: 0.1
+  ent_coef: 0
+  max_kl: 0.01
+  n_cg_iters: 10
+  n_vf_iters: 5
+  vf_lr: 0.001
+algorithm: OLBO
+ckpt:
+  base: /tmp/mbrl/logs
+  buf_load: null
+  buf_load_index: 0
+  model_load: null
+  n_save_stages: 10
+  policy_load: null
+  warm_up: null
+commit: 7315421ee85a3e3a88f7b5e93c10320b95aa84c5
+env:
+  id: AntUMaze-v1
+log_dir: ./experiments05/ant_umaze_1234
+model:
+  G_coef: 0.5
+  dev_batch_size: 512
+  hidden_sizes:
+  - 500
+  - 500
+  loss: L2
+  lr: 0.001
+  multi_step: 2
+  optimizer: Adam
+  train_batch_size: 128
+  validation_freq: 1
+  weight_decay: 1.0e-05
+pc:
+  bonus_scale: 0.5
+  bonus_stop_time: 30
+  lamb: 0.01
+plan:
+  max_steps: 1000
+  n_envs: 4
+  n_trpo_samples: 4000
+policy:
+  hidden_sizes:
+  - 32
+  - 32
+  init_std: 1.0
+rollout:
+  max_buf_size: 100000
+  n_dev_samples: 4000
+  n_test_samples: 10000
+  n_train_samples: 4000
+  normalizer: policy
+run_id: null
+runner:
+  gamma: 0.99
+  lambda_: 0.95
+  max_steps: 1000
+seed: 1234
+slbo:
+  n_evaluate_iters: 5
+  n_iters: 20
+  n_model_iters: 100
+  n_policy_iters: 40
+  n_stages: 50
+  opt_model: false
+  start: reset
+use_prev: true
diff --git a/experiments05/ant_umaze_1234/diff.patch b/experiments05/ant_umaze_1234/diff.patch
new file mode 100644
index 0000000..cf39b3b
--- /dev/null
+++ b/experiments05/ant_umaze_1234/diff.patch
@@ -0,0 +1,156 @@
+diff --git a/configs/env_tingwu/ant_umaze.yml b/configs/env_tingwu/ant_umaze.yml
+index f653d95..ab75516 100644
+--- a/configs/env_tingwu/ant_umaze.yml
++++ b/configs/env_tingwu/ant_umaze.yml
+@@ -5,6 +5,4 @@ runner:
+   max_steps: 1000
+ plan:
+   max_steps: 1000
+-pc: 
+-  bonus_scale: 0.1  
+ 
+diff --git a/lunzi/__pycache__/Logger.cpython-36.pyc b/lunzi/__pycache__/Logger.cpython-36.pyc
+index c62e4dc..cd65096 100644
+Binary files a/lunzi/__pycache__/Logger.cpython-36.pyc and b/lunzi/__pycache__/Logger.cpython-36.pyc differ
+diff --git a/lunzi/__pycache__/__init__.cpython-36.pyc b/lunzi/__pycache__/__init__.cpython-36.pyc
+index d0052a1..bb94071 100644
+Binary files a/lunzi/__pycache__/__init__.cpython-36.pyc and b/lunzi/__pycache__/__init__.cpython-36.pyc differ
+diff --git a/lunzi/__pycache__/config.cpython-36.pyc b/lunzi/__pycache__/config.cpython-36.pyc
+index 38e50cc..151d0cd 100644
+Binary files a/lunzi/__pycache__/config.cpython-36.pyc and b/lunzi/__pycache__/config.cpython-36.pyc differ
+diff --git a/lunzi/__pycache__/dataset.cpython-36.pyc b/lunzi/__pycache__/dataset.cpython-36.pyc
+index f5f173f..673cfda 100644
+Binary files a/lunzi/__pycache__/dataset.cpython-36.pyc and b/lunzi/__pycache__/dataset.cpython-36.pyc differ
+diff --git a/lunzi/__pycache__/stubs.cpython-36.pyc b/lunzi/__pycache__/stubs.cpython-36.pyc
+index 40d1d88..ac6d28a 100644
+Binary files a/lunzi/__pycache__/stubs.cpython-36.pyc and b/lunzi/__pycache__/stubs.cpython-36.pyc differ
+diff --git a/lunzi/nn/__pycache__/__init__.cpython-36.pyc b/lunzi/nn/__pycache__/__init__.cpython-36.pyc
+index 2082353..d28b7c9 100644
+Binary files a/lunzi/nn/__pycache__/__init__.cpython-36.pyc and b/lunzi/nn/__pycache__/__init__.cpython-36.pyc differ
+diff --git a/lunzi/nn/__pycache__/container.cpython-36.pyc b/lunzi/nn/__pycache__/container.cpython-36.pyc
+index fd7039b..f16264f 100644
+Binary files a/lunzi/nn/__pycache__/container.cpython-36.pyc and b/lunzi/nn/__pycache__/container.cpython-36.pyc differ
+diff --git a/lunzi/nn/__pycache__/flat_param.cpython-36.pyc b/lunzi/nn/__pycache__/flat_param.cpython-36.pyc
+index dc8e6f9..792725a 100644
+Binary files a/lunzi/nn/__pycache__/flat_param.cpython-36.pyc and b/lunzi/nn/__pycache__/flat_param.cpython-36.pyc differ
+diff --git a/lunzi/nn/__pycache__/layers.cpython-36.pyc b/lunzi/nn/__pycache__/layers.cpython-36.pyc
+index 3f22fba..9fd6751 100644
+Binary files a/lunzi/nn/__pycache__/layers.cpython-36.pyc and b/lunzi/nn/__pycache__/layers.cpython-36.pyc differ
+diff --git a/lunzi/nn/__pycache__/loss.cpython-36.pyc b/lunzi/nn/__pycache__/loss.cpython-36.pyc
+index 9fa218d..10a1d20 100644
+Binary files a/lunzi/nn/__pycache__/loss.cpython-36.pyc and b/lunzi/nn/__pycache__/loss.cpython-36.pyc differ
+diff --git a/lunzi/nn/__pycache__/module.cpython-36.pyc b/lunzi/nn/__pycache__/module.cpython-36.pyc
+index c0f0f4a..9088e26 100644
+Binary files a/lunzi/nn/__pycache__/module.cpython-36.pyc and b/lunzi/nn/__pycache__/module.cpython-36.pyc differ
+diff --git a/lunzi/nn/__pycache__/parameter.cpython-36.pyc b/lunzi/nn/__pycache__/parameter.cpython-36.pyc
+index 5191416..2131727 100644
+Binary files a/lunzi/nn/__pycache__/parameter.cpython-36.pyc and b/lunzi/nn/__pycache__/parameter.cpython-36.pyc differ
+diff --git a/lunzi/nn/__pycache__/patch.cpython-36.pyc b/lunzi/nn/__pycache__/patch.cpython-36.pyc
+index d535500..78a52be 100644
+Binary files a/lunzi/nn/__pycache__/patch.cpython-36.pyc and b/lunzi/nn/__pycache__/patch.cpython-36.pyc differ
+diff --git a/lunzi/nn/__pycache__/utils.cpython-36.pyc b/lunzi/nn/__pycache__/utils.cpython-36.pyc
+index 1c808e8..f1860e7 100644
+Binary files a/lunzi/nn/__pycache__/utils.cpython-36.pyc and b/lunzi/nn/__pycache__/utils.cpython-36.pyc differ
+diff --git a/run2.sh b/run2.sh
+new file mode 100644
+index 0000000..7440dc5
+--- /dev/null
++++ b/run2.sh
+@@ -0,0 +1,10 @@
++#!/usr/bin/env bash
++
++for env_name in $1; do
++    echo "=> Running environment ${env_name}"
++    #for random_seed in 1234 2314 2345 1235; do
++    for random_seed in 1234; do
++        python main.py -c configs/algos/slbo_bm_200k.yml configs/env_tingwu/${env_name}.yml \
++	    -s pc.bonus_scale=0.2 log_dir=./experiments02/${env_name}_${random_seed} seed=${random_seed}
++    done
++done
+diff --git a/run_experiments.sh b/run_experiments.sh
+index 37ce016..e3f2ad7 100644
+--- a/run_experiments.sh
++++ b/run_experiments.sh
+@@ -3,8 +3,8 @@
+ for env_name in $1; do
+     echo "=> Running environment ${env_name}"
+     #for random_seed in 1234 2314 2345 1235; do
+-    for random_seed in 19; do
++    for random_seed in 1234; do
+         python main.py -c configs/algos/slbo_bm_200k.yml configs/env_tingwu/${env_name}.yml \
+-	    -s log_dir=./experiments/${env_name}_${random_seed} seed=${random_seed}
++	    -s pc.bonus_scale=0.5 log_dir=./experiments05/${env_name}_${random_seed} seed=${random_seed}
+     done
+ done
+diff --git a/slbo/__pycache__/__init__.cpython-36.pyc b/slbo/__pycache__/__init__.cpython-36.pyc
+index 069fa36..6a33858 100644
+Binary files a/slbo/__pycache__/__init__.cpython-36.pyc and b/slbo/__pycache__/__init__.cpython-36.pyc differ
+diff --git a/slbo/__pycache__/dynamics_model.cpython-36.pyc b/slbo/__pycache__/dynamics_model.cpython-36.pyc
+index d9193cb..91423ce 100644
+Binary files a/slbo/__pycache__/dynamics_model.cpython-36.pyc and b/slbo/__pycache__/dynamics_model.cpython-36.pyc differ
+diff --git a/slbo/__pycache__/partial_envs.cpython-36.pyc b/slbo/__pycache__/partial_envs.cpython-36.pyc
+index f20520d..7ad21fc 100644
+Binary files a/slbo/__pycache__/partial_envs.cpython-36.pyc and b/slbo/__pycache__/partial_envs.cpython-36.pyc differ
+diff --git a/slbo/__pycache__/random_net.cpython-36.pyc b/slbo/__pycache__/random_net.cpython-36.pyc
+index 7688576..6e866b0 100644
+Binary files a/slbo/__pycache__/random_net.cpython-36.pyc and b/slbo/__pycache__/random_net.cpython-36.pyc differ
+diff --git a/slbo/algos/__pycache__/TRPO.cpython-36.pyc b/slbo/algos/__pycache__/TRPO.cpython-36.pyc
+index 99cdd6a..9ba8329 100644
+Binary files a/slbo/algos/__pycache__/TRPO.cpython-36.pyc and b/slbo/algos/__pycache__/TRPO.cpython-36.pyc differ
+diff --git a/slbo/algos/__pycache__/__init__.cpython-36.pyc b/slbo/algos/__pycache__/__init__.cpython-36.pyc
+index 09f2f9c..9c74708 100644
+Binary files a/slbo/algos/__pycache__/__init__.cpython-36.pyc and b/slbo/algos/__pycache__/__init__.cpython-36.pyc differ
+diff --git a/slbo/loss/__pycache__/__init__.cpython-36.pyc b/slbo/loss/__pycache__/__init__.cpython-36.pyc
+index f1c5979..cc5457b 100644
+Binary files a/slbo/loss/__pycache__/__init__.cpython-36.pyc and b/slbo/loss/__pycache__/__init__.cpython-36.pyc differ
+diff --git a/slbo/loss/__pycache__/multi_step_loss.cpython-36.pyc b/slbo/loss/__pycache__/multi_step_loss.cpython-36.pyc
+index 53c9a3d..695e469 100644
+Binary files a/slbo/loss/__pycache__/multi_step_loss.cpython-36.pyc and b/slbo/loss/__pycache__/multi_step_loss.cpython-36.pyc differ
+diff --git a/slbo/policies/__pycache__/__init__.cpython-36.pyc b/slbo/policies/__pycache__/__init__.cpython-36.pyc
+index a98e999..0f2ff6d 100644
+Binary files a/slbo/policies/__pycache__/__init__.cpython-36.pyc and b/slbo/policies/__pycache__/__init__.cpython-36.pyc differ
+diff --git a/slbo/policies/__pycache__/gaussian_mlp_policy.cpython-36.pyc b/slbo/policies/__pycache__/gaussian_mlp_policy.cpython-36.pyc
+index b355a90..3f0a156 100644
+Binary files a/slbo/policies/__pycache__/gaussian_mlp_policy.cpython-36.pyc and b/slbo/policies/__pycache__/gaussian_mlp_policy.cpython-36.pyc differ
+diff --git a/slbo/utils/__pycache__/OU_noise.cpython-36.pyc b/slbo/utils/__pycache__/OU_noise.cpython-36.pyc
+index 5819e5d..bdfb380 100644
+Binary files a/slbo/utils/__pycache__/OU_noise.cpython-36.pyc and b/slbo/utils/__pycache__/OU_noise.cpython-36.pyc differ
+diff --git a/slbo/utils/__pycache__/__init__.cpython-36.pyc b/slbo/utils/__pycache__/__init__.cpython-36.pyc
+index 0e85b1e..9df4fd9 100644
+Binary files a/slbo/utils/__pycache__/__init__.cpython-36.pyc and b/slbo/utils/__pycache__/__init__.cpython-36.pyc differ
+diff --git a/slbo/utils/__pycache__/average_meter.cpython-36.pyc b/slbo/utils/__pycache__/average_meter.cpython-36.pyc
+index 1bc0297..7fa8339 100644
+Binary files a/slbo/utils/__pycache__/average_meter.cpython-36.pyc and b/slbo/utils/__pycache__/average_meter.cpython-36.pyc differ
+diff --git a/slbo/utils/__pycache__/dataset.cpython-36.pyc b/slbo/utils/__pycache__/dataset.cpython-36.pyc
+index 0cc5d73..46a212c 100644
+Binary files a/slbo/utils/__pycache__/dataset.cpython-36.pyc and b/slbo/utils/__pycache__/dataset.cpython-36.pyc differ
+diff --git a/slbo/utils/__pycache__/flags.cpython-36.pyc b/slbo/utils/__pycache__/flags.cpython-36.pyc
+index dfd7feb..193bff4 100644
+Binary files a/slbo/utils/__pycache__/flags.cpython-36.pyc and b/slbo/utils/__pycache__/flags.cpython-36.pyc differ
+diff --git a/slbo/utils/__pycache__/multi_layer_perceptron.cpython-36.pyc b/slbo/utils/__pycache__/multi_layer_perceptron.cpython-36.pyc
+index ae136c1..eceaef9 100644
+Binary files a/slbo/utils/__pycache__/multi_layer_perceptron.cpython-36.pyc and b/slbo/utils/__pycache__/multi_layer_perceptron.cpython-36.pyc differ
+diff --git a/slbo/utils/__pycache__/normalizer.cpython-36.pyc b/slbo/utils/__pycache__/normalizer.cpython-36.pyc
+index 8c67648..574fc2b 100644
+Binary files a/slbo/utils/__pycache__/normalizer.cpython-36.pyc and b/slbo/utils/__pycache__/normalizer.cpython-36.pyc differ
+diff --git a/slbo/utils/__pycache__/np_utils.cpython-36.pyc b/slbo/utils/__pycache__/np_utils.cpython-36.pyc
+index e1f3150..d9d1d69 100644
+Binary files a/slbo/utils/__pycache__/np_utils.cpython-36.pyc and b/slbo/utils/__pycache__/np_utils.cpython-36.pyc differ
+diff --git a/slbo/utils/__pycache__/pc_utils.cpython-36.pyc b/slbo/utils/__pycache__/pc_utils.cpython-36.pyc
+index 0e894f3..b76bb46 100644
+Binary files a/slbo/utils/__pycache__/pc_utils.cpython-36.pyc and b/slbo/utils/__pycache__/pc_utils.cpython-36.pyc differ
+diff --git a/slbo/utils/__pycache__/runner.cpython-36.pyc b/slbo/utils/__pycache__/runner.cpython-36.pyc
+index 2678071..cce1028 100644
+Binary files a/slbo/utils/__pycache__/runner.cpython-36.pyc and b/slbo/utils/__pycache__/runner.cpython-36.pyc differ
+diff --git a/slbo/utils/__pycache__/tf_utils.cpython-36.pyc b/slbo/utils/__pycache__/tf_utils.cpython-36.pyc
+index 47606c4..59378e0 100644
+Binary files a/slbo/utils/__pycache__/tf_utils.cpython-36.pyc and b/slbo/utils/__pycache__/tf_utils.cpython-36.pyc differ
+diff --git a/slbo/utils/__pycache__/truncated_normal.cpython-36.pyc b/slbo/utils/__pycache__/truncated_normal.cpython-36.pyc
+index 6d8d1b4..a615a74 100644
+Binary files a/slbo/utils/__pycache__/truncated_normal.cpython-36.pyc and b/slbo/utils/__pycache__/truncated_normal.cpython-36.pyc differ
+diff --git a/slbo/v_function/__pycache__/__init__.cpython-36.pyc b/slbo/v_function/__pycache__/__init__.cpython-36.pyc
+index a38ef07..5932be7 100644
+Binary files a/slbo/v_function/__pycache__/__init__.cpython-36.pyc and b/slbo/v_function/__pycache__/__init__.cpython-36.pyc differ
+diff --git a/slbo/v_function/__pycache__/mlp_v_function.cpython-36.pyc b/slbo/v_function/__pycache__/mlp_v_function.cpython-36.pyc
+index dba380d..06cd147 100644
+Binary files a/slbo/v_function/__pycache__/mlp_v_function.cpython-36.pyc and b/slbo/v_function/__pycache__/mlp_v_function.cpython-36.pyc differ
diff --git a/experiments05/ant_umaze_1234/eval_real_returns.npy b/experiments05/ant_umaze_1234/eval_real_returns.npy
new file mode 100644
index 0000000..832b516
Binary files /dev/null and b/experiments05/ant_umaze_1234/eval_real_returns.npy differ
diff --git a/experiments05/ant_umaze_1234/final.npy b/experiments05/ant_umaze_1234/final.npy
new file mode 100644
index 0000000..e06d831
Binary files /dev/null and b/experiments05/ant_umaze_1234/final.npy differ
diff --git a/experiments05/ant_umaze_1234/log.json b/experiments05/ant_umaze_1234/log.json
new file mode 100644
index 0000000..ba20205
--- /dev/null
+++ b/experiments05/ant_umaze_1234/log.json
@@ -0,0 +1,1245 @@
+{"level": "info", "fmt": "log_dir = %s", "args": ["./experiments05/ant_umaze_1234"], "caller": "slbo/utils/flags.py:157", "time": "2021-02-04T22:06:16.576636"}
+{"level": "info", "fmt": "Enabling flattening... %s", "args": [["GaussianMLPPolicy_1/log_std:0", "GaussianMLPPolicy_1/Linear_1/weight:0", "GaussianMLPPolicy_1/Linear_1/bias:0", "GaussianMLPPolicy_1/Linear_1_2/weight:0", "GaussianMLPPolicy_1/Linear_1_2/bias:0", "GaussianMLPPolicy_1/Linear_2_1/weight:0", "GaussianMLPPolicy_1/Linear_2_1/bias:0"]], "caller": "lunzi/nn/flat_param.py:17", "time": "2021-02-04T22:06:18.960629"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [0], "caller": "main.py:129", "time": "2021-02-04T22:06:19.998601"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-04T22:06:28.500010"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["DynamicsModel", "[states actions] => [next_states]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:28.502822"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["RandomNet", "[states actions] => [features]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:28.538762"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, -7.284889785537853, 0.1432991688976139], "caller": "main.py:37", "time": "2021-02-04T22:06:33.022952"}
+{"level": "info", "fmt": "episode: %s", "args": [1.5049363137840386], "caller": "main.py:149", "time": "2021-02-04T22:06:42.796739"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["MultiStepLoss", "[states next_states_ actions masks] => [train loss grad_norm]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:43.195288"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["MultiStepLoss", "[states next_states_ actions masks] => [loss]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:45.545221"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.9378997683525085, 0.8658612370491028, 100, 0.0445103133518442], "caller": "main.py:200", "time": "2021-02-04T22:06:45.625833"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["MLPVFunction", "[states] => [values]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:47.284993"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["TRPO", "[] => [sync_old]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:47.342512"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["TRPO", "[states actions advantages ent_coef] => [loss flat_grad dist_std mean_kl dist_mean]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:47.366344"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["TRPO", "[states tangents actions] => [hessian_vec_prod]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:47.508742"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["FlatParam", "[] => [get_flat]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:47.808253"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["FlatParam", "[feed_flat] => [set_flat]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:47.854704"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["TRPO", "[states actions advantages ent_coef] => [loss mean_kl]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:47.886948"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["TRPO", "[states returns] => [train_vf vf_loss]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:47.948213"}
+{"level": "info", "fmt": "[%s] is making TensorFlow callables, key = %s", "args": ["TRPO", "[states returns] => [vf_loss]"], "caller": "lunzi/nn/module.py:71", "time": "2021-02-04T22:06:48.262897"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.016324544325470924, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-04T22:07:43.593863"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.7674996852874756, 0.6594032049179077, 100, 0.18592303832996326], "caller": "main.py:200", "time": "2021-02-04T22:08:10.697304"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.6140119433403015, 0.5865762829780579, 100, 0.29068448287322], "caller": "main.py:200", "time": "2021-02-04T22:09:35.164936"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.5615626573562622, 0.5586785078048706, 100, 0.3232737593029521], "caller": "main.py:200", "time": "2021-02-04T22:10:59.071856"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.5196434259414673, 0.5130995512008667, 100, 0.3634198417896201], "caller": "main.py:200", "time": "2021-02-04T22:12:25.863775"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.48745059967041016, 0.47261008620262146, 100, 0.381774088106125], "caller": "main.py:200", "time": "2021-02-04T22:13:49.000563"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.015538932755589485, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-04T22:14:04.970451"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.46413275599479675, 0.43267542123794556, 100, 0.40544601227758975], "caller": "main.py:200", "time": "2021-02-04T22:15:12.233352"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.4409376084804535, 0.43018847703933716, 100, 0.4347987842125016], "caller": "main.py:200", "time": "2021-02-04T22:16:35.703169"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.4228054881095886, 0.40959271788597107, 100, 0.4525770934963645], "caller": "main.py:200", "time": "2021-02-04T22:17:59.243764"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.4062086045742035, 0.39805346727371216, 100, 0.464379822863394], "caller": "main.py:200", "time": "2021-02-04T22:19:22.427830"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.38758179545402527, 0.3727763295173645, 100, 0.4922852912052919], "caller": "main.py:200", "time": "2021-02-04T22:20:45.706091"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.36915135383605957, 0.36561858654022217, 100, 0.5079268761961365], "caller": "main.py:200", "time": "2021-02-04T22:22:09.004981"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.3513016402721405, 0.3548884391784668, 100, 0.5363482841312979], "caller": "main.py:200", "time": "2021-02-04T22:23:32.073457"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.3346582353115082, 0.3185774087905884, 100, 0.5705401512409921], "caller": "main.py:200", "time": "2021-02-04T22:24:55.240207"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.3141838014125824, 0.3186214864253998, 100, 0.5858541428456963], "caller": "main.py:200", "time": "2021-02-04T22:26:18.546528"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2984384596347809, 0.29204872250556946, 100, 0.6008404158945059], "caller": "main.py:200", "time": "2021-02-04T22:27:41.838724"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.28569743037223816, 0.27806591987609863, 100, 0.672443496517648], "caller": "main.py:200", "time": "2021-02-04T22:29:05.087587"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.27058354020118713, 0.26003047823905945, 100, 0.7046934481750561], "caller": "main.py:200", "time": "2021-02-04T22:30:28.263191"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2567311227321625, 0.24795295298099518, 100, 0.6876876067427548], "caller": "main.py:200", "time": "2021-02-04T22:31:51.223327"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.24300675094127655, 0.24058246612548828, 100, 0.6691345238016474], "caller": "main.py:200", "time": "2021-02-04T22:33:14.495438"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [1], "caller": "main.py:129", "time": "2021-02-04T22:34:35.655817"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-04T22:34:44.369043"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 27.192308183330308, 24.760283378785978], "caller": "main.py:37", "time": "2021-02-04T22:34:48.267922"}
+{"level": "info", "fmt": "episode: %s", "args": [1.489976007330652], "caller": "main.py:149", "time": "2021-02-04T22:34:58.121211"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.3885601758956909, 0.350123792886734, 100, 0.5928953191045098], "caller": "main.py:200", "time": "2021-02-04T22:35:00.283383"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.34407249093055725, 0.34708529710769653, 100, 0.5578546842374846], "caller": "main.py:200", "time": "2021-02-04T22:36:23.252016"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.32253608107566833, 0.3255731463432312, 100, 0.5708341010834157], "caller": "main.py:200", "time": "2021-02-04T22:37:46.110918"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.313480943441391, 0.30371248722076416, 100, 0.6192356213308033], "caller": "main.py:200", "time": "2021-02-04T22:39:09.192702"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2979777455329895, 0.29178136587142944, 100, 0.5731983556770285], "caller": "main.py:200", "time": "2021-02-04T22:40:31.981213"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2904617190361023, 0.2777021527290344, 100, 0.6256044807404524], "caller": "main.py:200", "time": "2021-02-04T22:41:54.926837"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.28052788972854614, 0.2641128897666931, 100, 0.6070984631169711], "caller": "main.py:200", "time": "2021-02-04T22:43:17.806912"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2707897424697876, 0.27002131938934326, 100, 0.5908373644357169], "caller": "main.py:200", "time": "2021-02-04T22:44:40.655936"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2660534381866455, 0.2636871635913849, 100, 0.6013584124440604], "caller": "main.py:200", "time": "2021-02-04T22:46:03.514508"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.25895369052886963, 0.25655531883239746, 100, 0.6216240509722881], "caller": "main.py:200", "time": "2021-02-04T22:47:26.590334"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.25077924132347107, 0.2553086280822754, 100, 0.6017815517777431], "caller": "main.py:200", "time": "2021-02-04T22:48:49.516891"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.24886669218540192, 0.23380886018276215, 100, 0.6649544762900109], "caller": "main.py:200", "time": "2021-02-04T22:50:12.335035"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.24325209856033325, 0.23574472963809967, 100, 0.6566666015325962], "caller": "main.py:200", "time": "2021-02-04T22:51:35.301970"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23818263411521912, 0.23670199513435364, 100, 0.6738321166556774], "caller": "main.py:200", "time": "2021-02-04T22:52:58.254151"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23493073880672455, 0.2335895597934723, 100, 0.6719666754527466], "caller": "main.py:200", "time": "2021-02-04T22:54:21.252748"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2304995208978653, 0.23062093555927277, 100, 0.692142946650909], "caller": "main.py:200", "time": "2021-02-04T22:55:44.285615"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22534342110157013, 0.23156443238258362, 100, 0.6872498794702879], "caller": "main.py:200", "time": "2021-02-04T22:57:07.401667"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22134967148303986, 0.22219376266002655, 100, 0.648625861639487], "caller": "main.py:200", "time": "2021-02-04T22:58:30.390541"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.21619480848312378, 0.2310514897108078, 100, 0.6517196726147292], "caller": "main.py:200", "time": "2021-02-04T22:59:53.255547"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.21469898521900177, 0.2038353532552719, 100, 0.6756238823453481], "caller": "main.py:200", "time": "2021-02-04T23:01:16.160010"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [2], "caller": "main.py:129", "time": "2021-02-04T23:02:37.002820"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-04T23:02:45.523438"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 13.267047479582477, 1.9088918151755696], "caller": "main.py:37", "time": "2021-02-04T23:02:49.424144"}
+{"level": "info", "fmt": "episode: %s", "args": [1.487764008239687], "caller": "main.py:149", "time": "2021-02-04T23:02:58.913970"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.3144923150539398, 0.2991337478160858, 100, 0.6635317563347088], "caller": "main.py:200", "time": "2021-02-04T23:03:01.012835"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.28729701042175293, 0.3148777484893799, 100, 0.6241637703152196], "caller": "main.py:200", "time": "2021-02-04T23:04:23.954885"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.27259671688079834, 0.2707923650741577, 100, 0.5935929482581686], "caller": "main.py:200", "time": "2021-02-04T23:05:46.955610"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2694573998451233, 0.2647353410720825, 100, 0.5981501745579524], "caller": "main.py:200", "time": "2021-02-04T23:07:09.871475"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.26527607440948486, 0.258899450302124, 100, 0.6539499264301571], "caller": "main.py:200", "time": "2021-02-04T23:08:32.990118"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2576935589313507, 0.2569398283958435, 100, 0.6086306899177365], "caller": "main.py:200", "time": "2021-02-04T23:09:55.835832"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.25381457805633545, 0.24574750661849976, 100, 0.6171740683296058], "caller": "main.py:200", "time": "2021-02-04T23:11:18.832240"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.022534703835844994, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-04T23:11:26.694358"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2511337697505951, 0.25088873505592346, 100, 0.6123427557332909], "caller": "main.py:200", "time": "2021-02-04T23:12:41.665803"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.24774698913097382, 0.2308853566646576, 100, 0.6431975745511588], "caller": "main.py:200", "time": "2021-02-04T23:14:04.341207"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.24619396030902863, 0.24259039759635925, 100, 0.651422682339376], "caller": "main.py:200", "time": "2021-02-04T23:15:27.195008"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.24316495656967163, 0.265470027923584, 100, 0.6222608702752602], "caller": "main.py:200", "time": "2021-02-04T23:16:50.085719"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2408212423324585, 0.23947125673294067, 100, 0.6285275407997075], "caller": "main.py:200", "time": "2021-02-04T23:18:12.936986"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23787333071231842, 0.23752161860466003, 100, 0.6805508797680274], "caller": "main.py:200", "time": "2021-02-04T23:19:35.890560"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23594336211681366, 0.23638111352920532, 100, 0.6598031651313921], "caller": "main.py:200", "time": "2021-02-04T23:20:58.793084"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23323023319244385, 0.2264213263988495, 100, 0.6622032528453474], "caller": "main.py:200", "time": "2021-02-04T23:22:21.709722"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22890880703926086, 0.22115200757980347, 100, 0.6395868932561249], "caller": "main.py:200", "time": "2021-02-04T23:23:44.646134"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22666774690151215, 0.2260974496603012, 100, 0.6247618656725779], "caller": "main.py:200", "time": "2021-02-04T23:25:07.563841"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22472494840621948, 0.22024323046207428, 100, 0.6225398712727757], "caller": "main.py:200", "time": "2021-02-04T23:26:30.493156"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22352708876132965, 0.21198055148124695, 100, 0.6530370154323808], "caller": "main.py:200", "time": "2021-02-04T23:27:53.313411"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.22073757648468018, 0.2294287532567978, 100, 0.662848564320083], "caller": "main.py:200", "time": "2021-02-04T23:29:16.214194"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.015084369108080864, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-04T23:29:34.103943"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [3], "caller": "main.py:129", "time": "2021-02-04T23:30:36.847258"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-04T23:30:45.167955"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 47.932426984214715, 40.372009165803384], "caller": "main.py:37", "time": "2021-02-04T23:30:49.084754"}
+{"level": "info", "fmt": "episode: %s", "args": [-0.7433383015969701], "caller": "main.py:149", "time": "2021-02-04T23:30:58.423775"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2656884491443634, 0.2537093758583069, 100, 0.6232987427088188], "caller": "main.py:200", "time": "2021-02-04T23:31:00.517908"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.016556521877646446, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-04T23:31:42.621634"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2495730221271515, 0.2373039424419403, 100, 0.6469324394413607], "caller": "main.py:200", "time": "2021-02-04T23:32:23.377174"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.24598462879657745, 0.23024287819862366, 100, 0.6409468358114165], "caller": "main.py:200", "time": "2021-02-04T23:33:46.352348"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2429787665605545, 0.24423713982105255, 100, 0.6559720181762851], "caller": "main.py:200", "time": "2021-02-04T23:35:09.392199"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2384742945432663, 0.19784434139728546, 100, 0.6408335500314748], "caller": "main.py:200", "time": "2021-02-04T23:36:32.369834"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23314964771270752, 0.21497824788093567, 100, 0.6509717588965804], "caller": "main.py:200", "time": "2021-02-04T23:37:55.184743"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23205643892288208, 0.2285858690738678, 100, 0.663094974093885], "caller": "main.py:200", "time": "2021-02-04T23:39:18.300826"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23327691853046417, 0.24604706466197968, 100, 0.692660083701486], "caller": "main.py:200", "time": "2021-02-04T23:40:41.044261"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2292214035987854, 0.2330201268196106, 100, 0.6445126246761493], "caller": "main.py:200", "time": "2021-02-04T23:42:05.080223"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.227254256606102, 0.22167238593101501, 100, 0.661100497775725], "caller": "main.py:200", "time": "2021-02-04T23:43:29.647808"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.22531892359256744, 0.23452788591384888, 100, 0.6399301677302346], "caller": "main.py:200", "time": "2021-02-04T23:44:53.588470"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2212097942829132, 0.22138234972953796, 100, 0.6430232112956754], "caller": "main.py:200", "time": "2021-02-04T23:46:17.752448"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22144100069999695, 0.21072830259799957, 100, 0.6399233638600342], "caller": "main.py:200", "time": "2021-02-04T23:47:42.395341"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.22005149722099304, 0.21922963857650757, 100, 0.6763345169344723], "caller": "main.py:200", "time": "2021-02-04T23:49:07.661406"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.21930217742919922, 0.233090341091156, 100, 0.6710715168043452], "caller": "main.py:200", "time": "2021-02-04T23:50:33.401670"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22003845870494843, 0.22990548610687256, 100, 0.6849718516380077], "caller": "main.py:200", "time": "2021-02-04T23:51:58.923680"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.2167053073644638, 0.21754032373428345, 100, 0.6560989966332464], "caller": "main.py:200", "time": "2021-02-04T23:53:23.968621"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2148575782775879, 0.22517520189285278, 100, 0.6717091243787443], "caller": "main.py:200", "time": "2021-02-04T23:54:48.929845"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.21590504050254822, 0.2071198970079422, 100, 0.6604776447350921], "caller": "main.py:200", "time": "2021-02-04T23:56:14.541895"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.21348434686660767, 0.21213486790657043, 100, 0.6755247026006376], "caller": "main.py:200", "time": "2021-02-04T23:57:39.628160"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [4], "caller": "main.py:129", "time": "2021-02-04T23:59:02.374247"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-04T23:59:11.794486"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 47.5697844388983, 8.45114845500459], "caller": "main.py:37", "time": "2021-02-04T23:59:15.830087"}
+{"level": "info", "fmt": "episode: %s", "args": [-0.08707215349711947], "caller": "main.py:149", "time": "2021-02-04T23:59:26.105679"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.28529617190361023, 0.2699047923088074, 100, 0.6425333731724029], "caller": "main.py:200", "time": "2021-02-04T23:59:28.146791"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.25237181782722473, 0.27006638050079346, 100, 0.6509869917576209], "caller": "main.py:200", "time": "2021-02-05T00:00:52.611066"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2490644007921219, 0.21160146594047546, 100, 0.6431614606929394], "caller": "main.py:200", "time": "2021-02-05T00:02:16.985384"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2455822378396988, 0.2233978509902954, 100, 0.6878601347712257], "caller": "main.py:200", "time": "2021-02-05T00:03:41.167851"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.24017281830310822, 0.2073362171649933, 100, 0.6459101157734599], "caller": "main.py:200", "time": "2021-02-05T00:05:04.980196"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.01629333756864071, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T00:05:43.592875"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.24119457602500916, 0.23819997906684875, 100, 0.6432261037183312], "caller": "main.py:200", "time": "2021-02-05T00:06:28.952071"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23444733023643494, 0.22679953277111053, 100, 0.6725821780485866], "caller": "main.py:200", "time": "2021-02-05T00:07:52.628698"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23471219837665558, 0.24003352224826813, 100, 0.6270547621815191], "caller": "main.py:200", "time": "2021-02-05T00:09:16.770194"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23340551555156708, 0.23040629923343658, 100, 0.6539878200830693], "caller": "main.py:200", "time": "2021-02-05T00:10:40.424363"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2307969331741333, 0.21976877748966217, 100, 0.6609324424797977], "caller": "main.py:200", "time": "2021-02-05T00:12:05.175998"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.22864414751529694, 0.20312675833702087, 100, 0.643386617238556], "caller": "main.py:200", "time": "2021-02-05T00:13:31.007471"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22849096357822418, 0.23248913884162903, 100, 0.6442771189760615], "caller": "main.py:200", "time": "2021-02-05T00:14:56.249584"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.11905625462532043, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T00:15:16.554236"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22401410341262817, 0.23796740174293518, 100, 0.6648567023208473], "caller": "main.py:200", "time": "2021-02-05T00:16:20.959983"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2227412611246109, 0.21277067065238953, 100, 0.6710423564239847], "caller": "main.py:200", "time": "2021-02-05T00:17:46.208251"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22126063704490662, 0.22996008396148682, 100, 0.6321452336871288], "caller": "main.py:200", "time": "2021-02-05T00:19:11.215023"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22360236942768097, 0.20850680768489838, 100, 0.6653822349836632], "caller": "main.py:200", "time": "2021-02-05T00:20:35.425638"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22139175236225128, 0.21116870641708374, 100, 0.6491831558293613], "caller": "main.py:200", "time": "2021-02-05T00:21:59.360574"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22132712602615356, 0.20492494106292725, 100, 0.6365446376163992], "caller": "main.py:200", "time": "2021-02-05T00:23:23.652327"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.21965672075748444, 0.21849045157432556, 100, 0.6556047248184728], "caller": "main.py:200", "time": "2021-02-05T00:24:48.230219"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2139768749475479, 0.2538764476776123, 100, 0.6541388895453535], "caller": "main.py:200", "time": "2021-02-05T00:26:12.415637"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [5], "caller": "main.py:129", "time": "2021-02-05T00:27:34.654171"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T00:27:44.067842"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 71.39646059703323, 27.294946375464257], "caller": "main.py:37", "time": "2021-02-05T00:27:48.078799"}
+{"level": "info", "fmt": "episode: %s", "args": [2.2144787578124445], "caller": "main.py:149", "time": "2021-02-05T00:27:58.510817"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.25690239667892456, 0.2550359070301056, 100, 0.6484734683580424], "caller": "main.py:200", "time": "2021-02-05T00:28:00.551082"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2512354552745819, 0.23482318222522736, 100, 0.6363460748751395], "caller": "main.py:200", "time": "2021-02-05T00:29:24.865786"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2473858892917633, 0.256070077419281, 100, 0.6414153578236034], "caller": "main.py:200", "time": "2021-02-05T00:30:49.093594"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2443704456090927, 0.24331316351890564, 100, 0.6328067871295409], "caller": "main.py:200", "time": "2021-02-05T00:32:13.367040"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.24306240677833557, 0.22519540786743164, 100, 0.6352494182547468], "caller": "main.py:200", "time": "2021-02-05T00:33:37.748737"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2415125072002411, 0.21591416001319885, 100, 0.6393087732152593], "caller": "main.py:200", "time": "2021-02-05T00:35:02.285519"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23792116343975067, 0.25856488943099976, 100, 0.6170594009139982], "caller": "main.py:200", "time": "2021-02-05T00:36:26.670380"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23853355646133423, 0.26366886496543884, 100, 0.6307867293919156], "caller": "main.py:200", "time": "2021-02-05T00:37:50.972133"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.028958823531866074, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T00:38:54.334711"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2396746277809143, 0.24119020998477936, 100, 0.6517470752541155], "caller": "main.py:200", "time": "2021-02-05T00:39:15.239974"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23309510946273804, 0.2499125599861145, 100, 0.6181801092006485], "caller": "main.py:200", "time": "2021-02-05T00:40:39.644400"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23509331047534943, 0.21861311793327332, 100, 0.6366631203253184], "caller": "main.py:200", "time": "2021-02-05T00:42:05.209308"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2332783341407776, 0.2452094405889511, 100, 0.6322601532303836], "caller": "main.py:200", "time": "2021-02-05T00:43:30.972233"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23395492136478424, 0.23399657011032104, 100, 0.6183639811851119], "caller": "main.py:200", "time": "2021-02-05T00:44:56.542394"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.22825662791728973, 0.2198822796344757, 100, 0.6019069301480156], "caller": "main.py:200", "time": "2021-02-05T00:46:21.929313"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22814065217971802, 0.2401348501443863, 100, 0.6354995324572211], "caller": "main.py:200", "time": "2021-02-05T00:47:47.841328"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22969715297222137, 0.2128194272518158, 100, 0.6258110090464392], "caller": "main.py:200", "time": "2021-02-05T00:49:13.226694"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22897331416606903, 0.25535422563552856, 100, 0.6400332626060369], "caller": "main.py:200", "time": "2021-02-05T00:50:38.611967"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2271210253238678, 0.23252001404762268, 100, 0.6326220338904045], "caller": "main.py:200", "time": "2021-02-05T00:52:04.423472"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2274535596370697, 0.22818778455257416, 100, 0.6181319501377909], "caller": "main.py:200", "time": "2021-02-05T00:53:30.433158"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.22706808149814606, 0.24610774219036102, 100, 0.6464491026947164], "caller": "main.py:200", "time": "2021-02-05T00:54:56.077081"}
+{"level": "info", "fmt": "Surrogate didn't improve, shrinking step... %.6f => %.6f", "args": [-3.051757957450718e-08, -0.01571696437895298], "caller": "slbo/algos/TRPO.py:157", "time": "2021-02-05T00:55:04.229753"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.015451893210411072, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T00:55:25.178124"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.018789203837513924, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T00:55:43.953030"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [6], "caller": "main.py:129", "time": "2021-02-05T00:56:19.437729"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T00:56:28.764539"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 136.51673785451956, 11.694227744727254], "caller": "main.py:37", "time": "2021-02-05T00:56:32.840211"}
+{"level": "info", "fmt": "episode: %s", "args": [3.6217575034117115], "caller": "main.py:149", "time": "2021-02-05T00:56:43.304498"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2546501159667969, 0.24717777967453003, 100, 0.6139195119720706], "caller": "main.py:200", "time": "2021-02-05T00:56:45.382651"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2498742640018463, 0.2441970407962799, 100, 0.607142258225762], "caller": "main.py:200", "time": "2021-02-05T00:58:11.116696"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2473253309726715, 0.2449156939983368, 100, 0.5861406194577586], "caller": "main.py:200", "time": "2021-02-05T00:59:37.131977"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.24532003700733185, 0.25969821214675903, 100, 0.60262457752162], "caller": "main.py:200", "time": "2021-02-05T01:01:02.063764"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.24587509036064148, 0.2324397712945938, 100, 0.6281653305260011], "caller": "main.py:200", "time": "2021-02-05T01:02:27.728158"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2449197918176651, 0.21796435117721558, 100, 0.6051476474991281], "caller": "main.py:200", "time": "2021-02-05T01:03:53.111304"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.24188388884067535, 0.24208447337150574, 100, 0.6332119547687559], "caller": "main.py:200", "time": "2021-02-05T01:05:18.779830"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2419341653585434, 0.2251308709383011, 100, 0.5960266145467779], "caller": "main.py:200", "time": "2021-02-05T01:06:44.648018"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23830875754356384, 0.2722820043563843, 100, 0.6172737207391493], "caller": "main.py:200", "time": "2021-02-05T01:08:10.376450"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23798123002052307, 0.23688046634197235, 100, 0.5920388793353274], "caller": "main.py:200", "time": "2021-02-05T01:09:36.172002"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23926259577274323, 0.23515667021274567, 100, 0.6275042569009794], "caller": "main.py:200", "time": "2021-02-05T01:11:01.467436"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23478169739246368, 0.22522060573101044, 100, 0.5979346042273541], "caller": "main.py:200", "time": "2021-02-05T01:12:27.479881"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23851048946380615, 0.2303755134344101, 100, 0.6139878510814846], "caller": "main.py:200", "time": "2021-02-05T01:13:52.875639"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2373509258031845, 0.22519919276237488, 100, 0.6562608330785619], "caller": "main.py:200", "time": "2021-02-05T01:15:17.519687"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23446351289749146, 0.24777008593082428, 100, 0.5962043752670131], "caller": "main.py:200", "time": "2021-02-05T01:16:42.118260"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23261785507202148, 0.23913981020450592, 100, 0.6180589880921287], "caller": "main.py:200", "time": "2021-02-05T01:18:06.254920"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23437218368053436, 0.244974285364151, 100, 0.6461721449444237], "caller": "main.py:200", "time": "2021-02-05T01:19:30.282964"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23298360407352448, 0.24794146418571472, 100, 0.612424463270457], "caller": "main.py:200", "time": "2021-02-05T01:20:54.671182"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23008038103580475, 0.2533778250217438, 100, 0.6188328304267986], "caller": "main.py:200", "time": "2021-02-05T01:22:18.900541"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2296837419271469, 0.22406411170959473, 100, 0.5935192253592158], "caller": "main.py:200", "time": "2021-02-05T01:23:43.097294"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [7], "caller": "main.py:129", "time": "2021-02-05T01:25:05.359471"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T01:25:15.182537"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 139.8140261080573, 4.518970035887739], "caller": "main.py:37", "time": "2021-02-05T01:25:19.394568"}
+{"level": "info", "fmt": "episode: %s", "args": [4.135310105596485], "caller": "main.py:149", "time": "2021-02-05T01:25:30.177535"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2561134994029999, 0.25994595885276794, 100, 0.6046202277532587], "caller": "main.py:200", "time": "2021-02-05T01:25:32.192019"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2497568279504776, 0.23927614092826843, 100, 0.5928111844662087], "caller": "main.py:200", "time": "2021-02-05T01:26:56.423594"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.24596314132213593, 0.2588469386100769, 100, 0.583161360265113], "caller": "main.py:200", "time": "2021-02-05T01:28:20.662612"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.24499107897281647, 0.2503383755683899, 100, 0.583274904729213], "caller": "main.py:200", "time": "2021-02-05T01:29:44.847844"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.24114812910556793, 0.2564353048801422, 100, 0.5839961125862728], "caller": "main.py:200", "time": "2021-02-05T01:31:08.906064"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.24337728321552277, 0.2416534721851349, 100, 0.588257506430928], "caller": "main.py:200", "time": "2021-02-05T01:32:33.254828"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2448645979166031, 0.22732630372047424, 100, 0.587286156655234], "caller": "main.py:200", "time": "2021-02-05T01:33:57.881815"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.24099348485469818, 0.2605285346508026, 100, 0.5976805808542525], "caller": "main.py:200", "time": "2021-02-05T01:35:22.298907"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.018181294202804565, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T01:35:52.879023"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.24151098728179932, 0.25365400314331055, 100, 0.5922492610815985], "caller": "main.py:200", "time": "2021-02-05T01:36:46.551328"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.015179076232016087, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T01:37:58.422316"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.24082595109939575, 0.2583504915237427, 100, 0.5881023877271013], "caller": "main.py:200", "time": "2021-02-05T01:38:11.196762"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23825474083423615, 0.23263993859291077, 100, 0.597797863125626], "caller": "main.py:200", "time": "2021-02-05T01:39:35.396018"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23738840222358704, 0.24960659444332123, 100, 0.5840041639816883], "caller": "main.py:200", "time": "2021-02-05T01:40:58.892513"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23952694237232208, 0.22409279644489288, 100, 0.5866982250819018], "caller": "main.py:200", "time": "2021-02-05T01:42:23.366124"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23632682859897614, 0.24172060191631317, 100, 0.5999051349755243], "caller": "main.py:200", "time": "2021-02-05T01:43:47.399527"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23285011947155, 0.2527998685836792, 100, 0.5915127914552454], "caller": "main.py:200", "time": "2021-02-05T01:45:11.420897"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2346242070198059, 0.22603186964988708, 100, 0.6107030724868423], "caller": "main.py:200", "time": "2021-02-05T01:46:35.405940"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23699365556240082, 0.20847022533416748, 100, 0.5998176916833922], "caller": "main.py:200", "time": "2021-02-05T01:47:59.622041"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23287010192871094, 0.22177228331565857, 100, 0.5692973338988351], "caller": "main.py:200", "time": "2021-02-05T01:49:24.184153"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23394416272640228, 0.21613772213459015, 100, 0.6304995983208536], "caller": "main.py:200", "time": "2021-02-05T01:50:48.506470"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.020929701626300812, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T01:51:39.473435"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23125287890434265, 0.22990241646766663, 100, 0.5780823757666432], "caller": "main.py:200", "time": "2021-02-05T01:52:12.582247"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [8], "caller": "main.py:129", "time": "2021-02-05T01:53:34.465002"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T01:53:44.093999"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 88.55253829951161, 87.39300517162971], "caller": "main.py:37", "time": "2021-02-05T01:53:48.335951"}
+{"level": "info", "fmt": "episode: %s", "args": [3.8922143871487687], "caller": "main.py:149", "time": "2021-02-05T01:53:59.115312"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.253539115190506, 0.22343987226486206, 100, 0.5863700574050089], "caller": "main.py:200", "time": "2021-02-05T01:54:01.169165"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.24836164712905884, 0.2521657645702362, 100, 0.5813989674463614], "caller": "main.py:200", "time": "2021-02-05T01:55:25.717506"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.016245270147919655, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T01:56:37.220630"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.24847206473350525, 0.24373896420001984, 100, 0.5740000622890584], "caller": "main.py:200", "time": "2021-02-05T01:56:50.024475"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.24503283202648163, 0.23863857984542847, 100, 0.5974337088464303], "caller": "main.py:200", "time": "2021-02-05T01:58:14.396029"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.24531850218772888, 0.2348083108663559, 100, 0.5731535988472943], "caller": "main.py:200", "time": "2021-02-05T01:59:38.806482"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.24348850548267365, 0.24612316489219666, 100, 0.5851369237314644], "caller": "main.py:200", "time": "2021-02-05T02:01:03.128141"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2452690154314041, 0.2326495349407196, 100, 0.5687832664874638], "caller": "main.py:200", "time": "2021-02-05T02:02:27.663006"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.24529756605625153, 0.24180975556373596, 100, 0.603822085498986], "caller": "main.py:200", "time": "2021-02-05T02:03:52.674321"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.24310949444770813, 0.23828907310962677, 100, 0.5785147851127037], "caller": "main.py:200", "time": "2021-02-05T02:05:16.952472"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23852764070034027, 0.24449196457862854, 100, 0.5643907033832735], "caller": "main.py:200", "time": "2021-02-05T02:06:41.661180"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.24083974957466125, 0.23168952763080597, 100, 0.5675576298861382], "caller": "main.py:200", "time": "2021-02-05T02:08:05.775454"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23995639383792877, 0.22847026586532593, 100, 0.5822544237389051], "caller": "main.py:200", "time": "2021-02-05T02:09:30.158408"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23781640827655792, 0.2398337870836258, 100, 0.5658086707622249], "caller": "main.py:200", "time": "2021-02-05T02:10:54.427820"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2397816777229309, 0.24923667311668396, 100, 0.5814821594372055], "caller": "main.py:200", "time": "2021-02-05T02:12:18.393826"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23905906081199646, 0.2645703852176666, 100, 0.5894206138855884], "caller": "main.py:200", "time": "2021-02-05T02:13:42.743996"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23872387409210205, 0.2437550127506256, 100, 0.5740603714607291], "caller": "main.py:200", "time": "2021-02-05T02:15:06.978934"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.015456756576895714, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T02:16:19.066351"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.24034756422042847, 0.22683954238891602, 100, 0.5613427650367155], "caller": "main.py:200", "time": "2021-02-05T02:16:31.731813"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.015493057668209076, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T02:16:45.950359"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23633815348148346, 0.2204608917236328, 100, 0.5766333132405621], "caller": "main.py:200", "time": "2021-02-05T02:17:56.096867"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2375219762325287, 0.2395864576101303, 100, 0.5669500344424734], "caller": "main.py:200", "time": "2021-02-05T02:19:20.418073"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23295754194259644, 0.2422870397567749, 100, 0.5990837552544418], "caller": "main.py:200", "time": "2021-02-05T02:20:44.899150"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [9], "caller": "main.py:129", "time": "2021-02-05T02:22:06.496961"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T02:22:16.052393"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 9, 125.6131081194575, 7.950406983491776], "caller": "main.py:37", "time": "2021-02-05T02:22:20.073575"}
+{"level": "info", "fmt": "episode: %s", "args": [3.5405883202678954], "caller": "main.py:149", "time": "2021-02-05T02:22:30.389257"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.24905410408973694, 0.2457980513572693, 100, 0.5800446003095416], "caller": "main.py:200", "time": "2021-02-05T02:22:32.449680"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2463855892419815, 0.22956515848636627, 100, 0.5549776681702675], "caller": "main.py:200", "time": "2021-02-05T02:23:57.020694"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.015467201359570026, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T02:24:46.135826"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.24582207202911377, 0.25118064880371094, 100, 0.5890726458437264], "caller": "main.py:200", "time": "2021-02-05T02:25:21.395487"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.01569010689854622, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T02:25:33.293702"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.24758435785770416, 0.25752806663513184, 100, 0.5623095589313866], "caller": "main.py:200", "time": "2021-02-05T02:26:45.401406"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.244784876704216, 0.2173251211643219, 100, 0.5781899839061474], "caller": "main.py:200", "time": "2021-02-05T02:28:09.994430"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.24188640713691711, 0.2296314239501953, 100, 0.5653117042214612], "caller": "main.py:200", "time": "2021-02-05T02:29:34.689744"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.24323700368404388, 0.20339763164520264, 100, 0.5615577995215573], "caller": "main.py:200", "time": "2021-02-05T02:30:58.688968"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.017334330826997757, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T02:31:55.875214"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2464331090450287, 0.25175321102142334, 100, 0.5770685368199253], "caller": "main.py:200", "time": "2021-02-05T02:32:22.926969"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2391011118888855, 0.23390552401542664, 100, 0.5653912075788636], "caller": "main.py:200", "time": "2021-02-05T02:33:47.528885"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2409404218196869, 0.20986053347587585, 100, 0.5752314602753473], "caller": "main.py:200", "time": "2021-02-05T02:35:11.637064"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.017908740788698196, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T02:35:50.609471"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.24237947165966034, 0.24372419714927673, 100, 0.5785158389228232], "caller": "main.py:200", "time": "2021-02-05T02:36:36.103919"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2400665432214737, 0.21777789294719696, 100, 0.5976022767423028], "caller": "main.py:200", "time": "2021-02-05T02:38:00.566957"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2381182163953781, 0.2502570152282715, 100, 0.5699236452009575], "caller": "main.py:200", "time": "2021-02-05T02:39:24.897391"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2401796132326126, 0.21370282769203186, 100, 0.5730757083915793], "caller": "main.py:200", "time": "2021-02-05T02:40:48.696319"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23973819613456726, 0.25155067443847656, 100, 0.5649843462736674], "caller": "main.py:200", "time": "2021-02-05T02:42:13.115858"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2440023422241211, 0.258251816034317, 100, 0.5599292444622871], "caller": "main.py:200", "time": "2021-02-05T02:43:37.853070"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23790033161640167, 0.25619447231292725, 100, 0.568924965801567], "caller": "main.py:200", "time": "2021-02-05T02:45:02.882139"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2372003197669983, 0.23275581002235413, 100, 0.5709963428403155], "caller": "main.py:200", "time": "2021-02-05T02:46:27.548411"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23719394207000732, 0.21050511300563812, 100, 0.5694762694266187], "caller": "main.py:200", "time": "2021-02-05T02:47:52.218789"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23509658873081207, 0.2360454797744751, 100, 0.5652700763375541], "caller": "main.py:200", "time": "2021-02-05T02:49:16.310048"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [10], "caller": "main.py:129", "time": "2021-02-05T02:50:38.348384"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T02:50:47.688769"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 11.263155409933916, 0.3202181712846762], "caller": "main.py:37", "time": "2021-02-05T02:50:51.722002"}
+{"level": "info", "fmt": "episode: %s", "args": [5.225637714164765], "caller": "main.py:149", "time": "2021-02-05T02:51:02.118462"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.24933385848999023, 0.22712016105651855, 100, 0.5348168938817219], "caller": "main.py:200", "time": "2021-02-05T02:51:04.162140"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.24523530900478363, 0.2574393153190613, 100, 0.5553703873794225], "caller": "main.py:200", "time": "2021-02-05T02:52:28.211503"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.24492484331130981, 0.260459840297699, 100, 0.5427248081022179], "caller": "main.py:200", "time": "2021-02-05T02:53:52.421728"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.24384020268917084, 0.24149054288864136, 100, 0.5544856124561863], "caller": "main.py:200", "time": "2021-02-05T02:55:16.231559"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.24312061071395874, 0.25843894481658936, 100, 0.5625468634996535], "caller": "main.py:200", "time": "2021-02-05T02:56:40.810561"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.24364332854747772, 0.2466900646686554, 100, 0.549537045602681], "caller": "main.py:200", "time": "2021-02-05T02:58:05.463829"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.24341653287410736, 0.23161321878433228, 100, 0.5380850123821046], "caller": "main.py:200", "time": "2021-02-05T02:59:30.120038"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.020135419443249702, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T02:59:48.232277"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.24109594523906708, 0.22654855251312256, 100, 0.5348744087876457], "caller": "main.py:200", "time": "2021-02-05T03:00:54.322108"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2425806075334549, 0.2536276578903198, 100, 0.5351762875379398], "caller": "main.py:200", "time": "2021-02-05T03:02:18.582045"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2435699701309204, 0.23969092965126038, 100, 0.5359992575109448], "caller": "main.py:200", "time": "2021-02-05T03:03:43.132311"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.24210457503795624, 0.24198272824287415, 100, 0.5398506825622488], "caller": "main.py:200", "time": "2021-02-05T03:05:07.890619"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23934511840343475, 0.2345769703388214, 100, 0.5390978905019489], "caller": "main.py:200", "time": "2021-02-05T03:06:32.093428"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2399994432926178, 0.24925497174263, 100, 0.5466625263740332], "caller": "main.py:200", "time": "2021-02-05T03:07:56.403596"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2412506490945816, 0.2507460117340088, 100, 0.553088582400803], "caller": "main.py:200", "time": "2021-02-05T03:09:21.095898"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.24003919959068298, 0.24060767889022827, 100, 0.5654713203937588], "caller": "main.py:200", "time": "2021-02-05T03:10:44.652338"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.24188318848609924, 0.23756691813468933, 100, 0.5469116979290506], "caller": "main.py:200", "time": "2021-02-05T03:12:09.535505"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.2372056245803833, 0.2245379537343979, 100, 0.52646969849567], "caller": "main.py:200", "time": "2021-02-05T03:13:33.635652"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2389972060918808, 0.25206875801086426, 100, 0.5344039397897374], "caller": "main.py:200", "time": "2021-02-05T03:14:58.370612"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23647023737430573, 0.23285701870918274, 100, 0.5281276177831858], "caller": "main.py:200", "time": "2021-02-05T03:16:22.435685"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23651131987571716, 0.22310933470726013, 100, 0.5284125405021591], "caller": "main.py:200", "time": "2021-02-05T03:17:47.276076"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [11], "caller": "main.py:129", "time": "2021-02-05T03:19:09.482703"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T03:19:19.041102"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 85.63172215988948, 5.959366426256553], "caller": "main.py:37", "time": "2021-02-05T03:19:23.076384"}
+{"level": "info", "fmt": "episode: %s", "args": [4.4062887063878575], "caller": "main.py:149", "time": "2021-02-05T03:19:33.524642"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.25241827964782715, 0.2648758590221405, 100, 0.5299149658746094], "caller": "main.py:200", "time": "2021-02-05T03:19:35.578474"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.24533672630786896, 0.227889746427536, 100, 0.534048566168786], "caller": "main.py:200", "time": "2021-02-05T03:21:00.032674"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.24379630386829376, 0.25789421796798706, 100, 0.5375286307393418], "caller": "main.py:200", "time": "2021-02-05T03:22:24.329267"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.24161136150360107, 0.25691524147987366, 100, 0.5459577834060213], "caller": "main.py:200", "time": "2021-02-05T03:23:49.081648"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2444019466638565, 0.25747594237327576, 100, 0.5390114008841329], "caller": "main.py:200", "time": "2021-02-05T03:25:13.097501"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.24572782218456268, 0.22652515769004822, 100, 0.5313931023543161], "caller": "main.py:200", "time": "2021-02-05T03:26:37.340100"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2414115071296692, 0.2402125597000122, 100, 0.5279275530048779], "caller": "main.py:200", "time": "2021-02-05T03:28:02.351830"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.24199028313159943, 0.25564759969711304, 100, 0.5226727988793314], "caller": "main.py:200", "time": "2021-02-05T03:29:26.777640"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.24424071609973907, 0.23686924576759338, 100, 0.5538157745641977], "caller": "main.py:200", "time": "2021-02-05T03:30:50.906691"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.24205555021762848, 0.25030580163002014, 100, 0.5426893993669462], "caller": "main.py:200", "time": "2021-02-05T03:32:15.413585"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.24466800689697266, 0.26697665452957153, 100, 0.552499392871443], "caller": "main.py:200", "time": "2021-02-05T03:33:39.885007"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2400943785905838, 0.2581492066383362, 100, 0.5180592581111694], "caller": "main.py:200", "time": "2021-02-05T03:35:04.633509"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2377762347459793, 0.2515753507614136, 100, 0.5204582219795475], "caller": "main.py:200", "time": "2021-02-05T03:36:28.871706"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23807333409786224, 0.2624308466911316, 100, 0.533871942705173], "caller": "main.py:200", "time": "2021-02-05T03:37:52.747405"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.24054567515850067, 0.23535126447677612, 100, 0.5253694730471716], "caller": "main.py:200", "time": "2021-02-05T03:39:16.772160"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.24227501451969147, 0.22701257467269897, 100, 0.5417482643777787], "caller": "main.py:200", "time": "2021-02-05T03:40:40.583069"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23715631663799286, 0.21524301171302795, 100, 0.5459024458577709], "caller": "main.py:200", "time": "2021-02-05T03:42:04.700207"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23818954825401306, 0.2377251833677292, 100, 0.5447362285307696], "caller": "main.py:200", "time": "2021-02-05T03:43:29.517131"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23806962370872498, 0.23591193556785583, 100, 0.5366494377910883], "caller": "main.py:200", "time": "2021-02-05T03:44:53.854810"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2360640913248062, 0.2150774896144867, 100, 0.5389069836792865], "caller": "main.py:200", "time": "2021-02-05T03:46:18.218977"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [12], "caller": "main.py:129", "time": "2021-02-05T03:47:40.769117"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T03:47:50.254388"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 11.189342261381967, 0.6760811701034309], "caller": "main.py:37", "time": "2021-02-05T03:47:54.496397"}
+{"level": "info", "fmt": "episode: %s", "args": [0.5328876993732647], "caller": "main.py:149", "time": "2021-02-05T03:48:05.110950"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2515033483505249, 0.21494406461715698, 100, 0.5177868651825963], "caller": "main.py:200", "time": "2021-02-05T03:48:07.173586"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.24477031826972961, 0.22443941235542297, 100, 0.5448008328131423], "caller": "main.py:200", "time": "2021-02-05T03:49:31.252254"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2429143488407135, 0.22828984260559082, 100, 0.522679384596532], "caller": "main.py:200", "time": "2021-02-05T03:50:55.696041"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23806734383106232, 0.24920453131198883, 100, 0.5162591349562324], "caller": "main.py:200", "time": "2021-02-05T03:52:20.634927"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2411928027868271, 0.247531458735466, 100, 0.5180969735461938], "caller": "main.py:200", "time": "2021-02-05T03:53:44.950500"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23803992569446564, 0.23578707873821259, 100, 0.5119425326073828], "caller": "main.py:200", "time": "2021-02-05T03:55:10.106658"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23909364640712738, 0.2399097979068756, 100, 0.5028430088851672], "caller": "main.py:200", "time": "2021-02-05T03:56:34.462524"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2378530502319336, 0.2504572868347168, 100, 0.5469548904348828], "caller": "main.py:200", "time": "2021-02-05T03:57:59.160895"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.24005012214183807, 0.22631457448005676, 100, 0.5195754369573771], "caller": "main.py:200", "time": "2021-02-05T03:59:23.815819"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23848681151866913, 0.24351046979427338, 100, 0.5298279866046367], "caller": "main.py:200", "time": "2021-02-05T04:00:48.245883"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2332642376422882, 0.2573530673980713, 100, 0.5077650618045397], "caller": "main.py:200", "time": "2021-02-05T04:02:12.490455"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23668457567691803, 0.23947648704051971, 100, 0.529945661372596], "caller": "main.py:200", "time": "2021-02-05T04:03:36.789432"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2376016229391098, 0.26157575845718384, 100, 0.5273033809134562], "caller": "main.py:200", "time": "2021-02-05T04:05:01.526491"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23611308634281158, 0.24506834149360657, 100, 0.5296745350427242], "caller": "main.py:200", "time": "2021-02-05T04:06:26.117598"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23046736419200897, 0.218243807554245, 100, 0.5083927756039536], "caller": "main.py:200", "time": "2021-02-05T04:07:50.580767"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23397499322891235, 0.2504982054233551, 100, 0.539999282306077], "caller": "main.py:200", "time": "2021-02-05T04:09:14.647813"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23375195264816284, 0.2379341423511505, 100, 0.515980113991161], "caller": "main.py:200", "time": "2021-02-05T04:10:39.074613"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23357468843460083, 0.23717711865901947, 100, 0.5265748720834585], "caller": "main.py:200", "time": "2021-02-05T04:12:03.596183"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23349276185035706, 0.24050670862197876, 100, 0.5126517208898016], "caller": "main.py:200", "time": "2021-02-05T04:13:28.076350"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2310056835412979, 0.21687093377113342, 100, 0.5163893073280837], "caller": "main.py:200", "time": "2021-02-05T04:14:52.752482"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [13], "caller": "main.py:129", "time": "2021-02-05T04:16:14.907389"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T04:16:24.354396"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 18.82341581272284, 1.4422721413589146], "caller": "main.py:37", "time": "2021-02-05T04:16:28.405108"}
+{"level": "info", "fmt": "episode: %s", "args": [2.6244859576066673], "caller": "main.py:149", "time": "2021-02-05T04:16:38.621789"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.24924756586551666, 0.23691189289093018, 100, 0.5110208475078731], "caller": "main.py:200", "time": "2021-02-05T04:16:40.672976"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2432815432548523, 0.25454097986221313, 100, 0.49660386378567445], "caller": "main.py:200", "time": "2021-02-05T04:18:04.994709"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.24208784103393555, 0.22772693634033203, 100, 0.5100921809163217], "caller": "main.py:200", "time": "2021-02-05T04:19:29.396469"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.24017666280269623, 0.23231709003448486, 100, 0.5158924337705658], "caller": "main.py:200", "time": "2021-02-05T04:20:53.627468"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23981986939907074, 0.24187330901622772, 100, 0.5223041259720158], "caller": "main.py:200", "time": "2021-02-05T04:22:17.777196"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2411426603794098, 0.2267467975616455, 100, 0.525544334299462], "caller": "main.py:200", "time": "2021-02-05T04:23:42.111707"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23795433342456818, 0.232161745429039, 100, 0.518706341631994], "caller": "main.py:200", "time": "2021-02-05T04:25:06.927214"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2392052710056305, 0.2410559207201004, 100, 0.5176649364311399], "caller": "main.py:200", "time": "2021-02-05T04:26:31.545434"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23857130110263824, 0.2706950604915619, 100, 0.5121817966783465], "caller": "main.py:200", "time": "2021-02-05T04:27:56.035908"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23658007383346558, 0.25420814752578735, 100, 0.5211870017126556], "caller": "main.py:200", "time": "2021-02-05T04:29:20.081861"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23655787110328674, 0.2211454212665558, 100, 0.5032708921525271], "caller": "main.py:200", "time": "2021-02-05T04:30:44.408138"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2370852679014206, 0.23860886693000793, 100, 0.5074354665964046], "caller": "main.py:200", "time": "2021-02-05T04:32:08.674195"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23756422102451324, 0.2589957118034363, 100, 0.5177195175964482], "caller": "main.py:200", "time": "2021-02-05T04:33:32.777521"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.233832448720932, 0.2221498042345047, 100, 0.5059107189864649], "caller": "main.py:200", "time": "2021-02-05T04:34:57.452690"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23256249725818634, 0.22339260578155518, 100, 0.49993251527316884], "caller": "main.py:200", "time": "2021-02-05T04:36:21.800640"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2349739819765091, 0.2602747976779938, 100, 0.5019751029704705], "caller": "main.py:200", "time": "2021-02-05T04:37:45.800727"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23570579290390015, 0.2128116488456726, 100, 0.5314061411683827], "caller": "main.py:200", "time": "2021-02-05T04:39:09.630329"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23447303473949432, 0.2510994076728821, 100, 0.5256547307442485], "caller": "main.py:200", "time": "2021-02-05T04:40:33.597769"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23311756551265717, 0.2643945813179016, 100, 0.5105426462974222], "caller": "main.py:200", "time": "2021-02-05T04:41:58.397797"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23467139899730682, 0.2314663529396057, 100, 0.5277484365769973], "caller": "main.py:200", "time": "2021-02-05T04:43:23.886654"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [14], "caller": "main.py:129", "time": "2021-02-05T04:44:47.557671"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T04:44:56.897777"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 91.56676955723033, 13.419996211006787], "caller": "main.py:37", "time": "2021-02-05T04:45:00.956879"}
+{"level": "info", "fmt": "episode: %s", "args": [3.41369583617426], "caller": "main.py:149", "time": "2021-02-05T04:45:11.427928"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23886606097221375, 0.235527902841568, 100, 0.5094543733683394], "caller": "main.py:200", "time": "2021-02-05T04:45:13.507565"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23911406099796295, 0.21654824912548065, 100, 0.503934398541125], "caller": "main.py:200", "time": "2021-02-05T04:46:38.747929"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2362968474626541, 0.24300456047058105, 100, 0.5122026353562276], "caller": "main.py:200", "time": "2021-02-05T04:48:03.060611"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2348414957523346, 0.23777830600738525, 100, 0.5094810538974544], "caller": "main.py:200", "time": "2021-02-05T04:49:27.735214"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23345227539539337, 0.23192834854125977, 100, 0.49194535965768416], "caller": "main.py:200", "time": "2021-02-05T04:50:51.979865"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2364008128643036, 0.2220446765422821, 100, 0.5093232470241486], "caller": "main.py:200", "time": "2021-02-05T04:52:16.025716"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2345947027206421, 0.20215454697608948, 100, 0.4800924875852191], "caller": "main.py:200", "time": "2021-02-05T04:53:40.294209"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23348921537399292, 0.25329291820526123, 100, 0.5092735307899464], "caller": "main.py:200", "time": "2021-02-05T04:55:04.440854"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2327483743429184, 0.21947702765464783, 100, 0.5074549263208289], "caller": "main.py:200", "time": "2021-02-05T04:56:29.012129"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23554208874702454, 0.21928875148296356, 100, 0.49720309490953873], "caller": "main.py:200", "time": "2021-02-05T04:57:53.431341"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2320355921983719, 0.22462373971939087, 100, 0.5089302566138599], "caller": "main.py:200", "time": "2021-02-05T04:59:17.905153"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23384010791778564, 0.2517027258872986, 100, 0.5182106783467881], "caller": "main.py:200", "time": "2021-02-05T05:00:42.125702"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23119299113750458, 0.22643741965293884, 100, 0.495405487070611], "caller": "main.py:200", "time": "2021-02-05T05:02:06.568844"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.230971559882164, 0.2261505126953125, 100, 0.48972789431681474], "caller": "main.py:200", "time": "2021-02-05T05:03:30.685764"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.01686042547225952, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T05:04:36.006313"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.016561081632971764, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T05:04:37.995773"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23465225100517273, 0.22139891982078552, 100, 0.5060727542132706], "caller": "main.py:200", "time": "2021-02-05T05:04:54.687817"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23483304679393768, 0.22966304421424866, 100, 0.5076963585107462], "caller": "main.py:200", "time": "2021-02-05T05:06:19.304040"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.230827197432518, 0.24103735387325287, 100, 0.497536061356382], "caller": "main.py:200", "time": "2021-02-05T05:07:43.349252"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23072892427444458, 0.21429011225700378, 100, 0.49874498213310514], "caller": "main.py:200", "time": "2021-02-05T05:09:07.845440"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23117703199386597, 0.2353840470314026, 100, 0.506705076643818], "caller": "main.py:200", "time": "2021-02-05T05:10:32.183704"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2288881242275238, 0.23827148973941803, 100, 0.5036924093458024], "caller": "main.py:200", "time": "2021-02-05T05:11:56.519963"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [15], "caller": "main.py:129", "time": "2021-02-05T05:13:18.735773"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T05:13:28.027153"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 11.453584602286728, 0.20539113666074518], "caller": "main.py:37", "time": "2021-02-05T05:13:31.911390"}
+{"level": "info", "fmt": "episode: %s", "args": [7.4268915145181404], "caller": "main.py:149", "time": "2021-02-05T05:13:42.125575"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23891639709472656, 0.22890569269657135, 100, 0.48877179379343855], "caller": "main.py:200", "time": "2021-02-05T05:13:44.206490"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.025204673409461975, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T05:14:27.524724"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23955067992210388, 0.23774927854537964, 100, 0.5097321301188953], "caller": "main.py:200", "time": "2021-02-05T05:15:08.699585"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2384500503540039, 0.25526756048202515, 100, 0.48753717536819274], "caller": "main.py:200", "time": "2021-02-05T05:16:32.816271"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2345035970211029, 0.21408134698867798, 100, 0.48730273271694435], "caller": "main.py:200", "time": "2021-02-05T05:17:57.186340"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23491880297660828, 0.21782457828521729, 100, 0.48404910917209987], "caller": "main.py:200", "time": "2021-02-05T05:19:21.798152"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.233291894197464, 0.22658461332321167, 100, 0.4775457730411775], "caller": "main.py:200", "time": "2021-02-05T05:20:45.950542"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23734605312347412, 0.2349460870027542, 100, 0.4808020600195735], "caller": "main.py:200", "time": "2021-02-05T05:22:10.338591"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23496030271053314, 0.2222604751586914, 100, 0.48992744142878286], "caller": "main.py:200", "time": "2021-02-05T05:23:34.407124"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23416930437088013, 0.26103299856185913, 100, 0.47672855640650885], "caller": "main.py:200", "time": "2021-02-05T05:24:58.979986"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2355758249759674, 0.2250482738018036, 100, 0.491945232997814], "caller": "main.py:200", "time": "2021-02-05T05:26:23.355193"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2340431958436966, 0.2562577724456787, 100, 0.4767081030726562], "caller": "main.py:200", "time": "2021-02-05T05:27:47.616674"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23304826021194458, 0.24584999680519104, 100, 0.48654894227884454], "caller": "main.py:200", "time": "2021-02-05T05:29:11.817358"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.016263088211417198, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T05:29:19.829141"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2366795688867569, 0.24710440635681152, 100, 0.5014514460658255], "caller": "main.py:200", "time": "2021-02-05T05:30:36.186705"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23368015885353088, 0.23180612921714783, 100, 0.49089777583935323], "caller": "main.py:200", "time": "2021-02-05T05:32:00.212673"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23129817843437195, 0.25586360692977905, 100, 0.48633534396791844], "caller": "main.py:200", "time": "2021-02-05T05:33:24.359437"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23265472054481506, 0.22961172461509705, 100, 0.4775468044995552], "caller": "main.py:200", "time": "2021-02-05T05:34:48.926653"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23433057963848114, 0.22930845618247986, 100, 0.4875885197032668], "caller": "main.py:200", "time": "2021-02-05T05:36:12.412931"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23070505261421204, 0.22964432835578918, 100, 0.46407739515432167], "caller": "main.py:200", "time": "2021-02-05T05:37:36.059034"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23228776454925537, 0.2271398901939392, 100, 0.4877114605415914], "caller": "main.py:200", "time": "2021-02-05T05:39:00.053217"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2312116026878357, 0.25577878952026367, 100, 0.4820994597191317], "caller": "main.py:200", "time": "2021-02-05T05:40:24.265329"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [16], "caller": "main.py:129", "time": "2021-02-05T05:41:46.517249"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T05:41:55.850548"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 25.065238425284143, 0.5792627061726003], "caller": "main.py:37", "time": "2021-02-05T05:41:59.977562"}
+{"level": "info", "fmt": "episode: %s", "args": [3.985361198958518], "caller": "main.py:149", "time": "2021-02-05T05:42:10.647104"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.24025161564350128, 0.26333311200141907, 100, 0.5170236747939931], "caller": "main.py:200", "time": "2021-02-05T05:42:12.710147"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23790797591209412, 0.2280895709991455, 100, 0.46864045436464263], "caller": "main.py:200", "time": "2021-02-05T05:43:36.752029"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23451891541481018, 0.22655585408210754, 100, 0.4753780087353258], "caller": "main.py:200", "time": "2021-02-05T05:45:01.031404"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23855631053447723, 0.22669316828250885, 100, 0.4842498656623104], "caller": "main.py:200", "time": "2021-02-05T05:46:24.807846"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23838888108730316, 0.22859713435173035, 100, 0.47619896526343053], "caller": "main.py:200", "time": "2021-02-05T05:47:48.675652"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.016243992373347282, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T05:48:47.742427"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23376336693763733, 0.2072775810956955, 100, 0.4563882600804751], "caller": "main.py:200", "time": "2021-02-05T05:49:12.586784"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23402950167655945, 0.24270538985729218, 100, 0.4703045880324221], "caller": "main.py:200", "time": "2021-02-05T05:50:36.632287"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23294197022914886, 0.22113165259361267, 100, 0.4756015759230896], "caller": "main.py:200", "time": "2021-02-05T05:52:00.360528"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23153960704803467, 0.2677220106124878, 100, 0.4779888796328347], "caller": "main.py:200", "time": "2021-02-05T05:53:24.353629"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23349694907665253, 0.2504303753376007, 100, 0.45583146209926856], "caller": "main.py:200", "time": "2021-02-05T05:54:48.945961"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23394544422626495, 0.21827740967273712, 100, 0.4725258343939324], "caller": "main.py:200", "time": "2021-02-05T05:56:13.182649"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23416203260421753, 0.2744644284248352, 100, 0.4807754355188246], "caller": "main.py:200", "time": "2021-02-05T05:57:38.293281"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23501595854759216, 0.2465403974056244, 100, 0.48403556044029733], "caller": "main.py:200", "time": "2021-02-05T05:59:02.753728"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2312059849500656, 0.2330743968486786, 100, 0.46670614506107555], "caller": "main.py:200", "time": "2021-02-05T06:00:27.104523"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23236827552318573, 0.21455501019954681, 100, 0.4724990611795949], "caller": "main.py:200", "time": "2021-02-05T06:01:51.478992"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23072011768817902, 0.20852775871753693, 100, 0.4804426723238201], "caller": "main.py:200", "time": "2021-02-05T06:03:15.534774"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23141290247440338, 0.2099551260471344, 100, 0.47681363220215084], "caller": "main.py:200", "time": "2021-02-05T06:04:39.777595"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2330247312784195, 0.23137208819389343, 100, 0.4681860050090858], "caller": "main.py:200", "time": "2021-02-05T06:06:03.406477"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2297554761171341, 0.2378171682357788, 100, 0.4785816907404109], "caller": "main.py:200", "time": "2021-02-05T06:07:28.258744"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23352240025997162, 0.23339585959911346, 100, 0.4723723384265596], "caller": "main.py:200", "time": "2021-02-05T06:08:52.677242"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [17], "caller": "main.py:129", "time": "2021-02-05T06:10:15.130956"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T06:10:24.509238"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 13.151267625546446, 1.1674214710377575], "caller": "main.py:37", "time": "2021-02-05T06:10:28.579585"}
+{"level": "info", "fmt": "episode: %s", "args": [6.361894630352007], "caller": "main.py:149", "time": "2021-02-05T06:10:38.942348"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2340031862258911, 0.2409648299217224, 100, 0.460775562478718], "caller": "main.py:200", "time": "2021-02-05T06:10:41.023104"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23603001236915588, 0.2121797800064087, 100, 0.4718091517215035], "caller": "main.py:200", "time": "2021-02-05T06:12:05.178690"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23292867839336395, 0.22598688304424286, 100, 0.46211433316616424], "caller": "main.py:200", "time": "2021-02-05T06:13:29.180837"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23613639175891876, 0.24162890017032623, 100, 0.45539984162068164], "caller": "main.py:200", "time": "2021-02-05T06:14:53.112015"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23332369327545166, 0.25787824392318726, 100, 0.4539393400691996], "caller": "main.py:200", "time": "2021-02-05T06:16:17.852799"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23532316088676453, 0.23272372782230377, 100, 0.45803509350011024], "caller": "main.py:200", "time": "2021-02-05T06:17:41.768439"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2331160306930542, 0.24416548013687134, 100, 0.46992711330721343], "caller": "main.py:200", "time": "2021-02-05T06:19:06.276909"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.22992555797100067, 0.2131688892841339, 100, 0.46056539054079654], "caller": "main.py:200", "time": "2021-02-05T06:20:31.342999"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23451000452041626, 0.20407795906066895, 100, 0.4706508472091186], "caller": "main.py:200", "time": "2021-02-05T06:21:55.800406"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23527327179908752, 0.22056610882282257, 100, 0.46581905717959915], "caller": "main.py:200", "time": "2021-02-05T06:23:20.011747"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23198404908180237, 0.2381836622953415, 100, 0.45983206148317635], "caller": "main.py:200", "time": "2021-02-05T06:24:44.468037"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2338002622127533, 0.26579520106315613, 100, 0.47079243893683803], "caller": "main.py:200", "time": "2021-02-05T06:26:09.324985"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23447303473949432, 0.24053707718849182, 100, 0.46107379193463605], "caller": "main.py:200", "time": "2021-02-05T06:27:34.019540"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23031164705753326, 0.2078428864479065, 100, 0.4469660365134547], "caller": "main.py:200", "time": "2021-02-05T06:28:58.926141"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23225905001163483, 0.26054278016090393, 100, 0.4625957431926664], "caller": "main.py:200", "time": "2021-02-05T06:30:23.691610"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23172885179519653, 0.22829553484916687, 100, 0.4614923882022944], "caller": "main.py:200", "time": "2021-02-05T06:31:48.834340"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.2300747036933899, 0.22779832780361176, 100, 0.47626630897527683], "caller": "main.py:200", "time": "2021-02-05T06:33:14.121116"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22698825597763062, 0.2417001724243164, 100, 0.452276017024589], "caller": "main.py:200", "time": "2021-02-05T06:34:39.712053"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22937501966953278, 0.23394030332565308, 100, 0.4447970348151776], "caller": "main.py:200", "time": "2021-02-05T06:36:05.008397"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23091286420822144, 0.23570109903812408, 100, 0.45287754918576467], "caller": "main.py:200", "time": "2021-02-05T06:37:30.536882"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [18], "caller": "main.py:129", "time": "2021-02-05T06:38:54.253419"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T06:39:03.561044"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 22.591292451709577, 0.5066565546767223], "caller": "main.py:37", "time": "2021-02-05T06:39:07.622659"}
+{"level": "info", "fmt": "episode: %s", "args": [5.976062145430522], "caller": "main.py:149", "time": "2021-02-05T06:39:17.812316"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23678892850875854, 0.24572071433067322, 100, 0.4555732756397531], "caller": "main.py:200", "time": "2021-02-05T06:39:19.867024"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2353256344795227, 0.21990510821342468, 100, 0.4620146289005797], "caller": "main.py:200", "time": "2021-02-05T06:40:44.770408"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23664043843746185, 0.21498650312423706, 100, 0.4563225706955444], "caller": "main.py:200", "time": "2021-02-05T06:42:10.670834"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2338596135377884, 0.2312772572040558, 100, 0.4592355805176217], "caller": "main.py:200", "time": "2021-02-05T06:43:36.585366"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23109443485736847, 0.21659594774246216, 100, 0.45571222092124475], "caller": "main.py:200", "time": "2021-02-05T06:45:01.247250"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2330709844827652, 0.22419197857379913, 100, 0.4392592256822017], "caller": "main.py:200", "time": "2021-02-05T06:46:26.249443"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23462063074111938, 0.2433680146932602, 100, 0.4509230172183202], "caller": "main.py:200", "time": "2021-02-05T06:47:51.755178"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23405911028385162, 0.23898372054100037, 100, 0.457219464435155], "caller": "main.py:200", "time": "2021-02-05T06:49:16.641839"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23065179586410522, 0.2443445324897766, 100, 0.4577004450101916], "caller": "main.py:200", "time": "2021-02-05T06:50:41.440650"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23051807284355164, 0.23562215268611908, 100, 0.45325575376984856], "caller": "main.py:200", "time": "2021-02-05T06:52:07.094728"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2334284782409668, 0.24470052123069763, 100, 0.4477801662235707], "caller": "main.py:200", "time": "2021-02-05T06:53:32.089679"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2318325936794281, 0.2591647803783417, 100, 0.4474077340513095], "caller": "main.py:200", "time": "2021-02-05T06:54:57.564113"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22810064256191254, 0.2502260208129883, 100, 0.4496019193080481], "caller": "main.py:200", "time": "2021-02-05T06:56:23.513295"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2298877090215683, 0.22838044166564941, 100, 0.4577901687760327], "caller": "main.py:200", "time": "2021-02-05T06:57:48.873591"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23073039948940277, 0.21686507761478424, 100, 0.4480628573446438], "caller": "main.py:200", "time": "2021-02-05T06:59:14.672994"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2319490760564804, 0.23545992374420166, 100, 0.448813447609366], "caller": "main.py:200", "time": "2021-02-05T07:00:40.646248"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23144912719726562, 0.22021140158176422, 100, 0.449950697973079], "caller": "main.py:200", "time": "2021-02-05T07:02:06.424119"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23167894780635834, 0.23121041059494019, 100, 0.44263144756897616], "caller": "main.py:200", "time": "2021-02-05T07:03:31.328928"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2294718623161316, 0.22808435559272766, 100, 0.44457138712728816], "caller": "main.py:200", "time": "2021-02-05T07:04:56.828166"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2302701324224472, 0.23662367463111877, 100, 0.44259444083317817], "caller": "main.py:200", "time": "2021-02-05T07:06:22.345712"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [19], "caller": "main.py:129", "time": "2021-02-05T07:07:46.019125"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T07:07:55.420245"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 16.575898185210274, 3.0381880915286703], "caller": "main.py:37", "time": "2021-02-05T07:07:59.487268"}
+{"level": "info", "fmt": "episode: %s", "args": [6.801214068929713], "caller": "main.py:149", "time": "2021-02-05T07:08:09.828968"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.235059916973114, 0.2576184868812561, 100, 0.4515701269651885], "caller": "main.py:200", "time": "2021-02-05T07:08:11.905869"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2339038848876953, 0.2510763704776764, 100, 0.4475181898027175], "caller": "main.py:200", "time": "2021-02-05T07:09:37.355904"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23485197126865387, 0.23424509167671204, 100, 0.4572478055496732], "caller": "main.py:200", "time": "2021-02-05T07:11:03.091671"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23379234969615936, 0.2054198980331421, 100, 0.45768446470690566], "caller": "main.py:200", "time": "2021-02-05T07:12:28.448803"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23196735978126526, 0.23525120317935944, 100, 0.4272540342380516], "caller": "main.py:200", "time": "2021-02-05T07:13:52.989967"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23106621205806732, 0.23004399240016937, 100, 0.4548404702089372], "caller": "main.py:200", "time": "2021-02-05T07:15:17.490902"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23141609132289886, 0.22950252890586853, 100, 0.44996280457050136], "caller": "main.py:200", "time": "2021-02-05T07:16:41.697252"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23460924625396729, 0.23333299160003662, 100, 0.4517458426500616], "caller": "main.py:200", "time": "2021-02-05T07:18:05.622490"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23318763077259064, 0.21843907237052917, 100, 0.4524909567380456], "caller": "main.py:200", "time": "2021-02-05T07:19:30.024859"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2295101135969162, 0.24244031310081482, 100, 0.44473647023760415], "caller": "main.py:200", "time": "2021-02-05T07:20:54.611727"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.22887387871742249, 0.2252350151538849, 100, 0.4403767612136108], "caller": "main.py:200", "time": "2021-02-05T07:22:19.398363"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22893011569976807, 0.2130654752254486, 100, 0.4400187640942199], "caller": "main.py:200", "time": "2021-02-05T07:23:43.738687"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23231498897075653, 0.2196507453918457, 100, 0.44438503409948626], "caller": "main.py:200", "time": "2021-02-05T07:25:08.131308"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.226996511220932, 0.23967714607715607, 100, 0.4489899149088097], "caller": "main.py:200", "time": "2021-02-05T07:26:32.685513"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22925955057144165, 0.22958122193813324, 100, 0.44343450780248883], "caller": "main.py:200", "time": "2021-02-05T07:27:57.227510"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22911719977855682, 0.21450024843215942, 100, 0.43392856444549494], "caller": "main.py:200", "time": "2021-02-05T07:29:21.579913"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22940796613693237, 0.24871481955051422, 100, 0.442663846269687], "caller": "main.py:200", "time": "2021-02-05T07:30:45.930277"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2320886105298996, 0.2444828301668167, 100, 0.45582908506603514], "caller": "main.py:200", "time": "2021-02-05T07:32:09.774532"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23058828711509705, 0.2167779952287674, 100, 0.4496486699131498], "caller": "main.py:200", "time": "2021-02-05T07:33:33.972474"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23110555112361908, 0.23248390853405, 100, 0.44461273367727183], "caller": "main.py:200", "time": "2021-02-05T07:34:58.573966"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [20], "caller": "main.py:129", "time": "2021-02-05T07:36:21.104228"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T07:36:30.468602"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 35.50797603535172, 11.696723458647924], "caller": "main.py:37", "time": "2021-02-05T07:36:34.712406"}
+{"level": "info", "fmt": "episode: %s", "args": [4.954166266683951], "caller": "main.py:149", "time": "2021-02-05T07:36:45.244356"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23200687766075134, 0.24967879056930542, 100, 0.45397842462291427], "caller": "main.py:200", "time": "2021-02-05T07:36:47.323613"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23126055300235748, 0.22798220813274384, 100, 0.44671917195621313], "caller": "main.py:200", "time": "2021-02-05T07:38:11.698454"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23137202858924866, 0.20931392908096313, 100, 0.44132664169428215], "caller": "main.py:200", "time": "2021-02-05T07:39:35.568495"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23272937536239624, 0.21460652351379395, 100, 0.4390347024359359], "caller": "main.py:200", "time": "2021-02-05T07:41:00.056802"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23037250339984894, 0.2335653454065323, 100, 0.4434593867812156], "caller": "main.py:200", "time": "2021-02-05T07:42:24.175737"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23159682750701904, 0.2372710257768631, 100, 0.4469195755869266], "caller": "main.py:200", "time": "2021-02-05T07:43:48.464765"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.22598280012607574, 0.2419876605272293, 100, 0.4370839973847088], "caller": "main.py:200", "time": "2021-02-05T07:45:13.238346"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.22913362085819244, 0.2262241095304489, 100, 0.43579488243275033], "caller": "main.py:200", "time": "2021-02-05T07:46:37.437884"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2270323783159256, 0.23754096031188965, 100, 0.4389278637923957], "caller": "main.py:200", "time": "2021-02-05T07:48:02.173673"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.22833533585071564, 0.2510738670825958, 100, 0.4593681665795629], "caller": "main.py:200", "time": "2021-02-05T07:49:26.996687"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.22879809141159058, 0.2241210639476776, 100, 0.43778590078744395], "caller": "main.py:200", "time": "2021-02-05T07:50:51.050156"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22978082299232483, 0.20631016790866852, 100, 0.4411116504228078], "caller": "main.py:200", "time": "2021-02-05T07:52:15.371840"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23067785799503326, 0.24106743931770325, 100, 0.4416084253346386], "caller": "main.py:200", "time": "2021-02-05T07:53:39.894324"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23025988042354584, 0.23235741257667542, 100, 0.45072956557534943], "caller": "main.py:200", "time": "2021-02-05T07:55:04.020343"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22742120921611786, 0.26399409770965576, 100, 0.44182778324238875], "caller": "main.py:200", "time": "2021-02-05T07:56:28.576112"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.226710706949234, 0.2117944359779358, 100, 0.4477048766165302], "caller": "main.py:200", "time": "2021-02-05T07:57:53.343429"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22990193963050842, 0.2326594740152359, 100, 0.45201874460230107], "caller": "main.py:200", "time": "2021-02-05T07:59:17.354609"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22767825424671173, 0.23734712600708008, 100, 0.43725691433348685], "caller": "main.py:200", "time": "2021-02-05T08:00:41.488942"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22730767726898193, 0.2334766387939453, 100, 0.4256877526215205], "caller": "main.py:200", "time": "2021-02-05T08:02:05.313755"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2275833636522293, 0.21819943189620972, 100, 0.43759902711306997], "caller": "main.py:200", "time": "2021-02-05T08:03:29.411032"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [21], "caller": "main.py:129", "time": "2021-02-05T08:04:51.945383"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T08:05:01.397839"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 11.678316187586628, 0.8241494184373047], "caller": "main.py:37", "time": "2021-02-05T08:05:05.420591"}
+{"level": "info", "fmt": "episode: %s", "args": [0.7405185857933296], "caller": "main.py:149", "time": "2021-02-05T08:05:15.902418"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2329593151807785, 0.24336454272270203, 100, 0.4505266826894368], "caller": "main.py:200", "time": "2021-02-05T08:05:17.964751"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.22805869579315186, 0.25074946880340576, 100, 0.445466201558866], "caller": "main.py:200", "time": "2021-02-05T08:06:41.793054"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.22591181099414825, 0.23801615834236145, 100, 0.4421076917206208], "caller": "main.py:200", "time": "2021-02-05T08:08:06.477789"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2277001440525055, 0.2154579758644104, 100, 0.4349544584316175], "caller": "main.py:200", "time": "2021-02-05T08:09:30.524203"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.22770191729068756, 0.24047306180000305, 100, 0.4454101776631311], "caller": "main.py:200", "time": "2021-02-05T08:10:55.145713"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.22394296526908875, 0.23222608864307404, 100, 0.4421647125040395], "caller": "main.py:200", "time": "2021-02-05T08:12:19.538660"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.226701557636261, 0.22540119290351868, 100, 0.43667385752602716], "caller": "main.py:200", "time": "2021-02-05T08:13:43.817967"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2242944836616516, 0.20997388660907745, 100, 0.4257047745159737], "caller": "main.py:200", "time": "2021-02-05T08:15:08.088091"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2249254733324051, 0.2349366843700409, 100, 0.4411407994782858], "caller": "main.py:200", "time": "2021-02-05T08:16:32.457535"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.22629916667938232, 0.25230658054351807, 100, 0.44670336331430416], "caller": "main.py:200", "time": "2021-02-05T08:17:56.903873"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2267710119485855, 0.21963194012641907, 100, 0.4420914986287531], "caller": "main.py:200", "time": "2021-02-05T08:19:21.273463"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22553569078445435, 0.2366010695695877, 100, 0.4445815830978225], "caller": "main.py:200", "time": "2021-02-05T08:20:45.900155"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22712768614292145, 0.23524415493011475, 100, 0.4424032461200816], "caller": "main.py:200", "time": "2021-02-05T08:22:10.107624"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.22481849789619446, 0.2332494556903839, 100, 0.4573072552223662], "caller": "main.py:200", "time": "2021-02-05T08:23:35.138130"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22663724422454834, 0.22033077478408813, 100, 0.4414046316897539], "caller": "main.py:200", "time": "2021-02-05T08:24:59.874916"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22284114360809326, 0.24398528039455414, 100, 0.43693393494289795], "caller": "main.py:200", "time": "2021-02-05T08:26:24.743544"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22525501251220703, 0.2440469115972519, 100, 0.43492664034785716], "caller": "main.py:200", "time": "2021-02-05T08:27:49.213017"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2248300015926361, 0.21128100156784058, 100, 0.43459627236550263], "caller": "main.py:200", "time": "2021-02-05T08:29:13.499469"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2244674265384674, 0.19695927202701569, 100, 0.4352485620540246], "caller": "main.py:200", "time": "2021-02-05T08:30:37.422945"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2258348912000656, 0.22057051956653595, 100, 0.4496342607883], "caller": "main.py:200", "time": "2021-02-05T08:32:02.107271"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [22], "caller": "main.py:129", "time": "2021-02-05T08:33:24.288118"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T08:33:33.745374"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 13.646150891386148, 0.842932619768964], "caller": "main.py:37", "time": "2021-02-05T08:33:37.758700"}
+{"level": "info", "fmt": "episode: %s", "args": [7.495536238869844], "caller": "main.py:149", "time": "2021-02-05T08:33:48.196832"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23146165907382965, 0.2181902825832367, 100, 0.4400997525013468], "caller": "main.py:200", "time": "2021-02-05T08:33:50.246089"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2274530977010727, 0.2222861647605896, 100, 0.43325717623669596], "caller": "main.py:200", "time": "2021-02-05T08:35:14.319316"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.22841982543468475, 0.25896620750427246, 100, 0.44349357868766437], "caller": "main.py:200", "time": "2021-02-05T08:36:38.603080"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23159055411815643, 0.23844605684280396, 100, 0.43897746945998184], "caller": "main.py:200", "time": "2021-02-05T08:38:02.749225"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.22758233547210693, 0.23871368169784546, 100, 0.4212396233852671], "caller": "main.py:200", "time": "2021-02-05T08:39:27.332666"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.22957351803779602, 0.20552703738212585, 100, 0.4257119863799676], "caller": "main.py:200", "time": "2021-02-05T08:40:51.923725"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.227495938539505, 0.23718777298927307, 100, 0.43351626332239396], "caller": "main.py:200", "time": "2021-02-05T08:42:16.401375"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.22724592685699463, 0.2320299744606018, 100, 0.4378466274896512], "caller": "main.py:200", "time": "2021-02-05T08:43:41.479854"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.22853054106235504, 0.21745902299880981, 100, 0.432450433032183], "caller": "main.py:200", "time": "2021-02-05T08:45:05.995232"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.22813375294208527, 0.23637905716896057, 100, 0.42335310365496465], "caller": "main.py:200", "time": "2021-02-05T08:46:29.854553"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.22510850429534912, 0.21295496821403503, 100, 0.4336928450627556], "caller": "main.py:200", "time": "2021-02-05T08:47:54.331363"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22726497054100037, 0.2387245148420334, 100, 0.43300602550944906], "caller": "main.py:200", "time": "2021-02-05T08:49:19.054967"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2270524948835373, 0.20198146998882294, 100, 0.43288614060126174], "caller": "main.py:200", "time": "2021-02-05T08:50:43.795536"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2265285700559616, 0.2256505936384201, 100, 0.42524836357155754], "caller": "main.py:200", "time": "2021-02-05T08:52:08.558787"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2282010316848755, 0.22593548893928528, 100, 0.4417589661037238], "caller": "main.py:200", "time": "2021-02-05T08:53:33.687801"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22643284499645233, 0.22619250416755676, 100, 0.43485948141040964], "caller": "main.py:200", "time": "2021-02-05T08:54:58.175010"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22714722156524658, 0.22351756691932678, 100, 0.4322143324777404], "caller": "main.py:200", "time": "2021-02-05T08:56:23.196312"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22618955373764038, 0.22516003251075745, 100, 0.4376068985024582], "caller": "main.py:200", "time": "2021-02-05T08:57:47.865482"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2276214361190796, 0.22527214884757996, 100, 0.4364525720040652], "caller": "main.py:200", "time": "2021-02-05T08:59:12.533597"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2256065011024475, 0.23036065697669983, 100, 0.44000161106954816], "caller": "main.py:200", "time": "2021-02-05T09:00:36.196322"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [23], "caller": "main.py:129", "time": "2021-02-05T09:01:59.273442"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T09:02:08.809749"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 14.787321611438987, 1.5307683208637288], "caller": "main.py:37", "time": "2021-02-05T09:02:12.720756"}
+{"level": "info", "fmt": "episode: %s", "args": [7.9897001042202], "caller": "main.py:149", "time": "2021-02-05T09:02:23.366138"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2296789586544037, 0.22227759659290314, 100, 0.4259324451855066], "caller": "main.py:200", "time": "2021-02-05T09:02:25.471773"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.22773799300193787, 0.22642311453819275, 100, 0.4269938361217751], "caller": "main.py:200", "time": "2021-02-05T09:03:50.052160"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23173849284648895, 0.21476605534553528, 100, 0.4289557224083145], "caller": "main.py:200", "time": "2021-02-05T09:05:14.513368"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.22872401773929596, 0.21574732661247253, 100, 0.4191789629636249], "caller": "main.py:200", "time": "2021-02-05T09:06:38.897607"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.22903625667095184, 0.25186383724212646, 100, 0.427203625099854], "caller": "main.py:200", "time": "2021-02-05T09:08:03.326012"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2311437427997589, 0.2246394157409668, 100, 0.4329810389443385], "caller": "main.py:200", "time": "2021-02-05T09:09:27.817044"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.22891083359718323, 0.21505534648895264, 100, 0.4241666623525838], "caller": "main.py:200", "time": "2021-02-05T09:10:53.066694"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.22808845341205597, 0.24990767240524292, 100, 0.4356497180026234], "caller": "main.py:200", "time": "2021-02-05T09:12:17.791395"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2279646098613739, 0.20408862829208374, 100, 0.4239628907614177], "caller": "main.py:200", "time": "2021-02-05T09:13:42.922025"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2261800318956375, 0.2387159764766693, 100, 0.4183436679421744], "caller": "main.py:200", "time": "2021-02-05T09:15:07.300921"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23035933077335358, 0.21731385588645935, 100, 0.4316625675127382], "caller": "main.py:200", "time": "2021-02-05T09:16:31.730990"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22864408791065216, 0.23575985431671143, 100, 0.42866403813843856], "caller": "main.py:200", "time": "2021-02-05T09:17:55.986255"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22640617191791534, 0.23659692704677582, 100, 0.4224291613275577], "caller": "main.py:200", "time": "2021-02-05T09:19:20.690875"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23104728758335114, 0.23005177080631256, 100, 0.4218358432824777], "caller": "main.py:200", "time": "2021-02-05T09:20:44.811581"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2274480015039444, 0.23568496108055115, 100, 0.41976290907238456], "caller": "main.py:200", "time": "2021-02-05T09:22:09.530854"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2264994978904724, 0.22596633434295654, 100, 0.44431072826710766], "caller": "main.py:200", "time": "2021-02-05T09:23:34.285274"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22854375839233398, 0.22115981578826904, 100, 0.4283836993146713], "caller": "main.py:200", "time": "2021-02-05T09:24:58.282555"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22585396468639374, 0.20178104937076569, 100, 0.4179365950404894], "caller": "main.py:200", "time": "2021-02-05T09:26:22.806094"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22730234265327454, 0.21419626474380493, 100, 0.4266398757269512], "caller": "main.py:200", "time": "2021-02-05T09:27:47.103522"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.22416910529136658, 0.20088323950767517, 100, 0.4225049536997768], "caller": "main.py:200", "time": "2021-02-05T09:29:11.327784"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [24], "caller": "main.py:129", "time": "2021-02-05T09:30:33.645752"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T09:30:43.177271"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 16.75103532874253, 3.9489207613781168], "caller": "main.py:37", "time": "2021-02-05T09:30:47.392210"}
+{"level": "info", "fmt": "episode: %s", "args": [5.900852249399076], "caller": "main.py:149", "time": "2021-02-05T09:30:57.670285"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.22900953888893127, 0.27509045600891113, 100, 0.4177231987893091], "caller": "main.py:200", "time": "2021-02-05T09:30:59.702051"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23160240054130554, 0.22372379899024963, 100, 0.4212767836030254], "caller": "main.py:200", "time": "2021-02-05T09:32:24.561611"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2299179583787918, 0.2303573489189148, 100, 0.4205876698669417], "caller": "main.py:200", "time": "2021-02-05T09:33:49.260847"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2287469059228897, 0.22462165355682373, 100, 0.42782956178678405], "caller": "main.py:200", "time": "2021-02-05T09:35:14.162256"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.22891603410243988, 0.22504167258739471, 100, 0.4204216271218449], "caller": "main.py:200", "time": "2021-02-05T09:36:38.848223"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.22728127241134644, 0.2097894549369812, 100, 0.4230411925488862], "caller": "main.py:200", "time": "2021-02-05T09:38:03.600618"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.22944797575473785, 0.24224016070365906, 100, 0.4119073274319729], "caller": "main.py:200", "time": "2021-02-05T09:39:28.200316"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2270640730857849, 0.21584001183509827, 100, 0.42673471416736], "caller": "main.py:200", "time": "2021-02-05T09:40:53.535297"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.22915852069854736, 0.2426147758960724, 100, 0.43800278689057554], "caller": "main.py:200", "time": "2021-02-05T09:42:18.051178"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.22880497574806213, 0.2463330626487732, 100, 0.4217616739447356], "caller": "main.py:200", "time": "2021-02-05T09:43:42.760416"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2265554964542389, 0.21464776992797852, 100, 0.4245857256226339], "caller": "main.py:200", "time": "2021-02-05T09:45:06.968245"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22774738073349, 0.2462335228919983, 100, 0.4232103192383034], "caller": "main.py:200", "time": "2021-02-05T09:46:31.346817"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2273605912923813, 0.23120859265327454, 100, 0.43475142713013937], "caller": "main.py:200", "time": "2021-02-05T09:47:55.170977"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.224029079079628, 0.2174343317747116, 100, 0.411934139985376], "caller": "main.py:200", "time": "2021-02-05T09:49:19.214888"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22686201333999634, 0.20543088018894196, 100, 0.4160663929165576], "caller": "main.py:200", "time": "2021-02-05T09:50:43.391757"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22835072875022888, 0.24601656198501587, 100, 0.44281969036204055], "caller": "main.py:200", "time": "2021-02-05T09:52:07.946370"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22853542864322662, 0.20579226315021515, 100, 0.4137307950440571], "caller": "main.py:200", "time": "2021-02-05T09:53:32.367886"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22480060160160065, 0.23445820808410645, 100, 0.42250672991819094], "caller": "main.py:200", "time": "2021-02-05T09:54:56.512440"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22941723465919495, 0.22701774537563324, 100, 0.4239478328400096], "caller": "main.py:200", "time": "2021-02-05T09:56:20.708633"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.22432689368724823, 0.21706244349479675, 100, 0.41671932961349856], "caller": "main.py:200", "time": "2021-02-05T09:57:44.819640"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [25], "caller": "main.py:129", "time": "2021-02-05T09:59:06.557949"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T09:59:15.695398"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 16.3812468206838, 1.4244652225023984], "caller": "main.py:37", "time": "2021-02-05T09:59:19.634018"}
+{"level": "info", "fmt": "episode: %s", "args": [8.50590461653519], "caller": "main.py:149", "time": "2021-02-05T09:59:30.032019"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.22846826910972595, 0.24267610907554626, 100, 0.41908556457143376], "caller": "main.py:200", "time": "2021-02-05T09:59:32.079547"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2289559692144394, 0.21861332654953003, 100, 0.4243272152118741], "caller": "main.py:200", "time": "2021-02-05T10:00:56.726452"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.22599942982196808, 0.20319610834121704, 100, 0.4104893636293002], "caller": "main.py:200", "time": "2021-02-05T10:02:21.199092"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2259712666273117, 0.229896679520607, 100, 0.41839934315073746], "caller": "main.py:200", "time": "2021-02-05T10:03:45.506357"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.22949257493019104, 0.2119535505771637, 100, 0.4146450998964513], "caller": "main.py:200", "time": "2021-02-05T10:05:10.063254"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2281280755996704, 0.24138349294662476, 100, 0.4111149799412646], "caller": "main.py:200", "time": "2021-02-05T10:06:34.800400"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.22875478863716125, 0.2389909029006958, 100, 0.42499280835908537], "caller": "main.py:200", "time": "2021-02-05T10:07:59.554896"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.22700923681259155, 0.23604990541934967, 100, 0.4210035693224474], "caller": "main.py:200", "time": "2021-02-05T10:09:24.607377"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2254626303911209, 0.21443504095077515, 100, 0.40838811065643066], "caller": "main.py:200", "time": "2021-02-05T10:10:49.131774"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2253580093383789, 0.22282874584197998, 100, 0.4131205570284664], "caller": "main.py:200", "time": "2021-02-05T10:12:13.282174"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.22705651819705963, 0.2334049642086029, 100, 0.40926069076737015], "caller": "main.py:200", "time": "2021-02-05T10:13:37.886222"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22620481252670288, 0.2126861810684204, 100, 0.40500274505141487], "caller": "main.py:200", "time": "2021-02-05T10:15:02.221105"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22564832866191864, 0.22062227129936218, 100, 0.4231458064490604], "caller": "main.py:200", "time": "2021-02-05T10:16:26.845031"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2277829349040985, 0.21216678619384766, 100, 0.4147589745702833], "caller": "main.py:200", "time": "2021-02-05T10:17:51.286822"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22410330176353455, 0.22810602188110352, 100, 0.4143692043009284], "caller": "main.py:200", "time": "2021-02-05T10:19:15.984912"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22607174515724182, 0.22563862800598145, 100, 0.41305404927176304], "caller": "main.py:200", "time": "2021-02-05T10:20:40.304892"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.2249542772769928, 0.23390032351016998, 100, 0.41753046866056326], "caller": "main.py:200", "time": "2021-02-05T10:22:04.518181"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2251235395669937, 0.2272515594959259, 100, 0.41293341274900514], "caller": "main.py:200", "time": "2021-02-05T10:23:28.789063"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2215394228696823, 0.23634155094623566, 100, 0.40192043032203256], "caller": "main.py:200", "time": "2021-02-05T10:24:53.567802"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.22681652009487152, 0.23759952187538147, 100, 0.41267661477968953], "caller": "main.py:200", "time": "2021-02-05T10:26:18.526006"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [26], "caller": "main.py:129", "time": "2021-02-05T10:27:40.729666"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T10:27:50.225197"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 21.384929069126127, 2.4757067411925124], "caller": "main.py:37", "time": "2021-02-05T10:27:54.300445"}
+{"level": "info", "fmt": "episode: %s", "args": [7.895967041952188], "caller": "main.py:149", "time": "2021-02-05T10:28:04.841765"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23329991102218628, 0.22833693027496338, 100, 0.4220623680527149], "caller": "main.py:200", "time": "2021-02-05T10:28:06.996864"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23172339797019958, 0.23100614547729492, 100, 0.40140476848641365], "caller": "main.py:200", "time": "2021-02-05T10:29:31.278788"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2287292331457138, 0.20986971259117126, 100, 0.420338766830849], "caller": "main.py:200", "time": "2021-02-05T10:30:55.518094"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.22908878326416016, 0.23271626234054565, 100, 0.40362275954024995], "caller": "main.py:200", "time": "2021-02-05T10:32:20.328142"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.22901897132396698, 0.21118101477622986, 100, 0.4191519692121017], "caller": "main.py:200", "time": "2021-02-05T10:33:44.632411"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.22908951342105865, 0.20597173273563385, 100, 0.41529044564385764], "caller": "main.py:200", "time": "2021-02-05T10:35:08.494269"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23022235929965973, 0.20681267976760864, 100, 0.4091203826256758], "caller": "main.py:200", "time": "2021-02-05T10:36:33.287465"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23036514222621918, 0.2261263132095337, 100, 0.41862295981035635], "caller": "main.py:200", "time": "2021-02-05T10:37:57.617723"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2267182320356369, 0.2392967790365219, 100, 0.4269524520208653], "caller": "main.py:200", "time": "2021-02-05T10:39:22.240096"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23092392086982727, 0.23132500052452087, 100, 0.4138744058791181], "caller": "main.py:200", "time": "2021-02-05T10:40:46.719596"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2277345061302185, 0.24620458483695984, 100, 0.41361489738858165], "caller": "main.py:200", "time": "2021-02-05T10:42:11.052093"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22896669805049896, 0.23505553603172302, 100, 0.4173951890647249], "caller": "main.py:200", "time": "2021-02-05T10:43:35.788154"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23045678436756134, 0.23032410442829132, 100, 0.4195325469551171], "caller": "main.py:200", "time": "2021-02-05T10:45:00.576550"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23094232380390167, 0.2291749119758606, 100, 0.41659733350879347], "caller": "main.py:200", "time": "2021-02-05T10:46:24.993796"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22946952283382416, 0.2070923149585724, 100, 0.41184596266204343], "caller": "main.py:200", "time": "2021-02-05T10:47:49.505333"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22914983332157135, 0.22693291306495667, 100, 0.41179357464963945], "caller": "main.py:200", "time": "2021-02-05T10:49:14.100705"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.230529323220253, 0.22341805696487427, 100, 0.4097350674457895], "caller": "main.py:200", "time": "2021-02-05T10:50:38.628013"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22557421028614044, 0.22160880267620087, 100, 0.41909192796330935], "caller": "main.py:200", "time": "2021-02-05T10:52:03.337872"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22521667182445526, 0.21596431732177734, 100, 0.4132467937056236], "caller": "main.py:200", "time": "2021-02-05T10:53:28.003274"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2251421958208084, 0.23713986575603485, 100, 0.4093473034687775], "caller": "main.py:200", "time": "2021-02-05T10:54:52.190476"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [27], "caller": "main.py:129", "time": "2021-02-05T10:56:14.311704"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T10:56:23.166765"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 16.469842913766026, 0.09926198037178974], "caller": "main.py:37", "time": "2021-02-05T10:56:27.111106"}
+{"level": "info", "fmt": "episode: %s", "args": [6.4530090679476775], "caller": "main.py:149", "time": "2021-02-05T10:56:36.838545"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23150356113910675, 0.23053812980651855, 100, 0.4171253275454152], "caller": "main.py:200", "time": "2021-02-05T10:56:38.881956"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23342232406139374, 0.23725609481334686, 100, 0.4240858834557833], "caller": "main.py:200", "time": "2021-02-05T10:58:03.030095"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23197823762893677, 0.23863019049167633, 100, 0.4262582617571865], "caller": "main.py:200", "time": "2021-02-05T10:59:27.579552"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23243603110313416, 0.21856436133384705, 100, 0.4165192022503136], "caller": "main.py:200", "time": "2021-02-05T11:00:52.076103"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23066078126430511, 0.25932490825653076, 100, 0.4037162559820817], "caller": "main.py:200", "time": "2021-02-05T11:02:16.829212"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23150569200515747, 0.23925775289535522, 100, 0.4088419663497256], "caller": "main.py:200", "time": "2021-02-05T11:03:41.322846"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2295193076133728, 0.23605120182037354, 100, 0.4078849261591246], "caller": "main.py:200", "time": "2021-02-05T11:05:05.551972"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.22871188819408417, 0.24074429273605347, 100, 0.4220809688742081], "caller": "main.py:200", "time": "2021-02-05T11:06:29.695068"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.22955270111560822, 0.23456184566020966, 100, 0.42274019207303287], "caller": "main.py:200", "time": "2021-02-05T11:07:54.085462"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.22809921205043793, 0.21796569228172302, 100, 0.4144427930894304], "caller": "main.py:200", "time": "2021-02-05T11:09:19.464274"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2308286875486374, 0.21301540732383728, 100, 0.4151793610634473], "caller": "main.py:200", "time": "2021-02-05T11:10:43.731066"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22956305742263794, 0.19841308891773224, 100, 0.4059887873720446], "caller": "main.py:200", "time": "2021-02-05T11:12:08.391923"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22883471846580505, 0.2482808530330658, 100, 0.4158454325379328], "caller": "main.py:200", "time": "2021-02-05T11:13:33.259111"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.22951200604438782, 0.22678108513355255, 100, 0.4141053592744617], "caller": "main.py:200", "time": "2021-02-05T11:14:57.768216"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22774146497249603, 0.24708041548728943, 100, 0.4124291714613473], "caller": "main.py:200", "time": "2021-02-05T11:16:22.435084"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23062698543071747, 0.2055921107530594, 100, 0.4074673246929874], "caller": "main.py:200", "time": "2021-02-05T11:17:46.859204"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.2290666252374649, 0.23233084380626678, 100, 0.4134982019249359], "caller": "main.py:200", "time": "2021-02-05T11:19:11.523058"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2289571315050125, 0.23866552114486694, 100, 0.416087848502512], "caller": "main.py:200", "time": "2021-02-05T11:20:35.582599"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22857166826725006, 0.225029855966568, 100, 0.42010896380514945], "caller": "main.py:200", "time": "2021-02-05T11:22:00.056773"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23065416514873505, 0.23923705518245697, 100, 0.4240023025446321], "caller": "main.py:200", "time": "2021-02-05T11:23:24.467596"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [28], "caller": "main.py:129", "time": "2021-02-05T11:24:46.817523"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T11:24:56.062007"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 16.684407853856555, 1.3412064134297486], "caller": "main.py:37", "time": "2021-02-05T11:25:00.092112"}
+{"level": "info", "fmt": "episode: %s", "args": [8.503659082553982], "caller": "main.py:149", "time": "2021-02-05T11:25:10.055582"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23239833116531372, 0.24972394108772278, 100, 0.413342157560976], "caller": "main.py:200", "time": "2021-02-05T11:25:12.090043"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23339633643627167, 0.22776781022548676, 100, 0.41854795332537853], "caller": "main.py:200", "time": "2021-02-05T11:26:35.721152"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2340412735939026, 0.23336540162563324, 100, 0.4145519020738898], "caller": "main.py:200", "time": "2021-02-05T11:27:59.796825"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23419791460037231, 0.22607091069221497, 100, 0.40377140219942237], "caller": "main.py:200", "time": "2021-02-05T11:29:24.407799"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23655575513839722, 0.23231931030750275, 100, 0.41740736361144226], "caller": "main.py:200", "time": "2021-02-05T11:30:48.524239"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23474888503551483, 0.20632554590702057, 100, 0.4167611580670352], "caller": "main.py:200", "time": "2021-02-05T11:32:13.449658"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23529760539531708, 0.245508074760437, 100, 0.42162671293572357], "caller": "main.py:200", "time": "2021-02-05T11:33:37.975492"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23239010572433472, 0.21756422519683838, 100, 0.420489930468472], "caller": "main.py:200", "time": "2021-02-05T11:35:02.272829"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2352842092514038, 0.21713164448738098, 100, 0.43001732404700227], "caller": "main.py:200", "time": "2021-02-05T11:36:26.765907"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2345941960811615, 0.245948925614357, 100, 0.4147019585552362], "caller": "main.py:200", "time": "2021-02-05T11:37:50.822348"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2350664585828781, 0.24084067344665527, 100, 0.4213339811142158], "caller": "main.py:200", "time": "2021-02-05T11:39:15.201492"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23126445710659027, 0.23010146617889404, 100, 0.4159559982599105], "caller": "main.py:200", "time": "2021-02-05T11:40:39.984054"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23539108037948608, 0.25763484835624695, 100, 0.42377742286259057], "caller": "main.py:200", "time": "2021-02-05T11:42:04.405351"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23336774110794067, 0.23329317569732666, 100, 0.4098870002813532], "caller": "main.py:200", "time": "2021-02-05T11:43:28.652708"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2304171323776245, 0.2293580174446106, 100, 0.41469353731299996], "caller": "main.py:200", "time": "2021-02-05T11:44:52.872375"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23218156397342682, 0.25721579790115356, 100, 0.427809523301259], "caller": "main.py:200", "time": "2021-02-05T11:46:17.194362"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23411718010902405, 0.23872634768486023, 100, 0.4327312305136918], "caller": "main.py:200", "time": "2021-02-05T11:47:41.558991"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23090137541294098, 0.23732367157936096, 100, 0.4145799055280954], "caller": "main.py:200", "time": "2021-02-05T11:49:06.050880"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2310263067483902, 0.24542877078056335, 100, 0.4092729749866339], "caller": "main.py:200", "time": "2021-02-05T11:50:30.758690"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23053394258022308, 0.24504883587360382, 100, 0.4130575689260366], "caller": "main.py:200", "time": "2021-02-05T11:51:55.582638"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [29], "caller": "main.py:129", "time": "2021-02-05T11:53:17.720796"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T11:53:26.711047"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 17.67494603701322, 0.8739401003799816], "caller": "main.py:37", "time": "2021-02-05T11:53:30.741301"}
+{"level": "info", "fmt": "episode: %s", "args": [8.661982128020187], "caller": "main.py:149", "time": "2021-02-05T11:53:40.996868"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2381761521100998, 0.23598472774028778, 100, 0.4222311517174014], "caller": "main.py:200", "time": "2021-02-05T11:53:43.070720"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23682613670825958, 0.2304438054561615, 100, 0.421128698843328], "caller": "main.py:200", "time": "2021-02-05T11:55:06.805024"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2360369861125946, 0.2586919665336609, 100, 0.42265974815433593], "caller": "main.py:200", "time": "2021-02-05T11:56:31.498841"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23618778586387634, 0.24411262571811676, 100, 0.41642680342801525], "caller": "main.py:200", "time": "2021-02-05T11:57:55.772239"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23721590638160706, 0.24810925126075745, 100, 0.4376433294573877], "caller": "main.py:200", "time": "2021-02-05T11:59:20.268434"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23670722544193268, 0.23183277249336243, 100, 0.4099792655892473], "caller": "main.py:200", "time": "2021-02-05T12:00:44.667506"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2345290333032608, 0.23624348640441895, 100, 0.41669586743717796], "caller": "main.py:200", "time": "2021-02-05T12:02:09.111457"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23672974109649658, 0.2091682404279709, 100, 0.4167809795916532], "caller": "main.py:200", "time": "2021-02-05T12:03:33.518430"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23708535730838776, 0.24444802105426788, 100, 0.4118888464158157], "caller": "main.py:200", "time": "2021-02-05T12:04:58.015219"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2346968650817871, 0.23961228132247925, 100, 0.41664938386088485], "caller": "main.py:200", "time": "2021-02-05T12:06:22.799334"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23421983420848846, 0.24035978317260742, 100, 0.42020185824006906], "caller": "main.py:200", "time": "2021-02-05T12:07:47.262738"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2352622002363205, 0.22993043065071106, 100, 0.4154114821137819], "caller": "main.py:200", "time": "2021-02-05T12:09:11.627562"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2370234876871109, 0.24542613327503204, 100, 0.4202005496200932], "caller": "main.py:200", "time": "2021-02-05T12:10:35.672454"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23560455441474915, 0.24280783534049988, 100, 0.41709310438038316], "caller": "main.py:200", "time": "2021-02-05T12:12:00.553355"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2339141070842743, 0.251403272151947, 100, 0.4289647334385107], "caller": "main.py:200", "time": "2021-02-05T12:13:25.078846"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23480769991874695, 0.2288515567779541, 100, 0.419814060884456], "caller": "main.py:200", "time": "2021-02-05T12:14:49.662666"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23552098870277405, 0.23452982306480408, 100, 0.41811583514800776], "caller": "main.py:200", "time": "2021-02-05T12:16:13.852735"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2345278412103653, 0.20782023668289185, 100, 0.4268315252234874], "caller": "main.py:200", "time": "2021-02-05T12:17:38.201763"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2350294142961502, 0.21054431796073914, 100, 0.4229721888476831], "caller": "main.py:200", "time": "2021-02-05T12:19:02.867732"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23374561965465546, 0.24860715866088867, 100, 0.41840132917750117], "caller": "main.py:200", "time": "2021-02-05T12:20:26.821685"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [30], "caller": "main.py:129", "time": "2021-02-05T12:21:48.854460"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T12:21:57.912694"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 14.12015962914338, 0.8329277295227031], "caller": "main.py:37", "time": "2021-02-05T12:22:01.899295"}
+{"level": "info", "fmt": "episode: %s", "args": [6.495148597439268], "caller": "main.py:149", "time": "2021-02-05T12:22:12.159595"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.24087347090244293, 0.23570066690444946, 100, 0.44291278894286845], "caller": "main.py:200", "time": "2021-02-05T12:22:14.254656"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23839399218559265, 0.24759666621685028, 100, 0.4167204796851194], "caller": "main.py:200", "time": "2021-02-05T12:23:38.820248"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.24042661488056183, 0.2576863765716553, 100, 0.42316026206553314], "caller": "main.py:200", "time": "2021-02-05T12:25:02.760070"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.24062447249889374, 0.2318393737077713, 100, 0.42783335860265587], "caller": "main.py:200", "time": "2021-02-05T12:26:27.160149"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23700514435768127, 0.23862163722515106, 100, 0.4222817128474432], "caller": "main.py:200", "time": "2021-02-05T12:27:51.348986"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23731762170791626, 0.24572665989398956, 100, 0.425904621439305], "caller": "main.py:200", "time": "2021-02-05T12:29:15.909664"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2386789470911026, 0.2255786955356598, 100, 0.41363421495592817], "caller": "main.py:200", "time": "2021-02-05T12:30:40.809362"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23940858244895935, 0.23716990649700165, 100, 0.4196282228407849], "caller": "main.py:200", "time": "2021-02-05T12:32:04.947898"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23519867658615112, 0.2258324921131134, 100, 0.42837205077894397], "caller": "main.py:200", "time": "2021-02-05T12:33:29.345358"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23893900215625763, 0.24659866094589233, 100, 0.41242582525658994], "caller": "main.py:200", "time": "2021-02-05T12:34:53.645737"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2348192036151886, 0.22183822095394135, 100, 0.4172090580642247], "caller": "main.py:200", "time": "2021-02-05T12:36:18.171291"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23468193411827087, 0.25687962770462036, 100, 0.41769120450611297], "caller": "main.py:200", "time": "2021-02-05T12:37:42.772134"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23422767221927643, 0.2751650810241699, 100, 0.41792973603599254], "caller": "main.py:200", "time": "2021-02-05T12:39:07.228924"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23627272248268127, 0.2229863703250885, 100, 0.40623720463028234], "caller": "main.py:200", "time": "2021-02-05T12:40:31.746330"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23384620249271393, 0.24161316454410553, 100, 0.4131784599606277], "caller": "main.py:200", "time": "2021-02-05T12:41:56.569131"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2361495941877365, 0.24747774004936218, 100, 0.41806205655924134], "caller": "main.py:200", "time": "2021-02-05T12:43:21.470662"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23527920246124268, 0.23463165760040283, 100, 0.41901475484949413], "caller": "main.py:200", "time": "2021-02-05T12:44:45.761954"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23700012266635895, 0.2550811171531677, 100, 0.4318387049004731], "caller": "main.py:200", "time": "2021-02-05T12:46:10.188695"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2351173758506775, 0.222858265042305, 100, 0.43174424584362825], "caller": "main.py:200", "time": "2021-02-05T12:47:34.389529"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2320203185081482, 0.2480468600988388, 100, 0.44091161425472936], "caller": "main.py:200", "time": "2021-02-05T12:48:58.547547"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [31], "caller": "main.py:129", "time": "2021-02-05T12:50:21.122723"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T12:50:30.384272"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 16.697821233148765, 0.30912444907498643], "caller": "main.py:37", "time": "2021-02-05T12:50:34.394165"}
+{"level": "info", "fmt": "episode: %s", "args": [9.169208715600298], "caller": "main.py:149", "time": "2021-02-05T12:50:44.747463"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2377040833234787, 0.22912120819091797, 100, 0.4209805997785158], "caller": "main.py:200", "time": "2021-02-05T12:50:46.800747"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23814602196216583, 0.24404509365558624, 100, 0.42376475598150737], "caller": "main.py:200", "time": "2021-02-05T12:52:11.288214"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2378983497619629, 0.2189067006111145, 100, 0.428547434763969], "caller": "main.py:200", "time": "2021-02-05T12:53:35.000764"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2393874228000641, 0.2398446500301361, 100, 0.4260482903888947], "caller": "main.py:200", "time": "2021-02-05T12:54:59.082277"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23936985433101654, 0.2397918552160263, 100, 0.43385960634184506], "caller": "main.py:200", "time": "2021-02-05T12:56:23.021147"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23854666948318481, 0.23105022311210632, 100, 0.4286573198009034], "caller": "main.py:200", "time": "2021-02-05T12:57:47.320796"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.237352192401886, 0.2207583636045456, 100, 0.4213483792122063], "caller": "main.py:200", "time": "2021-02-05T12:59:11.400784"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23783451318740845, 0.23574337363243103, 100, 0.4272542154361717], "caller": "main.py:200", "time": "2021-02-05T13:00:35.544690"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23485563695430756, 0.23206695914268494, 100, 0.42615252460748165], "caller": "main.py:200", "time": "2021-02-05T13:01:59.901128"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23651722073554993, 0.24865156412124634, 100, 0.41357602413333266], "caller": "main.py:200", "time": "2021-02-05T13:03:24.218038"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2384539395570755, 0.23909595608711243, 100, 0.42830428685435207], "caller": "main.py:200", "time": "2021-02-05T13:04:48.461475"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23857122659683228, 0.24559161067008972, 100, 0.42500417943519125], "caller": "main.py:200", "time": "2021-02-05T13:06:12.631152"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23505067825317383, 0.24092169106006622, 100, 0.41728905673622524], "caller": "main.py:200", "time": "2021-02-05T13:07:37.234523"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2341098189353943, 0.22506234049797058, 100, 0.42034740354387606], "caller": "main.py:200", "time": "2021-02-05T13:09:02.040913"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23649783432483673, 0.24484096467494965, 100, 0.4175079369127475], "caller": "main.py:200", "time": "2021-02-05T13:10:26.993488"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23523801565170288, 0.23190897703170776, 100, 0.43320647563178544], "caller": "main.py:200", "time": "2021-02-05T13:11:51.953413"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23464426398277283, 0.2470315843820572, 100, 0.4191250803647533], "caller": "main.py:200", "time": "2021-02-05T13:13:15.997424"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23636837303638458, 0.21350640058517456, 100, 0.41733471598028116], "caller": "main.py:200", "time": "2021-02-05T13:14:40.140620"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23347193002700806, 0.21690112352371216, 100, 0.42662587817217357], "caller": "main.py:200", "time": "2021-02-05T13:16:04.355843"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2360200732946396, 0.21441778540611267, 100, 0.4228243621760976], "caller": "main.py:200", "time": "2021-02-05T13:17:29.395140"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [32], "caller": "main.py:129", "time": "2021-02-05T13:18:51.662160"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.125, 0.33071891388307384], "caller": "main.py:37", "time": "2021-02-05T13:19:01.013242"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 17.099058986130874, 1.7798287210530355], "caller": "main.py:37", "time": "2021-02-05T13:19:05.053803"}
+{"level": "info", "fmt": "episode: %s", "args": [8.565944982148569], "caller": "main.py:149", "time": "2021-02-05T13:19:15.315175"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23948675394058228, 0.23987612128257751, 100, 0.42646844978836207], "caller": "main.py:200", "time": "2021-02-05T13:19:17.365887"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.24067042768001556, 0.25892701745033264, 100, 0.44055053557520385], "caller": "main.py:200", "time": "2021-02-05T13:20:41.664596"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23807695508003235, 0.22899705171585083, 100, 0.42106666977963253], "caller": "main.py:200", "time": "2021-02-05T13:22:05.537730"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2392447143793106, 0.24144624173641205, 100, 0.4186602171836856], "caller": "main.py:200", "time": "2021-02-05T13:23:29.904634"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23904016613960266, 0.21896162629127502, 100, 0.42560269142901647], "caller": "main.py:200", "time": "2021-02-05T13:24:54.663434"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23877044022083282, 0.2499300241470337, 100, 0.4275844841767927], "caller": "main.py:200", "time": "2021-02-05T13:26:19.066096"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23693706095218658, 0.24342837929725647, 100, 0.4273141294290475], "caller": "main.py:200", "time": "2021-02-05T13:27:43.790929"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2399091273546219, 0.25587087869644165, 100, 0.4236603754335175], "caller": "main.py:200", "time": "2021-02-05T13:29:08.237148"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23627637326717377, 0.25549060106277466, 100, 0.42418322558929555], "caller": "main.py:200", "time": "2021-02-05T13:30:33.224667"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23902390897274017, 0.20502103865146637, 100, 0.42355473961170875], "caller": "main.py:200", "time": "2021-02-05T13:31:57.809784"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2399158924818039, 0.2382073551416397, 100, 0.4336112463040663], "caller": "main.py:200", "time": "2021-02-05T13:33:22.194398"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.240621417760849, 0.23510566353797913, 100, 0.42685991253160527], "caller": "main.py:200", "time": "2021-02-05T13:34:46.854647"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23872441053390503, 0.2289065271615982, 100, 0.4315379246637821], "caller": "main.py:200", "time": "2021-02-05T13:36:11.415894"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23564963042736053, 0.23947851359844208, 100, 0.4248603757791044], "caller": "main.py:200", "time": "2021-02-05T13:37:35.841108"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23709522187709808, 0.25841057300567627, 100, 0.42342515255746904], "caller": "main.py:200", "time": "2021-02-05T13:39:00.621741"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2372925579547882, 0.2379949986934662, 100, 0.43167001600717175], "caller": "main.py:200", "time": "2021-02-05T13:40:25.108156"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23735913634300232, 0.25828880071640015, 100, 0.4309478002355689], "caller": "main.py:200", "time": "2021-02-05T13:41:49.368863"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2367773950099945, 0.21956667304039001, 100, 0.4325681665107174], "caller": "main.py:200", "time": "2021-02-05T13:43:13.721830"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23755569756031036, 0.2184622883796692, 100, 0.4298303043412486], "caller": "main.py:200", "time": "2021-02-05T13:44:38.381074"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23451197147369385, 0.2360258847475052, 100, 0.4252517526916591], "caller": "main.py:200", "time": "2021-02-05T13:46:03.151867"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [33], "caller": "main.py:129", "time": "2021-02-05T13:47:25.842634"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T13:47:34.810358"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 15.886760522581632, 1.1767567972316233], "caller": "main.py:37", "time": "2021-02-05T13:47:38.766004"}
+{"level": "info", "fmt": "episode: %s", "args": [7.719284116312], "caller": "main.py:149", "time": "2021-02-05T13:47:48.940601"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23749251663684845, 0.26370951533317566, 100, 0.42872104819302115], "caller": "main.py:200", "time": "2021-02-05T13:47:51.007474"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23904304206371307, 0.257124125957489, 100, 0.4240120741300956], "caller": "main.py:200", "time": "2021-02-05T13:49:15.180386"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23855793476104736, 0.24073252081871033, 100, 0.42927595104454724], "caller": "main.py:200", "time": "2021-02-05T13:50:39.224698"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23926374316215515, 0.23413440585136414, 100, 0.4359536328475828], "caller": "main.py:200", "time": "2021-02-05T13:52:03.074670"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23968790471553802, 0.24098461866378784, 100, 0.44108751113772265], "caller": "main.py:200", "time": "2021-02-05T13:53:27.752219"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2387489676475525, 0.25625818967819214, 100, 0.4318105727002348], "caller": "main.py:200", "time": "2021-02-05T13:54:52.394826"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2387760728597641, 0.22157692909240723, 100, 0.42476598258539205], "caller": "main.py:200", "time": "2021-02-05T13:56:16.923092"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.238718643784523, 0.23324137926101685, 100, 0.44272000487191915], "caller": "main.py:200", "time": "2021-02-05T13:57:41.327498"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2367924153804779, 0.22596576809883118, 100, 0.4319116028711422], "caller": "main.py:200", "time": "2021-02-05T13:59:05.813711"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23771192133426666, 0.21937647461891174, 100, 0.42305089290679926], "caller": "main.py:200", "time": "2021-02-05T14:00:29.955446"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23766852915287018, 0.23832356929779053, 100, 0.43651761348650725], "caller": "main.py:200", "time": "2021-02-05T14:01:54.484419"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23686212301254272, 0.2618093490600586, 100, 0.4242093851738152], "caller": "main.py:200", "time": "2021-02-05T14:03:18.532927"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23533739149570465, 0.23121842741966248, 100, 0.4188086410938118], "caller": "main.py:200", "time": "2021-02-05T14:04:42.818848"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23593096435070038, 0.22437205910682678, 100, 0.4359389477532277], "caller": "main.py:200", "time": "2021-02-05T14:06:07.395909"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23821483552455902, 0.2103254497051239, 100, 0.4307230090664248], "caller": "main.py:200", "time": "2021-02-05T14:07:31.775786"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23536552488803864, 0.23153045773506165, 100, 0.4308979728387042], "caller": "main.py:200", "time": "2021-02-05T14:08:56.264529"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23494167625904083, 0.22412732243537903, 100, 0.4251875417999419], "caller": "main.py:200", "time": "2021-02-05T14:10:20.958052"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2359275370836258, 0.2462773323059082, 100, 0.4354424178164869], "caller": "main.py:200", "time": "2021-02-05T14:11:45.518425"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23791499435901642, 0.22724346816539764, 100, 0.42970363225216135], "caller": "main.py:200", "time": "2021-02-05T14:13:10.680238"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2339869886636734, 0.2379150092601776, 100, 0.43236031140533804], "caller": "main.py:200", "time": "2021-02-05T14:14:34.812700"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [34], "caller": "main.py:129", "time": "2021-02-05T14:15:57.364073"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T14:16:06.372003"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 16.192045044128847, 0.7575395368910914], "caller": "main.py:37", "time": "2021-02-05T14:16:10.387689"}
+{"level": "info", "fmt": "episode: %s", "args": [6.395255066637142], "caller": "main.py:149", "time": "2021-02-05T14:16:20.470112"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23781335353851318, 0.23270709812641144, 100, 0.4347409334344045], "caller": "main.py:200", "time": "2021-02-05T14:16:22.515395"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23825393617153168, 0.23006302118301392, 100, 0.4344273003501387], "caller": "main.py:200", "time": "2021-02-05T14:17:46.942636"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.237502321600914, 0.2277696579694748, 100, 0.43311202581366054], "caller": "main.py:200", "time": "2021-02-05T14:19:11.514237"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.2376207411289215, 0.2404472678899765, 100, 0.4429929229174126], "caller": "main.py:200", "time": "2021-02-05T14:20:35.241102"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23481245338916779, 0.2037070393562317, 100, 0.43196579213591113], "caller": "main.py:200", "time": "2021-02-05T14:21:59.265012"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23767055571079254, 0.22123804688453674, 100, 0.4385248896041082], "caller": "main.py:200", "time": "2021-02-05T14:23:23.399045"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23719264566898346, 0.24608291685581207, 100, 0.4276111384798639], "caller": "main.py:200", "time": "2021-02-05T14:24:47.631237"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23622705042362213, 0.2545086741447449, 100, 0.42928492362974763], "caller": "main.py:200", "time": "2021-02-05T14:26:12.198994"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23601047694683075, 0.2600865364074707, 100, 0.42611467118770474], "caller": "main.py:200", "time": "2021-02-05T14:27:36.854485"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23404468595981598, 0.21421319246292114, 100, 0.43548491533216105], "caller": "main.py:200", "time": "2021-02-05T14:29:01.465800"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23602552711963654, 0.24709810316562653, 100, 0.4299552431272659], "caller": "main.py:200", "time": "2021-02-05T14:30:26.119870"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23526757955551147, 0.2240152359008789, 100, 0.4319307842418755], "caller": "main.py:200", "time": "2021-02-05T14:31:51.010692"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2372729331254959, 0.22626934945583344, 100, 0.4417975943838827], "caller": "main.py:200", "time": "2021-02-05T14:33:15.310482"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23424343764781952, 0.22903117537498474, 100, 0.42993096198312614], "caller": "main.py:200", "time": "2021-02-05T14:34:40.025759"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2365448772907257, 0.2350548505783081, 100, 0.4305069025920468], "caller": "main.py:200", "time": "2021-02-05T14:36:04.269262"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.236988827586174, 0.2287195324897766, 100, 0.4285960372853837], "caller": "main.py:200", "time": "2021-02-05T14:37:28.218347"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23330436646938324, 0.24983106553554535, 100, 0.43563556458169966], "caller": "main.py:200", "time": "2021-02-05T14:38:52.586121"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23299342393875122, 0.23864655196666718, 100, 0.4219574874217554], "caller": "main.py:200", "time": "2021-02-05T14:40:16.932503"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2366781085729599, 0.2608482837677002, 100, 0.44517964418250994], "caller": "main.py:200", "time": "2021-02-05T14:41:41.146047"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2360677868127823, 0.22395992279052734, 100, 0.44664738322798314], "caller": "main.py:200", "time": "2021-02-05T14:43:05.510929"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [35], "caller": "main.py:129", "time": "2021-02-05T14:44:27.543353"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T14:44:36.949433"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 16.07003529814674, 1.4102033693818403], "caller": "main.py:37", "time": "2021-02-05T14:44:41.190623"}
+{"level": "info", "fmt": "episode: %s", "args": [7.796053246965693], "caller": "main.py:149", "time": "2021-02-05T14:44:51.761336"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2365526705980301, 0.24877668917179108, 100, 0.4317367097303669], "caller": "main.py:200", "time": "2021-02-05T14:44:53.875604"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23518867790699005, 0.2669808566570282, 100, 0.4393281870641071], "caller": "main.py:200", "time": "2021-02-05T14:46:18.325217"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.2349936068058014, 0.2309490442276001, 100, 0.4435668202791106], "caller": "main.py:200", "time": "2021-02-05T14:47:42.542378"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23637159168720245, 0.24333137273788452, 100, 0.43340994174914715], "caller": "main.py:200", "time": "2021-02-05T14:49:07.331679"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23605068027973175, 0.2603378891944885, 100, 0.45242935474162993], "caller": "main.py:200", "time": "2021-02-05T14:50:31.092440"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23492220044136047, 0.23333224654197693, 100, 0.43906125630518067], "caller": "main.py:200", "time": "2021-02-05T14:51:55.813310"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23721659183502197, 0.22387422621250153, 100, 0.44958124066863825], "caller": "main.py:200", "time": "2021-02-05T14:53:20.595855"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23519177734851837, 0.26265671849250793, 100, 0.4389218303122289], "caller": "main.py:200", "time": "2021-02-05T14:54:44.889657"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2324686199426651, 0.24487073719501495, 100, 0.43499790217102735], "caller": "main.py:200", "time": "2021-02-05T14:56:09.270113"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2343348115682602, 0.23988091945648193, 100, 0.43775589580926705], "caller": "main.py:200", "time": "2021-02-05T14:57:33.605840"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23576343059539795, 0.21531075239181519, 100, 0.4470671706944672], "caller": "main.py:200", "time": "2021-02-05T14:58:58.071053"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.235397607088089, 0.23561596870422363, 100, 0.44364880229566583], "caller": "main.py:200", "time": "2021-02-05T15:00:22.451206"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23462079465389252, 0.22523468732833862, 100, 0.4437022062692399], "caller": "main.py:200", "time": "2021-02-05T15:01:46.922047"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23248769342899323, 0.23762226104736328, 100, 0.4392147603191805], "caller": "main.py:200", "time": "2021-02-05T15:03:11.671476"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23266635835170746, 0.23227518796920776, 100, 0.45691894288262236], "caller": "main.py:200", "time": "2021-02-05T15:04:36.522940"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23156234622001648, 0.26443108916282654, 100, 0.44116029109119514], "caller": "main.py:200", "time": "2021-02-05T15:06:01.182974"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23231396079063416, 0.23749807476997375, 100, 0.432681173936968], "caller": "main.py:200", "time": "2021-02-05T15:07:25.357948"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2337011843919754, 0.2353040724992752, 100, 0.44144203807469506], "caller": "main.py:200", "time": "2021-02-05T15:08:50.514927"}
+{"level": "info", "fmt": "Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f", "args": [0.01744777150452137, 0.01], "caller": "slbo/algos/TRPO.py:155", "time": "2021-02-05T15:09:04.588706"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23156818747520447, 0.23994848132133484, 100, 0.4446518089922666], "caller": "main.py:200", "time": "2021-02-05T15:10:15.294081"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2324523776769638, 0.22867780923843384, 100, 0.43622030224014186], "caller": "main.py:200", "time": "2021-02-05T15:11:39.888415"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [36], "caller": "main.py:129", "time": "2021-02-05T15:13:02.186232"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T15:13:11.354617"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 11, 47.7793342786651, 43.29094637235998], "caller": "main.py:37", "time": "2021-02-05T15:13:15.375009"}
+{"level": "info", "fmt": "episode: %s", "args": [6.381818275124899], "caller": "main.py:149", "time": "2021-02-05T15:13:25.645080"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23447206616401672, 0.25502774119377136, 100, 0.4389031278648164], "caller": "main.py:200", "time": "2021-02-05T15:13:27.697276"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23463214933872223, 0.22088029980659485, 100, 0.4422418269072796], "caller": "main.py:200", "time": "2021-02-05T15:14:52.439801"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23609282076358795, 0.23534086346626282, 100, 0.44238919104682173], "caller": "main.py:200", "time": "2021-02-05T15:16:17.168400"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23479114472866058, 0.22984054684638977, 100, 0.4486183639912214], "caller": "main.py:200", "time": "2021-02-05T15:17:42.303831"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2357025146484375, 0.22867605090141296, 100, 0.4437407838854508], "caller": "main.py:200", "time": "2021-02-05T15:19:06.538565"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23482657968997955, 0.23247504234313965, 100, 0.44627387489386067], "caller": "main.py:200", "time": "2021-02-05T15:20:31.311711"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2337576448917389, 0.2553703486919403, 100, 0.44064461614255435], "caller": "main.py:200", "time": "2021-02-05T15:21:56.533278"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2337253987789154, 0.24719248712062836, 100, 0.44526715959626234], "caller": "main.py:200", "time": "2021-02-05T15:23:21.277578"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2331402599811554, 0.24935288727283478, 100, 0.4487931111006171], "caller": "main.py:200", "time": "2021-02-05T15:24:46.153578"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2303377389907837, 0.24499808251857758, 100, 0.44641821290558786], "caller": "main.py:200", "time": "2021-02-05T15:26:11.362364"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23437154293060303, 0.2196826934814453, 100, 0.4396262818135279], "caller": "main.py:200", "time": "2021-02-05T15:27:36.429765"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.234903484582901, 0.21552646160125732, 100, 0.4560560094854155], "caller": "main.py:200", "time": "2021-02-05T15:29:00.747854"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2311568260192871, 0.2392803132534027, 100, 0.44523297007399937], "caller": "main.py:200", "time": "2021-02-05T15:30:25.021057"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2343774288892746, 0.23209825158119202, 100, 0.4414717241680225], "caller": "main.py:200", "time": "2021-02-05T15:31:49.992852"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23242951929569244, 0.22001047432422638, 100, 0.44852951045319395], "caller": "main.py:200", "time": "2021-02-05T15:33:14.445809"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23309946060180664, 0.23485936224460602, 100, 0.44256120766744056], "caller": "main.py:200", "time": "2021-02-05T15:34:40.167213"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23405028879642487, 0.2377026379108429, 100, 0.4399806636131904], "caller": "main.py:200", "time": "2021-02-05T15:36:05.087798"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23154667019844055, 0.23546873033046722, 100, 0.45059944625163195], "caller": "main.py:200", "time": "2021-02-05T15:37:30.529509"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23077325522899628, 0.21887142956256866, 100, 0.44336399222947115], "caller": "main.py:200", "time": "2021-02-05T15:38:55.023889"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2315177321434021, 0.21584421396255493, 100, 0.44704461808502577], "caller": "main.py:200", "time": "2021-02-05T15:40:19.411496"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [37], "caller": "main.py:129", "time": "2021-02-05T15:41:41.930513"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T15:41:51.122922"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 9, 35.357172018840316, 39.786886946678024], "caller": "main.py:37", "time": "2021-02-05T15:41:55.320568"}
+{"level": "info", "fmt": "episode: %s", "args": [6.821140146659908], "caller": "main.py:149", "time": "2021-02-05T15:42:05.659104"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23729945719242096, 0.2485862374305725, 100, 0.4377822371879532], "caller": "main.py:200", "time": "2021-02-05T15:42:07.730142"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.24083834886550903, 0.23290413618087769, 100, 0.45639808441604923], "caller": "main.py:200", "time": "2021-02-05T15:43:33.129303"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23600073158740997, 0.2510356307029724, 100, 0.44584267194581034], "caller": "main.py:200", "time": "2021-02-05T15:44:57.496314"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23666322231292725, 0.25105157494544983, 100, 0.4489169474987294], "caller": "main.py:200", "time": "2021-02-05T15:46:22.452411"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23867517709732056, 0.2212589979171753, 100, 0.4496200441864528], "caller": "main.py:200", "time": "2021-02-05T15:47:46.898588"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23725463449954987, 0.2463301718235016, 100, 0.44295683766942273], "caller": "main.py:200", "time": "2021-02-05T15:49:10.702061"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2360650897026062, 0.20841102302074432, 100, 0.45246503885036055], "caller": "main.py:200", "time": "2021-02-05T15:50:35.238066"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2341122180223465, 0.23713630437850952, 100, 0.4481073420791746], "caller": "main.py:200", "time": "2021-02-05T15:52:00.116404"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2355661243200302, 0.23870781064033508, 100, 0.4505618050243904], "caller": "main.py:200", "time": "2021-02-05T15:53:25.209439"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23613151907920837, 0.23676055669784546, 100, 0.45018335099486817], "caller": "main.py:200", "time": "2021-02-05T15:54:50.299281"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23620253801345825, 0.22988729178905487, 100, 0.4595228579180866], "caller": "main.py:200", "time": "2021-02-05T15:56:14.960139"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2347993403673172, 0.25713762640953064, 100, 0.4628220027222344], "caller": "main.py:200", "time": "2021-02-05T15:57:39.978815"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23726676404476166, 0.23476187884807587, 100, 0.45196552421103287], "caller": "main.py:200", "time": "2021-02-05T15:59:04.716921"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23566505312919617, 0.2243373990058899, 100, 0.4432498561892718], "caller": "main.py:200", "time": "2021-02-05T16:00:29.263138"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23520460724830627, 0.23808325827121735, 100, 0.45351276899809434], "caller": "main.py:200", "time": "2021-02-05T16:01:53.688232"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2337217777967453, 0.2389431893825531, 100, 0.4425728675280503], "caller": "main.py:200", "time": "2021-02-05T16:03:18.380476"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23605051636695862, 0.23428596556186676, 100, 0.46343779171839244], "caller": "main.py:200", "time": "2021-02-05T16:04:42.779370"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23371444642543793, 0.2260482907295227, 100, 0.4534006732248855], "caller": "main.py:200", "time": "2021-02-05T16:06:07.403679"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23334650695323944, 0.2171487808227539, 100, 0.4525491636538952], "caller": "main.py:200", "time": "2021-02-05T16:07:31.570184"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23317846655845642, 0.22572065889835358, 100, 0.45201565052519105], "caller": "main.py:200", "time": "2021-02-05T16:08:56.246592"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [38], "caller": "main.py:129", "time": "2021-02-05T16:10:18.931983"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T16:10:28.124588"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 12.66067063209116, 1.311320977982055], "caller": "main.py:37", "time": "2021-02-05T16:10:32.196172"}
+{"level": "info", "fmt": "episode: %s", "args": [7.445363677587325], "caller": "main.py:149", "time": "2021-02-05T16:10:42.255784"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23890022933483124, 0.23938998579978943, 100, 0.4657935887109356], "caller": "main.py:200", "time": "2021-02-05T16:10:44.319486"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23700255155563354, 0.23058485984802246, 100, 0.45863689507428024], "caller": "main.py:200", "time": "2021-02-05T16:12:08.140097"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23692908883094788, 0.22706934809684753, 100, 0.44248439277754037], "caller": "main.py:200", "time": "2021-02-05T16:13:32.681377"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23685938119888306, 0.2705398201942444, 100, 0.45962886507683226], "caller": "main.py:200", "time": "2021-02-05T16:14:57.272046"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2374970018863678, 0.2398618459701538, 100, 0.4673483660349798], "caller": "main.py:200", "time": "2021-02-05T16:16:21.550742"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23822666704654694, 0.2499352991580963, 100, 0.4634103124870376], "caller": "main.py:200", "time": "2021-02-05T16:17:46.033078"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.24027858674526215, 0.22413185238838196, 100, 0.4552451788923875], "caller": "main.py:200", "time": "2021-02-05T16:19:09.571044"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23918001353740692, 0.24901127815246582, 100, 0.4630592521562049], "caller": "main.py:200", "time": "2021-02-05T16:20:33.680601"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.2364547848701477, 0.21402013301849365, 100, 0.46073687191170526], "caller": "main.py:200", "time": "2021-02-05T16:21:58.335095"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2374955117702484, 0.22699925303459167, 100, 0.4589631086128943], "caller": "main.py:200", "time": "2021-02-05T16:23:22.497531"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23571506142616272, 0.23066388070583344, 100, 0.4506577696945621], "caller": "main.py:200", "time": "2021-02-05T16:24:46.770558"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23716305196285248, 0.24088333547115326, 100, 0.44996294464141656], "caller": "main.py:200", "time": "2021-02-05T16:26:10.887105"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23452675342559814, 0.24013462662696838, 100, 0.4544199585460192], "caller": "main.py:200", "time": "2021-02-05T16:27:35.193150"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23638513684272766, 0.220822274684906, 100, 0.4642534496916004], "caller": "main.py:200", "time": "2021-02-05T16:28:59.042992"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2367001324892044, 0.23808611929416656, 100, 0.4628039633764949], "caller": "main.py:200", "time": "2021-02-05T16:30:23.114480"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2377249300479889, 0.2499973475933075, 100, 0.46370454545152384], "caller": "main.py:200", "time": "2021-02-05T16:31:47.282548"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23765848577022552, 0.24133643507957458, 100, 0.46034740204976], "caller": "main.py:200", "time": "2021-02-05T16:33:11.859854"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23551064729690552, 0.24532556533813477, 100, 0.4618444472089502], "caller": "main.py:200", "time": "2021-02-05T16:34:36.528212"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23487350344657898, 0.22199775278568268, 100, 0.4485794609336501], "caller": "main.py:200", "time": "2021-02-05T16:36:00.873248"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2369215190410614, 0.21715672314167023, 100, 0.4638096850646649], "caller": "main.py:200", "time": "2021-02-05T16:37:25.416794"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [39], "caller": "main.py:129", "time": "2021-02-05T16:38:47.344284"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 9, 0.1111111111111111, 0.31426968052735443], "caller": "main.py:37", "time": "2021-02-05T16:38:56.326384"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 12, 46.60995600016309, 41.61974942089652], "caller": "main.py:37", "time": "2021-02-05T16:39:00.390423"}
+{"level": "info", "fmt": "episode: %s", "args": [7.831795865859383], "caller": "main.py:149", "time": "2021-02-05T16:39:10.613499"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23940563201904297, 0.24000173807144165, 100, 0.4556088554403553], "caller": "main.py:200", "time": "2021-02-05T16:39:12.705442"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.24010291695594788, 0.22536233067512512, 100, 0.46268804247520795], "caller": "main.py:200", "time": "2021-02-05T16:40:37.202070"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23819111287593842, 0.2453872263431549, 100, 0.4549729492805411], "caller": "main.py:200", "time": "2021-02-05T16:42:02.127035"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23886407911777496, 0.24499860405921936, 100, 0.46671989972692574], "caller": "main.py:200", "time": "2021-02-05T16:43:26.619434"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2359938770532608, 0.24747754633426666, 100, 0.473377745104182], "caller": "main.py:200", "time": "2021-02-05T16:44:51.346530"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23762211203575134, 0.26559343934059143, 100, 0.4572366266985467], "caller": "main.py:200", "time": "2021-02-05T16:46:15.551122"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2367493063211441, 0.24643544852733612, 100, 0.46333090092078805], "caller": "main.py:200", "time": "2021-02-05T16:47:39.493770"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23652799427509308, 0.2459586262702942, 100, 0.46399085397848805], "caller": "main.py:200", "time": "2021-02-05T16:49:04.354159"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23747214674949646, 0.22770676016807556, 100, 0.4616109358802763], "caller": "main.py:200", "time": "2021-02-05T16:50:28.811933"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23804622888565063, 0.2379261702299118, 100, 0.46881674016794306], "caller": "main.py:200", "time": "2021-02-05T16:51:52.957039"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2366502583026886, 0.2281028777360916, 100, 0.4605443790094456], "caller": "main.py:200", "time": "2021-02-05T16:53:17.335690"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.24046926200389862, 0.22899511456489563, 100, 0.4729809140686077], "caller": "main.py:200", "time": "2021-02-05T16:54:41.778009"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.2344689518213272, 0.24827739596366882, 100, 0.4585713779467729], "caller": "main.py:200", "time": "2021-02-05T16:56:05.940041"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2360691875219345, 0.2483687698841095, 100, 0.4530075516843784], "caller": "main.py:200", "time": "2021-02-05T16:57:30.139462"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2335922122001648, 0.21956270933151245, 100, 0.46115696693815633], "caller": "main.py:200", "time": "2021-02-05T16:58:54.753625"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23474366962909698, 0.2425473928451538, 100, 0.46162178899201917], "caller": "main.py:200", "time": "2021-02-05T17:00:18.969066"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23636573553085327, 0.24002552032470703, 100, 0.47494047249574556], "caller": "main.py:200", "time": "2021-02-05T17:01:43.030969"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2347142994403839, 0.22413673996925354, 100, 0.4643352904451475], "caller": "main.py:200", "time": "2021-02-05T17:03:07.415895"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2357916682958603, 0.23858897387981415, 100, 0.4632125779523169], "caller": "main.py:200", "time": "2021-02-05T17:04:32.363631"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23276594281196594, 0.22114039957523346, 100, 0.463919931901793], "caller": "main.py:200", "time": "2021-02-05T17:05:56.460084"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [40], "caller": "main.py:129", "time": "2021-02-05T17:07:18.078165"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T17:07:27.329245"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 15.124017847207309, 0.28528974452359573], "caller": "main.py:37", "time": "2021-02-05T17:07:31.255131"}
+{"level": "info", "fmt": "episode: %s", "args": [9.697748136440897], "caller": "main.py:149", "time": "2021-02-05T17:07:41.385241"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.2365410029888153, 0.23173415660858154, 100, 0.4638031962050121], "caller": "main.py:200", "time": "2021-02-05T17:07:43.438689"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23551255464553833, 0.23486334085464478, 100, 0.44146309907790116], "caller": "main.py:200", "time": "2021-02-05T17:09:07.376226"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23637953400611877, 0.24855856597423553, 100, 0.46553566570395], "caller": "main.py:200", "time": "2021-02-05T17:10:31.772476"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23681625723838806, 0.24269446730613708, 100, 0.4728387307647889], "caller": "main.py:200", "time": "2021-02-05T17:11:55.848486"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23484741151332855, 0.23005159199237823, 100, 0.4580779978174849], "caller": "main.py:200", "time": "2021-02-05T17:13:20.594428"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.2333139032125473, 0.23746713995933533, 100, 0.465999935520779], "caller": "main.py:200", "time": "2021-02-05T17:14:45.282273"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23630741238594055, 0.22891543805599213, 100, 0.46809616769639645], "caller": "main.py:200", "time": "2021-02-05T17:16:09.311914"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23384201526641846, 0.24054472148418427, 100, 0.45685986067256434], "caller": "main.py:200", "time": "2021-02-05T17:17:33.200397"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23456571996212006, 0.2068147361278534, 100, 0.463514721645257], "caller": "main.py:200", "time": "2021-02-05T17:18:57.465298"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23193418979644775, 0.25503289699554443, 100, 0.45326829965350285], "caller": "main.py:200", "time": "2021-02-05T17:20:22.389380"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23169326782226562, 0.23497025668621063, 100, 0.4574880983968057], "caller": "main.py:200", "time": "2021-02-05T17:21:47.383778"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23387950658798218, 0.2411385178565979, 100, 0.4659276970755043], "caller": "main.py:200", "time": "2021-02-05T17:23:12.013347"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23092979192733765, 0.2197592556476593, 100, 0.4552060314557726], "caller": "main.py:200", "time": "2021-02-05T17:24:36.703172"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23433426022529602, 0.2350253462791443, 100, 0.46855393107020243], "caller": "main.py:200", "time": "2021-02-05T17:26:01.177761"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23441879451274872, 0.25800663232803345, 100, 0.47044169331852975], "caller": "main.py:200", "time": "2021-02-05T17:27:25.321513"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23138310015201569, 0.2278004288673401, 100, 0.47250546540065214], "caller": "main.py:200", "time": "2021-02-05T17:28:49.368077"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23155656456947327, 0.250644713640213, 100, 0.4659938376675959], "caller": "main.py:200", "time": "2021-02-05T17:30:13.811586"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2288067489862442, 0.23767447471618652, 100, 0.45564381147835], "caller": "main.py:200", "time": "2021-02-05T17:31:38.263373"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23200878500938416, 0.274647057056427, 100, 0.47133394385631283], "caller": "main.py:200", "time": "2021-02-05T17:33:02.767657"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23171889781951904, 0.24073174595832825, 100, 0.4693618452079619], "caller": "main.py:200", "time": "2021-02-05T17:34:27.165881"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [41], "caller": "main.py:129", "time": "2021-02-05T17:35:49.734219"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T17:35:58.928262"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 14.610597435197008, 0.20168119447474012], "caller": "main.py:37", "time": "2021-02-05T17:36:02.858771"}
+{"level": "info", "fmt": "episode: %s", "args": [8.994818925302278], "caller": "main.py:149", "time": "2021-02-05T17:36:13.044759"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23364654183387756, 0.2309800535440445, 100, 0.4646813427937266], "caller": "main.py:200", "time": "2021-02-05T17:36:15.121470"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23508639633655548, 0.22453373670578003, 100, 0.4582451217908378], "caller": "main.py:200", "time": "2021-02-05T17:37:39.036139"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23268760740756989, 0.2241072654724121, 100, 0.4593551626342136], "caller": "main.py:200", "time": "2021-02-05T17:39:03.300531"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23521193861961365, 0.23345908522605896, 100, 0.46309759284395285], "caller": "main.py:200", "time": "2021-02-05T17:40:27.963135"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2312266230583191, 0.25381773710250854, 100, 0.465148411644674], "caller": "main.py:200", "time": "2021-02-05T17:41:52.264102"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23395231366157532, 0.23021820187568665, 100, 0.45632604296412543], "caller": "main.py:200", "time": "2021-02-05T17:43:16.356661"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23484255373477936, 0.2140742689371109, 100, 0.4673682477602891], "caller": "main.py:200", "time": "2021-02-05T17:44:40.917930"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23192815482616425, 0.21908962726593018, 100, 0.45590506578893175], "caller": "main.py:200", "time": "2021-02-05T17:46:04.774127"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23490121960639954, 0.23499897122383118, 100, 0.4631168198122394], "caller": "main.py:200", "time": "2021-02-05T17:47:29.212155"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23557274043560028, 0.23344430327415466, 100, 0.4626564684047575], "caller": "main.py:200", "time": "2021-02-05T17:48:52.965645"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23555535078048706, 0.23915982246398926, 100, 0.47032128865784], "caller": "main.py:200", "time": "2021-02-05T17:50:16.811453"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23265983164310455, 0.23585115373134613, 100, 0.46894380594570895], "caller": "main.py:200", "time": "2021-02-05T17:51:41.243802"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23307280242443085, 0.2441948503255844, 100, 0.4674197235236049], "caller": "main.py:200", "time": "2021-02-05T17:53:05.724150"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23254962265491486, 0.24308297038078308, 100, 0.46505317862151685], "caller": "main.py:200", "time": "2021-02-05T17:54:29.711804"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2321709394454956, 0.23377324640750885, 100, 0.4609811898604991], "caller": "main.py:200", "time": "2021-02-05T17:55:54.368558"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.2336745709180832, 0.2340405434370041, 100, 0.46743837173079067], "caller": "main.py:200", "time": "2021-02-05T17:57:18.813142"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.2315642535686493, 0.21046295762062073, 100, 0.47278398836891655], "caller": "main.py:200", "time": "2021-02-05T17:58:43.219020"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23300492763519287, 0.22849087417125702, 100, 0.4623856710925249], "caller": "main.py:200", "time": "2021-02-05T18:00:07.351219"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23181341588497162, 0.23458531498908997, 100, 0.4607338165776144], "caller": "main.py:200", "time": "2021-02-05T18:01:31.292111"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.2319052517414093, 0.2307671308517456, 100, 0.4746281381966893], "caller": "main.py:200", "time": "2021-02-05T18:02:55.681504"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [42], "caller": "main.py:129", "time": "2021-02-05T18:04:18.102392"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T18:04:27.421708"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 16.116155259408565, 0.1274442548976258], "caller": "main.py:37", "time": "2021-02-05T18:04:31.405825"}
+{"level": "info", "fmt": "episode: %s", "args": [9.431063333938603], "caller": "main.py:149", "time": "2021-02-05T18:04:41.573099"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23416437208652496, 0.2291576862335205, 100, 0.4671823659072041], "caller": "main.py:200", "time": "2021-02-05T18:04:43.693955"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23463822901248932, 0.23889963328838348, 100, 0.4669704731831881], "caller": "main.py:200", "time": "2021-02-05T18:06:08.001388"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23466309905052185, 0.23367956280708313, 100, 0.4673080107102387], "caller": "main.py:200", "time": "2021-02-05T18:07:32.117974"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23281075060367584, 0.22789326310157776, 100, 0.45964034671955417], "caller": "main.py:200", "time": "2021-02-05T18:08:56.286841"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23283201456069946, 0.22092163562774658, 100, 0.4670510774383795], "caller": "main.py:200", "time": "2021-02-05T18:10:20.986294"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23235748708248138, 0.21843545138835907, 100, 0.47269287998717746], "caller": "main.py:200", "time": "2021-02-05T18:11:45.557433"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23447531461715698, 0.22478915750980377, 100, 0.46682204480271355], "caller": "main.py:200", "time": "2021-02-05T18:13:09.720901"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23181818425655365, 0.2377874255180359, 100, 0.45957107777768186], "caller": "main.py:200", "time": "2021-02-05T18:14:34.215797"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23140497505664825, 0.23559217154979706, 100, 0.4631696584357578], "caller": "main.py:200", "time": "2021-02-05T18:15:58.223265"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2355061173439026, 0.23316287994384766, 100, 0.4896620720135262], "caller": "main.py:200", "time": "2021-02-05T18:17:23.364242"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23194879293441772, 0.21628467738628387, 100, 0.4619773432150442], "caller": "main.py:200", "time": "2021-02-05T18:18:48.082579"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23048175871372223, 0.22827085852622986, 100, 0.4604702636139071], "caller": "main.py:200", "time": "2021-02-05T18:20:12.026566"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23188123106956482, 0.24102386832237244, 100, 0.4682589074501033], "caller": "main.py:200", "time": "2021-02-05T18:21:36.579017"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23481091856956482, 0.2174474000930786, 100, 0.4677760383018972], "caller": "main.py:200", "time": "2021-02-05T18:23:00.862949"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22840501368045807, 0.23471394181251526, 100, 0.47372134770186114], "caller": "main.py:200", "time": "2021-02-05T18:24:25.147086"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23129074275493622, 0.22378644347190857, 100, 0.47538288379922156], "caller": "main.py:200", "time": "2021-02-05T18:25:49.439168"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23150911927223206, 0.23688039183616638, 100, 0.4801199778434332], "caller": "main.py:200", "time": "2021-02-05T18:27:14.084411"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22986909747123718, 0.2253774106502533, 100, 0.4696335845716051], "caller": "main.py:200", "time": "2021-02-05T18:28:38.643773"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22855894267559052, 0.21949489414691925, 100, 0.46261659736775507], "caller": "main.py:200", "time": "2021-02-05T18:30:03.461292"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23176822066307068, 0.22483499348163605, 100, 0.47336183751856276], "caller": "main.py:200", "time": "2021-02-05T18:31:27.648942"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [43], "caller": "main.py:129", "time": "2021-02-05T18:32:49.926868"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T18:32:59.225855"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 10, 32.23234062640965, 37.73759662851746], "caller": "main.py:37", "time": "2021-02-05T18:33:03.469957"}
+{"level": "info", "fmt": "episode: %s", "args": [7.061682403152367], "caller": "main.py:149", "time": "2021-02-05T18:33:13.987653"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23501363396644592, 0.24171724915504456, 100, 0.4791563596842354], "caller": "main.py:200", "time": "2021-02-05T18:33:16.058267"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23186944425106049, 0.2336975634098053, 100, 0.46517771358606586], "caller": "main.py:200", "time": "2021-02-05T18:34:40.441045"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.22913645207881927, 0.20746156573295593, 100, 0.46805175746767164], "caller": "main.py:200", "time": "2021-02-05T18:36:04.779333"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23197656869888306, 0.2278231382369995, 100, 0.47495843346137717], "caller": "main.py:200", "time": "2021-02-05T18:37:29.917690"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23107802867889404, 0.2408502995967865, 100, 0.47539059395804567], "caller": "main.py:200", "time": "2021-02-05T18:38:56.197463"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23261617124080658, 0.2184026539325714, 100, 0.46323307986395507], "caller": "main.py:200", "time": "2021-02-05T18:40:21.464726"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23077671229839325, 0.23495712876319885, 100, 0.4567836156030682], "caller": "main.py:200", "time": "2021-02-05T18:41:46.895969"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2289884388446808, 0.23075240850448608, 100, 0.4672619437704125], "caller": "main.py:200", "time": "2021-02-05T18:43:13.039202"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23088832199573517, 0.22697916626930237, 100, 0.46794995303242864], "caller": "main.py:200", "time": "2021-02-05T18:44:39.566697"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2297772765159607, 0.2300540804862976, 100, 0.4635061275495509], "caller": "main.py:200", "time": "2021-02-05T18:46:05.019663"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23051989078521729, 0.21583767235279083, 100, 0.463618773473631], "caller": "main.py:200", "time": "2021-02-05T18:47:30.697229"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22972959280014038, 0.2265666127204895, 100, 0.4616295203104922], "caller": "main.py:200", "time": "2021-02-05T18:48:57.156782"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22795400023460388, 0.20594000816345215, 100, 0.46383491839300484], "caller": "main.py:200", "time": "2021-02-05T18:50:24.083160"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.228134885430336, 0.2269469052553177, 100, 0.47926149065470297], "caller": "main.py:200", "time": "2021-02-05T18:51:49.467285"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22915731370449066, 0.217448890209198, 100, 0.4680880528220343], "caller": "main.py:200", "time": "2021-02-05T18:53:14.636464"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22820226848125458, 0.2241438627243042, 100, 0.46676346744399677], "caller": "main.py:200", "time": "2021-02-05T18:54:38.888222"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22893396019935608, 0.2208709418773651, 100, 0.469509198618668], "caller": "main.py:200", "time": "2021-02-05T18:56:03.882108"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22945378720760345, 0.2362232804298401, 100, 0.460065672351607], "caller": "main.py:200", "time": "2021-02-05T18:57:27.990035"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2289971113204956, 0.22503507137298584, 100, 0.4686274471410378], "caller": "main.py:200", "time": "2021-02-05T18:58:53.347071"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.22823798656463623, 0.21780389547348022, 100, 0.474462799084424], "caller": "main.py:200", "time": "2021-02-05T19:00:19.684043"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [44], "caller": "main.py:129", "time": "2021-02-05T19:01:42.295972"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T19:01:51.466764"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 14.258784939982974, 0.24517499736323603], "caller": "main.py:37", "time": "2021-02-05T19:01:55.484815"}
+{"level": "info", "fmt": "episode: %s", "args": [8.406760120541438], "caller": "main.py:149", "time": "2021-02-05T19:02:05.678494"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23213092982769012, 0.23817282915115356, 100, 0.4635558262003829], "caller": "main.py:200", "time": "2021-02-05T19:02:07.730572"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2316037118434906, 0.22696052491664886, 100, 0.46802279318659235], "caller": "main.py:200", "time": "2021-02-05T19:03:32.375010"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.22950096428394318, 0.2125900685787201, 100, 0.4568561848541214], "caller": "main.py:200", "time": "2021-02-05T19:04:57.129866"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.22905586659908295, 0.2141202688217163, 100, 0.4549707850358895], "caller": "main.py:200", "time": "2021-02-05T19:06:22.816055"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.22969721257686615, 0.2127482295036316, 100, 0.4507078852798842], "caller": "main.py:200", "time": "2021-02-05T19:07:47.717828"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23025457561016083, 0.22138462960720062, 100, 0.44630853946514243], "caller": "main.py:200", "time": "2021-02-05T19:09:12.812544"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.22852087020874023, 0.22912491858005524, 100, 0.47040582234923783], "caller": "main.py:200", "time": "2021-02-05T19:10:37.437411"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.22803257405757904, 0.22517292201519012, 100, 0.46268190766714506], "caller": "main.py:200", "time": "2021-02-05T19:12:02.938898"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.22869230806827545, 0.2344784438610077, 100, 0.46490285570740836], "caller": "main.py:200", "time": "2021-02-05T19:13:29.623064"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.22820283472537994, 0.21168017387390137, 100, 0.4689333715565956], "caller": "main.py:200", "time": "2021-02-05T19:14:55.762921"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2270478457212448, 0.2522195279598236, 100, 0.46559837783687447], "caller": "main.py:200", "time": "2021-02-05T19:16:22.065784"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22895774245262146, 0.2275615930557251, 100, 0.45045422430302284], "caller": "main.py:200", "time": "2021-02-05T19:17:48.345885"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22638103365898132, 0.2203412652015686, 100, 0.4709950768476607], "caller": "main.py:200", "time": "2021-02-05T19:19:15.220494"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.2282118797302246, 0.21506839990615845, 100, 0.45094802971145126], "caller": "main.py:200", "time": "2021-02-05T19:20:42.458195"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22585946321487427, 0.21138951182365417, 100, 0.4668118640313637], "caller": "main.py:200", "time": "2021-02-05T19:22:06.879043"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22909626364707947, 0.24506303668022156, 100, 0.46538613821382596], "caller": "main.py:200", "time": "2021-02-05T19:23:32.192052"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22779367864131927, 0.22811123728752136, 100, 0.45699875946243307], "caller": "main.py:200", "time": "2021-02-05T19:24:58.191789"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22845982015132904, 0.23889517784118652, 100, 0.4600446214811905], "caller": "main.py:200", "time": "2021-02-05T19:26:23.427902"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22960838675498962, 0.22347328066825867, 100, 0.4790293901679708], "caller": "main.py:200", "time": "2021-02-05T19:27:48.457412"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.22553662955760956, 0.21772021055221558, 100, 0.46328476096527205], "caller": "main.py:200", "time": "2021-02-05T19:29:12.897589"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [45], "caller": "main.py:129", "time": "2021-02-05T19:30:35.518025"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T19:30:44.538925"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 11, 40.13436685365309, 40.18736587977387], "caller": "main.py:37", "time": "2021-02-05T19:30:48.560234"}
+{"level": "info", "fmt": "episode: %s", "args": [8.62976318372353], "caller": "main.py:149", "time": "2021-02-05T19:30:58.642302"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23011822998523712, 0.232692152261734, 100, 0.4524687030458977], "caller": "main.py:200", "time": "2021-02-05T19:31:00.698884"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23076528310775757, 0.21686731278896332, 100, 0.4510462623383564], "caller": "main.py:200", "time": "2021-02-05T19:32:25.737647"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23004254698753357, 0.21855789422988892, 100, 0.4584873318213693], "caller": "main.py:200", "time": "2021-02-05T19:33:50.212251"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23001764714717865, 0.23706457018852234, 100, 0.46331836248771435], "caller": "main.py:200", "time": "2021-02-05T19:35:15.335619"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.22957977652549744, 0.24067118763923645, 100, 0.45802524472663175], "caller": "main.py:200", "time": "2021-02-05T19:36:40.187297"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.22943958640098572, 0.21898028254508972, 100, 0.4679334133395516], "caller": "main.py:200", "time": "2021-02-05T19:38:04.490048"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.22930634021759033, 0.24478605389595032, 100, 0.458701540781881], "caller": "main.py:200", "time": "2021-02-05T19:39:28.848508"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23049387335777283, 0.22885125875473022, 100, 0.45989406491687457], "caller": "main.py:200", "time": "2021-02-05T19:40:53.354139"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.22954364120960236, 0.2177695333957672, 100, 0.460505422307694], "caller": "main.py:200", "time": "2021-02-05T19:42:17.609497"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.23029354214668274, 0.22158978879451752, 100, 0.46216199780849343], "caller": "main.py:200", "time": "2021-02-05T19:43:42.294804"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.22750791907310486, 0.249235138297081, 100, 0.4531095954322701], "caller": "main.py:200", "time": "2021-02-05T19:45:07.239887"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22906608879566193, 0.2325197011232376, 100, 0.4636426800025783], "caller": "main.py:200", "time": "2021-02-05T19:46:31.545670"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22804033756256104, 0.23286661505699158, 100, 0.4657698663603406], "caller": "main.py:200", "time": "2021-02-05T19:47:55.880718"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23212066292762756, 0.21859483420848846, 100, 0.46731715942716434], "caller": "main.py:200", "time": "2021-02-05T19:49:20.399416"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2280958592891693, 0.2006756067276001, 100, 0.46467684711339485], "caller": "main.py:200", "time": "2021-02-05T19:50:44.962924"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22788649797439575, 0.23170270025730133, 100, 0.46039297725126677], "caller": "main.py:200", "time": "2021-02-05T19:52:09.529653"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.22822493314743042, 0.23586460947990417, 100, 0.46138194466983856], "caller": "main.py:200", "time": "2021-02-05T19:53:34.107485"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.2271745800971985, 0.22622612118721008, 100, 0.44730476613340064], "caller": "main.py:200", "time": "2021-02-05T19:54:58.651869"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.2303430140018463, 0.23525357246398926, 100, 0.4697997730500583], "caller": "main.py:200", "time": "2021-02-05T19:56:23.209726"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.22983571887016296, 0.22176602482795715, 100, 0.461194014503136], "caller": "main.py:200", "time": "2021-02-05T19:57:47.508916"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [46], "caller": "main.py:129", "time": "2021-02-05T19:59:10.119994"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T19:59:19.273285"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 14.813534787165244, 0.28962878770687583], "caller": "main.py:37", "time": "2021-02-05T19:59:23.254469"}
+{"level": "info", "fmt": "episode: %s", "args": [7.955724235104869], "caller": "main.py:149", "time": "2021-02-05T19:59:33.356646"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23124147951602936, 0.24199455976486206, 100, 0.45598742510288837], "caller": "main.py:200", "time": "2021-02-05T19:59:35.406396"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.23225905001163483, 0.24713754653930664, 100, 0.45438319708333447], "caller": "main.py:200", "time": "2021-02-05T20:00:59.704077"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23367109894752502, 0.23191742599010468, 100, 0.46011726792025875], "caller": "main.py:200", "time": "2021-02-05T20:02:24.356441"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.23435162007808685, 0.2255796492099762, 100, 0.46175262982996157], "caller": "main.py:200", "time": "2021-02-05T20:03:48.854892"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.23386016488075256, 0.23463553190231323, 100, 0.457032225980355], "caller": "main.py:200", "time": "2021-02-05T20:05:13.572685"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.23244932293891907, 0.22502173483371735, 100, 0.4566225939532468], "caller": "main.py:200", "time": "2021-02-05T20:06:38.087195"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.23458778858184814, 0.2175971418619156, 100, 0.45944935883457494], "caller": "main.py:200", "time": "2021-02-05T20:08:02.663606"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.23241685330867767, 0.2350248098373413, 100, 0.45650989766324185], "caller": "main.py:200", "time": "2021-02-05T20:09:27.122864"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.23207446932792664, 0.2272978574037552, 100, 0.455430918886418], "caller": "main.py:200", "time": "2021-02-05T20:10:51.147600"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2322351634502411, 0.2205696702003479, 100, 0.44634530003375755], "caller": "main.py:200", "time": "2021-02-05T20:12:16.158460"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2311272770166397, 0.264732301235199, 100, 0.45203510428915095], "caller": "main.py:200", "time": "2021-02-05T20:13:40.830620"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.23432961106300354, 0.2439839392900467, 100, 0.45667372997008737], "caller": "main.py:200", "time": "2021-02-05T20:15:05.298487"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.23189692199230194, 0.22299537062644958, 100, 0.45470782871467635], "caller": "main.py:200", "time": "2021-02-05T20:16:29.684980"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.23097367584705353, 0.22457081079483032, 100, 0.46671206916671915], "caller": "main.py:200", "time": "2021-02-05T20:17:54.379560"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.23255066573619843, 0.24191905558109283, 100, 0.4566012277742595], "caller": "main.py:200", "time": "2021-02-05T20:19:18.947973"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.23191513121128082, 0.26025325059890747, 100, 0.46062200303239853], "caller": "main.py:200", "time": "2021-02-05T20:20:43.230244"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23106732964515686, 0.23045578598976135, 100, 0.45903049941241], "caller": "main.py:200", "time": "2021-02-05T20:22:07.545736"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.23064184188842773, 0.23573477566242218, 100, 0.4516975184823381], "caller": "main.py:200", "time": "2021-02-05T20:23:32.147649"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.23027195036411285, 0.23115095496177673, 100, 0.4599777715938262], "caller": "main.py:200", "time": "2021-02-05T20:24:56.690207"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.23138432204723358, 0.21993502974510193, 100, 0.45969882215432845], "caller": "main.py:200", "time": "2021-02-05T20:26:21.167537"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [47], "caller": "main.py:129", "time": "2021-02-05T20:27:43.609465"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 9, 0.1111111111111111, 0.31426968052735443], "caller": "main.py:37", "time": "2021-02-05T20:27:52.677692"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 10, 32.97890659917072, 36.70491684517064], "caller": "main.py:37", "time": "2021-02-05T20:27:56.713515"}
+{"level": "info", "fmt": "episode: %s", "args": [8.73831902074591], "caller": "main.py:149", "time": "2021-02-05T20:28:06.971571"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.23065166175365448, 0.23906102776527405, 100, 0.4540774878405443], "caller": "main.py:200", "time": "2021-02-05T20:28:09.011243"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.2307441085577011, 0.24170057475566864, 100, 0.45598073656767557], "caller": "main.py:200", "time": "2021-02-05T20:29:33.818564"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.23038333654403687, 0.23406276106834412, 100, 0.45088257576486734], "caller": "main.py:200", "time": "2021-02-05T20:30:58.242586"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.22911608219146729, 0.23017892241477966, 100, 0.4508431085493333], "caller": "main.py:200", "time": "2021-02-05T20:32:22.531082"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.2312256395816803, 0.24091270565986633, 100, 0.46020629788803474], "caller": "main.py:200", "time": "2021-02-05T20:33:47.094717"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.229876309633255, 0.21086038649082184, 100, 0.4439142635020754], "caller": "main.py:200", "time": "2021-02-05T20:35:11.668702"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.22842366993427277, 0.23558300733566284, 100, 0.4462212934643844], "caller": "main.py:200", "time": "2021-02-05T20:36:35.850977"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.22938168048858643, 0.231635183095932, 100, 0.4469598865062074], "caller": "main.py:200", "time": "2021-02-05T20:38:00.229170"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.22722621262073517, 0.21877889335155487, 100, 0.44054767008190654], "caller": "main.py:200", "time": "2021-02-05T20:39:24.700291"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.22950123250484467, 0.23072856664657593, 100, 0.46004531647134855], "caller": "main.py:200", "time": "2021-02-05T20:40:49.511907"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.23145227134227753, 0.22434483468532562, 100, 0.45679420556030437], "caller": "main.py:200", "time": "2021-02-05T20:42:13.916880"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.2284998893737793, 0.2257922738790512, 100, 0.46221815849688813], "caller": "main.py:200", "time": "2021-02-05T20:43:38.275278"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22702130675315857, 0.22987332940101624, 100, 0.44866223449988785], "caller": "main.py:200", "time": "2021-02-05T20:45:02.671929"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.22855013608932495, 0.22451722621917725, 100, 0.45741283710197145], "caller": "main.py:200", "time": "2021-02-05T20:46:26.959128"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22433950006961823, 0.23093745112419128, 100, 0.45217687572487436], "caller": "main.py:200", "time": "2021-02-05T20:47:51.347839"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22967976331710815, 0.2380084991455078, 100, 0.4621173807516513], "caller": "main.py:200", "time": "2021-02-05T20:49:15.717032"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.23036788403987885, 0.2280646562576294, 100, 0.45943514580900646], "caller": "main.py:200", "time": "2021-02-05T20:50:39.998999"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22758251428604126, 0.2241745889186859, 100, 0.46527213360186837], "caller": "main.py:200", "time": "2021-02-05T20:52:04.488847"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22946380078792572, 0.23824399709701538, 100, 0.45161248172773844], "caller": "main.py:200", "time": "2021-02-05T20:53:29.285742"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.22718115150928497, 0.21703898906707764, 100, 0.4475149195938823], "caller": "main.py:200", "time": "2021-02-05T20:54:53.748122"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [48], "caller": "main.py:129", "time": "2021-02-05T20:56:15.929288"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T20:56:25.241142"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 16.51380972265385, 0.1820444452392426], "caller": "main.py:37", "time": "2021-02-05T20:56:29.485854"}
+{"level": "info", "fmt": "episode: %s", "args": [8.890210054481447], "caller": "main.py:149", "time": "2021-02-05T20:56:39.801727"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.22970624268054962, 0.23037654161453247, 100, 0.45432604010330246], "caller": "main.py:200", "time": "2021-02-05T20:56:41.867865"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.22949573397636414, 0.21296051144599915, 100, 0.4488744955806085], "caller": "main.py:200", "time": "2021-02-05T20:58:06.111392"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.22544075548648834, 0.24675486981868744, 100, 0.4543122723244304], "caller": "main.py:200", "time": "2021-02-05T20:59:30.663241"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.22649045288562775, 0.22680403292179108, 100, 0.45500608618477745], "caller": "main.py:200", "time": "2021-02-05T21:00:55.029328"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.224647656083107, 0.22210437059402466, 100, 0.45442345435843495], "caller": "main.py:200", "time": "2021-02-05T21:02:19.220923"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.22553607821464539, 0.24753545224666595, 100, 0.4499808129218255], "caller": "main.py:200", "time": "2021-02-05T21:03:43.736917"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.22608008980751038, 0.2315455973148346, 100, 0.45481855566721807], "caller": "main.py:200", "time": "2021-02-05T21:05:08.274649"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.22711843252182007, 0.21715010702610016, 100, 0.458205540432409], "caller": "main.py:200", "time": "2021-02-05T21:06:32.538903"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.22481755912303925, 0.2304503321647644, 100, 0.4449699437173286], "caller": "main.py:200", "time": "2021-02-05T21:07:56.482087"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.2265903502702713, 0.22031566500663757, 100, 0.4492558812645837], "caller": "main.py:200", "time": "2021-02-05T21:09:20.728933"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.22514083981513977, 0.23209263384342194, 100, 0.461561665488817], "caller": "main.py:200", "time": "2021-02-05T21:10:45.363606"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22505861520767212, 0.25201570987701416, 100, 0.4446360930355802], "caller": "main.py:200", "time": "2021-02-05T21:12:09.891111"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22550366818904877, 0.23932233452796936, 100, 0.4546886720798707], "caller": "main.py:200", "time": "2021-02-05T21:13:34.020904"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.22784537076950073, 0.23966604471206665, 100, 0.4507768493439986], "caller": "main.py:200", "time": "2021-02-05T21:14:58.251918"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.22616051137447357, 0.20994681119918823, 100, 0.44935030604875775], "caller": "main.py:200", "time": "2021-02-05T21:16:22.990847"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22391757369041443, 0.24089692533016205, 100, 0.4466719483882552], "caller": "main.py:200", "time": "2021-02-05T21:17:47.639776"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.2258712202310562, 0.22992601990699768, 100, 0.45492082740287604], "caller": "main.py:200", "time": "2021-02-05T21:19:11.829994"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22392091155052185, 0.25136464834213257, 100, 0.4556815835377868], "caller": "main.py:200", "time": "2021-02-05T21:20:35.931966"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22696909308433533, 0.19530263543128967, 100, 0.44611323530981223], "caller": "main.py:200", "time": "2021-02-05T21:22:00.606079"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.22636108100414276, 0.21151208877563477, 100, 0.44498257274946235], "caller": "main.py:200", "time": "2021-02-05T21:23:24.805560"}
+{"level": "info", "fmt": "------ Starting Stage %d --------", "args": [49], "caller": "main.py:129", "time": "2021-02-05T21:24:46.700269"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T21:24:55.737022"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 10, 33.454236625118604, 36.58440751537775], "caller": "main.py:37", "time": "2021-02-05T21:24:59.778858"}
+{"level": "info", "fmt": "episode: %s", "args": [8.612110514550421], "caller": "main.py:149", "time": "2021-02-05T21:25:09.867361"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [0, 0.22677846252918243, 0.2141023576259613, 100, 0.4489411282090427], "caller": "main.py:200", "time": "2021-02-05T21:25:11.932657"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [1, 0.22743871808052063, 0.20570001006126404, 100, 0.44619335587331127], "caller": "main.py:200", "time": "2021-02-05T21:26:35.880346"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [2, 0.22545818984508514, 0.2284492552280426, 100, 0.44411145772069127], "caller": "main.py:200", "time": "2021-02-05T21:28:00.203846"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [3, 0.22451549768447876, 0.22394105792045593, 100, 0.44977468635114454], "caller": "main.py:200", "time": "2021-02-05T21:29:24.319056"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [4, 0.224447101354599, 0.2260664701461792, 100, 0.4450153538139694], "caller": "main.py:200", "time": "2021-02-05T21:30:48.868265"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [5, 0.22404465079307556, 0.22218182682991028, 100, 0.4518319215918751], "caller": "main.py:200", "time": "2021-02-05T21:32:13.051485"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [6, 0.2244257777929306, 0.21632204949855804, 100, 0.4567461853762691], "caller": "main.py:200", "time": "2021-02-05T21:33:37.747845"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [7, 0.2258124202489853, 0.22551798820495605, 100, 0.44655383880441235], "caller": "main.py:200", "time": "2021-02-05T21:35:02.435210"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [8, 0.22594498097896576, 0.21023118495941162, 100, 0.45169191326154395], "caller": "main.py:200", "time": "2021-02-05T21:36:26.547961"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [9, 0.22501353919506073, 0.20948831737041473, 100, 0.4483976199774572], "caller": "main.py:200", "time": "2021-02-05T21:37:50.911234"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [10, 0.2277127206325531, 0.2167503535747528, 100, 0.4553521027705697], "caller": "main.py:200", "time": "2021-02-05T21:39:15.747912"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [11, 0.22520712018013, 0.22566339373588562, 100, 0.4574934711594852], "caller": "main.py:200", "time": "2021-02-05T21:40:40.142691"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [12, 0.22380632162094116, 0.20672887563705444, 100, 0.44236133243190295], "caller": "main.py:200", "time": "2021-02-05T21:42:04.240015"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [13, 0.22627809643745422, 0.22971618175506592, 100, 0.44045560295945635], "caller": "main.py:200", "time": "2021-02-05T21:43:28.696580"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [14, 0.2249078005552292, 0.20753884315490723, 100, 0.4522518157506733], "caller": "main.py:200", "time": "2021-02-05T21:44:52.978090"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [15, 0.22444534301757812, 0.22223594784736633, 100, 0.4453567680271158], "caller": "main.py:200", "time": "2021-02-05T21:46:17.195860"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [16, 0.2239004522562027, 0.22277352213859558, 100, 0.44363299573991943], "caller": "main.py:200", "time": "2021-02-05T21:47:41.729496"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [17, 0.22574563324451447, 0.22675205767154694, 100, 0.45525534357048325], "caller": "main.py:200", "time": "2021-02-05T21:49:06.124808"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [18, 0.22572964429855347, 0.19745349884033203, 100, 0.4531894102239415], "caller": "main.py:200", "time": "2021-02-05T21:50:30.510158"}
+{"level": "info", "fmt": "# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f", "args": [19, 0.22378362715244293, 0.23681676387786865, 100, 0.45859046663228353], "caller": "main.py:200", "time": "2021-02-05T21:51:55.342569"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Real Env", 8, 0.0, 0.0], "caller": "main.py:37", "time": "2021-02-05T21:53:27.123728"}
+{"level": "info", "fmt": "Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f", "args": ["episode", "Virt Env", 8, 15.773014155492273, 0.31361383936605874], "caller": "main.py:37", "time": "2021-02-05T21:53:31.109118"}
diff --git a/experiments05/ant_umaze_1234/src/.gitignore b/experiments05/ant_umaze_1234/src/.gitignore
new file mode 100644
index 0000000..01e12c7
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/.gitignore
@@ -0,0 +1,132 @@
+# Byte-compiled / optimized / DLL files
+__pycache__/
+*.py[cod]
+*$py.class
+
+# C extensions
+*.so
+
+# Distribution / packaging
+.Python
+build/
+develop-eggs/
+dist/
+downloads/
+eggs/
+.eggs/
+lib/
+lib64/
+parts/
+sdist/
+var/
+wheels/
+pip-wheel-metadata/
+share/python-wheels/
+*.egg-info/
+.installed.cfg
+*.egg
+MANIFEST
+vendor/
+rllab/
+tmp/
+log/
+experiments/
+# PyInstaller
+#  Usually these files are written by a python script from a template
+#  before PyInstaller builds the exe, so as to inject date/other infos into it.
+*.manifest
+*.spec
+
+# Installer logs
+pip-log.txt
+pip-delete-this-directory.txt
+
+# Unit test / coverage reports
+htmlcov/
+.tox/
+.nox/
+.coverage
+.coverage.*
+.cache
+nosetests.xml
+coverage.xml
+*.cover
+*.py,cover
+.hypothesis/
+.pytest_cache/
+
+# Translations
+*.mo
+*.pot
+
+# Django stuff:
+*.log
+local_settings.py
+db.sqlite3
+db.sqlite3-journal
+
+# Flask stuff:
+instance/
+.webassets-cache
+
+# Scrapy stuff:
+.scrapy
+
+# Sphinx documentation
+docs/_build/
+
+# PyBuilder
+target/
+
+# Jupyter Notebook
+.ipynb_checkpoints
+
+# IPython
+profile_default/
+ipython_config.py
+
+# pyenv
+.python-version
+
+# pipenv
+#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
+#   However, in case of collaboration, if having platform-specific dependencies or dependencies
+#   having no cross-platform support, pipenv may install dependencies that don't work, or not
+#   install all needed dependencies.
+#Pipfile.lock
+
+# PEP 582; used by e.g. github.com/David-OConnor/pyflow
+__pypackages__/
+
+# Celery stuff
+celerybeat-schedule
+celerybeat.pid
+
+# SageMath parsed files
+*.sage.py
+
+# Environments
+.env
+.venv
+venv/
+ENV/
+env.bak/
+venv.bak/
+
+# Spyder project settings
+.spyderproject
+.spyproject
+
+# Rope project settings
+.ropeproject
+
+# mkdocs documentation
+/site
+
+# mypy
+.mypy_cache/
+.dmypy.json
+dmypy.json
+
+# Pyre type checker
+.pyre/
diff --git a/experiments05/ant_umaze_1234/src/CODE_OF_CONDUCT.md b/experiments05/ant_umaze_1234/src/CODE_OF_CONDUCT.md
new file mode 100644
index 0000000..0d31b1f
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/CODE_OF_CONDUCT.md
@@ -0,0 +1,5 @@
+# Code of Conduct
+
+Facebook has adopted a Code of Conduct that we expect project participants to adhere to.
+Please read the [full text](https://code.fb.com/codeofconduct/)
+so that you can understand what actions will and will not be tolerated.
\ No newline at end of file
diff --git a/experiments05/ant_umaze_1234/src/CONTRIBUTING.md b/experiments05/ant_umaze_1234/src/CONTRIBUTING.md
new file mode 100644
index 0000000..07780f7
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/CONTRIBUTING.md
@@ -0,0 +1,31 @@
+# Contributing to SLBO
+We want to make contributing to this project as easy and transparent as
+possible.
+
+## Pull Requests
+We actively welcome your pull requests.
+
+1. Fork the repo and create your branch from `master`.
+2. Ensure the examples still run.
+3. If you haven't already, complete the Contributor License Agreement ("CLA").
+
+## Contributor License Agreement ("CLA")
+In order to accept your pull request, we need you to submit a CLA. You only need
+to do this once to work on any of Facebook's open source projects.
+
+Complete your CLA here: <https://code.facebook.com/cla>
+
+## Issues
+We use GitHub issues to track public bugs. Please ensure your description is
+clear and has sufficient instructions to be able to reproduce the issue.
+
+Facebook has a [bounty program](https://www.facebook.com/whitehat/) for the safe
+disclosure of security bugs. In those cases, please go through the process
+outlined on that page and do not file a public issue.
+
+## Coding Style  
+We try to follow the PEP style guidelines and encourage you to as well.
+
+## License
+By contributing to SparseConvNet, you agree that your contributions will be licensed
+under the LICENSE file in the root directory of this source tree.
\ No newline at end of file
diff --git a/experiments05/ant_umaze_1234/src/LICENSE b/experiments05/ant_umaze_1234/src/LICENSE
new file mode 100644
index 0000000..1fe4148
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/LICENSE
@@ -0,0 +1,407 @@
+Attribution-NonCommercial 4.0 International
+
+=======================================================================
+
+Creative Commons Corporation ("Creative Commons") is not a law firm and
+does not provide legal services or legal advice. Distribution of
+Creative Commons public licenses does not create a lawyer-client or
+other relationship. Creative Commons makes its licenses and related
+information available on an "as-is" basis. Creative Commons gives no
+warranties regarding its licenses, any material licensed under their
+terms and conditions, or any related information. Creative Commons
+disclaims all liability for damages resulting from their use to the
+fullest extent possible.
+
+Using Creative Commons Public Licenses
+
+Creative Commons public licenses provide a standard set of terms and
+conditions that creators and other rights holders may use to share
+original works of authorship and other material subject to copyright
+and certain other rights specified in the public license below. The
+following considerations are for informational purposes only, are not
+exhaustive, and do not form part of our licenses.
+
+     Considerations for licensors: Our public licenses are
+     intended for use by those authorized to give the public
+     permission to use material in ways otherwise restricted by
+     copyright and certain other rights. Our licenses are
+     irrevocable. Licensors should read and understand the terms
+     and conditions of the license they choose before applying it.
+     Licensors should also secure all rights necessary before
+     applying our licenses so that the public can reuse the
+     material as expected. Licensors should clearly mark any
+     material not subject to the license. This includes other CC-
+     licensed material, or material used under an exception or
+     limitation to copyright. More considerations for licensors:
+	wiki.creativecommons.org/Considerations_for_licensors
+
+     Considerations for the public: By using one of our public
+     licenses, a licensor grants the public permission to use the
+     licensed material under specified terms and conditions. If
+     the licensor's permission is not necessary for any reason--for
+     example, because of any applicable exception or limitation to
+     copyright--then that use is not regulated by the license. Our
+     licenses grant only permissions under copyright and certain
+     other rights that a licensor has authority to grant. Use of
+     the licensed material may still be restricted for other
+     reasons, including because others have copyright or other
+     rights in the material. A licensor may make special requests,
+     such as asking that all changes be marked or described.
+     Although not required by our licenses, you are encouraged to
+     respect those requests where reasonable. More_considerations
+     for the public:
+	wiki.creativecommons.org/Considerations_for_licensees
+
+=======================================================================
+
+Creative Commons Attribution-NonCommercial 4.0 International Public
+License
+
+By exercising the Licensed Rights (defined below), You accept and agree
+to be bound by the terms and conditions of this Creative Commons
+Attribution-NonCommercial 4.0 International Public License ("Public
+License"). To the extent this Public License may be interpreted as a
+contract, You are granted the Licensed Rights in consideration of Your
+acceptance of these terms and conditions, and the Licensor grants You
+such rights in consideration of benefits the Licensor receives from
+making the Licensed Material available under these terms and
+conditions.
+
+
+Section 1 -- Definitions.
+
+  a. Adapted Material means material subject to Copyright and Similar
+     Rights that is derived from or based upon the Licensed Material
+     and in which the Licensed Material is translated, altered,
+     arranged, transformed, or otherwise modified in a manner requiring
+     permission under the Copyright and Similar Rights held by the
+     Licensor. For purposes of this Public License, where the Licensed
+     Material is a musical work, performance, or sound recording,
+     Adapted Material is always produced where the Licensed Material is
+     synched in timed relation with a moving image.
+
+  b. Adapter's License means the license You apply to Your Copyright
+     and Similar Rights in Your contributions to Adapted Material in
+     accordance with the terms and conditions of this Public License.
+
+  c. Copyright and Similar Rights means copyright and/or similar rights
+     closely related to copyright including, without limitation,
+     performance, broadcast, sound recording, and Sui Generis Database
+     Rights, without regard to how the rights are labeled or
+     categorized. For purposes of this Public License, the rights
+     specified in Section 2(b)(1)-(2) are not Copyright and Similar
+     Rights.
+  d. Effective Technological Measures means those measures that, in the
+     absence of proper authority, may not be circumvented under laws
+     fulfilling obligations under Article 11 of the WIPO Copyright
+     Treaty adopted on December 20, 1996, and/or similar international
+     agreements.
+
+  e. Exceptions and Limitations means fair use, fair dealing, and/or
+     any other exception or limitation to Copyright and Similar Rights
+     that applies to Your use of the Licensed Material.
+
+  f. Licensed Material means the artistic or literary work, database,
+     or other material to which the Licensor applied this Public
+     License.
+
+  g. Licensed Rights means the rights granted to You subject to the
+     terms and conditions of this Public License, which are limited to
+     all Copyright and Similar Rights that apply to Your use of the
+     Licensed Material and that the Licensor has authority to license.
+
+  h. Licensor means the individual(s) or entity(ies) granting rights
+     under this Public License.
+
+  i. NonCommercial means not primarily intended for or directed towards
+     commercial advantage or monetary compensation. For purposes of
+     this Public License, the exchange of the Licensed Material for
+     other material subject to Copyright and Similar Rights by digital
+     file-sharing or similar means is NonCommercial provided there is
+     no payment of monetary compensation in connection with the
+     exchange.
+
+  j. Share means to provide material to the public by any means or
+     process that requires permission under the Licensed Rights, such
+     as reproduction, public display, public performance, distribution,
+     dissemination, communication, or importation, and to make material
+     available to the public including in ways that members of the
+     public may access the material from a place and at a time
+     individually chosen by them.
+
+  k. Sui Generis Database Rights means rights other than copyright
+     resulting from Directive 96/9/EC of the European Parliament and of
+     the Council of 11 March 1996 on the legal protection of databases,
+     as amended and/or succeeded, as well as other essentially
+     equivalent rights anywhere in the world.
+
+  l. You means the individual or entity exercising the Licensed Rights
+     under this Public License. Your has a corresponding meaning.
+
+
+Section 2 -- Scope.
+
+  a. License grant.
+
+       1. Subject to the terms and conditions of this Public License,
+          the Licensor hereby grants You a worldwide, royalty-free,
+          non-sublicensable, non-exclusive, irrevocable license to
+          exercise the Licensed Rights in the Licensed Material to:
+
+            a. reproduce and Share the Licensed Material, in whole or
+               in part, for NonCommercial purposes only; and
+
+            b. produce, reproduce, and Share Adapted Material for
+               NonCommercial purposes only.
+
+       2. Exceptions and Limitations. For the avoidance of doubt, where
+          Exceptions and Limitations apply to Your use, this Public
+          License does not apply, and You do not need to comply with
+          its terms and conditions.
+
+       3. Term. The term of this Public License is specified in Section
+          6(a).
+
+       4. Media and formats; technical modifications allowed. The
+          Licensor authorizes You to exercise the Licensed Rights in
+          all media and formats whether now known or hereafter created,
+          and to make technical modifications necessary to do so. The
+          Licensor waives and/or agrees not to assert any right or
+          authority to forbid You from making technical modifications
+          necessary to exercise the Licensed Rights, including
+          technical modifications necessary to circumvent Effective
+          Technological Measures. For purposes of this Public License,
+          simply making modifications authorized by this Section 2(a)
+          (4) never produces Adapted Material.
+
+       5. Downstream recipients.
+
+            a. Offer from the Licensor -- Licensed Material. Every
+               recipient of the Licensed Material automatically
+               receives an offer from the Licensor to exercise the
+               Licensed Rights under the terms and conditions of this
+               Public License.
+
+            b. No downstream restrictions. You may not offer or impose
+               any additional or different terms or conditions on, or
+               apply any Effective Technological Measures to, the
+               Licensed Material if doing so restricts exercise of the
+               Licensed Rights by any recipient of the Licensed
+               Material.
+
+       6. No endorsement. Nothing in this Public License constitutes or
+          may be construed as permission to assert or imply that You
+          are, or that Your use of the Licensed Material is, connected
+          with, or sponsored, endorsed, or granted official status by,
+          the Licensor or others designated to receive attribution as
+          provided in Section 3(a)(1)(A)(i).
+
+  b. Other rights.
+
+       1. Moral rights, such as the right of integrity, are not
+          licensed under this Public License, nor are publicity,
+          privacy, and/or other similar personality rights; however, to
+          the extent possible, the Licensor waives and/or agrees not to
+          assert any such rights held by the Licensor to the limited
+          extent necessary to allow You to exercise the Licensed
+          Rights, but not otherwise.
+
+       2. Patent and trademark rights are not licensed under this
+          Public License.
+
+       3. To the extent possible, the Licensor waives any right to
+          collect royalties from You for the exercise of the Licensed
+          Rights, whether directly or through a collecting society
+          under any voluntary or waivable statutory or compulsory
+          licensing scheme. In all other cases the Licensor expressly
+          reserves any right to collect such royalties, including when
+          the Licensed Material is used other than for NonCommercial
+          purposes.
+
+
+Section 3 -- License Conditions.
+
+Your exercise of the Licensed Rights is expressly made subject to the
+following conditions.
+
+  a. Attribution.
+
+       1. If You Share the Licensed Material (including in modified
+          form), You must:
+
+            a. retain the following if it is supplied by the Licensor
+               with the Licensed Material:
+
+                 i. identification of the creator(s) of the Licensed
+                    Material and any others designated to receive
+                    attribution, in any reasonable manner requested by
+                    the Licensor (including by pseudonym if
+                    designated);
+
+                ii. a copyright notice;
+
+               iii. a notice that refers to this Public License;
+
+                iv. a notice that refers to the disclaimer of
+                    warranties;
+
+                 v. a URI or hyperlink to the Licensed Material to the
+                    extent reasonably practicable;
+
+            b. indicate if You modified the Licensed Material and
+               retain an indication of any previous modifications; and
+
+            c. indicate the Licensed Material is licensed under this
+               Public License, and include the text of, or the URI or
+               hyperlink to, this Public License.
+
+       2. You may satisfy the conditions in Section 3(a)(1) in any
+          reasonable manner based on the medium, means, and context in
+          which You Share the Licensed Material. For example, it may be
+          reasonable to satisfy the conditions by providing a URI or
+          hyperlink to a resource that includes the required
+          information.
+
+       3. If requested by the Licensor, You must remove any of the
+          information required by Section 3(a)(1)(A) to the extent
+          reasonably practicable.
+
+       4. If You Share Adapted Material You produce, the Adapter's
+          License You apply must not prevent recipients of the Adapted
+          Material from complying with this Public License.
+
+
+Section 4 -- Sui Generis Database Rights.
+
+Where the Licensed Rights include Sui Generis Database Rights that
+apply to Your use of the Licensed Material:
+
+  a. for the avoidance of doubt, Section 2(a)(1) grants You the right
+     to extract, reuse, reproduce, and Share all or a substantial
+     portion of the contents of the database for NonCommercial purposes
+     only;
+
+  b. if You include all or a substantial portion of the database
+     contents in a database in which You have Sui Generis Database
+     Rights, then the database in which You have Sui Generis Database
+     Rights (but not its individual contents) is Adapted Material; and
+
+  c. You must comply with the conditions in Section 3(a) if You Share
+     all or a substantial portion of the contents of the database.
+
+For the avoidance of doubt, this Section 4 supplements and does not
+replace Your obligations under this Public License where the Licensed
+Rights include other Copyright and Similar Rights.
+
+
+Section 5 -- Disclaimer of Warranties and Limitation of Liability.
+
+  a. UNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE
+     EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS
+     AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF
+     ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS,
+     IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION,
+     WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR
+     PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS,
+     ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT
+     KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT
+     ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU.
+
+  b. TO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE
+     TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION,
+     NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT,
+     INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES,
+     COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR
+     USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN
+     ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR
+     DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR
+     IN PART, THIS LIMITATION MAY NOT APPLY TO YOU.
+
+  c. The disclaimer of warranties and limitation of liability provided
+     above shall be interpreted in a manner that, to the extent
+     possible, most closely approximates an absolute disclaimer and
+     waiver of all liability.
+
+
+Section 6 -- Term and Termination.
+
+  a. This Public License applies for the term of the Copyright and
+     Similar Rights licensed here. However, if You fail to comply with
+     this Public License, then Your rights under this Public License
+     terminate automatically.
+
+  b. Where Your right to use the Licensed Material has terminated under
+     Section 6(a), it reinstates:
+
+       1. automatically as of the date the violation is cured, provided
+          it is cured within 30 days of Your discovery of the
+          violation; or
+
+       2. upon express reinstatement by the Licensor.
+
+     For the avoidance of doubt, this Section 6(b) does not affect any
+     right the Licensor may have to seek remedies for Your violations
+     of this Public License.
+
+  c. For the avoidance of doubt, the Licensor may also offer the
+     Licensed Material under separate terms or conditions or stop
+     distributing the Licensed Material at any time; however, doing so
+     will not terminate this Public License.
+
+  d. Sections 1, 5, 6, 7, and 8 survive termination of this Public
+     License.
+
+
+Section 7 -- Other Terms and Conditions.
+
+  a. The Licensor shall not be bound by any additional or different
+     terms or conditions communicated by You unless expressly agreed.
+
+  b. Any arrangements, understandings, or agreements regarding the
+     Licensed Material not stated herein are separate from and
+     independent of the terms and conditions of this Public License.
+
+
+Section 8 -- Interpretation.
+
+  a. For the avoidance of doubt, this Public License does not, and
+     shall not be interpreted to, reduce, limit, restrict, or impose
+     conditions on any use of the Licensed Material that could lawfully
+     be made without permission under this Public License.
+
+  b. To the extent possible, if any provision of this Public License is
+     deemed unenforceable, it shall be automatically reformed to the
+     minimum extent necessary to make it enforceable. If the provision
+     cannot be reformed, it shall be severed from this Public License
+     without affecting the enforceability of the remaining terms and
+     conditions.
+
+  c. No term or condition of this Public License will be waived and no
+     failure to comply consented to unless expressly agreed to by the
+     Licensor.
+
+  d. Nothing in this Public License constitutes or may be interpreted
+     as a limitation upon, or waiver of, any privileges and immunities
+     that apply to the Licensor or You, including from the legal
+     processes of any jurisdiction or authority.
+
+=======================================================================
+
+Creative Commons is not a party to its public
+licenses. Notwithstanding, Creative Commons may elect to apply one of
+its public licenses to material it publishes and in those instances
+will be considered the “Licensor.” The text of the Creative Commons
+public licenses is dedicated to the public domain under the CC0 Public
+Domain Dedication. Except for the limited purpose of indicating that
+material is shared under a Creative Commons public license or as
+otherwise permitted by the Creative Commons policies published at
+creativecommons.org/policies, Creative Commons does not authorize the
+use of the trademark "Creative Commons" or any other trademark or logo
+of Creative Commons without its prior written consent including,
+without limitation, in connection with any unauthorized modifications
+to any of its public licenses or any other arrangements,
+understandings, or agreements concerning use of licensed material. For
+the avoidance of doubt, this paragraph does not form part of the
+public licenses.
+
+Creative Commons may be contacted at creativecommons.org.
diff --git a/experiments05/ant_umaze_1234/src/README.md b/experiments05/ant_umaze_1234/src/README.md
new file mode 100644
index 0000000..ec09da9
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/README.md
@@ -0,0 +1,29 @@
+# Stochastic Lower Bound Optimization
+
+This is the TensorFlow implementation for the paper [
+Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees](https://arxiv.org/abs/1807.03858).
+A PyTorch version will be released later.  
+
+
+## Requirements
+1. OpenAI Baselines
+2. rllab (commit number `b3a2899`)
+3. MuJoCo (1.5)
+4. TensorFlow (>= 1.9)
+5. NumPy (>= 1.14.5)
+6. Python 3.6
+
+## Run
+
+Before running, please make sure that `rllab` and `baselines` are available 
+
+```bash
+python main.py -c configs/algos/slbo.yml configs/envs/half_cheetah.yml -s log_dir=/tmp
+```
+
+If you want to change hyper-parameters, you can either modify a corresponding `yml` file or 
+change it temporarily by appending `model.hidden_sizes='[1000,1000]'` in the command line.
+
+## License
+
+See [LICENSE](LICENSE) for additional details.
diff --git a/experiments05/ant_umaze_1234/src/check_result.py b/experiments05/ant_umaze_1234/src/check_result.py
new file mode 100644
index 0000000..d1c8161
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/check_result.py
@@ -0,0 +1,12 @@
+import numpy as np
+
+seed_list = ['1234', '1235', '2314', '2345']
+env_name = 'gym_reacher'
+lasts = []
+for seed in seed_list:
+    cur = np.load('experiments/'+env_name+'_'+seed+'/eval_real_returns.npy')
+    lasts.append(cur[-1])
+
+print(lasts)
+print(np.mean(lasts))
+print(np.std(lasts))
diff --git a/experiments05/ant_umaze_1234/src/configs/algos/mb_trpo.yml b/experiments05/ant_umaze_1234/src/configs/algos/mb_trpo.yml
new file mode 100644
index 0000000..cce36ec
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/algos/mb_trpo.yml
@@ -0,0 +1,12 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 2
+  dev_batch_size: 512
+  train_batch_size: 128
+slbo:
+  n_stages: 100
+  n_iters: 1
+  n_model_iters: 2000
+  n_policy_iters: 200
+TRPO:
+  ent_coef: 0.005
diff --git a/experiments05/ant_umaze_1234/src/configs/algos/mf.yml b/experiments05/ant_umaze_1234/src/configs/algos/mf.yml
new file mode 100644
index 0000000..5d5e317
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/algos/mf.yml
@@ -0,0 +1,11 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+algorithm: MF
+rollout:
+  n_train_samples: 10000
+  n_dev_samples: 128
+  n_test_samples: 10000
+slbo:
+  n_stages: 100
+  n_iters: 1
+  n_policy_iters: 20
+  n_model_iters: 1
diff --git a/experiments05/ant_umaze_1234/src/configs/algos/slbo.yml b/experiments05/ant_umaze_1234/src/configs/algos/slbo.yml
new file mode 100644
index 0000000..2b5eecb
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/algos/slbo.yml
@@ -0,0 +1,12 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 2
+  dev_batch_size: 512
+  train_batch_size: 128
+slbo:
+  n_stages: 100
+  n_iters: 20
+  n_model_iters: 100
+  n_policy_iters: 40
+TRPO:
+  ent_coef: 0
diff --git a/experiments05/ant_umaze_1234/src/configs/algos/slbo_bm_1m.yml b/experiments05/ant_umaze_1234/src/configs/algos/slbo_bm_1m.yml
new file mode 100644
index 0000000..dcf3777
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/algos/slbo_bm_1m.yml
@@ -0,0 +1,12 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 2
+  dev_batch_size: 512
+  train_batch_size: 128
+slbo:
+  n_stages: 100
+  n_iters: 20
+  n_model_iters: 100
+  n_policy_iters: 40
+TRPO:
+  ent_coef: 0.005
diff --git a/experiments05/ant_umaze_1234/src/configs/algos/slbo_bm_200k.yml b/experiments05/ant_umaze_1234/src/configs/algos/slbo_bm_200k.yml
new file mode 100644
index 0000000..c0e39d4
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/algos/slbo_bm_200k.yml
@@ -0,0 +1,15 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 2
+  dev_batch_size: 512
+  train_batch_size: 128
+slbo:
+  n_stages: 80
+  n_iters: 20
+  n_model_iters: 100
+  n_policy_iters: 40
+TRPO:
+  ent_coef: 0
+rollout:
+  n_train_samples: 6000
+
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/ant_umaze.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/ant_umaze.yml
new file mode 100644
index 0000000..ab75516
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/ant_umaze.yml
@@ -0,0 +1,8 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: AntUMaze-v1
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
+
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/fetch_push.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/fetch_push.yml
new file mode 100644
index 0000000..43a36d0
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/fetch_push.yml
@@ -0,0 +1,8 @@
+env:
+  id: FetchPush
+runner:
+  max_steps: 50
+plan:
+  max_steps: 50
+pc:
+  bonus_scale: 0.1
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_acrobot.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_acrobot.yml
new file mode 100644
index 0000000..fe01e4a
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_acrobot.yml
@@ -0,0 +1,6 @@
+env:
+  id: Acrobot
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_ant.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_ant.yml
new file mode 100644
index 0000000..07d4c3e
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_ant.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Ant
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_ant_depth_search.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_ant_depth_search.yml
new file mode 100644
index 0000000..1d50248
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_ant_depth_search.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Ant
+runner:
+  max_steps: ENV_LENGTH
+plan:
+  max_steps: PLAN_LENGTH
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cartpole.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cartpole.yml
new file mode 100644
index 0000000..a608f7c
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cartpole.yml
@@ -0,0 +1,6 @@
+env:
+  id: CartPole
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cartpoleO001.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cartpoleO001.yml
new file mode 100644
index 0000000..f83770b
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cartpoleO001.yml
@@ -0,0 +1,6 @@
+env:
+  id: CartPoleO001
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cartpoleO01.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cartpoleO01.yml
new file mode 100644
index 0000000..3f7674f
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cartpoleO01.yml
@@ -0,0 +1,6 @@
+env:
+  id: CartPoleO01
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetah.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetah.yml
new file mode 100644
index 0000000..ae2c095
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetah.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetah
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahA003.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahA003.yml
new file mode 100644
index 0000000..a87b6be
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahA003.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetahA003
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahA01.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahA01.yml
new file mode 100644
index 0000000..5312055
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahA01.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetahA01
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahO001.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahO001.yml
new file mode 100644
index 0000000..960e36e
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahO001.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetahO001
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahO01.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahO01.yml
new file mode 100644
index 0000000..4a5d67d
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetahO01.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetahO01
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetah_depth_search.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetah_depth_search.yml
new file mode 100644
index 0000000..c147ec4
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_cheetah_depth_search.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetah
+runner:
+  max_steps: ENV_LENGTH
+plan:
+  max_steps: PLAN_LENGTH
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_fant.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_fant.yml
new file mode 100644
index 0000000..6773c7b
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_fant.yml
@@ -0,0 +1,6 @@
+env:
+  id: FixedAnt
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_fhopper.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_fhopper.yml
new file mode 100644
index 0000000..2efc44e
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_fhopper.yml
@@ -0,0 +1,6 @@
+env:
+  id: FixedHopper
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_fswimmer.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_fswimmer.yml
new file mode 100644
index 0000000..888b87b
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_fswimmer.yml
@@ -0,0 +1,6 @@
+env:
+  id: FixedSwimmer
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_fwalker2d.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_fwalker2d.yml
new file mode 100644
index 0000000..ee63b73
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_fwalker2d.yml
@@ -0,0 +1,6 @@
+env:
+  id: FixedWalker
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_hopper.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_hopper.yml
new file mode 100644
index 0000000..a061802
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_hopper.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Hopper
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_humanoid.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_humanoid.yml
new file mode 100644
index 0000000..426bdfb
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_humanoid.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: gym_humanoid
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_invertedPendulum.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_invertedPendulum.yml
new file mode 100644
index 0000000..d1062d9
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_invertedPendulum.yml
@@ -0,0 +1,6 @@
+env:
+  id: InvertedPendulum
+runner:
+  max_steps: 100
+plan:
+  max_steps: 100
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_mountain.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_mountain.yml
new file mode 100644
index 0000000..6c335d5
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_mountain.yml
@@ -0,0 +1,11 @@
+env:
+  id: MountainCar
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
+pc:
+  bonus_scale: 2
+  bonus_stop_time: 50
+rollout:
+  n_train_samples: 1000
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_nostopslimhumanoid.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_nostopslimhumanoid.yml
new file mode 100644
index 0000000..dabe168
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_nostopslimhumanoid.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: gym_nostopslimhumanoid
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_pendulum.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_pendulum.yml
new file mode 100644
index 0000000..c0caff5
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_pendulum.yml
@@ -0,0 +1,6 @@
+env:
+  id: Pendulum
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_pendulumO001.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_pendulumO001.yml
new file mode 100644
index 0000000..1ebd722
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_pendulumO001.yml
@@ -0,0 +1,6 @@
+env:
+  id: PendulumO001
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_pendulumO01.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_pendulumO01.yml
new file mode 100644
index 0000000..4658d7f
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_pendulumO01.yml
@@ -0,0 +1,6 @@
+env:
+  id: PendulumO01
+runner:
+  max_steps: 200
+plan:
+  max_steps: 200
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_reacher.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_reacher.yml
new file mode 100644
index 0000000..427ad36
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_reacher.yml
@@ -0,0 +1,6 @@
+env:
+  id: Reacher
+runner:
+  max_steps: 50
+plan:
+  max_steps: 50
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_slimhumanoid.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_slimhumanoid.yml
new file mode 100644
index 0000000..ecd0976
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_slimhumanoid.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: gym_slimhumanoid
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_swimmer.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_swimmer.yml
new file mode 100644
index 0000000..b9ba125
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_swimmer.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Swimmer
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_walker2d.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_walker2d.yml
new file mode 100644
index 0000000..8403547
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/gym_walker2d.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Walker2D
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/hand_egg.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/hand_egg.yml
new file mode 100644
index 0000000..d4f5160
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/hand_egg.yml
@@ -0,0 +1,8 @@
+env:
+  id: HandEgg
+runner:
+  max_steps: 100
+plan:
+  max_steps: 100
+pc:
+  bonus_scale: 1
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/hand_reach.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/hand_reach.yml
new file mode 100644
index 0000000..40ae08b
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/hand_reach.yml
@@ -0,0 +1,8 @@
+env:
+  id: HandReach
+runner:
+  max_steps: 50
+plan:
+  max_steps: 50
+pc:
+  bonus_scale: 0.5
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/point_fall.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/point_fall.yml
new file mode 100644
index 0000000..ac26219
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/point_fall.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: PointFall-v1
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/point_push.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/point_push.yml
new file mode 100644
index 0000000..22d4f65
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/point_push.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: PointPush-v1
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments05/ant_umaze_1234/src/configs/env_tingwu/point_umaze.yml b/experiments05/ant_umaze_1234/src/configs/env_tingwu/point_umaze.yml
new file mode 100644
index 0000000..b459aa6
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/env_tingwu/point_umaze.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: PointUMaze-v1
+runner:
+  max_steps: 1000
+plan:
+  max_steps: 1000
diff --git a/experiments05/ant_umaze_1234/src/configs/envs/acrobot.yml b/experiments05/ant_umaze_1234/src/configs/envs/acrobot.yml
new file mode 100644
index 0000000..4ca200d
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/envs/acrobot.yml
@@ -0,0 +1,4 @@
+env:
+  id: Acrobot
+runner:
+  max_steps: 200
\ No newline at end of file
diff --git a/experiments05/ant_umaze_1234/src/configs/envs/ant.yml b/experiments05/ant_umaze_1234/src/configs/envs/ant.yml
new file mode 100644
index 0000000..f24b431
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/envs/ant.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Ant
+runner:
+  max_steps: 1000
\ No newline at end of file
diff --git a/experiments05/ant_umaze_1234/src/configs/envs/cartpole.yml b/experiments05/ant_umaze_1234/src/configs/envs/cartpole.yml
new file mode 100644
index 0000000..9b49dbe
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/envs/cartpole.yml
@@ -0,0 +1,4 @@
+env:
+  id: CartPole
+runner:
+  max_steps: 200
\ No newline at end of file
diff --git a/experiments05/ant_umaze_1234/src/configs/envs/fswimmer.yml b/experiments05/ant_umaze_1234/src/configs/envs/fswimmer.yml
new file mode 100644
index 0000000..2aee16a
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/envs/fswimmer.yml
@@ -0,0 +1,4 @@
+env:
+  id: FixedSwimmer
+runner:
+  max_steps: 1000
\ No newline at end of file
diff --git a/experiments05/ant_umaze_1234/src/configs/envs/half_cheetah.yml b/experiments05/ant_umaze_1234/src/configs/envs/half_cheetah.yml
new file mode 100644
index 0000000..36cfffa
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/envs/half_cheetah.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetah
+runner:
+  max_steps: 1000
diff --git a/experiments05/ant_umaze_1234/src/configs/envs/half_cheetah_short.yml b/experiments05/ant_umaze_1234/src/configs/envs/half_cheetah_short.yml
new file mode 100644
index 0000000..e0e21ed
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/envs/half_cheetah_short.yml
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: HalfCheetah-v2
+plan:
+  max_steps: 200
+runner:
+  max_steps: 200
diff --git a/experiments05/ant_umaze_1234/src/configs/envs/hopper.yml b/experiments05/ant_umaze_1234/src/configs/envs/hopper.yml
new file mode 100644
index 0000000..7b34a11
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/envs/hopper.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Hopper
+runner:
+  max_steps: 1000
\ No newline at end of file
diff --git a/experiments05/ant_umaze_1234/src/configs/envs/humanoid.yml b/experiments05/ant_umaze_1234/src/configs/envs/humanoid.yml
new file mode 100644
index 0000000..1eb1236
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/envs/humanoid.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: POMDPHumanoid-v2
+runner:
+  max_steps: 500
diff --git a/experiments05/ant_umaze_1234/src/configs/envs/inverted_pendulum.yml b/experiments05/ant_umaze_1234/src/configs/envs/inverted_pendulum.yml
new file mode 100644
index 0000000..1d60732
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/envs/inverted_pendulum.yml
@@ -0,0 +1,4 @@
+env:
+  id: InvertedPendulum
+runner:
+  max_steps: 100
\ No newline at end of file
diff --git a/experiments05/ant_umaze_1234/src/configs/envs/mountain.yml b/experiments05/ant_umaze_1234/src/configs/envs/mountain.yml
new file mode 100644
index 0000000..e4cb7f6
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/envs/mountain.yml
@@ -0,0 +1,4 @@
+env:
+  id: MountainCar
+runner:
+  max_steps: 200
\ No newline at end of file
diff --git a/experiments05/ant_umaze_1234/src/configs/envs/pendulum.yml b/experiments05/ant_umaze_1234/src/configs/envs/pendulum.yml
new file mode 100644
index 0000000..8ea91c3
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/envs/pendulum.yml
@@ -0,0 +1,4 @@
+env:
+  id: Pendulum
+runner:
+  max_steps: 200
\ No newline at end of file
diff --git a/experiments05/ant_umaze_1234/src/configs/envs/reacher.yml b/experiments05/ant_umaze_1234/src/configs/envs/reacher.yml
new file mode 100644
index 0000000..bfaf65b
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/envs/reacher.yml
@@ -0,0 +1,4 @@
+env:
+  id: Reacher
+runner:
+  max_steps: 50
\ No newline at end of file
diff --git a/experiments05/ant_umaze_1234/src/configs/envs/swimmer.yml b/experiments05/ant_umaze_1234/src/configs/envs/swimmer.yml
new file mode 100644
index 0000000..abe2842
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/envs/swimmer.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Swimmer
+runner:
+  max_steps: 1000
\ No newline at end of file
diff --git a/experiments05/ant_umaze_1234/src/configs/envs/walker.yml b/experiments05/ant_umaze_1234/src/configs/envs/walker.yml
new file mode 100644
index 0000000..e42fa53
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/envs/walker.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+env:
+  id: Walker2D
+runner:
+  max_steps: 1000
diff --git a/experiments05/ant_umaze_1234/src/configs/multi_step/1.yml b/experiments05/ant_umaze_1234/src/configs/multi_step/1.yml
new file mode 100644
index 0000000..e102872
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/multi_step/1.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 1
+  dev_batch_size: 1024
+  train_batch_size: 256
diff --git a/experiments05/ant_umaze_1234/src/configs/multi_step/2.yml b/experiments05/ant_umaze_1234/src/configs/multi_step/2.yml
new file mode 100644
index 0000000..c866952
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/multi_step/2.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 2
+  dev_batch_size: 512
+  train_batch_size: 128
diff --git a/experiments05/ant_umaze_1234/src/configs/multi_step/4.yml b/experiments05/ant_umaze_1234/src/configs/multi_step/4.yml
new file mode 100644
index 0000000..16a1e5c
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/multi_step/4.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 4
+  dev_batch_size: 256
+  train_batch_size: 64
diff --git a/experiments05/ant_umaze_1234/src/configs/multi_step/8.yml b/experiments05/ant_umaze_1234/src/configs/multi_step/8.yml
new file mode 100644
index 0000000..7262945
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/configs/multi_step/8.yml
@@ -0,0 +1,5 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+model:
+  multi_step: 8
+  dev_batch_size: 128
+  train_batch_size: 32
diff --git a/experiments05/ant_umaze_1234/src/cpu_requirements.txt b/experiments05/ant_umaze_1234/src/cpu_requirements.txt
new file mode 100644
index 0000000..141cfbb
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/cpu_requirements.txt
@@ -0,0 +1,8 @@
+tensorflow==1.13.1
+pyyaml==5.1
+termcolor==1.1.0
+gym
+mujoco-py
+json_tricks==3.13.1
+baselines==0.1.5
+
diff --git a/experiments05/ant_umaze_1234/src/lunzi/Logger.py b/experiments05/ant_umaze_1234/src/lunzi/Logger.py
new file mode 100644
index 0000000..9f41014
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/lunzi/Logger.py
@@ -0,0 +1,133 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from termcolor import colored
+import datetime
+import sys
+import os
+from collections import Counter, defaultdict
+import json_tricks
+
+
+def a():
+    pass
+
+
+_srcfile = os.path.normcase(a.__code__.co_filename)
+
+
+class BaseSink(object):
+    @staticmethod
+    def _time():
+        return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S,%f')
+
+    def info(self, fmt, *args, **kwargs):
+        raise NotImplementedError
+
+    def warning(self, fmt, *args, **kwargs):
+        self.info(fmt, *args, **kwargs)
+
+    def verbose(self, fmt, *args, **kwargs):
+        pass
+
+
+class StdoutSink(BaseSink):
+    def __init__(self):
+        self.freq_count = Counter()
+
+    def info(self, fmt, *args, freq=1, caller=None):
+        if args:
+            fmt = fmt % args
+        self.freq_count[caller] += 1
+        if self.freq_count[caller] % freq == 0:
+            print("%s - %s - %s" % (colored(self._time(), 'green'),
+                                    colored(caller, 'cyan'), fmt), flush=True)
+
+    def warning(self, fmt, *args, **kwargs):
+        if args:
+            fmt = fmt % args
+        self.info(colored(fmt, 'yellow'), **kwargs)
+
+
+class FileSink(BaseSink):
+    def __init__(self, fn):
+        self.log_file = open(fn, 'w')
+        self.callers = {}
+
+    def info(self, fmt, *args, **kwargs):
+        self._kv(level='info', fmt=fmt, args=args, **kwargs)
+
+    def warning(self, fmt, *args, **kwargs):
+        self._kv(level='warning', fmt=fmt, args=args, **kwargs)
+
+    def _kv(self, **kwargs):
+        kwargs.update(time=datetime.datetime.now())
+        self.log_file.write(json_tricks.dumps(kwargs, primitives=True) + '\n')
+        self.log_file.flush()
+
+    def verbose(self, fmt, *args, **kwargs):
+        self._kv(level='verbose', fmt=fmt, args=args, **kwargs)
+
+
+class LibLogger(object):
+    logfile = ""
+
+    def __init__(self, name='logger', is_root=True):
+        self.name = name
+        self.is_root = is_root
+        self.tab_keys = None
+        self.sinks = []
+        self.key_prior = defaultdict(np.random.randn)
+
+    def add_sink(self, sink):
+        self.sinks.append(sink)
+
+    def info(self, fmt, *args, **kwargs):
+        caller = self.find_caller()
+        for sink in self.sinks:
+            sink.info(fmt, *args, caller=caller, **kwargs)
+
+    def warning(self, fmt, *args, **kwargs):
+        caller = self.find_caller()
+        for sink in self.sinks:
+            sink.warning(fmt, *args, caller=caller, **kwargs)
+
+    def verbose(self, fmt, *args, **kwargs):
+        caller = self.find_caller()
+        for sink in self.sinks:
+            sink.verbose(fmt, *args, caller=caller, **kwargs)
+
+    def find_caller(self):
+        """
+        Copy from `python.logging` module
+
+        Find the stack frame of the caller so that we can note the source
+        file name, line number and function name.
+        """
+        f = sys._getframe(1)
+        if f is not None:
+            f = f.f_back
+        caller = ''
+        while hasattr(f, "f_code"):
+            co = f.f_code
+            filename = os.path.normcase(co.co_filename)
+            if filename == _srcfile:
+                f = f.f_back
+                continue
+            # if stack_info:
+            #     sio = io.StringIO()
+            #     sio.write('Stack (most recent call last):\n')
+            #     traceback.print_stack(f, file=sio)
+            #     sio.close()
+            # rv = (co.co_filename, f.f_lineno, co.co_name, sinfo)
+            rel_path = os.path.relpath(co.co_filename, '')
+            caller = f'{rel_path}:{f.f_lineno}'
+            break
+        return caller
+
+
+def get_logger(name):
+    return LibLogger(name)
+
+
+logger = get_logger('Logger')
+logger.add_sink(StdoutSink())
diff --git a/experiments05/ant_umaze_1234/src/lunzi/__init__.py b/experiments05/ant_umaze_1234/src/lunzi/__init__.py
new file mode 100644
index 0000000..0c5417b
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/lunzi/__init__.py
@@ -0,0 +1,4 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from .stubs import Tensor
+import lunzi.nn
+import lunzi.Logger
diff --git a/experiments05/ant_umaze_1234/src/lunzi/config.py b/experiments05/ant_umaze_1234/src/lunzi/config.py
new file mode 100644
index 0000000..5c39b1e
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/lunzi/config.py
@@ -0,0 +1,97 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import argparse
+import os
+import yaml
+from lunzi.Logger import logger
+
+
+_frozen = False
+_initialized = False
+
+
+def expand(path):
+    return os.path.abspath(os.path.expanduser(path))
+
+
+class MetaFLAGS(type):
+    _initialized = False
+
+    def __setattr__(self, key, value):
+        assert not _frozen, 'Modifying FLAGS after dumping is not allowed!'
+        super().__setattr__(key, value)
+
+    def __getitem__(self, item):
+        return self.__dict__[item]
+
+    def __iter__(self):
+        for key, value in self.__dict__.items():
+            if not key.startswith('_') and not isinstance(value, classmethod):
+                if isinstance(value, MetaFLAGS):
+                    value = dict(value)
+                yield key, value
+
+    def as_dict(self):
+        return dict(self)
+
+    def merge(self, other: dict):
+        for key in other:
+            assert key in self.__dict__, f"Can't find key `{key}`"
+            if isinstance(self[key], MetaFLAGS) and isinstance(other[key], dict):
+                self[key].merge(other[key])
+            else:
+                setattr(self, key, other[key])
+
+    def set_value(self, path, value):
+        key, *rest = path
+        assert key in self.__dict__, f"Can't find key `{key}`"
+        if not rest:
+            setattr(self, key, value)
+        else:
+            self[key]: MetaFLAGS
+            self[key].set_value(rest, value)
+
+    @staticmethod
+    def set_frozen():
+        global _frozen
+        _frozen = True
+
+    def freeze(self):
+        for key, value in self.__dict__.items():
+            if not key.startswith('_'):
+                if isinstance(value, MetaFLAGS):
+                    value.freeze()
+        self.finalize()
+
+    def finalize(self):
+        pass
+
+
+class BaseFLAGS(metaclass=MetaFLAGS):
+    pass
+
+
+def parse(cls):
+    global _initialized
+
+    if _initialized:
+        return
+    parser = argparse.ArgumentParser(description='Stochastic Lower Bound Optimization')
+    parser.add_argument('-c', '--config', type=str, help='configuration file (YAML)', nargs='+', action='append')
+    parser.add_argument('-s', '--set', type=str, help='additional options', nargs='*', action='append')
+
+    args, unknown = parser.parse_known_args()
+    for a in unknown:
+        logger.info('unknown arguments: %s', a)
+    # logger.info('parsed arguments = %s, unknown arguments: %s', args, unknown)
+    if args.config:
+        for config in sum(args.config, []):
+            cls.merge(yaml.load(open(expand(config))))
+    else:
+        logger.info('no config file specified.')
+    if args.set:
+        for instruction in sum(args.set, []):
+            path, *value = instruction.split('=')
+            cls.set_value(path.split('.'), yaml.load('='.join(value)))
+
+    _initialized = True
+
diff --git a/experiments05/ant_umaze_1234/src/lunzi/dataset.py b/experiments05/ant_umaze_1234/src/lunzi/dataset.py
new file mode 100644
index 0000000..7cb5d5f
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/lunzi/dataset.py
@@ -0,0 +1,82 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+
+
+class Dataset(np.recarray):
+    """
+        Overallocation can be supported, by making examinations before
+        each `append` and `extend`.
+    """
+
+    @staticmethod
+    def fromarrays(array_lists, dtype):
+        array = np.rec.fromarrays(array_lists, dtype=dtype)
+        ret = Dataset(dtype, len(array))
+        ret.extend(array)
+        return ret
+
+    def __init__(self, dtype, max_size, verbose=False):
+        super().__init__()
+        self.max_size = max_size
+        self._index = 0
+        self._buf_size = 0
+        self._len = 0
+
+        self.resize(max_size)
+        self._buf_size = max_size
+
+    def __new__(cls, dtype, max_size):
+        return np.recarray.__new__(cls, max_size, dtype=dtype)
+
+    def size(self):
+        return self._len
+
+    def reserve(self, size):
+        cur_size = max(self._buf_size, 1)
+        while cur_size < size:
+            cur_size *= 2
+        if cur_size != self._buf_size:
+            self.resize(cur_size)
+
+    def clear(self):
+        self._index = 0
+        self._len = 0
+        return self
+
+    def append(self, item):
+        self[self._index] = item
+        self._index = (self._index + 1) % self.max_size
+        self._len = min(self._len + 1, self.max_size)
+        return self
+
+    def extend(self, items):
+        n_new = len(items)
+        if n_new > self.max_size:
+            items = items[-self.max_size:]
+            n_new = self.max_size
+
+        n_tail = self.max_size - self._index
+        if n_new <= n_tail:
+            self[self._index:self._index + n_new] = items
+        else:
+            n_head = n_new - n_tail
+            self[self._index:] = items[:n_tail]
+            self[:n_head] = items[n_tail:]
+
+        self._index = (self._index + n_new) % self.max_size
+        self._len = min(self._len + n_new, self.max_size)
+        return self
+
+    def sample(self, size, indices=None):
+        if indices is None:
+            indices = np.random.randint(0, self._len, size=size)
+        return self[indices]
+
+    def iterator(self, batch_size):
+        indices = np.arange(self._len, dtype=np.int32)
+        np.random.shuffle(indices)
+        index = 0
+        while index + batch_size <= self._len:
+            end = index + batch_size
+            yield self[indices[index:end]]
+            index = end
diff --git a/experiments05/ant_umaze_1234/src/lunzi/nn/__init__.py b/experiments05/ant_umaze_1234/src/lunzi/nn/__init__.py
new file mode 100644
index 0000000..2315809
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/lunzi/nn/__init__.py
@@ -0,0 +1,10 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from . import patch
+from .parameter import Parameter
+from .module import Module
+from .container import *
+from . import utils
+from .utils import make_method
+from .layers import *
+from .loss import *
+from .flat_param import FlatParam
diff --git a/experiments05/ant_umaze_1234/src/lunzi/nn/container.py b/experiments05/ant_umaze_1234/src/lunzi/nn/container.py
new file mode 100644
index 0000000..6dbafdc
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/lunzi/nn/container.py
@@ -0,0 +1,35 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import Dict, Any
+from .module import Module
+from .parameter import Parameter
+
+_dict_methods = ['__setitem__', '__getitem__', '__delitem__', '__len__', '__iter__', '__contains__',
+                 'update', 'keys', 'values', 'items', 'clear', 'pop']
+
+
+class ModuleDict(Module, dict):  # use dict for auto-complete
+    """
+        Essentially this exposes some methods of `Module._modules`.
+    """
+    def __init__(self, modules: Dict[Any, Module] = None):
+        super().__init__()
+        for method in _dict_methods:
+            setattr(self, method, getattr(self._modules, method))
+        if modules is not None:
+            self.update(modules)
+
+    def forward(self):
+        raise RuntimeError("ModuleDict is not callable")
+
+
+# Do we need a factory for it?
+class ParameterDict(Module, dict):
+    def __init__(self, parameters: Dict[Any, Parameter] = None):
+        super().__init__()
+        for method in _dict_methods:
+            setattr(self, method, getattr(self._modules, method))
+        if parameters is not None:
+            self.update(parameters)
+
+    def forward(self):
+        raise RuntimeError("ParameterDict is not callable")
diff --git a/experiments05/ant_umaze_1234/src/lunzi/nn/flat_param.py b/experiments05/ant_umaze_1234/src/lunzi/nn/flat_param.py
new file mode 100644
index 0000000..5c4bb52
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/lunzi/nn/flat_param.py
@@ -0,0 +1,34 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+from lunzi.Logger import logger
+from .module import Module
+from .utils import make_method, n_parameters, parameters_to_vector, vector_to_parameters
+
+
+class FlatParam(Module):
+    def __init__(self, parameters):
+        super().__init__()
+        self.params = parameters
+        self.op_feed_flat, self.op_set_flat, self.op_get_flat = \
+            self.enable_flat()
+
+    def enable_flat(self):
+        params = self.params
+        logger.info('Enabling flattening... %s', [p.name for p in params])
+        n_params = n_parameters(params)
+        feed_flat = tf.placeholder(tf.float32, [n_params])
+        get_flat = parameters_to_vector(params)
+        set_flat = tf.group(*[tf.assign(param, value) for param, value in
+                            zip(params, vector_to_parameters(feed_flat, params))])
+        return feed_flat, set_flat, get_flat
+
+    def forward(self):
+        return self.op_get_flat
+
+    @make_method(feed='feed_flat', fetch='set_flat')
+    def set_flat(self, feed_flat):
+        pass
+
+    @make_method(fetch='get_flat')
+    def get_flat(self):
+        pass
diff --git a/experiments05/ant_umaze_1234/src/lunzi/nn/layers.py b/experiments05/ant_umaze_1234/src/lunzi/nn/layers.py
new file mode 100644
index 0000000..a0cd9c2
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/lunzi/nn/layers.py
@@ -0,0 +1,88 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+import numpy as np
+from .module import Module
+from .parameter import Parameter
+
+
+class Linear(Module):
+    def __init__(self, in_features: int, out_features: int, bias=True, weight_initializer=None):
+        super().__init__()
+        self.in_features = in_features
+        self.out_features = out_features
+        if weight_initializer is None:
+            init_range = tf.sqrt(6.0 / (in_features + out_features))
+            weight_initializer = tf.random_uniform_initializer(-init_range, init_range, dtype=tf.float32)
+
+        self.use_bias = bias
+        with self.scope:
+            self.op_input = tf.placeholder(dtype=tf.float32, shape=[None, in_features], name='input')
+            self.weight = Parameter(weight_initializer([in_features, out_features],dtype=tf.float32), name='weight')
+            if bias:
+                self.bias = Parameter(tf.zeros([out_features], dtype=tf.float32), name='bias')
+
+        self.op_output = self(self.op_input)
+
+    def forward(self, x):
+        shape = x.get_shape().as_list()
+        if len(shape) > 2:
+            y = tf.tensordot(x, self.weight, [[len(shape) - 1], [0]])
+        else:
+            y = x.matmul(self.weight)
+        if self.use_bias:
+            y = y + self.bias
+        return y
+
+    def fast(self, x):
+        x = x.dot(self.weight.numpy())
+        if self.use_bias:
+            x = x + self.bias.numpy()
+        return x
+
+    def extra_repr(self):
+        return f'in_features={self.in_features}, out_features={self.out_features}, bias={self.use_bias}'
+
+
+class Sequential(Module):
+    def __init__(self, *modules):
+        super().__init__()
+        for i, module in enumerate(modules):
+            self._modules[i] = module
+
+    def forward(self, x):
+        for module in self._modules.values():
+            x = module(x)
+        return x
+
+    def fast(self, x):
+        for module in self._modules.values():
+            x = module.fast(x)
+        return x
+
+
+class ReLU(Module):
+    def forward(self, x):
+        return tf.nn.relu(x)
+
+    def fast(self, x: np.ndarray):
+        return np.maximum(x, 0)
+
+
+class Tanh(Module):
+    def forward(self, x):
+        return tf.nn.tanh(x)
+
+    def fast(self, x: np.ndarray):
+        return np.tanh(x)
+
+
+class Squeeze(Module):
+    def __init__(self, axis=None):
+        super().__init__()
+        self._axis = axis
+
+    def forward(self, x):
+        return x.squeeze(axis=self._axis)
+
+    def fast(self, x):
+        return x.squeeze(axis=self._axis)
diff --git a/experiments05/ant_umaze_1234/src/lunzi/nn/loss.py b/experiments05/ant_umaze_1234/src/lunzi/nn/loss.py
new file mode 100644
index 0000000..26e662b
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/lunzi/nn/loss.py
@@ -0,0 +1,40 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from lunzi import Tensor
+from .module import Module
+
+
+class PointwiseLoss(Module):
+    def __init__(self, size_average=True, reduce=True):
+        super().__init__()
+        self.size_average = size_average
+        self.reduce = reduce
+
+    def pointwise(self, output: Tensor, target: Tensor):
+        raise NotImplementedError
+
+    def forward(self, output: Tensor, target: Tensor, input: Tensor = None):
+        loss = self.pointwise(output, target)
+        if self.reduce and len(loss.shape) > 1:
+            if self.size_average:
+                loss = loss.reduce_mean(axis=1)
+            else:
+                loss = loss.reduce_sum(axis=1)
+        return loss
+
+
+class L1Loss(PointwiseLoss):
+    def pointwise(self, output: Tensor, target: Tensor):
+        return output.sub(target).abs()
+
+
+class L2Loss(PointwiseLoss):
+    def pointwise(self, output: Tensor, target: Tensor):
+        return output.sub(target).pow(2)
+
+    def forward(self, output: Tensor, target: Tensor, input: Tensor = None):
+        return super().forward(output, target).sqrt()
+
+
+class MSELoss(PointwiseLoss):
+    def pointwise(self, output: Tensor, target: Tensor):
+        return output.sub(target).pow(2)
diff --git a/experiments05/ant_umaze_1234/src/lunzi/nn/module.py b/experiments05/ant_umaze_1234/src/lunzi/nn/module.py
new file mode 100644
index 0000000..8eacf3b
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/lunzi/nn/module.py
@@ -0,0 +1,158 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import Dict, Any, Callable, List
+from collections import Counter
+import tensorflow as tf
+import numpy as np
+
+from lunzi import Tensor
+from lunzi.Logger import logger
+from .parameter import Parameter
+
+
+class Module(object):
+    """
+        A front-end for TensorFlow, heavily inspired by PyTorch's design and implementation.
+
+        Deepcopy is not supported since I didn't find a good way to duplicate `tf.Variables` and `tf.variable_scope`.
+    """
+
+    # To generate unique name scope
+    # The only reason we keep variable scope here is that we want the variables have meaning names,
+    # since the internal operations always look messy, I put no hope maintaining their names,
+    # So let's just do it for variables.
+    prefix_count = Counter()
+
+    @staticmethod
+    def _create_uid(prefix: str) -> str:
+        scope = tf.get_variable_scope().name + '/'
+        uid = Module.prefix_count[scope + prefix]
+        Module.prefix_count[scope + prefix] += 1
+        if uid == 0:
+            return prefix
+        return f'{prefix}_{uid}'
+
+    def __init__(self):
+        scope = Module._create_uid(self.__class__.__name__)
+        with tf.variable_scope(scope, reuse=False) as self._scope:
+            pass
+        # Since we only plan to support Python 3.6+, in which dict is already ordered, we don't use OrderedDict here.
+        self._parameters: Dict[Any, Parameter] = {}
+        self._modules: Dict[Any, Module] = {}
+        self._callables: Dict[Any, Callable] = {}
+
+    def forward(self, *args: List[Any], **kwargs: Dict[str, Any]) -> Tensor:
+        raise NotImplementedError
+
+    def fast(self, *args, **kwargs):
+        pass
+
+    def __setattr__(self, key, value):
+        # dynamically maintain sub modules.
+        modules = self.__dict__.get('_modules')
+        if isinstance(value, Parameter):
+            self._parameters[key] = value
+        if isinstance(value, Module):
+            assert modules is not None, 'Call `super().__init__` before assigning modules'
+            modules[key] = value
+        else:
+            if modules and key in modules:
+                del modules[key]
+        object.__setattr__(self, key, value)
+
+    def __call__(self, *args, **kwargs):
+        return self.forward(*args, **kwargs)
+
+    def register_callable(self, key, callable):
+        self._callables[key] = callable
+
+    def eval(self, fetch: str, **feed: Dict[str, np.ndarray]):
+        cache_key = f'[{" ".join(feed.keys())}] => [{fetch}]'
+        if cache_key not in self._callables:
+            logger.info('[%s] is making TensorFlow callables, key = %s', self.__class__.__name__, cache_key)
+            feed_ops = []
+            for key in feed.keys():
+                feed_ops.append(self.__dict__['op_' + key])
+            if isinstance(fetch, str):
+                fetch_ops = [self.__dict__['op_' + key] for key in fetch.split(' ')]
+                if len(fetch_ops) == 1:
+                    fetch_ops = fetch_ops[0]
+            else:
+                fetch_ops = fetch
+            self.register_callable(cache_key, tf.get_default_session().make_callable(fetch_ops, feed_ops))
+        return self._callables[cache_key](*feed.values())
+
+    def parameters(self, trainable=True, non_trainable=False, recursive=True, out=None) -> List[Parameter]:
+        """
+            We don't introduce `buffers` here. PyTorch has it since it doesn't have non-trainable Parameter.
+            A tensor in `buffers` is essentially a non-trainable Parameter (part of state_dict but isn't
+            optimized over).
+        """
+        if out is None:
+            out = []
+        for param in self._parameters.values():
+            if param.trainable and trainable or not param.trainable and non_trainable:
+                out.append(param)
+        if recursive:
+            for module in self._modules.values():
+                module.parameters(trainable=trainable, non_trainable=non_trainable, recursive=True, out=out)
+        # probably we don't need to sort since we're using `OrderedDict`
+        return out
+
+    @property
+    def scope(self) -> tf.variable_scope:
+        return tf.variable_scope(self._scope, reuse=tf.AUTO_REUSE)
+
+    def extra_repr(self) -> str:
+        return ''
+
+    def named_modules(self) -> dict:
+        return self._modules
+
+    def __repr__(self):
+        def dfs(node, prefix):
+            root_info = node.__class__.__name__
+            modules = node.named_modules()
+            if not modules:
+                return root_info + f'({node.extra_repr()})'
+
+            root_info += '(\n'
+            for key, module in modules.items():
+                module_repr = dfs(module, prefix + '    ')
+                root_info += f'{prefix}    ({key}): {module_repr}\n'
+            root_info += prefix + ')'
+            return root_info
+        return dfs(self, '')
+
+    def state_dict(self, recursive=True):
+        """
+            A better option is to find all parameters and then sess.run(state) but I assume this can't be the
+            bottleneck.
+        """
+        state = {}
+        for key, parameter in self._parameters.items():
+            # although we can use `.numpy()` here, for safety I'd use `.eval()`
+            state[key] = parameter.eval()
+        if recursive:
+            for key, module in self._modules.items():
+                state[key] = module.state_dict()
+        return state
+
+    def load_state_dict(self, state_dict: Dict[Any, Any], recursive=True, strict=True):
+        for key, parameter in self._parameters.items():
+            if key in state_dict:
+                parameter.load(state_dict[key])
+                parameter.invalidate()
+            else:
+                assert not strict, f'Missing Parameter {key} in state_dict'
+        if recursive:
+            for key, module in self._modules.items():
+                if key in state_dict:
+                    module.load_state_dict(state_dict[key], recursive=recursive, strict=strict)
+                else:
+                    assert not strict, f'Missing Module {key} in state_dict.'
+
+    def apply(self, fn):
+        for module in self._modules.values():
+            module.apply(fn)
+        fn(self)
+        return self
diff --git a/experiments05/ant_umaze_1234/src/lunzi/nn/parameter.py b/experiments05/ant_umaze_1234/src/lunzi/nn/parameter.py
new file mode 100644
index 0000000..969d79f
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/lunzi/nn/parameter.py
@@ -0,0 +1,24 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+from lunzi import Tensor
+
+
+def numpy(self):
+    if self.__dict__.get('_numpy_cache', None) is None:
+        self._numpy_cache: Tensor = self.eval()
+    return self._numpy_cache
+
+
+def invalidate(self):
+    self._numpy_cache = None
+
+
+# Q: Why not inherit from `tf.Variable`?
+# A: Since TensorFlow 1.11, `tf.Variable` has a meta class VariableMetaClass, which overrides `__call__`.
+#    And it's `_variable_call` function doesn't explicitly call `tf.Variable` so the return value must
+#    be a `tf.Variable`, which makes inheritance impossible.
+Parameter = tf.Variable
+
+Parameter.numpy = numpy
+Parameter.invalidate = invalidate
+
diff --git a/experiments05/ant_umaze_1234/src/lunzi/nn/patch.py b/experiments05/ant_umaze_1234/src/lunzi/nn/patch.py
new file mode 100644
index 0000000..5db96d8
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/lunzi/nn/patch.py
@@ -0,0 +1,60 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+
+from lunzi.Logger import logger
+
+
+def find_monkey_patch_keys(avoid_set=None):
+    if avoid_set is None:
+        avoid_set = {"shape"}  # tf.shape conflicts with Tensor.shape
+    patched = []
+    for key, value in tf.__dict__.items():
+        if not callable(value) or key in avoid_set:
+            continue
+        doc = value.__doc__
+        if doc is None:
+            continue
+        loc = doc.find('Args:\n')
+        if loc == -1:
+            continue
+
+        # Am I doing NLP?
+        # It seems that PyTorch has better doc. They always write `x (Tensor): ...` which is much easier to parse.
+        first_arg_doc = doc[loc + 6:].split('\n')[0].split(': ')[1]
+        if first_arg_doc.startswith('A `Tensor`') or first_arg_doc.startswith('`Tensor`') or key.startswith('reduce_'):
+            patched.append(key)
+    logger.warning(f'Monkey patched TensorFlow: {patched}')
+    return patched
+
+
+def monkey_patch(avoid_set=None):
+    logger.warning('Monkey patching TensorFlow...')
+
+    patched = ['abs', 'acos', 'acosh', 'add', 'angle', 'argmax', 'argmin', 'asin', 'asinh', 'atan', 'atan2', 'atanh',
+            'betainc', 'cast', 'ceil', 'check_numerics', 'clip_by_average_norm', 'clip_by_norm', 'clip_by_value',
+            'complex', 'conj', 'cos', 'cosh', 'cross', 'cumprod', 'cumsum', 'dequantize', 'diag', 'digamma', 'div',
+            'equal', 'erf', 'erfc', 'exp', 'expand_dims', 'expm1', 'fill', 'floor', 'floor_div', 'floordiv', 'floormod',
+            'gather', 'gather_nd', 'greater', 'greater_equal', 'hessians', 'identity', 'igamma', 'igammac', 'imag',
+            'is_finite', 'is_inf', 'is_nan', 'less', 'less_equal', 'lgamma', 'log', 'log1p', 'logical_and',
+            'logical_not', 'logical_or', 'matmul', 'maximum', 'meshgrid', 'minimum', 'mod', 'multiply', 'negative',
+            'norm', 'not_equal', 'one_hot', 'ones_like', 'pad', 'polygamma', 'pow', 'quantize', 'real', 'realdiv',
+            'reciprocal', 'reduce_all', 'reduce_any', 'reduce_logsumexp', 'reduce_max', 'reduce_mean', 'reduce_min',
+            'reduce_prod', 'reduce_sum', 'reshape', 'reverse', 'rint', 'round', 'rsqrt', 'scatter_nd', 'sign', 'sin',
+            'sinh', 'size', 'slice', 'sqrt', 'square', 'squeeze', 'stop_gradient', 'subtract', 'tan', 'tensordot',
+            'tile', 'to_bfloat16', 'to_complex128', 'to_complex64', 'to_double', 'to_float', 'to_int32', 'to_int64',
+            'transpose', 'truediv', 'truncatediv', 'truncatemod', 'unique', 'where', 'zeros_like', 'zeta']
+    alias = {
+        'mul': 'multiply',
+        'sub': 'subtract',
+    }
+
+    # use the code below for more ops
+    # patched = find_monkey_patch_keys(avoid_set)
+
+    for key, method in list(zip(patched, patched)) + list(alias.items()):
+        value = tf.__dict__[method]
+        setattr(tf.Tensor, key, value)
+        setattr(tf.Variable, key, value)
+
+
+monkey_patch()
diff --git a/experiments05/ant_umaze_1234/src/lunzi/nn/utils.py b/experiments05/ant_umaze_1234/src/lunzi/nn/utils.py
new file mode 100644
index 0000000..9fa5707
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/lunzi/nn/utils.py
@@ -0,0 +1,85 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import Callable, List, Union
+import inspect
+from functools import wraps
+import tensorflow as tf
+import numpy as np
+from lunzi import Tensor
+
+from .parameter import Parameter
+
+
+def make_method(feed: str = None, fetch: str = ''):
+    """
+        The following code:
+
+            @make_method('. w', fetch='d)
+            def func(a, c):
+                pass
+
+        will be converted to
+
+            def func(a, c, fetch='d'):
+                return self.eval(fetch, a=a, w=c)
+
+        Note that `func(1, c=2, b=1)` is also supported. This is
+        useful when writing PyTorch-like object method.
+
+    """
+
+    def decorator(func: Callable):
+        arg_names = inspect.signature(func).parameters.keys()
+        arg_map = {}
+        if feed is None:
+            arg_map = {op_name: op_name for op_name in arg_names if op_name != 'self'}
+        else:
+            feeds = ['-'] + feed.split(' ')  # ignore first `self`
+            for op_name, arg_name in zip(feeds, arg_names):
+                if op_name == '.':
+                    arg_map[op_name] = op_name
+                elif op_name != '-':  # deprecated
+                    arg_map[op_name] = arg_name
+
+        @wraps(func)
+        def wrapper(self, *args, **kwargs):
+            cur_fetch = kwargs.pop('fetch', fetch)
+            call_args = inspect.getcallargs(func, self, *args, **kwargs)
+            feed_dict = {op_name: call_args[arg_name] for op_name, arg_name in arg_map.items()}
+            return self.eval(cur_fetch, **feed_dict)
+
+        return wrapper
+
+    return decorator
+
+
+def n_parameters(params: List[Parameter]) -> int:
+    return sum([np.prod(p.shape) for p in params])
+
+
+def parameters_to_vector(parameters: List[Union[Parameter, Tensor]]) -> Tensor:
+    return tf.concat([param.reshape([-1]) for param in parameters], axis=0)
+
+
+def vector_to_parameters(vec: Tensor, parameters: List[Parameter]) -> List[Tensor]:
+    params: List[Tensor] = []
+    start = 0
+    for p in parameters:
+        end = start + np.prod(p.shape)
+        params.append(vec[start:end].reshape(p.shape))
+        start = end
+    return params
+
+
+def hessian_vec_prod(ys: Tensor, xs: List[Parameter], vs: Tensor) -> Tensor:
+    grad = parameters_to_vector(tf.gradients(ys, xs))
+    aux = (grad * vs).reduce_sum()
+    return parameters_to_vector(tf.gradients(aux, xs))
+
+
+# credit to https://github.com/renmengye/tensorflow-forward-ad/issues/2#issue-234418055
+def jacobian_vec_prod(ys: Tensor, xs: List[Parameter], vs: Tensor) -> Tensor:
+    u = tf.zeros_like(ys)  # dummy variable
+    grad = tf.gradients(ys, xs, grad_ys=u)
+    return tf.gradients(grad, u, grad_ys=vs)
+
+
diff --git a/experiments05/ant_umaze_1234/src/lunzi/stubs.py b/experiments05/ant_umaze_1234/src/lunzi/stubs.py
new file mode 100644
index 0000000..644c477
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/lunzi/stubs.py
@@ -0,0 +1,6 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+
+
+class Tensor(tf.Tensor):
+    pass
\ No newline at end of file
diff --git a/experiments05/ant_umaze_1234/src/lunzi/stubs.pyi b/experiments05/ant_umaze_1234/src/lunzi/stubs.pyi
new file mode 100644
index 0000000..7bab814
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/lunzi/stubs.pyi
@@ -0,0 +1,138 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+
+class Tensor(tf.Tensor):
+    def abs(x, name=None) -> Tensor: ...
+    def acos(x, name=None) -> Tensor: ...
+    def acosh(x, name=None) -> Tensor: ...
+    def add(x, y, name=None) -> Tensor: ...
+    def angle(input, name=None) -> Tensor: ...
+    def argmax(input, axis=None, name=None, dimension=None, output_type=tf.int64) -> Tensor: ...
+    def argmin(input, axis=None, name=None, dimension=None, output_type=tf.int64) -> Tensor: ...
+    def asin(x, name=None) -> Tensor: ...
+    def asinh(x, name=None) -> Tensor: ...
+    def atan(x, name=None) -> Tensor: ...
+    def atan2(y, x, name=None) -> Tensor: ...
+    def atanh(x, name=None) -> Tensor: ...
+    def betainc(a, b, x, name=None) -> Tensor: ...
+    def cast(x, dtype, name=None) -> Tensor: ...
+    def ceil(x, name=None) -> Tensor: ...
+    def check_numerics(tensor, message, name=None) -> Tensor: ...
+    def clip_by_average_norm(t, clip_norm, name=None) -> Tensor: ...
+    def clip_by_norm(t, clip_norm, axes=None, name=None) -> Tensor: ...
+    def clip_by_value(t, clip_value_min, clip_value_max, name=None) -> Tensor: ...
+    def complex(real, imag, name=None) -> Tensor: ...
+    def conj(x, name=None) -> Tensor: ...
+    def cos(x, name=None) -> Tensor: ...
+    def cosh(x, name=None) -> Tensor: ...
+    def cross(a, b, name=None) -> Tensor: ...
+    def cumprod(x, axis=0, exclusive=False, reverse=False, name=None) -> Tensor: ...
+    def cumsum(x, axis=0, exclusive=False, reverse=False, name=None) -> Tensor: ...
+    def dequantize(input, min_range, max_range, mode='MIN_COMBINED', name=None) -> Tensor: ...
+    def diag(diagonal, name=None) -> Tensor: ...
+    def digamma(x, name=None) -> Tensor: ...
+    def div(x, y, name=None) -> Tensor: ...
+    def equal(x, y, name=None) -> Tensor: ...
+    def erf(x, name=None) -> Tensor: ...
+    def erfc(x, name=None) -> Tensor: ...
+    def exp(x, name=None) -> Tensor: ...
+    def expand_dims(input, axis=None, name=None, dim=None) -> Tensor: ...
+    def expm1(x, name=None) -> Tensor: ...
+    def fill(dims, value, name=None) -> Tensor: ...
+    def floor(x, name=None) -> Tensor: ...
+    def floor_div(x, y, name=None) -> Tensor: ...
+    def floordiv(x, y, name=None) -> Tensor: ...
+    def floormod(x, y, name=None) -> Tensor: ...
+    def gather(params, indices, validate_indices=None, name=None, axis=0) -> Tensor: ...
+    def gather_nd(params, indices, name=None) -> Tensor: ...
+    def greater(x, y, name=None) -> Tensor: ...
+    def greater_equal(x, y, name=None) -> Tensor: ...
+    def hessians(ys, xs, name='hessians', colocate_gradients_with_ops=False, gate_gradients=False, aggregation_method=None) -> Tensor: ...
+    def identity(input, name=None) -> Tensor: ...
+    def igamma(a, x, name=None) -> Tensor: ...
+    def igammac(a, x, name=None) -> Tensor: ...
+    def imag(input, name=None) -> Tensor: ...
+    def is_finite(x, name=None) -> Tensor: ...
+    def is_inf(x, name=None) -> Tensor: ...
+    def is_nan(x, name=None) -> Tensor: ...
+    def less(x, y, name=None) -> Tensor: ...
+    def less_equal(x, y, name=None) -> Tensor: ...
+    def lgamma(x, name=None) -> Tensor: ...
+    def log(x, name=None) -> Tensor: ...
+    def log1p(x, name=None) -> Tensor: ...
+    def logical_and(x, y, name=None) -> Tensor: ...
+    def logical_not(x, name=None) -> Tensor: ...
+    def logical_or(x, y, name=None) -> Tensor: ...
+    def matmul(a, b, transpose_a=False, transpose_b=False, adjoint_a=False, adjoint_b=False, a_is_sparse=False, b_is_sparse=False, name=None) -> Tensor: ...
+    def maximum(x, y, name=None) -> Tensor: ...
+    def meshgrid(*args, **kwargs) -> Tensor: ...
+    def minimum(x, y, name=None) -> Tensor: ...
+    def mod(x, y, name=None) -> Tensor: ...
+    def mul(x, y, name=None) -> Tensor: ...
+    def multiply(x, y, name=None) -> Tensor: ...
+    def negative(x, name=None) -> Tensor: ...
+    def norm(tensor, ord='euclidean', axis=None, keepdims=None, name=None) -> Tensor: ...
+    def not_equal(x, y, name=None) -> Tensor: ...
+    def one_hot(indices, depth, on_value=None, off_value=None, axis=None, dtype=None, name=None) -> Tensor: ...
+    def ones_like(tensor, dtype=None, name=None, optimize=True) -> Tensor: ...
+    def pad(tensor, paddings, mode='CONSTANT', name=None, constant_values=0) -> Tensor: ...
+    def polygamma(a, x, name=None) -> Tensor: ...
+    def pow(x, y, name=None) -> Tensor: ...
+    def quantize(input, min_range, max_range, T, mode='MIN_COMBINED', round_mode='HALF_AWAY_FROM_ZERO', name=None) -> Tensor: ...
+    def real(input, name=None) -> Tensor: ...
+    def realdiv(x, y, name=None) -> Tensor: ...
+    def reciprocal(x, name=None) -> Tensor: ...
+    def reduce_all(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_any(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_logsumexp(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_max(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_mean(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_min(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_prod(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_sum(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reshape(tensor, shape, name=None) -> Tensor: ...
+    def reverse(tensor, axis, name=None) -> Tensor: ...
+    def rint(x, name=None) -> Tensor: ...
+    def round(x, name=None) -> Tensor: ...
+    def rsqrt(x, name=None) -> Tensor: ...
+    def scatter_nd(indices, updates, shape, name=None) -> Tensor: ...
+    def sign(x, name=None) -> Tensor: ...
+    def sin(x, name=None) -> Tensor: ...
+    def sinh(x, name=None) -> Tensor: ...
+    def size(input, name=None, out_type=tf.int32) -> Tensor: ...
+    def slice(input_, begin, size, name=None) -> Tensor: ...
+    def sqrt(x, name=None) -> Tensor: ...
+    def square(x, name=None) -> Tensor: ...
+    def squeeze(input, axis=None, name=None, squeeze_dims=None) -> Tensor: ...
+    def stop_gradient(input, name=None) -> Tensor: ...
+    def sub(x, y, name=None) -> Tensor: ...
+    def subtract(x, y, name=None) -> Tensor: ...
+    def tan(x, name=None) -> Tensor: ...
+    def tensordot(a, b, axes, name=None) -> Tensor: ...
+    def tile(input, multiples, name=None) -> Tensor: ...
+    def to_bfloat16(x, name='ToBFloat16') -> Tensor: ...
+    def to_complex128(x, name='ToComplex128') -> Tensor: ...
+    def to_complex64(x, name='ToComplex64') -> Tensor: ...
+    def to_double(x, name='ToDouble') -> Tensor: ...
+    def to_float(x, name='ToFloat') -> Tensor: ...
+    def to_int32(x, name='ToInt32') -> Tensor: ...
+    def to_int64(x, name='ToInt64') -> Tensor: ...
+    def transpose(a, perm=None, name='transpose', conjugate=False) -> Tensor: ...
+    def truediv(x, y, name=None) -> Tensor: ...
+    def truncatediv(x, y, name=None) -> Tensor: ...
+    def truncatemod(x, y, name=None) -> Tensor: ...
+    def unique(x, out_idx=tf.int32, name=None) -> Tensor: ...
+    def where(condition, x=None, y=None, name=None) -> Tensor: ...
+    def zeros_like(tensor, dtype=None, name=None, optimize=True) -> Tensor: ...
+    def zeta(x, q, name=None) -> Tensor: ...
+
+    def __add__(self, other) -> Tensor: ...
+    def __sub__(self, other) -> Tensor: ...
+    def __mul__(self, other) -> Tensor: ...
+    def __rdiv__(self, other) -> Tensor: ...
+    def __itruediv__(self, other) -> Tensor: ...
+    def __rsub__(self, other) -> Tensor: ...
+    def __isub__(self, other) -> Tensor: ...
+    def __imul__(self, other) -> Tensor: ...
+    def __rmul__(self, other) -> Tensor: ...
+    def __radd__(self, other) -> Tensor: ...
\ No newline at end of file
diff --git a/experiments05/ant_umaze_1234/src/main.py b/experiments05/ant_umaze_1234/src/main.py
new file mode 100644
index 0000000..38aafb2
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/main.py
@@ -0,0 +1,232 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import pickle
+from collections import deque
+import tensorflow as tf
+import numpy as np
+from tqdm import tqdm
+
+import lunzi.nn as nn
+from lunzi.Logger import logger
+from slbo.utils.average_meter import AverageMeter
+from slbo.utils.flags import FLAGS
+from slbo.utils.dataset import Dataset, gen_dtype
+from slbo.utils.OU_noise import OUNoise
+from slbo.utils.normalizer import Normalizers
+from slbo.utils.tf_utils import get_tf_config
+from slbo.utils.runner import Runner
+from slbo.policies.gaussian_mlp_policy import GaussianMLPPolicy
+from slbo.envs.virtual_env import VirtualEnv
+from slbo.dynamics_model import DynamicsModel
+from slbo.v_function.mlp_v_function import MLPVFunction
+from slbo.partial_envs import make_env
+from slbo.loss.multi_step_loss import MultiStepLoss
+from slbo.algos.TRPO import TRPO
+from slbo.random_net import RandomNet
+
+
+def evaluate(settings, tag):
+    return_means = []
+    for runner, policy, name in settings:
+        runner.reset()
+        _, ep_infos = runner.run(policy, FLAGS.rollout.n_test_samples)
+        if name == 'Real Env':
+            returns = np.array([ep_info['success'] for ep_info in ep_infos])
+        else:
+            returns = np.array([ep_info['return'] for ep_info in ep_infos])
+        logger.info('Tag = %s, Reward on %s (%d episodes): mean = %.6f, std = %.6f', tag, name,
+                    len(returns), np.mean(returns), np.std(returns))
+
+        return_means.append(np.mean(returns))
+
+    return return_means
+
+
+
+def add_multi_step(src: Dataset, dst: Dataset):
+    n_envs = 1
+    dst.extend(src[:-n_envs])
+
+    ending = src[-n_envs:].copy()
+    ending.timeout = True
+    dst.extend(ending)
+
+
+def make_real_runner(n_envs):
+    from slbo.envs.batched_env import BatchedEnv
+    batched_env = BatchedEnv([make_env(FLAGS.env.id) for _ in range(n_envs)])
+    return Runner(batched_env, rescale_action=True, **FLAGS.runner.as_dict())
+
+
+def main():
+    FLAGS.set_seed()
+    FLAGS.freeze()
+
+    env = make_env(FLAGS.env.id)
+    dim_state = int(np.prod(env.observation_space.shape))
+    dim_action = int(np.prod(env.action_space.shape))
+
+    env.verify()
+
+    normalizers = Normalizers(dim_action=dim_action, dim_state=dim_state)
+
+    dtype = gen_dtype(env, 'state action next_state reward done timeout')
+    train_set = Dataset(dtype, FLAGS.rollout.max_buf_size)
+    dev_set = Dataset(dtype, FLAGS.rollout.max_buf_size)
+
+    policy = GaussianMLPPolicy(dim_state, dim_action, normalizer=normalizers.state, **FLAGS.policy.as_dict())
+    # batched noises
+    #noise = OUNoise(env.action_space, theta=FLAGS.OUNoise.theta, sigma=FLAGS.OUNoise.sigma, shape=(1, dim_action))
+    vfn = MLPVFunction(dim_state, [64, 64], normalizers.state)
+    model = DynamicsModel(dim_state, dim_action, normalizers, FLAGS.model.hidden_sizes)
+    random_net = RandomNet(dim_state, dim_action, normalizers, FLAGS.model.hidden_sizes)
+
+    virt_env = VirtualEnv(model, make_env(FLAGS.env.id), random_net, FLAGS.plan.n_envs,FLAGS.model.hidden_sizes[-1], 
+                            FLAGS.pc.bonus_scale,FLAGS.pc.lamb, opt_model=FLAGS.slbo.opt_model)
+    virt_runner = Runner(virt_env, **{**FLAGS.runner.as_dict(), 'max_steps': FLAGS.plan.max_steps})
+
+    criterion_map = {
+        'L1': nn.L1Loss(),
+        'L2': nn.L2Loss(),
+        'MSE': nn.MSELoss(),
+    }
+    criterion = criterion_map[FLAGS.model.loss]
+    loss_mod = MultiStepLoss(model, normalizers, dim_state, dim_action, criterion, FLAGS.model.multi_step)
+    loss_mod.build_backward(FLAGS.model.lr, FLAGS.model.weight_decay)
+    algo = TRPO(vfn=vfn, policy=policy, dim_state=dim_state, dim_action=dim_action, **FLAGS.TRPO.as_dict())
+
+    tf.get_default_session().run(tf.global_variables_initializer())
+
+    runners = {
+        'test': make_real_runner(4),
+        'collect': make_real_runner(1),
+        'dev': make_real_runner(1),
+        'train': make_real_runner(FLAGS.plan.n_envs) if FLAGS.algorithm == 'MF' else virt_runner,
+    }
+    settings = [(runners['test'], policy, 'Real Env'), (runners['train'], policy, 'Virt Env')]
+
+    saver = nn.ModuleDict({'policy': policy, 'model': model, 'vfn': vfn})
+    print(saver)
+
+    eval_real_returns = []
+    timesteps = []
+
+    if FLAGS.ckpt.model_load:
+        saver.load_state_dict(np.load(FLAGS.ckpt.model_load)[()])
+        logger.warning('Load model from %s', FLAGS.ckpt.model_load)
+
+    if FLAGS.ckpt.buf_load:
+        n_samples = 0
+        for i in range(FLAGS.ckpt.buf_load_index):
+            data = pickle.load(open(f'{FLAGS.ckpt.buf_load}/stage-{i}.inc-buf.pkl', 'rb'))
+            add_multi_step(data, train_set)
+            n_samples += len(data)
+        logger.warning('Loading %d samples from %s', n_samples, FLAGS.ckpt.buf_load)
+
+    max_ent_coef = 0
+    print("!!!!!!!!")
+    print(virt_env.bonus_scale)
+    for T in range(FLAGS.slbo.n_stages):
+        logger.info('------ Starting Stage %d --------', T)
+        eval_returns = evaluate(settings, 'episode')
+        eval_real_returns.append(eval_returns[0])
+        timesteps.append(T*FLAGS.rollout.n_train_samples)
+
+        if not FLAGS.use_prev:
+            train_set.clear()
+            dev_set.clear()
+
+        # collect data
+        recent_train_set, ep_infos = runners['collect'].run(policy, FLAGS.rollout.n_train_samples, render=False)
+        add_multi_step(recent_train_set, train_set)
+        add_multi_step(
+            runners['dev'].run(policy, FLAGS.rollout.n_dev_samples)[0],
+            dev_set,
+        )
+
+        returns = np.array([ep_info['return'] for ep_info in ep_infos])
+
+        if len(returns) > 0:
+            logger.info("episode: %s", np.mean(returns))
+
+        if T == 0:  # check
+            samples = train_set.sample_multi_step(100, 1, FLAGS.model.multi_step)
+            for i in range(FLAGS.model.multi_step - 1):
+                masks = 1 - (samples.done[i] | samples.timeout[i])[..., np.newaxis]
+                assert np.allclose(samples.state[i + 1] * masks, samples.next_state[i] * masks)
+
+        # recent_states = obsvs
+        # ref_actions = policy.eval('actions_mean actions_std', states=recent_states)
+        if FLAGS.rollout.normalizer == 'policy' or FLAGS.rollout.normalizer == 'uniform' and T == 0:
+            normalizers.state.update(recent_train_set.state)
+            normalizers.action.update(recent_train_set.action)
+            normalizers.diff.update(recent_train_set.next_state - recent_train_set.state)
+        #print(recent_train_set.state.shape)
+        virt_env.update_cov(recent_train_set.state,recent_train_set.action)
+
+        #if T == FLAGS.pc.bonus_stop_time:
+        #    virt_env.bonus_scale = 0.
+
+        for i in range(FLAGS.slbo.n_iters):
+            #if i % FLAGS.slbo.n_evaluate_iters == 0 and i != 0:
+                # cur_actions = policy.eval('actions_mean actions_std', states=recent_states)
+                # kl_old_new = gaussian_kl(*ref_actions, *cur_actions).sum(axis=1).mean()
+                # logger.info('KL(old || cur) = %.6f', kl_old_new)
+            #    evaluate(settings, 'iteration')
+
+            losses = deque(maxlen=FLAGS.slbo.n_model_iters)
+            grad_norm_meter = AverageMeter()
+            n_model_iters = FLAGS.slbo.n_model_iters
+            for _ in range(n_model_iters):
+                samples = train_set.sample_multi_step(FLAGS.model.train_batch_size, 1, FLAGS.model.multi_step)
+                _, train_loss, grad_norm = loss_mod.get_loss(
+                    samples.state, samples.next_state, samples.action, ~samples.done & ~samples.timeout,
+                    fetch='train loss grad_norm')
+                losses.append(train_loss.mean())
+                grad_norm_meter.update(grad_norm)
+                # ideally, we should define an Optimizer class, which takes parameters as inputs.
+                # The `update` method of `Optimizer` will invalidate all parameters during updates.
+                for param in model.parameters():
+                    param.invalidate()
+
+            if i % FLAGS.model.validation_freq == 0:
+                samples = train_set.sample_multi_step(
+                    FLAGS.model.train_batch_size, 1, FLAGS.model.multi_step)
+                loss = loss_mod.get_loss(
+                    samples.state, samples.next_state, samples.action, ~samples.done & ~samples.timeout)
+                loss = loss.mean()
+                if np.isnan(loss) or np.isnan(np.mean(losses)):
+                    logger.info('nan! %s %s', np.isnan(loss), np.isnan(np.mean(losses)))
+                logger.info('# Iter %3d: Loss = [train = %.3f, dev = %.3f], after %d steps, grad_norm = %.6f',
+                            i, np.mean(losses), loss, n_model_iters, grad_norm_meter.get())
+
+            for n_updates in tqdm(range(FLAGS.slbo.n_policy_iters)):
+                if FLAGS.algorithm != 'MF' and FLAGS.slbo.start == 'buffer':
+                    runners['train'].set_state(train_set.sample(FLAGS.plan.n_envs).state)
+                else:
+                    runners['train'].reset()
+
+                data, ep_infos = runners['train'].run(policy, FLAGS.plan.n_trpo_samples)
+                advantages, values = runners['train'].compute_advantage(vfn, data)
+                dist_mean, dist_std, vf_loss = algo.train(max_ent_coef, data, advantages, values)
+                returns = [info['return'] for info in ep_infos]
+                #logger.info('[TRPO] # %d: n_episodes = %d, returns: {mean = %.0f, std = %.0f}, '
+                #            'dist std = %.10f, dist mean = %.10f, vf_loss = %.3f',
+                #            n_updates, len(returns), np.mean(returns), np.std(returns) / np.sqrt(len(returns)),
+                #            dist_std, dist_mean, vf_loss)
+
+        if T % FLAGS.ckpt.n_save_stages == 0:
+            np.save(f'{FLAGS.log_dir}/stage-{T}', saver.state_dict())
+            np.save(f'{FLAGS.log_dir}/final', saver.state_dict())
+        if FLAGS.ckpt.n_save_stages == 1:
+            pickle.dump(recent_train_set, open(f'{FLAGS.log_dir}/stage-{T}.inc-buf.pkl', 'wb'))
+
+    eval_returns = evaluate(settings, 'episode')
+    eval_real_returns.append(eval_returns[0])
+    timesteps.append(T*FLAGS.rollout.n_train_samples)
+    np.save(f'{FLAGS.log_dir}/eval_real_returns', eval_real_returns)
+    np.save(f'{FLAGS.log_dir}/timesteps', timesteps)
+
+
+if __name__ == '__main__':
+    with tf.Session(config=get_tf_config()):
+        main()
diff --git a/experiments05/ant_umaze_1234/src/requirements.txt b/experiments05/ant_umaze_1234/src/requirements.txt
new file mode 100644
index 0000000..8991cc0
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/requirements.txt
@@ -0,0 +1,6 @@
+tensorflow
+numpy
+pyyaml
+termcolor
+gym
+json_tricks
diff --git a/experiments05/ant_umaze_1234/src/rllab_requirements.txt b/experiments05/ant_umaze_1234/src/rllab_requirements.txt
new file mode 100644
index 0000000..9fb2d37
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/rllab_requirements.txt
@@ -0,0 +1,7 @@
+theano
+cached_property
+pyopengl
+joblib
+mako
+mujoco_py
+
diff --git a/experiments05/ant_umaze_1234/src/run2.sh b/experiments05/ant_umaze_1234/src/run2.sh
new file mode 100644
index 0000000..7440dc5
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/run2.sh
@@ -0,0 +1,10 @@
+#!/usr/bin/env bash
+
+for env_name in $1; do
+    echo "=> Running environment ${env_name}"
+    #for random_seed in 1234 2314 2345 1235; do
+    for random_seed in 1234; do
+        python main.py -c configs/algos/slbo_bm_200k.yml configs/env_tingwu/${env_name}.yml \
+	    -s pc.bonus_scale=0.2 log_dir=./experiments02/${env_name}_${random_seed} seed=${random_seed}
+    done
+done
diff --git a/experiments05/ant_umaze_1234/src/run_experiments.sh b/experiments05/ant_umaze_1234/src/run_experiments.sh
new file mode 100644
index 0000000..e3f2ad7
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/run_experiments.sh
@@ -0,0 +1,10 @@
+#!/usr/bin/env bash
+
+for env_name in $1; do
+    echo "=> Running environment ${env_name}"
+    #for random_seed in 1234 2314 2345 1235; do
+    for random_seed in 1234; do
+        python main.py -c configs/algos/slbo_bm_200k.yml configs/env_tingwu/${env_name}.yml \
+	    -s pc.bonus_scale=0.5 log_dir=./experiments05/${env_name}_${random_seed} seed=${random_seed}
+    done
+done
diff --git a/experiments05/ant_umaze_1234/src/slbo/__init__.py b/experiments05/ant_umaze_1234/src/slbo/__init__.py
new file mode 100644
index 0000000..5c7f19c
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/__init__.py
@@ -0,0 +1 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
diff --git a/experiments05/ant_umaze_1234/src/slbo/algos/TRPO.py b/experiments05/ant_umaze_1234/src/slbo/algos/TRPO.py
new file mode 100644
index 0000000..e8469e3
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/algos/TRPO.py
@@ -0,0 +1,183 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List, Callable
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi import Tensor
+from lunzi.Logger import logger
+from slbo.utils.dataset import Dataset
+from slbo.policies import BaseNNPolicy
+from slbo.v_function import BaseVFunction
+
+
+def average_l2_norm(x):
+    return np.sqrt((x**2).mean())
+
+
+# for damping, modify func_Ax
+def conj_grad(mat_mul_vec: Callable[[np.ndarray], np.ndarray], b, n_iters=10, residual_tol=1e-10, verbose=False):
+    p = b.copy()
+    r = b.copy()
+    x = np.zeros_like(b)
+    r_dot_r = r.dot(r)
+
+    for i in range(n_iters):
+        if verbose:
+            logger.info('[CG] iters = %d, |Res| = %.6f, |x| = %.6f', i, r_dot_r, np.linalg.norm(x))
+        z = mat_mul_vec(p)
+        v = r_dot_r / p.dot(z)
+        x += v * p
+        r -= v * z
+        new_r_dot_r = r.dot(r)
+        if new_r_dot_r < residual_tol:
+            break
+        mu = new_r_dot_r / r_dot_r
+        p = r + mu * p
+        r_dot_r = new_r_dot_r
+    return x
+
+
+class TRPO(nn.Module):
+    def __init__(self, dim_state: int, dim_action: int, policy: BaseNNPolicy, vfn: BaseVFunction, max_kl: float,
+                 n_cg_iters: int, ent_coef=0.0, cg_damping=0.01, vf_lr=3e-4, n_vf_iters=3):
+        super().__init__()
+        self.dim_state = dim_state
+        self.policy = policy
+        self.ent_coef = ent_coef
+        self.vf = vfn
+        self.n_cg_iters = n_cg_iters
+        self.max_kl = max_kl
+        self.cg_damping = cg_damping
+        self.n_vf_iters = n_vf_iters
+        self.vf_lr = vf_lr
+
+        # doing backtrace, so don't need to separate.
+        self.flatten = nn.FlatParam(self.policy.parameters())
+        self.old_policy: nn.Module = policy.clone()
+
+        with self.scope:
+            self.op_returns = tf.placeholder(dtype=tf.float32, shape=[None], name='returns')
+            self.op_advantages = tf.placeholder(dtype=tf.float32, shape=[None], name='advantages')
+            self.op_states = tf.placeholder(dtype=tf.float32, shape=[None, dim_state], name='states')
+            self.op_actions = tf.placeholder(dtype=tf.float32, shape=[None, dim_action], name='actions')
+            self.op_feed_params = tf.placeholder(dtype=tf.float32, shape=[None], name='feed_params')
+
+            self.op_tangents = tf.placeholder(
+                dtype=tf.float32, shape=[nn.utils.n_parameters(self.policy.parameters())])
+            self.op_ent_coef = tf.placeholder(dtype=tf.float32, shape=[], name='ent_coef')
+
+        self.op_mean_kl, self.op_loss, self.op_dist_std, self.op_dist_mean, self.op_policy_loss = \
+            self(self.op_states, self.op_actions, self.op_advantages, self.op_ent_coef)
+
+        self.op_sync_old, self.op_hessian_vec_prod, self.op_flat_grad = \
+            self.compute_natural_grad(self.op_loss, self.op_mean_kl, self.op_tangents)
+
+        self.op_vf_loss, self.op_train_vf = self.compute_vf(self.op_states, self.op_returns)
+
+    def forward(self, states, actions, advantages, ent_coef):
+        old_distribution: tf.distributions.Normal = self.old_policy(states)
+        distribution: tf.distributions.Normal = self.policy(states)
+        mean_kl = old_distribution.kl_divergence(distribution).reduce_sum(axis=1).reduce_mean()
+        entropy = distribution.entropy().reduce_sum(axis=1).reduce_mean()
+        entropy_bonus = ent_coef * entropy
+
+        ratios: Tensor = (distribution.log_prob(actions) - old_distribution.log_prob(actions)) \
+            .reduce_sum(axis=1).exp()
+        # didn't output op_policy_loss since in principle it should be 0.
+        policy_loss = ratios.mul(advantages).reduce_mean()
+
+        # We're doing Gradient Ascent so this is, in fact, gain.
+        loss = policy_loss + entropy_bonus
+
+        return mean_kl, loss, distribution.stddev().log().reduce_mean().exp(), \
+            distribution.mean().norm(axis=1).reduce_mean() / np.sqrt(10), policy_loss
+
+    def compute_natural_grad(self, loss, mean_kl, tangents):
+        params = self.policy.parameters()
+        old_params = self.old_policy.parameters()
+        hessian_vec_prod = nn.utils.hessian_vec_prod(mean_kl, params, tangents)
+        flat_grad = nn.utils.parameters_to_vector(tf.gradients(loss, params))
+        sync_old = tf.group(*[tf.assign(old_v, new_v) for old_v, new_v in zip(old_params, params)])
+
+        return sync_old, hessian_vec_prod, flat_grad
+
+    def compute_vf(self, states, returns):
+        vf_loss = nn.MSELoss()(self.vf(states), returns).reduce_mean()
+        optimizer = tf.train.AdamOptimizer(self.vf_lr)
+        train_vf = optimizer.minimize(vf_loss)
+
+        return vf_loss, train_vf
+
+    @nn.make_method()
+    def get_vf_loss(self, states, returns) -> List[np.ndarray]: pass
+
+    @nn.make_method(fetch='sync_old')
+    def sync_old(self) -> List[np.ndarray]: pass
+
+    @nn.make_method(fetch='hessian_vec_prod')
+    def get_hessian_vec_prod(self, states, tangents, actions) -> List[np.ndarray]: pass
+
+    @nn.make_method(fetch='loss')
+    def get_loss(self, states, actions, advantages, ent_coef) -> List[np.ndarray]: pass
+
+    def train(self, ent_coef, samples, advantages, values):
+        returns = advantages + values
+        advantages = (advantages - advantages.mean()) / np.maximum(advantages.std(), 1e-8)
+        assert np.isfinite(advantages).all()
+        self.sync_old()
+        old_loss, grad, dist_std, mean_kl, dist_mean = self.get_loss(
+            samples.state, samples.action, advantages, ent_coef, fetch='loss flat_grad dist_std mean_kl dist_mean')
+
+        if np.allclose(grad, 0):
+            logger.info('Zero gradient, not updating...')
+            return
+
+        def fisher_vec_prod(x):
+            return self.get_hessian_vec_prod(samples.state, x, samples.action) + self.cg_damping * x
+
+        assert np.isfinite(grad).all()
+        nat_grad = conj_grad(fisher_vec_prod, grad, n_iters=self.n_cg_iters, verbose=False)
+
+        assert np.isfinite(nat_grad).all()
+
+        old_params = self.flatten.get_flat()
+        step_size = np.sqrt(2 * self.max_kl / nat_grad.dot(fisher_vec_prod(nat_grad)))
+
+        for _ in range(10):
+            new_params = old_params + nat_grad * step_size
+            self.flatten.set_flat(new_params)
+            loss, mean_kl = self.get_loss(samples.state, samples.action, advantages, ent_coef, fetch='loss mean_kl')
+            improve = loss - old_loss
+            if not np.isfinite([loss, mean_kl]).all():
+                logger.info('Got non-finite loss.')
+            elif mean_kl > self.max_kl * 1.5:
+                logger.info('Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f',
+                            mean_kl, self.max_kl)
+            elif improve < 0:
+                logger.info("Surrogate didn't improve, shrinking step... %.6f => %.6f", old_loss, loss)
+            else:
+                break
+            step_size *= 0.5
+        else:
+            logger.info("Couldn't find a good step.")
+            self.flatten.set_flat(old_params)
+        for param in self.policy.parameters():
+            param.invalidate()
+
+        # optimize value function
+        vf_dataset = Dataset.fromarrays([samples.state, returns],
+                                        dtype=[('state', ('f8', self.dim_state)), ('return_', 'f8')])
+        vf_loss = self.train_vf(vf_dataset)
+
+        return dist_mean, dist_std, vf_loss
+
+    def train_vf(self, dataset: Dataset):
+        for _ in range(self.n_vf_iters):
+            for subset in dataset.iterator(64):
+                self.get_vf_loss(subset.state, subset.return_, fetch='train_vf vf_loss')
+        for param in self.parameters():
+            param.invalidate()
+        vf_loss = self.get_vf_loss(dataset.state, dataset.return_, fetch='vf_loss')
+        return vf_loss
+
+
diff --git a/experiments05/ant_umaze_1234/src/slbo/algos/__init__.py b/experiments05/ant_umaze_1234/src/slbo/algos/__init__.py
new file mode 100644
index 0000000..5c7f19c
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/algos/__init__.py
@@ -0,0 +1 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
diff --git a/experiments05/ant_umaze_1234/src/slbo/dynamics_model.py b/experiments05/ant_umaze_1234/src/slbo/dynamics_model.py
new file mode 100644
index 0000000..19662ec
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/dynamics_model.py
@@ -0,0 +1,43 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List
+import tensorflow as tf
+from lunzi import Tensor
+import lunzi.nn as nn
+from slbo.utils.normalizer import Normalizers
+from slbo.utils.multi_layer_perceptron import MultiLayerPerceptron
+
+
+class DynamicsModel(MultiLayerPerceptron):
+    op_loss: Tensor
+    op_train: Tensor
+    op_grad_norm: Tensor
+
+    def __init__(self, dim_state: int, dim_action: int, normalizers: Normalizers, hidden_sizes: List[int]):
+        initializer = tf.truncated_normal_initializer(mean=0.0, stddev=1e-5)
+
+        self.dim_state = dim_state
+        self.dim_action = dim_action
+        self.hidden_sizes = hidden_sizes
+        self.op_states = tf.placeholder(tf.float32, shape=[None, self.dim_state], name='states')
+        self.op_actions = tf.placeholder(tf.float32, shape=[None, self.dim_action], name='actions')
+        super().__init__([dim_state + dim_action, *hidden_sizes, dim_state],
+                         activation=nn.ReLU,
+                         weight_initializer=initializer, build=False)
+
+        self.normalizers = normalizers
+        self.build()
+
+    def build(self):
+        self.op_next_states = self.forward(self.op_states, self.op_actions)
+
+    def forward(self, states, actions):
+        assert actions.shape[-1] == self.dim_action
+        inputs = tf.concat([self.normalizers.state(states), actions.clip_by_value(-1., 1.)], axis=1)
+
+        normalized_diffs = super().forward(inputs)
+        next_states = states + self.normalizers.diff(normalized_diffs, inverse=True)
+        next_states = self.normalizers.state(self.normalizers.state(next_states).clip_by_value(-100, 100), inverse=True)
+        return next_states
+
+    def clone(self):
+        return DynamicsModel(self.dim_state, self.dim_action, self.normalizers, self.hidden_sizes)
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/__init__.py b/experiments05/ant_umaze_1234/src/slbo/envs/__init__.py
new file mode 100644
index 0000000..7efdf01
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/__init__.py
@@ -0,0 +1,55 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+import abc
+import gym
+from slbo.utils.dataset import Dataset, gen_dtype
+from lunzi.Logger import logger
+
+
+class BaseBatchedEnv(gym.Env, abc.ABC):
+    # thought about using `@property @abc.abstractmethod` here but we don't need explicit `@property` function here.
+    n_envs: int
+
+    @abc.abstractmethod
+    def step(self, actions):
+        pass
+
+    def reset(self):
+        return self.partial_reset(range(self.n_envs))
+
+    @abc.abstractmethod
+    def partial_reset(self, indices):
+        pass
+
+    def set_state(self, state):
+        logger.warning('`set_state` is not implemented')
+
+
+class BaseModelBasedEnv(gym.Env, abc.ABC):
+    @abc.abstractmethod
+    def mb_step(self, states: np.ndarray, actions: np.ndarray, next_states: np.ndarray):
+        raise NotImplementedError
+
+    def verify(self, n=2000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
+
+    def seed(self, seed: int = None):
+        pass
+
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/batched_env.py b/experiments05/ant_umaze_1234/src/slbo/envs/batched_env.py
new file mode 100644
index 0000000..cbe4bac
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/batched_env.py
@@ -0,0 +1,37 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from gym import Wrapper
+from . import BaseBatchedEnv
+
+
+class BatchedEnv(BaseBatchedEnv, Wrapper):
+    def __init__(self, envs):
+        super().__init__(envs[0])
+        self.envs = envs
+        self.n_envs = len(envs)
+
+    def step(self, actions):
+
+        buf, infos = [], []
+        for env, action in zip(self.envs, actions):
+            next_state, reward, done, info = env.step(action)
+            buf.append((next_state, reward, done))
+            infos.append(info)
+
+        return [*(np.array(x) for x in zip(*buf)), infos]
+
+    def reset(self):
+        return self.partial_reset(range(self.n_envs))
+
+    def partial_reset(self, indices):
+        states = []
+        for index in indices:
+            states.append(self.envs[index].reset())
+        return np.array(states)
+
+    def __repr__(self):
+        return f'Batch<{self.n_envs}x {self.env}>'
+
+    def set_state(self, state):
+        pass
+
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/__init__.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/ant.xml b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/ant.xml
new file mode 100644
index 0000000..18ad38b
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/ant.xml
@@ -0,0 +1,80 @@
+<mujoco model="ant">
+  <compiler angle="degree" coordinate="local" inertiafromgeom="true"/>
+  <option integrator="RK4" timestep="0.01"/>
+  <custom>
+    <numeric data="0.0 0.0 0.55 1.0 0.0 0.0 0.0 0.0 1.0 0.0 -1.0 0.0 -1.0 0.0 1.0" name="init_qpos"/>
+  </custom>
+  <default>
+    <joint armature="1" damping="1" limited="true"/>
+    <geom conaffinity="0" condim="3" density="5.0" friction="1 0.5 0.5" margin="0.01" rgba="0.8 0.6 0.4 1"/>
+  </default>
+  <asset>
+    <texture builtin="gradient" height="100" rgb1="1 1 1" rgb2="0 0 0" type="skybox" width="100"/>
+    <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+    <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+    <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="60 60" texture="texplane"/>
+    <material name="geom" texture="texgeom" texuniform="true"/>
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="40 40 40" type="plane"/>
+    <body name="torso" pos="0 0 0.75">
+      <geom name="torso_geom" pos="0 0 0" size="0.25" type="sphere"/>
+      <joint armature="0" damping="0" limited="false" margin="0.01" name="root" pos="0 0 0" type="free"/>
+      <body name="front_left_leg" pos="0 0 0">
+        <geom fromto="0.0 0.0 0.0 0.2 0.2 0.0" name="aux_1_geom" size="0.08" type="capsule"/>
+        <body name="aux_1" pos="0.2 0.2 0">
+          <joint axis="0 0 1" name="hip_1" pos="0.0 0.0 0.0" range="-30 30" type="hinge"/>
+          <geom fromto="0.0 0.0 0.0 0.2 0.2 0.0" name="left_leg_geom" size="0.08" type="capsule"/>
+          <body pos="0.2 0.2 0">
+            <joint axis="-1 1 0" name="ankle_1" pos="0.0 0.0 0.0" range="30 70" type="hinge"/>
+            <geom fromto="0.0 0.0 0.0 0.4 0.4 0.0" name="left_ankle_geom" size="0.08" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <body name="front_right_leg" pos="0 0 0">
+        <geom fromto="0.0 0.0 0.0 -0.2 0.2 0.0" name="aux_2_geom" size="0.08" type="capsule"/>
+        <body name="aux_2" pos="-0.2 0.2 0">
+          <joint axis="0 0 1" name="hip_2" pos="0.0 0.0 0.0" range="-30 30" type="hinge"/>
+          <geom fromto="0.0 0.0 0.0 -0.2 0.2 0.0" name="right_leg_geom" size="0.08" type="capsule"/>
+          <body pos="-0.2 0.2 0">
+            <joint axis="1 1 0" name="ankle_2" pos="0.0 0.0 0.0" range="-70 -30" type="hinge"/>
+            <geom fromto="0.0 0.0 0.0 -0.4 0.4 0.0" name="right_ankle_geom" size="0.08" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <body name="back_leg" pos="0 0 0">
+        <geom fromto="0.0 0.0 0.0 -0.2 -0.2 0.0" name="aux_3_geom" size="0.08" type="capsule"/>
+        <body name="aux_3" pos="-0.2 -0.2 0">
+          <joint axis="0 0 1" name="hip_3" pos="0.0 0.0 0.0" range="-30 30" type="hinge"/>
+          <geom fromto="0.0 0.0 0.0 -0.2 -0.2 0.0" name="back_leg_geom" size="0.08" type="capsule"/>
+          <body pos="-0.2 -0.2 0">
+            <joint axis="-1 1 0" name="ankle_3" pos="0.0 0.0 0.0" range="-70 -30" type="hinge"/>
+            <geom fromto="0.0 0.0 0.0 -0.4 -0.4 0.0" name="third_ankle_geom" size="0.08" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <body name="right_back_leg" pos="0 0 0">
+        <geom fromto="0.0 0.0 0.0 0.2 -0.2 0.0" name="aux_4_geom" size="0.08" type="capsule"/>
+        <body name="aux_4" pos="0.2 -0.2 0">
+          <joint axis="0 0 1" name="hip_4" pos="0.0 0.0 0.0" range="-30 30" type="hinge"/>
+          <geom fromto="0.0 0.0 0.0 0.2 -0.2 0.0" name="rightback_leg_geom" size="0.08" type="capsule"/>
+          <body pos="0.2 -0.2 0">
+            <joint axis="1 1 0" name="ankle_4" pos="0.0 0.0 0.0" range="30 70" type="hinge"/>
+            <geom fromto="0.0 0.0 0.0 0.4 -0.4 0.0" name="fourth_ankle_geom" size="0.08" type="capsule"/>
+          </body>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="hip_4" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="ankle_4" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="hip_1" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="ankle_1" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="hip_2" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="ankle_2" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="hip_3" gear="150"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" joint="ankle_3" gear="150"/>
+  </actuator>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/half_cheetah.xml b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/half_cheetah.xml
new file mode 100644
index 0000000..b07aada
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/half_cheetah.xml
@@ -0,0 +1,95 @@
+<!-- Cheetah Model
+
+    The state space is populated with joints in the order that they are
+    defined in this file. The actuators also operate on joints.
+
+    State-Space (name/joint/parameter):
+        - rootx     slider      position (m)
+        - rootz     slider      position (m)
+        - rooty     hinge       angle (rad)
+        - bthigh    hinge       angle (rad)
+        - bshin     hinge       angle (rad)
+        - bfoot     hinge       angle (rad)
+        - fthigh    hinge       angle (rad)
+        - fshin     hinge       angle (rad)
+        - ffoot     hinge       angle (rad)
+        - rootx     slider      velocity (m/s)
+        - rootz     slider      velocity (m/s)
+        - rooty     hinge       angular velocity (rad/s)
+        - bthigh    hinge       angular velocity (rad/s)
+        - bshin     hinge       angular velocity (rad/s)
+        - bfoot     hinge       angular velocity (rad/s)
+        - fthigh    hinge       angular velocity (rad/s)
+        - fshin     hinge       angular velocity (rad/s)
+        - ffoot     hinge       angular velocity (rad/s)
+
+    Actuators (name/actuator/parameter):
+        - bthigh    hinge       torque (N m)
+        - bshin     hinge       torque (N m)
+        - bfoot     hinge       torque (N m)
+        - fthigh    hinge       torque (N m)
+        - fshin     hinge       torque (N m)
+        - ffoot     hinge       torque (N m)
+
+-->
+<mujoco model="cheetah">
+  <compiler angle="radian" coordinate="local" inertiafromgeom="true" settotalmass="14"/>
+  <default>
+    <joint armature=".1" damping=".01" limited="true" solimplimit="0 .8 .03" solreflimit=".02 1" stiffness="8"/>
+    <geom conaffinity="0" condim="3" contype="1" friction=".4 .1 .1" rgba="0.8 0.6 .4 1" solimp="0.0 0.8 0.01" solref="0.02 1"/>
+    <motor ctrllimited="true" ctrlrange="-1 1"/>
+  </default>
+  <size nstack="300000" nuser_geom="1"/>
+  <option gravity="0 0 -9.81" timestep="0.01"/>
+  <asset>
+    <texture builtin="gradient" height="100" rgb1="1 1 1" rgb2="0 0 0" type="skybox" width="100"/>
+    <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+    <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+    <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="60 60" texture="texplane"/>
+    <material name="geom" texture="texgeom" texuniform="true"/>
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="40 40 40" type="plane"/>
+    <body name="torso" pos="0 0 .7">
+      <joint armature="0" axis="1 0 0" damping="0" limited="false" name="rootx" pos="0 0 0" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 0 1" damping="0" limited="false" name="rootz" pos="0 0 0" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 1 0" damping="0" limited="false" name="rooty" pos="0 0 0" stiffness="0" type="hinge"/>
+      <geom fromto="-.5 0 0 .5 0 0" name="torso" size="0.046" type="capsule"/>
+      <geom axisangle="0 1 0 .87" name="head" pos=".6 0 .1" size="0.046 .15" type="capsule"/>
+      <!-- <site name='tip'  pos='.15 0 .11'/>-->
+      <body name="bthigh" pos="-.5 0 0">
+        <joint axis="0 1 0" damping="6" name="bthigh" pos="0 0 0" range="-.52 1.05" stiffness="240" type="hinge"/>
+        <geom axisangle="0 1 0 -3.8" name="bthigh" pos=".1 0 -.13" size="0.046 .145" type="capsule"/>
+        <body name="bshin" pos=".16 0 -.25">
+          <joint axis="0 1 0" damping="4.5" name="bshin" pos="0 0 0" range="-.785 .785" stiffness="180" type="hinge"/>
+          <geom axisangle="0 1 0 -2.03" name="bshin" pos="-.14 0 -.07" rgba="0.9 0.6 0.6 1" size="0.046 .15" type="capsule"/>
+          <body name="bfoot" pos="-.28 0 -.14">
+            <joint axis="0 1 0" damping="3" name="bfoot" pos="0 0 0" range="-.4 .785" stiffness="120" type="hinge"/>
+            <geom axisangle="0 1 0 -.27" name="bfoot" pos=".03 0 -.097" rgba="0.9 0.6 0.6 1" size="0.046 .094" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <body name="fthigh" pos=".5 0 0">
+        <joint axis="0 1 0" damping="4.5" name="fthigh" pos="0 0 0" range="-1 .7" stiffness="180" type="hinge"/>
+        <geom axisangle="0 1 0 .52" name="fthigh" pos="-.07 0 -.12" size="0.046 .133" type="capsule"/>
+        <body name="fshin" pos="-.14 0 -.24">
+          <joint axis="0 1 0" damping="3" name="fshin" pos="0 0 0" range="-1.2 .87" stiffness="120" type="hinge"/>
+          <geom axisangle="0 1 0 -.6" name="fshin" pos=".065 0 -.09" rgba="0.9 0.6 0.6 1" size="0.046 .106" type="capsule"/>
+          <body name="ffoot" pos=".13 0 -.18">
+            <joint axis="0 1 0" damping="1.5" name="ffoot" pos="0 0 0" range="-.5 .5" stiffness="60" type="hinge"/>
+            <geom axisangle="0 1 0 -.6" name="ffoot" pos=".045 0 -.07" rgba="0.9 0.6 0.6 1" size="0.046 .07" type="capsule"/>
+          </body>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor gear="120" joint="bthigh" name="bthigh"/>
+    <motor gear="90" joint="bshin" name="bshin"/>
+    <motor gear="60" joint="bfoot" name="bfoot"/>
+    <motor gear="120" joint="fthigh" name="fthigh"/>
+    <motor gear="60" joint="fshin" name="fshin"/>
+    <motor gear="30" joint="ffoot" name="ffoot"/>
+  </actuator>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/hopper.xml b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/hopper.xml
new file mode 100644
index 0000000..b0ebc0e
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/hopper.xml
@@ -0,0 +1,44 @@
+<mujoco model="hopper">
+  <compiler angle="degree" coordinate="global" inertiafromgeom="true"/>
+  <default>
+    <joint armature="1" damping="1" limited="true"/>
+    <geom conaffinity="1" condim="1" contype="1" margin="0.001" material="geom" rgba="0.8 0.6 .4 1" solimp=".8 .8 .01" solref=".02 1"/>
+    <motor ctrllimited="true" ctrlrange="-.4 .4"/>
+  </default>
+  <option integrator="RK4" timestep="0.002"/>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="20 20 .125" type="plane" material="MatPlane"/>
+    <body name="torso" pos="0 0 1.25">
+      <joint armature="0" axis="1 0 0" damping="0" limited="false" name="rootx" pos="0 0 0" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 0 1" damping="0" limited="false" name="rootz" pos="0 0 0" ref="1.25" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 1 0" damping="0" limited="false" name="rooty" pos="0 0 1.25" stiffness="0" type="hinge"/>
+      <geom friction="0.9" fromto="0 0 1.45 0 0 1.05" name="torso_geom" size="0.05" type="capsule"/>
+      <body name="thigh" pos="0 0 1.05">
+        <joint axis="0 -1 0" name="thigh_joint" pos="0 0 1.05" range="-150 0" type="hinge"/>
+        <geom friction="0.9" fromto="0 0 1.05 0 0 0.6" name="thigh_geom" size="0.05" type="capsule"/>
+        <body name="leg" pos="0 0 0.35">
+          <joint axis="0 -1 0" name="leg_joint" pos="0 0 0.6" range="-150 0" type="hinge"/>
+          <geom friction="0.9" fromto="0 0 0.6 0 0 0.1" name="leg_geom" size="0.04" type="capsule"/>
+          <body name="foot" pos="0.13/2 0 0.1">
+            <joint axis="0 -1 0" name="foot_joint" pos="0 0 0.1" range="-45 45" type="hinge"/>
+            <geom friction="2.0" fromto="-0.13 0 0.1 0.26 0 0.1" name="foot_geom" size="0.06" type="capsule"/>
+          </body>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="thigh_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="leg_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="foot_joint"/>
+  </actuator>
+    <asset>
+        <texture type="skybox" builtin="gradient" rgb1=".4 .5 .6" rgb2="0 0 0"
+            width="100" height="100"/>        
+        <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+        <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+        <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="60 60" texture="texplane"/>
+        <material name="geom" texture="texgeom" texuniform="true"/>
+    </asset>
+</mujoco>
\ No newline at end of file
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/point.xml b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/point.xml
new file mode 100644
index 0000000..e35ef3d
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/point.xml
@@ -0,0 +1,31 @@
+<mujoco>
+  <compiler angle="degree" coordinate="local" inertiafromgeom="true"/>
+  <option integrator="RK4" timestep="0.02"/>
+  <default>
+    <joint armature="0" damping="0" limited="false"/>
+    <geom conaffinity="0" condim="3" density="100" friction="1 0.5 0.5" margin="0" rgba="0.8 0.6 0.4 1"/>
+  </default>
+  <asset>
+    <texture builtin="gradient" height="100" rgb1="1 1 1" rgb2="0 0 0" type="skybox" width="100"/>
+    <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+    <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+    <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="30 30" texture="texplane"/>
+    <material name="geom" texture="texgeom" texuniform="true"/>
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="40 40 40" type="plane"/>
+    <body name="torso" pos="0 0 0">
+      <geom name="pointbody" pos="0 0 0.5" size="0.5" type="sphere"/>
+      <geom name="pointarrow" pos="0.6 0 0.5" size="0.5 0.1 0.1" type="box"/>
+      <joint axis="1 0 0" name="ballx" pos="0 0 0" type="slide"/>
+      <joint axis="0 1 0" name="bally" pos="0 0 0" type="slide"/>
+      <joint axis="0 0 1" limited="false" name="rot" pos="0 0 0" type="hinge"/>
+    </body>
+  </worldbody>
+  <actuator>
+    <!-- Those are just dummy actuators for providing ranges -->
+    <motor ctrllimited="true" ctrlrange="-1 1" joint="ballx"/>
+    <motor ctrllimited="true" ctrlrange="-0.25 0.25" joint="rot"/>
+  </actuator>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/pusher.xml b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/pusher.xml
new file mode 100644
index 0000000..31a5ef7
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/pusher.xml
@@ -0,0 +1,91 @@
+<mujoco model="arm3d">
+  <compiler inertiafromgeom="true" angle="radian" coordinate="local"/>
+  <option timestep="0.01" gravity="0 0 0" iterations="20" integrator="Euler" />
+  
+  <default>
+    <joint armature='0.04' damping="1" limited="true"/>
+    <geom friction=".8 .1 .1" density="300" margin="0.002" condim="1" contype="0" conaffinity="0"/>
+  </default>
+  
+  <worldbody>
+    <light diffuse=".5 .5 .5" pos="0 0 3" dir="0 0 -1"/>
+    <geom name="table" type="plane" pos="0 0.5 -0.325" size="1 1 0.1" contype="1" conaffinity="1"/>
+
+    <body name="r_shoulder_pan_link" pos="0 -0.6 0">
+      <geom name="e1" type="sphere" rgba="0.6 0.6 0.6 1" pos="-0.06 0.05 0.2" size="0.05" />
+      <geom name="e2" type="sphere" rgba="0.6 0.6 0.6 1" pos=" 0.06 0.05 0.2" size="0.05" />
+      <geom name="e1p" type="sphere" rgba="0.1 0.1 0.1 1" pos="-0.06 0.09 0.2" size="0.03" />
+      <geom name="e2p" type="sphere" rgba="0.1 0.1 0.1 1" pos=" 0.06 0.09 0.2" size="0.03" />
+      <geom name="sp" type="capsule" fromto="0 0 -0.4 0 0 0.2" size="0.1" />
+      <joint name="r_shoulder_pan_joint" type="hinge" pos="0 0 0" axis="0 0 1" range="-2.2854 1.714602" damping="1.0" />
+
+      <body name="r_shoulder_lift_link" pos="0.1 0 0">
+        <geom name="sl" type="capsule" fromto="0 -0.1 0 0 0.1 0" size="0.1" />
+        <joint name="r_shoulder_lift_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.5236 1.3963" damping="1.0" />
+
+        <body name="r_upper_arm_roll_link" pos="0 0 0">
+          <geom name="uar" type="capsule" fromto="-0.1 0 0 0.1 0 0" size="0.02" />
+          <joint name="r_upper_arm_roll_joint" type="hinge" pos="0 0 0" axis="1 0 0" range="-1.5 1.7" damping="0.1" />
+
+          <body name="r_upper_arm_link" pos="0 0 0">
+            <geom name="ua" type="capsule" fromto="0 0 0 0.4 0 0" size="0.06" />
+
+            <body name="r_elbow_flex_link" pos="0.4 0 0">
+              <geom name="ef" type="capsule" fromto="0 -0.02 0 0.0 0.02 0" size="0.06" />
+              <joint name="r_elbow_flex_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-2.3213 0" damping="0.1" />
+
+              <body name="r_forearm_roll_link" pos="0 0 0">
+                <geom name="fr" type="capsule" fromto="-0.1 0 0 0.1 0 0" size="0.02" />
+                <joint name="r_forearm_roll_joint" type="hinge" limited="true" pos="0 0 0" axis="1 0 0" damping=".1" range="-1.5 1.5"/>
+
+                <body name="r_forearm_link" pos="0 0 0">
+                  <geom name="fa" type="capsule" fromto="0 0 0 0.291 0 0" size="0.05" />
+
+                  <body name="r_wrist_flex_link" pos="0.321 0 0">
+                    <geom name="wf" type="capsule" fromto="0 -0.02 0 0 0.02 0" size="0.01" />
+                    <joint name="r_wrist_flex_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-1.094 0" damping=".1" />
+
+                    <body name="r_wrist_roll_link" pos="0 0 0">
+                      <joint name="r_wrist_roll_joint" type="hinge" pos="0 0 0" limited="true" axis="1 0 0" damping="0.1" range="-1.5 1.5"/>
+                      <body name="tips_arm" pos="0 0 0">
+                        <geom name="tip_arml" type="sphere" pos="0.1 -0.1 0." size="0.01" />
+                        <geom name="tip_armr" type="sphere" pos="0.1 0.1 0." size="0.01" />
+                      </body>
+                      <geom type="capsule" fromto="0 -0.1 0. 0.0 +0.1 0" size="0.02" contype="1" conaffinity="1" />
+                      <geom type="capsule" fromto="0 -0.1 0. 0.1 -0.1 0" size="0.02" contype="1" conaffinity="1" />
+                      <geom type="capsule" fromto="0 +0.1 0. 0.1 +0.1 0." size="0.02" contype="1" conaffinity="1" />
+                    </body>
+                  </body>
+                </body>
+              </body>
+            </body>
+          </body>
+        </body>
+      </body>
+    </body>
+
+    <!--<body name="object" pos="0.55 -0.3 -0.275" >-->
+    <body name="object" pos="0.45 -0.05 -0.275" >
+      <geom rgba="1 1 1 0" type="sphere" size="0.05 0.05 0.05" density="0.00001" conaffinity="0"/>
+      <geom rgba="1 1 1 1" type="cylinder" size="0.05 0.05 0.05" density="0.00001" contype="1" conaffinity="0"/>
+      <joint name="obj_slidey" type="slide" pos="0 0 0" axis="0 1 0" range="-10.3213 10.3" damping="0.5"/>
+      <joint name="obj_slidex" type="slide" pos="0 0 0" axis="1 0 0" range="-10.3213 10.3" damping="0.5"/>
+    </body>
+
+    <body name="goal" pos="0.45 -0.05 -0.3230">
+      <geom rgba="1 0 0 1" type="cylinder" size="0.08 0.001 0.1" density='0.00001' contype="0" conaffinity="0"/>
+      <joint name="goal_slidey" type="slide" pos="0 0 0" axis="0 1 0" range="-10.3213 10.3" damping="0.5"/>
+      <joint name="goal_slidex" type="slide" pos="0 0 0" axis="1 0 0" range="-10.3213 10.3" damping="0.5"/> 
+    </body>
+  </worldbody>
+
+  <actuator>
+    <motor joint="r_shoulder_pan_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_shoulder_lift_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_upper_arm_roll_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_elbow_flex_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_forearm_roll_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_wrist_flex_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_wrist_roll_joint" ctrlrange="-2.0 2.0" ctrllimited="true"/>
+  </actuator>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/reacher.xml b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/reacher.xml
new file mode 100644
index 0000000..64a67b9
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/reacher.xml
@@ -0,0 +1,39 @@
+<mujoco model="reacher">
+	<compiler angle="radian" inertiafromgeom="true"/>
+	<default>
+		<joint armature="1" damping="1" limited="true"/>
+		<geom contype="0" friction="1 0.1 0.1" rgba="0.7 0.7 0 1"/>
+	</default>
+	<option gravity="0 0 -9.81" integrator="RK4" timestep="0.01"/>
+	<worldbody>
+		<!-- Arena -->
+		<geom conaffinity="0" contype="0" name="ground" pos="0 0 0" rgba="0.9 0.9 0.9 1" size="1 1 10" type="plane"/>
+		<geom conaffinity="0" fromto="-.3 -.3 .01 .3 -.3 .01" name="sideS" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
+		<geom conaffinity="0" fromto=" .3 -.3 .01 .3  .3 .01" name="sideE" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
+		<geom conaffinity="0" fromto="-.3  .3 .01 .3  .3 .01" name="sideN" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
+		<geom conaffinity="0" fromto="-.3 -.3 .01 -.3 .3 .01" name="sideW" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
+		<!-- Arm -->
+		<geom conaffinity="0" contype="0" fromto="0 0 0 0 0 0.02" name="root" rgba="0.9 0.4 0.6 1" size=".011" type="cylinder"/>
+		<body name="body0" pos="0 0 .01">
+			<geom fromto="0 0 0 0.1 0 0" name="link0" rgba="0.0 0.4 0.6 1" size=".01" type="capsule"/>
+			<joint axis="0 0 1" limited="false" name="joint0" pos="0 0 0" type="hinge"/>
+			<body name="body1" pos="0.1 0 0">
+				<joint axis="0 0 1" limited="true" name="joint1" pos="0 0 0" range="-3.0 3.0" type="hinge"/>
+				<geom fromto="0 0 0 0.1 0 0" name="link1" rgba="0.0 0.4 0.6 1" size=".01" type="capsule"/>
+				<body name="fingertip" pos="0.11 0 0">
+					<geom contype="0" name="fingertip" pos="0 0 0" rgba="0.0 0.8 0.6 1" size=".01" type="sphere"/>
+				</body>
+			</body>
+		</body>
+		<!-- Target -->
+		<body name="target" pos=".1 -.1 .01">
+			<joint armature="0" axis="1 0 0" damping="0" limited="true" name="target_x" pos="0 0 0" range="-.27 .27" ref=".1" stiffness="0" type="slide"/>
+			<joint armature="0" axis="0 1 0" damping="0" limited="true" name="target_y" pos="0 0 0" range="-.27 .27" ref="-.1" stiffness="0" type="slide"/>
+			<geom conaffinity="0" contype="0" name="target" pos="0 0 0" rgba="0.9 0.2 0.2 1" size=".009" type="sphere"/>
+		</body>
+	</worldbody>
+	<actuator>
+		<motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="joint0"/>
+		<motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="joint1"/>
+	</actuator>
+</mujoco>
\ No newline at end of file
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/swimmer.xml b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/swimmer.xml
new file mode 100644
index 0000000..cda25da
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/swimmer.xml
@@ -0,0 +1,38 @@
+<mujoco model="swimmer">
+  <compiler angle="degree" coordinate="local" inertiafromgeom="true"/>
+  <option collision="predefined" density="4000" integrator="RK4" timestep="0.01" viscosity="0.1"/>
+  <default>
+    <geom conaffinity="1" condim="1" contype="1" material="geom" rgba="0.8 0.6 .4 1"/>
+    <joint armature='0.1'  />
+  </default>
+  <asset>
+    <texture builtin="gradient" height="100" rgb1="1 1 1" rgb2="0 0 0" type="skybox" width="100"/>
+    <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+    <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+    <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="30 30" texture="texplane"/>
+    <material name="geom" texture="texgeom" texuniform="true"/>
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 -0.1" rgba="0.8 0.9 0.8 1" size="40 40 0.1" type="plane"/>
+    <!--  ================= SWIMMER ================= /-->
+    <body name="torso" pos="0 0 0">
+      <geom density="1000" fromto="1.5 0 0 0.5 0 0" size="0.1" type="capsule"/>
+      <joint axis="1 0 0" name="slider1" pos="0 0 0" type="slide"/>
+      <joint axis="0 1 0" name="slider2" pos="0 0 0" type="slide"/>
+      <joint axis="0 0 1" name="rot" pos="0 0 0" type="hinge"/>
+      <body name="mid" pos="0.5 0 0">
+        <geom density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule"/>
+        <joint axis="0 0 1" limited="true" name="rot2" pos="0 0 0" range="-100 100" type="hinge"/>
+        <body name="back" pos="-1 0 0">
+          <geom density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule"/>
+          <joint axis="0 0 1" limited="true" name="rot3" pos="0 0 0" range="-100 100" type="hinge"/>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot2"/>
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot3"/>
+  </actuator>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/walker2d.xml b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/walker2d.xml
new file mode 100644
index 0000000..cbc074d
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/assets/walker2d.xml
@@ -0,0 +1,61 @@
+<mujoco model="walker2d">
+  <compiler angle="degree" coordinate="global" inertiafromgeom="true"/>
+  <default>
+    <joint armature="0.01" damping=".1" limited="true"/>
+    <geom conaffinity="0" condim="3" contype="1" density="1000" friction=".7 .1 .1" rgba="0.8 0.6 .4 1"/>
+  </default>
+  <option integrator="RK4" timestep="0.002"/>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="40 40 40" type="plane" material="MatPlane"/>
+    <body name="torso" pos="0 0 1.25">
+      <joint armature="0" axis="1 0 0" damping="0" limited="false" name="rootx" pos="0 0 0" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 0 1" damping="0" limited="false" name="rootz" pos="0 0 0" ref="1.25" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 1 0" damping="0" limited="false" name="rooty" pos="0 0 1.25" stiffness="0" type="hinge"/>
+      <geom friction="0.9" fromto="0 0 1.45 0 0 1.05" name="torso_geom" size="0.05" type="capsule"/>
+      <body name="thigh" pos="0 0 1.05">
+        <joint axis="0 -1 0" name="thigh_joint" pos="0 0 1.05" range="-150 0" type="hinge"/>
+        <geom friction="0.9" fromto="0 0 1.05 0 0 0.6" name="thigh_geom" size="0.05" type="capsule"/>
+        <body name="leg" pos="0 0 0.35">
+          <joint axis="0 -1 0" name="leg_joint" pos="0 0 0.6" range="-150 0" type="hinge"/>
+          <geom friction="0.9" fromto="0 0 0.6 0 0 0.1" name="leg_geom" size="0.04" type="capsule"/>
+          <body name="foot" pos="0.2/2 0 0.1">
+            <joint axis="0 -1 0" name="foot_joint" pos="0 0 0.1" range="-45 45" type="hinge"/>
+            <geom friction="0.9" fromto="-0.0 0 0.1 0.2 0 0.1" name="foot_geom" size="0.06" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <!-- copied and then replace thigh->thigh_left, leg->leg_left, foot->foot_right -->
+      <body name="thigh_left" pos="0 0 1.05">
+        <joint axis="0 -1 0" name="thigh_left_joint" pos="0 0 1.05" range="-150 0" type="hinge"/>
+        <geom friction="0.9" fromto="0 0 1.05 0 0 0.6" name="thigh_left_geom" rgba=".7 .3 .6 1" size="0.05" type="capsule"/>
+        <body name="leg_left" pos="0 0 0.35">
+          <joint axis="0 -1 0" name="leg_left_joint" pos="0 0 0.6" range="-150 0" type="hinge"/>
+          <geom friction="0.9" fromto="0 0 0.6 0 0 0.1" name="leg_left_geom" rgba=".7 .3 .6 1" size="0.04" type="capsule"/>
+          <body name="foot_left" pos="0.2/2 0 0.1">
+            <joint axis="0 -1 0" name="foot_left_joint" pos="0 0 0.1" range="-45 45" type="hinge"/>
+            <geom friction="1.9" fromto="-0.0 0 0.1 0.2 0 0.1" name="foot_left_geom" rgba=".7 .3 .6 1" size="0.06" type="capsule"/>
+          </body>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <!-- <motor joint="torso_joint" ctrlrange="-100.0 100.0" isctrllimited="true"/>-->
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="100" joint="thigh_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="100" joint="leg_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="100" joint="foot_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="100" joint="thigh_left_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="100" joint="leg_left_joint"/>
+    <motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="100" joint="foot_left_joint"/>
+    <!-- <motor joint="finger2_rot" ctrlrange="-20.0 20.0" isctrllimited="true"/>-->
+  </actuator>
+    <asset>
+        <texture type="skybox" builtin="gradient" rgb1=".4 .5 .6" rgb2="0 0 0"
+            width="100" height="100"/>        
+        <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+        <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+        <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="60 60" texture="texplane"/>
+        <material name="geom" texture="texgeom" texuniform="true"/>
+    </asset>
+</mujoco>
\ No newline at end of file
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/__init__.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/__init__.py
new file mode 100644
index 0000000..eff23c0
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/__init__.py
@@ -0,0 +1,210 @@
+from gym.envs.registration import register
+
+register(
+    id='MBRLHalfCheetah-v0',
+    entry_point='envs.gym.half_cheetah:HalfCheetahEnv',
+    kwargs={'frame_skip': 5},
+    max_episode_steps=1000,
+)
+
+register(
+    id='MBRLWalker2d-v0',
+    entry_point='envs.gym.walker2d:Walker2dEnv',
+    kwargs={'frame_skip': 4},
+    max_episode_steps=1000,
+)
+
+register(
+    id='MBRLSwimmer-v0',
+    entry_point='envs.gym.swimmer:SwimmerEnv',
+    kwargs={'frame_skip': 4},
+    max_episode_steps=1000,
+)
+
+register(
+    id='MBRLAnt-v0',
+    entry_point='envs.gym.ant:AntEnv',
+    kwargs={'frame_skip': 5},
+    max_episode_steps=1000,
+)
+
+register(
+    id='MBRLHopper-v0',
+    entry_point='envs.gym.hopper:HopperEnv',
+    kwargs={'frame_skip': 4},
+    max_episode_steps=1000,
+)
+
+register(
+    id='MBRLReacher-v0',
+    entry_point='envs.gym.reacher:ReacherEnv',
+    max_episode_steps=50,
+)
+
+
+# second batch of environments
+
+register(
+    id='MBRLInvertedPendulum-v0',
+    entry_point='envs.gym.inverted_pendulum:InvertedPendulumEnv',
+    max_episode_steps=100,
+)
+register(
+    id='MBRLAcrobot-v0',
+    entry_point='envs.gym.acrobot:AcrobotEnv',
+    max_episode_steps=200,
+)
+register(
+    id='MBRLCartpole-v0',
+    entry_point='envs.gym.cartpole:CartPoleEnv',
+    max_episode_steps=200,
+
+)
+register(
+    id='MBRLMountain-v0',
+    entry_point='envs.gym.mountain_car:Continuous_MountainCarEnv',
+    max_episode_steps=200,
+
+)
+register(
+    id='MBRLPendulum-v0',
+    entry_point='envs.gym.pendulum:PendulumEnv',
+    max_episode_steps=200,
+)
+
+register(
+    id='gym_petsPusher-v0',
+    entry_point='envs.gym.pets_pusher:PusherEnv',
+    max_episode_steps=150,
+)
+register(
+    id='gym_petsReacher-v0',
+    entry_point='envs.gym.pets_reacher:Reacher3DEnv',
+    max_episode_steps=150,
+)
+register(
+    id='gym_petsCheetah-v0',
+    entry_point='envs.gym.pets_cheetah:HalfCheetahEnv',
+    max_episode_steps=1000,
+)
+
+# noisy env
+register(
+    id='gym_cheetahO01-v0',
+    entry_point='envs.gym.gym_cheetahO01:HalfCheetahEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_cheetahO001-v0',
+    entry_point='envs.gym.gym_cheetahO001:HalfCheetahEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_cheetahA01-v0',
+    entry_point='envs.gym.gym_cheetahA01:HalfCheetahEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_cheetahA003-v0',
+    entry_point='envs.gym.gym_cheetahA003:HalfCheetahEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_pendulumO01-v0',
+    entry_point='envs.gym.gym_pendulumO01:PendulumEnv',
+    max_episode_steps=200,
+)
+register(
+    id='gym_pendulumO001-v0',
+    entry_point='envs.gym.gym_pendulumO001:PendulumEnv',
+    max_episode_steps=200,
+)
+register(
+    id='gym_cartpoleO01-v0',
+    entry_point='envs.gym.gym_cartpoleO01:CartPoleEnv',
+    max_episode_steps=200,
+)
+register(
+    id='gym_cartpoleO001-v0',
+    entry_point='envs.gym.gym_cartpoleO001:CartPoleEnv',
+    max_episode_steps=200,
+)
+
+register(
+    id='gym_fant-v0',
+    entry_point='envs.gym.gym_fant:AntEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_fhopper-v0',
+    entry_point='envs.gym.gym_fhopper:HopperEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_fwalker2d-v0',
+    entry_point='envs.gym.gym_fwalker2d:Walker2dEnv',
+    max_episode_steps=1000,
+)
+register(
+    id='gym_fswimmer-v0',
+    entry_point='envs.gym.gym_fswimmer:fixedSwimmerEnv',
+    max_episode_steps=1000,
+)
+register(
+    id="gym_humanoid-v0",
+    entry_point='envs.gym.gym_humanoid:HumanoidEnv',
+    max_episode_steps=1000,
+)
+register(
+    id="gym_slimhumanoid-v0",
+    entry_point='envs.gym.gym_slimhumanoid:HumanoidEnv',
+    max_episode_steps=1000,
+)
+register(
+    id="gym_nostopslimhumanoid-v0",
+    entry_point='envs.gym.gym_nostopslimhumanoid:HumanoidEnv',
+    max_episode_steps=1000,
+)
+
+env_name_to_gym_registry = {
+    # first batch
+    "half_cheetah": "MBRLHalfCheetah-v0",
+    "swimmer": "MBRLSwimmer-v0",
+    "ant": "MBRLAnt-v0",
+    "hopper": "MBRLHopper-v0",
+    "reacher": "MBRLReacher-v0",
+    "walker2d": "MBRLWalker2d-v0",
+
+    # second batch
+    "invertedPendulum": "MBRLInvertedPendulum-v0",
+    "acrobot": 'MBRLAcrobot-v0',
+    "cartpole": 'MBRLCartpole-v0',
+    "mountain": 'MBRLMountain-v0',
+    "pendulum": 'MBRLPendulum-v0',
+
+    # the pets env
+    "gym_petsPusher": "gym_petsPusher-v0",
+    "gym_petsReacher": "gym_petsReacher-v0",
+    "gym_petsCheetah": "gym_petsCheetah-v0",
+
+    # the noise env
+    "gym_cheetahO01": "gym_cheetahO01-v0",
+    "gym_cheetahO001": "gym_cheetahO001-v0",
+    "gym_cheetahA01": "gym_cheetahA01-v0",
+    "gym_cheetahA003": "gym_cheetahA003-v0",
+
+    "gym_pendulumO01": "gym_pendulumO01-v0",
+    "gym_pendulumO001": "gym_pendulumO001-v0",
+
+    "gym_cartpoleO01": "gym_cartpoleO01-v0",
+    "gym_cartpoleO001": "gym_cartpoleO001-v0",
+
+    "gym_fant": "gym_fant-v0",
+    "gym_fswimmer": "gym_fswimmer-v0",
+    "gym_fhopper": "gym_fhopper-v0",
+    "gym_fwalker2d": "gym_fwalker2d-v0",
+
+    "gym_humanoid": "gym_humanoid-v0",
+    "gym_slimhumanoid": "gym_slimhumanoid-v0",
+    "gym_nostopslimhumanoid": "gym_nostopslimhumanoid-v0",
+}
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/acrobot.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/acrobot.py
new file mode 100644
index 0000000..1af57a6
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/acrobot.py
@@ -0,0 +1,357 @@
+"""classic Acrobot task"""
+from gym import core, spaces
+from gym.utils import seeding
+import numpy as np
+from numpy import sin, cos, pi
+from slbo.utils.dataset import Dataset, gen_dtype
+from lunzi.Logger import logger
+
+
+__copyright__ = "Copyright 2013, RLPy http://acl.mit.edu/RLPy"
+__credits__ = ["Alborz Geramifard", "Robert H. Klein", "Christoph Dann",
+               "William Dabney", "Jonathan P. How"]
+__license__ = "BSD 3-Clause"
+__author__ = "Christoph Dann <cdann@cdann.de>"
+
+# SOURCE:
+# https://github.com/rlpy/rlpy/blob/master/rlpy/Domains/Acrobot.py
+
+
+class AcrobotEnv(core.Env):
+
+    """
+    Acrobot is a 2-link pendulum with only the second joint actuated
+    Intitially, both links point downwards. The goal is to swing the
+    end-effector at a height at least the length of one link above the base.
+    Both links can swing freely and can pass by each other, i.e., they don't
+    collide when they have the same angle.
+    **STATE:**
+    The state consists of the two rotational joint angles and their velocities
+    [theta1 theta2 thetaDot1 thetaDot2]. An angle of 0 corresponds to corresponds
+    to the respective link pointing downwards (angles are in world coordinates).
+    **ACTIONS:**
+    The action is either applying +1, 0 or -1 torque on the joint between
+    the two pendulum links.
+    .. note::
+        The dynamics equations were missing some terms in the NIPS paper which
+        are present in the book. R. Sutton confirmed in personal correspondance
+        that the experimental results shown in the paper and the book were
+        generated with the equations shown in the book.
+        However, there is the option to run the domain with the paper equations
+        by setting book_or_nips = 'nips'
+    **REFERENCE:**
+    .. seealso::
+        R. Sutton: Generalization in Reinforcement Learning:
+        Successful Examples Using Sparse Coarse Coding (NIPS 1996)
+    .. seealso::
+        R. Sutton and A. G. Barto:
+        Reinforcement learning: An introduction.
+        Cambridge: MIT press, 1998.
+    .. warning::
+        This version of the domain uses the Runge-Kutta method for integrating
+        the system dynamics and is more realistic, but also considerably harder
+        than the original version which employs Euler integration,
+        see the AcrobotLegacy class.
+    """
+
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 15
+    }
+
+    dt = .2
+
+    LINK_LENGTH_1 = 1.  # [m]
+    LINK_LENGTH_2 = 1.  # [m]
+    LINK_MASS_1 = 1.  #: [kg] mass of link 1
+    LINK_MASS_2 = 1.  #: [kg] mass of link 2
+    LINK_COM_POS_1 = 0.5  #: [m] position of the center of mass of link 1
+    LINK_COM_POS_2 = 0.5  #: [m] position of the center of mass of link 2
+    LINK_MOI = 1.  #: moments of inertia for both links
+
+    MAX_VEL_1 = 4 * np.pi
+    MAX_VEL_2 = 9 * np.pi
+
+    AVAIL_TORQUE = [-1., 0., +1]
+
+    torque_noise_max = 0.
+
+    #: use dynamics equations from the nips paper or the book
+    book_or_nips = "book"
+    action_arrow = None
+    domain_fig = None
+    actions_num = 3
+
+    def __init__(self):
+        self.viewer = None
+        high = np.array([1.0, 1.0, 1.0, 1.0, self.MAX_VEL_1, self.MAX_VEL_2])
+        low = -high
+        self.observation_space = spaces.Box(low, high)
+        self.action_space = spaces.Box(np.array([-1.0]), np.array([1.0]))
+        self.state = None
+        self._seed()
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _reset(self):
+        self.state = self.np_random.uniform(low=-0.1, high=0.1, size=(4,))
+        return self._get_ob()
+
+    def _step(self, a):
+        # Discretize
+        if a[0] < -.33:
+            action = 0
+        elif a[0] < .33:
+            action = 1
+        else:
+            action = 2
+
+        s = self.state
+        reward = -np.cos(s[0]) - np.cos(s[1] + s[0])
+        torque = self.AVAIL_TORQUE[action]
+
+        # Add noise to the force action
+        if self.torque_noise_max > 0:
+            torque += self.np_random.uniform(-self.torque_noise_max, self.torque_noise_max)
+
+        # Now, augment the state with our force action so it can be passed to
+        # _dsdt
+        s_augmented = np.append(s, torque)
+
+        ns = rk4(self._dsdt, s_augmented, [0, self.dt])
+        # only care about final timestep of integration returned by integrator
+        ns = ns[-1]
+        ns = ns[:4]  # omit action
+        # ODEINT IS TOO SLOW!
+        # ns_continuous = integrate.odeint(self._dsdt, self.s_continuous, [0, self.dt])
+        # self.s_continuous = ns_continuous[-1] # We only care about the state
+        # at the ''final timestep'', self.dt
+
+        ns[0] = wrap(ns[0], -pi, pi)
+        ns[1] = wrap(ns[1], -pi, pi)
+        ns[2] = bound(ns[2], -self.MAX_VEL_1, self.MAX_VEL_1)
+        ns[3] = bound(ns[3], -self.MAX_VEL_2, self.MAX_VEL_2)
+        self.state = ns
+        # terminal = self._terminal()
+        terminal = False
+        # reward = -1. if not terminal else 0.
+        return (self._get_ob(), reward, terminal, {})
+
+    def _get_ob(self):
+        s = self.state
+        return np.array([cos(s[0]), np.sin(s[0]), cos(s[1]), sin(s[1]), s[2], s[3]])
+
+    def _terminal(self):
+        s = self.state
+        return bool(-np.cos(s[0]) - np.cos(s[1] + s[0]) > 1.)
+
+    def _dsdt(self, s_augmented, t):
+        m1 = self.LINK_MASS_1
+        m2 = self.LINK_MASS_2
+        l1 = self.LINK_LENGTH_1
+        lc1 = self.LINK_COM_POS_1
+        lc2 = self.LINK_COM_POS_2
+        I1 = self.LINK_MOI
+        I2 = self.LINK_MOI
+        g = 9.8
+        a = s_augmented[-1]
+        s = s_augmented[:-1]
+        theta1 = s[0]
+        theta2 = s[1]
+        dtheta1 = s[2]
+        dtheta2 = s[3]
+        d1 = m1 * lc1 ** 2 + m2 * \
+            (l1 ** 2 + lc2 ** 2 + 2 * l1 * lc2 * np.cos(theta2)) + I1 + I2
+        d2 = m2 * (lc2 ** 2 + l1 * lc2 * np.cos(theta2)) + I2
+        phi2 = m2 * lc2 * g * np.cos(theta1 + theta2 - np.pi / 2.)
+        phi1 = - m2 * l1 * lc2 * dtheta2 ** 2 * np.sin(theta2) \
+               - 2 * m2 * l1 * lc2 * dtheta2 * dtheta1 * np.sin(theta2)  \
+            + (m1 * lc1 + m2 * l1) * g * np.cos(theta1 - np.pi / 2) + phi2
+        if self.book_or_nips == "nips":
+            # the following line is consistent with the description in the
+            # paper
+            ddtheta2 = (a + d2 / d1 * phi1 - phi2) / \
+                (m2 * lc2 ** 2 + I2 - d2 ** 2 / d1)
+        else:
+            # the following line is consistent with the java implementation and the
+            # book
+            ddtheta2 = (a + d2 / d1 * phi1 - m2 * l1 * lc2 * dtheta1 ** 2 * np.sin(theta2) - phi2) \
+                / (m2 * lc2 ** 2 + I2 - d2 ** 2 / d1)
+        ddtheta1 = -(d2 * ddtheta2 + phi1) / d1
+        return (dtheta1, dtheta2, ddtheta1, ddtheta2, 0.)
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+        from gym.envs.classic_control import rendering
+
+        s = self.state
+
+        if self.viewer is None:
+            self.viewer = rendering.Viewer(500, 500)
+            self.viewer.set_bounds(-2.2, 2.2, -2.2, 2.2)
+
+        if s is None:
+            return None
+
+        p1 = [-self.LINK_LENGTH_1 *
+              np.cos(s[0]), self.LINK_LENGTH_1 * np.sin(s[0])]
+
+        p2 = [p1[0] - self.LINK_LENGTH_2 * np.cos(s[0] + s[1]),
+              p1[1] + self.LINK_LENGTH_2 * np.sin(s[0] + s[1])]
+
+        xys = np.array([[0, 0], p1, p2])[:, ::-1]
+        thetas = [s[0] - np.pi / 2, s[0] + s[1] - np.pi / 2]
+
+        self.viewer.draw_line((-2.2, 1), (2.2, 1))
+        for ((x, y), th) in zip(xys, thetas):
+            l, r, t, b = 0, 1, .1, -.1
+            jtransform = rendering.Transform(rotation=th, translation=(x, y))
+            link = self.viewer.draw_polygon([(l, b), (l, t), (r, t), (r, b)])
+            link.add_attr(jtransform)
+            link.set_color(0, .8, .8)
+            circ = self.viewer.draw_circle(.1)
+            circ.set_color(.8, .8, 0)
+            circ.add_attr(jtransform)
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+            def height(obs):
+                h1 = obs[0]  # Height of first arm
+                h2 = obs[0] * obs[2] - obs[1] * obs[3]  # Height of second arm
+                return -(h1 + h2)  # total height
+
+            start_height = height(data_dict['start_state'])
+        """
+        h1 = obs[:, 0]  # Height of first arm
+        h2 = obs[:, 0] * obs[:, 2] - obs[:, 1] * obs[:, 3]  # Height of second arm
+        return (h1 + h2)
+
+    def verify(self, n=2000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
+
+
+def wrap(x, m, M):
+    """
+    :param x: a scalar
+    :param m: minimum possible value in range
+    :param M: maximum possible value in range
+    Wraps ``x`` so m <= x <= M; but unlike ``bound()`` which
+    truncates, ``wrap()`` wraps x around the coordinate system defined by m,M.\n
+    For example, m = -180, M = 180 (degrees), x = 360 --> returns 0.
+    """
+    diff = M - m
+    while x > M:
+        x = x - diff
+    while x < m:
+        x = x + diff
+    return x
+
+
+def bound(x, m, M=None):
+    """
+    :param x: scalar
+    Either have m as scalar, so bound(x,m,M) which returns m <= x <= M *OR*
+    have m as length 2 vector, bound(x,m, <IGNORED>) returns m[0] <= x <= m[1].
+    """
+    if M is None:
+        M = m[1]
+        m = m[0]
+    # bound x between min (m) and Max (M)
+    return min(max(x, m), M)
+
+
+def rk4(derivs, y0, t, *args, **kwargs):
+    """
+    Integrate 1D or ND system of ODEs using 4-th order Runge-Kutta.
+    This is a toy implementation which may be useful if you find
+    yourself stranded on a system w/o scipy.  Otherwise use
+    :func:`scipy.integrate`.
+    *y0*
+        initial state vector
+    *t*
+        sample times
+    *derivs*
+        returns the derivative of the system and has the
+        signature ``dy = derivs(yi, ti)``
+    *args*
+        additional arguments passed to the derivative function
+    *kwargs*
+        additional keyword arguments passed to the derivative function
+    Example 1 ::
+        ## 2D system
+        def derivs6(x,t):
+            d1 =  x[0] + 2*x[1]
+            d2 =  -3*x[0] + 4*x[1]
+            return (d1, d2)
+        dt = 0.0005
+        t = arange(0.0, 2.0, dt)
+        y0 = (1,2)
+        yout = rk4(derivs6, y0, t)
+    Example 2::
+        ## 1D system
+        alpha = 2
+        def derivs(x,t):
+            return -alpha*x + exp(-t)
+        y0 = 1
+        yout = rk4(derivs, y0, t)
+    If you have access to scipy, you should probably be using the
+    scipy.integrate tools rather than this function.
+    """
+
+    try:
+        Ny = len(y0)
+    except TypeError:
+        yout = np.zeros((len(t),), np.float_)
+    else:
+        yout = np.zeros((len(t), Ny), np.float_)
+
+    yout[0] = y0
+    i = 0
+
+    for i in np.arange(len(t) - 1):
+
+        thist = t[i]
+        dt = t[i + 1] - thist
+        dt2 = dt / 2.0
+        y0 = yout[i]
+
+        k1 = np.asarray(derivs(y0, thist, *args, **kwargs))
+        k2 = np.asarray(derivs(y0 + dt2 * k1, thist + dt2, *args, **kwargs))
+        k3 = np.asarray(derivs(y0 + dt2 * k2, thist + dt2, *args, **kwargs))
+        k4 = np.asarray(derivs(y0 + dt * k3, thist + dt, *args, **kwargs))
+        yout[i + 1] = y0 + dt / 6.0 * (k1 + 2 * k2 + 2 * k3 + k4)
+    return yout
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/ant.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/ant.py
new file mode 100644
index 0000000..a2699aa
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/ant.py
@@ -0,0 +1,81 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class AntEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/ant.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        ob = self._get_obs()
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = old_ob[13]
+        reward_height = -3.0 * np.square(old_ob[0] - 0.57)
+        reward = reward_run + reward_ctrl + reward_height + 1.0
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            # (self.model.data.qpos.flat[:1] - self.prev_qpos[:1]) / self.dt,
+            # self.get_body_comvel("torso")[:1],
+            self.model.data.qpos.flat[2:],
+            self.model.data.qvel.flat,
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        qpos = self.init_qpos + self.np_random.uniform(size=self.model.nq, low=-.1, high=.1)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        # self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 13]
+        reward_height = -3.0 * np.square(obs[:, 0] - 0.57)
+        reward = reward_run + reward_ctrl + reward_height + 1.0
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        # reward_height = -3.0 * tf.square(next_obs[:, 0] - 0.57)
+        reward = reward_run + reward_ctrl # + reward_height
+        return -reward
+        """
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/cartpole.xml b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/cartpole.xml
new file mode 100644
index 0000000..284a58c
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/cartpole.xml
@@ -0,0 +1,35 @@
+<mujoco model="cartpole">
+	<compiler inertiafromgeom="true"/>
+	<default>
+		<joint armature="0" damping="1" limited="true"/>
+		<geom contype="0" friction="1 0.1 0.1" rgba="0.7 0.7 0 1"/>
+		<tendon/>
+		<motor ctrlrange="-3 3" ctrllimited='true'/>
+	</default>
+    <asset>
+		<texture type="skybox" builtin="checker" rgb1="1 1 1" rgb2="1 1 1"
+                    width="256" height="256"/>
+        <texture name="texgeom" type="cube" builtin="flat" mark="cross" width="127" height="1278" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" markrgb="1 1 1" random="0.01" />
+		<texture name="texplane" type="2d" builtin="checker" rgb1=".5 .5 .5" rgb2=".5 .5 .5" width="100" height="100" />
+        <texture name="texplane_show" type="2d" builtin="checker" rgb1="0 0 0" rgb2="0.9 0.9 0.9" width="100" height="100" />
+        <material name='MatPlane' texture="texplane" shininess="1" texrepeat="30 30" specular="1"  reflectance="0.5" />
+        <material name='geom' texture="texgeom" texuniform="true" />
+ 	</asset>
+	<option gravity="0 0 -9.81" integrator="RK4" timestep="0.02"/>
+	<size nstack="3000"/>
+	<worldbody>
+		<geom name="rail" pos="0 0 0" quat="0.707 0 0.707 0" rgba="0.3 0.3 0.7 1" size="0.02 3" type="capsule"/>
+		<body name="cart" pos="0 0 0">
+			<joint axis="1 0 0" limited="true" name="slider" pos="0 0 0" range="-2.5 2.5" type="slide"/>
+			<geom name="cart" pos="0 0 0" quat="0.707 0 0.707 0" size="0.1 0.1" type="capsule"/>
+			<body name="pole" pos="0 0 0">
+				<joint axis="0 1 0" limited="false" name="hinge" pos="0 0 0" range="-180 180" type="hinge"/>
+				<geom fromto="0 0 0 0.001 0 -0.6" name="cpole" rgba="0 0.7 0.7 1" size="0.049 0.3" type="capsule"/>
+			</body>
+		</body>
+	</worldbody>
+	<actuator>
+		<motor gear="100" joint="slider" name="slide"/>
+	</actuator>
+</mujoco>
+
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/fixed_swimmer.xml b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/fixed_swimmer.xml
new file mode 100644
index 0000000..142f344
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/fixed_swimmer.xml
@@ -0,0 +1,43 @@
+<mujoco model="swimmer">
+  <compiler angle="degree" coordinate="local" inertiafromgeom="true"/>
+  <option collision="predefined" density="4000" integrator="RK4" timestep="0.01" viscosity="0.1"/>
+  <default>
+    <geom conaffinity="1" condim="1" contype="1" material="geom" rgba="0.8 0.6 .4 1"/>
+    <joint armature='0.1'  />
+  </default>
+  <asset>
+    <texture builtin="gradient" height="100" rgb1="1 1 1" rgb2="0 0 0" type="skybox" width="100"/>
+    <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+    <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+    <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="30 30" texture="texplane"/>
+    <material name="geom" texture="texgeom" texuniform="true"/>
+  </asset>
+
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 -0.1" rgba="0.8 0.9 0.8 1" size="40 40 0.1" type="plane"/>
+    <body name="podBody_1" pos="0 0 0">
+      <geom name='pod_1' density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule"/>
+      <joint axis="1 0 0" name="slider1" pos="0 0 0" type="slide"/>
+      <joint axis="0 1 0" name="slider2" pos="0 0 0" type="slide"/>
+      <joint axis="0 0 1" name="rot_1" pos="-1.5 0 0" type="hinge"/>
+      <site name="tip" pos="0 0 0" size="0.02 0.02"/>
+      <body name="podBody_2" pos="-1 0 0">
+        <geom name='pod_2' density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule"/>
+        <joint axis="0 0 1" limited="true" name="rot_2" pos="0 0 0" range="-100 100" type="hinge"/>
+        
+        <body name="podBody_3" pos="-1 0 0">
+          <geom name='pod_3' density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule"/>
+          <joint axis="0 0 1" limited="true" name="rot_3" pos="0 0 0" range="-100 100" type="hinge"/>
+        
+        </body>
+
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot_2"/>
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot_3"/>
+  </actuator>
+
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/half_cheetah.xml b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/half_cheetah.xml
new file mode 100644
index 0000000..40a1cb6
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/half_cheetah.xml
@@ -0,0 +1,95 @@
+<!-- Cheetah Model
+
+    The state space is populated with joints in the order that they are
+    defined in this file. The actuators also operate on joints.
+
+    State-Space (name/joint/parameter):
+        - rootx     slider      position (m)
+        - rootz     slider      position (m)
+        - rooty     hinge       angle (rad)
+        - bthigh    hinge       angle (rad)
+        - bshin     hinge       angle (rad)
+        - bfoot     hinge       angle (rad)
+        - fthigh    hinge       angle (rad)
+        - fshin     hinge       angle (rad)
+        - ffoot     hinge       angle (rad)
+        - rootx     slider      velocity (m/s)
+        - rootz     slider      velocity (m/s)
+        - rooty     hinge       angular velocity (rad/s)
+        - bthigh    hinge       angular velocity (rad/s)
+        - bshin     hinge       angular velocity (rad/s)
+        - bfoot     hinge       angular velocity (rad/s)
+        - fthigh    hinge       angular velocity (rad/s)
+        - fshin     hinge       angular velocity (rad/s)
+        - ffoot     hinge       angular velocity (rad/s)
+
+    Actuators (name/actuator/parameter):
+        - bthigh    hinge       torque (N m)
+        - bshin     hinge       torque (N m)
+        - bfoot     hinge       torque (N m)
+        - fthigh    hinge       torque (N m)
+        - fshin     hinge       torque (N m)
+        - ffoot     hinge       torque (N m)
+
+-->
+<mujoco model="cheetah">
+  <compiler angle="radian" coordinate="local" inertiafromgeom="true" settotalmass="14"/>
+  <default>
+    <joint armature=".1" damping=".01" limited="true" solimplimit="0 .8 .03" solreflimit=".02 1" stiffness="8"/>
+    <geom conaffinity="0" condim="3" contype="1" friction=".4 .1 .1" rgba="0.8 0.6 .4 1" solimp="0.0 0.8 0.01" solref="0.02 1"/>
+    <motor ctrllimited="true" ctrlrange="-1 1"/>
+  </default>
+  <size nstack="300000" nuser_geom="1"/>
+  <option gravity="0 0 -9.81" timestep="0.01"/>
+  <asset>
+    <texture builtin="gradient" height="100" rgb1="1 1 1" rgb2="0 0 0" type="skybox" width="100"/>
+    <texture builtin="flat" height="1278" mark="cross" markrgb="1 1 1" name="texgeom" random="0.01" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" type="cube" width="127"/>
+    <texture builtin="checker" height="100" name="texplane" rgb1="0 0 0" rgb2="0.8 0.8 0.8" type="2d" width="100"/>
+    <material name="MatPlane" reflectance="0.5" shininess="1" specular="1" texrepeat="150 150" texture="texplane"/>
+    <material name="geom" texture="texgeom" texuniform="true"/>
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="200 200 200" type="plane"/>
+    <body name="torso" pos="0 0 .7">
+      <joint armature="0" axis="1 0 0" damping="0" limited="false" name="rootx" pos="0 0 0" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 0 1" damping="0" limited="false" name="rootz" pos="0 0 0" stiffness="0" type="slide"/>
+      <joint armature="0" axis="0 1 0" damping="0" limited="false" name="rooty" pos="0 0 0" stiffness="0" type="hinge"/>
+      <geom fromto="-.5 0 0 .5 0 0" name="torso" size="0.046" type="capsule"/>
+      <geom axisangle="0 1 0 .87" name="head" pos=".6 0 .1" size="0.046 .15" type="capsule"/>
+      <!-- <site name='tip'  pos='.15 0 .11'/>-->
+      <body name="bthigh" pos="-.5 0 0">
+        <joint axis="0 1 0" damping="6" name="bthigh" pos="0 0 0" range="-.52 1.05" stiffness="240" type="hinge"/>
+        <geom axisangle="0 1 0 -3.8" name="bthigh" pos=".1 0 -.13" size="0.046 .145" type="capsule"/>
+        <body name="bshin" pos=".16 0 -.25">
+          <joint axis="0 1 0" damping="4.5" name="bshin" pos="0 0 0" range="-.785 .785" stiffness="180" type="hinge"/>
+          <geom axisangle="0 1 0 -2.03" name="bshin" pos="-.14 0 -.07" rgba="0.9 0.6 0.6 1" size="0.046 .15" type="capsule"/>
+          <body name="bfoot" pos="-.28 0 -.14">
+            <joint axis="0 1 0" damping="3" name="bfoot" pos="0 0 0" range="-.4 .785" stiffness="120" type="hinge"/>
+            <geom axisangle="0 1 0 -.27" name="bfoot" pos=".03 0 -.097" rgba="0.9 0.6 0.6 1" size="0.046 .094" type="capsule"/>
+          </body>
+        </body>
+      </body>
+      <body name="fthigh" pos=".5 0 0">
+        <joint axis="0 1 0" damping="4.5" name="fthigh" pos="0 0 0" range="-1 .7" stiffness="180" type="hinge"/>
+        <geom axisangle="0 1 0 .52" name="fthigh" pos="-.07 0 -.12" size="0.046 .133" type="capsule"/>
+        <body name="fshin" pos="-.14 0 -.24">
+          <joint axis="0 1 0" damping="3" name="fshin" pos="0 0 0" range="-1.2 .87" stiffness="120" type="hinge"/>
+          <geom axisangle="0 1 0 -.6" name="fshin" pos=".065 0 -.09" rgba="0.9 0.6 0.6 1" size="0.046 .106" type="capsule"/>
+          <body name="ffoot" pos=".13 0 -.18">
+            <joint axis="0 1 0" damping="1.5" name="ffoot" pos="0 0 0" range="-.5 .5" stiffness="60" type="hinge"/>
+            <geom axisangle="0 1 0 -.6" name="ffoot" pos=".045 0 -.07" rgba="0.9 0.6 0.6 1" size="0.046 .07" type="capsule"/>
+          </body>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor gear="120" joint="bthigh" name="bthigh"/>
+    <motor gear="90" joint="bshin" name="bshin"/>
+    <motor gear="60" joint="bfoot" name="bfoot"/>
+    <motor gear="120" joint="fthigh" name="fthigh"/>
+    <motor gear="60" joint="fshin" name="fshin"/>
+    <motor gear="30" joint="ffoot" name="ffoot"/>
+  </actuator>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/pusher.xml b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/pusher.xml
new file mode 100644
index 0000000..9e81b01
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/pusher.xml
@@ -0,0 +1,101 @@
+<mujoco model="arm3d">
+  <compiler inertiafromgeom="true" angle="radian" coordinate="local"/>
+  <option timestep="0.01" gravity="0 0 0" iterations="20" integrator="Euler" />
+  
+  <default>
+    <joint armature='0.04' damping="1" limited="true"/>
+    <geom friction=".8 .1 .1" density="300" margin="0.002" condim="1" contype="0" conaffinity="0"/>
+  </default>
+  <asset>
+    <texture type="skybox" builtin="checker" rgb1="1 1 1" rgb2="1 1 1"
+             width="256" height="256"/>
+    <texture name="texgeom" type="cube" builtin="flat" mark="cross" width="127" height="1278" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" markrgb="1 1 1" random="0.01" />
+    <texture name="texplane" type="2d" builtin="checker" rgb1=".5 .5 .5" rgb2=".5 .5 .5" width="100" height="100" />
+    <texture name="texplane_show" type="2d" builtin="checker" rgb1="0 0 0" rgb2="0.9 0.9 0.9" width="100" height="100" />
+    <material name='MatPlane' texture="texplane" shininess="1" texrepeat="30 30" specular="1"  reflectance="0.5" />
+    <material name='geom' texture="texgeom" texuniform="true" />
+  </asset>
+
+  <worldbody>
+    <light diffuse=".5 .5 .5" pos="0 0 3" dir="0 0 -1"/>
+    <geom name="table" type="plane" pos="0 0.5 -0.325" size="1 1 0.1" contype="1" conaffinity="1"/>
+
+    <body name="r_shoulder_pan_link" pos="0 -0.6 0">
+      <geom name="e1" type="sphere" rgba="0.6 0.6 0.6 1" pos="-0.06 0.05 0.2" size="0.05" />
+      <geom name="e2" type="sphere" rgba="0.6 0.6 0.6 1" pos=" 0.06 0.05 0.2" size="0.05" />
+      <geom name="e1p" type="sphere" rgba="0.1 0.1 0.1 1" pos="-0.06 0.09 0.2" size="0.03" />
+      <geom name="e2p" type="sphere" rgba="0.1 0.1 0.1 1" pos=" 0.06 0.09 0.2" size="0.03" />
+      <geom name="sp" type="capsule" fromto="0 0 -0.4 0 0 0.2" size="0.1" />
+      <!--<joint name="r_shoulder_pan_joint" type="hinge" pos="0 0 0" axis="0 0 1" range="-2.2854 1.714602" damping="1.0" />-->
+      <joint name="r_shoulder_pan_joint" type="hinge" pos="0 0 0" axis="0 0 1" range="-2.2854 2.0" damping="1.0" />
+
+      <body name="r_shoulder_lift_link" pos="0.1 0 0">
+        <geom name="sl" type="capsule" fromto="0 -0.1 0 0 0.1 0" size="0.1" />
+        <joint name="r_shoulder_lift_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.5236 1.3963" damping="1.0" />
+
+        <body name="r_upper_arm_roll_link" pos="0 0 0">
+          <geom name="uar" type="capsule" fromto="-0.1 0 0 0.1 0 0" size="0.02" />
+          <joint name="r_upper_arm_roll_joint" type="hinge" pos="0 0 0" axis="1 0 0" range="-1.5 1.7" damping="0.1" />
+
+          <body name="r_upper_arm_link" pos="0 0 0">
+            <geom name="ua" type="capsule" fromto="0 0 0 0.4 0 0" size="0.06" />
+
+            <body name="r_elbow_flex_link" pos="0.4 0 0">
+              <geom name="ef" type="capsule" fromto="0 -0.02 0 0.0 0.02 0" size="0.06" />
+              <joint name="r_elbow_flex_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-2.3213 0" damping="0.1" />
+
+              <body name="r_forearm_roll_link" pos="0 0 0">
+                <geom name="fr" type="capsule" fromto="-0.1 0 0 0.1 0 0" size="0.02" />
+                <joint name="r_forearm_roll_joint" type="hinge" limited="true" pos="0 0 0" axis="1 0 0" damping=".1" range="-1.5 1.5"/>
+
+                <body name="r_forearm_link" pos="0 0 0">
+                  <geom name="fa" type="capsule" fromto="0 0 0 0.291 0 0" size="0.05" />
+
+                  <body name="r_wrist_flex_link" pos="0.321 0 0">
+                    <geom name="wf" type="capsule" fromto="0 -0.02 0 0 0.02 0" size="0.01" />
+                    <joint name="r_wrist_flex_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-1.094 0" damping=".1" />
+
+                    <body name="r_wrist_roll_link" pos="0 0 0">
+                      <joint name="r_wrist_roll_joint" type="hinge" pos="0 0 0" limited="true" axis="1 0 0" damping="0.1" range="-1.5 1.5"/>
+                      <body name="tips_arm" pos="0 0 0">
+                        <geom name="tip_arml" type="sphere" pos="0.1 -0.1 0." size="0.01" />
+                        <geom name="tip_armr" type="sphere" pos="0.1 0.1 0." size="0.01" />
+                      </body>
+                      <geom type="capsule" fromto="0 -0.1 0. 0.0 +0.1 0" size="0.02" contype="1" conaffinity="1" />
+                      <geom type="capsule" fromto="0 -0.1 0. 0.1 -0.1 0" size="0.02" contype="1" conaffinity="1" />
+                      <geom type="capsule" fromto="0 +0.1 0. 0.1 +0.1 0." size="0.02" contype="1" conaffinity="1" />
+                    </body>
+                  </body>
+                </body>
+              </body>
+            </body>
+          </body>
+        </body>
+      </body>
+    </body>
+
+    <!--<body name="object" pos="0.55 -0.3 -0.275" >-->
+    <body name="object" pos="0.45 -0.05 -0.275" >
+      <geom rgba="1 1 1 0" type="sphere" size="0.05 0.05 0.05" density="0.00001" conaffinity="0"/>
+      <geom rgba="1 1 1 1" type="cylinder" size="0.05 0.05 0.05" density="0.00001" contype="1" conaffinity="0"/>
+      <joint name="obj_slidey" type="slide" pos="0 0 0" axis="0 1 0" range="-10.3213 10.3" damping="0.5"/>
+      <joint name="obj_slidex" type="slide" pos="0 0 0" axis="1 0 0" range="-10.3213 10.3" damping="0.5"/>
+    </body>
+
+    <body name="goal" pos="0.45 -0.05 -0.3230">
+      <geom rgba="1 0 0 1" type="cylinder" size="0.08 0.001 0.1" density='0.00001' contype="0" conaffinity="0"/>
+      <joint name="goal_slidey" type="slide" pos="0 0 0" axis="0 1 0" range="-10.3213 10.3" damping="0.5"/>
+      <joint name="goal_slidex" type="slide" pos="0 0 0" axis="1 0 0" range="-10.3213 10.3" damping="0.5"/> 
+    </body>
+  </worldbody>
+
+  <actuator>
+    <motor joint="r_shoulder_pan_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_shoulder_lift_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_upper_arm_roll_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_elbow_flex_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_forearm_roll_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_wrist_flex_joint" ctrlrange="-2.0 2.0" ctrllimited="true" />
+    <motor joint="r_wrist_roll_joint" ctrlrange="-2.0 2.0" ctrllimited="true"/>
+  </actuator>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/reacher3d.xml b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/reacher3d.xml
new file mode 100644
index 0000000..a51c71b
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/assets/reacher3d.xml
@@ -0,0 +1,154 @@
+<mujoco model="arm3d">
+
+    <compiler inertiafromgeom="true" angle="radian" coordinate="local" />
+    <option timestep="0.01" gravity="0 0 0" iterations="20" integrator="RK4" />
+    <default>
+        <joint armature="0.04" damping="1" limited="true" />
+        <geom friction=".5 .1 .1" margin="0.002" condim="1" contype="0" conaffinity="0" />
+    </default>
+    <asset>
+        <texture type="skybox" builtin="checker" rgb1="1 1 1" rgb2="1 1 1"
+                 width="256" height="256"/>
+        <texture name="texgeom" type="cube" builtin="flat" mark="cross" width="127" height="1278" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" markrgb="1 1 1" random="0.01" />
+        <texture name="texplane" type="2d" builtin="checker" rgb1=".5 .5 .5" rgb2=".5 .5 .5" width="100" height="100" />
+        <texture name="texplane_show" type="2d" builtin="checker" rgb1="0 0 0" rgb2="0.9 0.9 0.9" width="100" height="100" />
+        <material name='MatPlane' texture="texplane" shininess="1" texrepeat="30 30" specular="1"  reflectance="0.5" />
+        <material name='geom' texture="texgeom" texuniform="true" />
+    </asset>
+    <worldbody>
+        <light pos="0 0 5" />
+        <body name="r_shoulder_pan_link" pos="0 -0.188 0">
+            <geom name="e1" type="sphere" rgba="0.6 0.6 0.6 1" pos="-0.06 0.05 0.2" size="0.05" />
+            <geom name="e2" type="sphere" rgba="0.6 0.6 0.6 1" pos=" 0.06 0.05 0.2" size="0.05" />
+            <geom name="e1p" type="sphere" rgba="0.1 0.1 0.1 1" pos="-0.06 0.09 0.2" size="0.03" />
+            <geom name="e2p" type="sphere" rgba="0.1 0.1 0.1 1" pos=" 0.06 0.09 0.2" size="0.03" />
+            <geom name="sp" type="capsule" fromto="0 0 -0.4 0 0 0.2" size="0.1" />
+            <joint name="r_shoulder_pan_joint" type="hinge" pos="0 0 0" axis="0 0 1" range="-2.2854 1.714602" damping="10.0" />
+
+            <body name="r_shoulder_lift_link" pos="0.1 0 0">
+                <geom name="sl" type="capsule" fromto="0 -0.1 0 0 0.1 0" size="0.1" />
+                <joint name="r_shoulder_lift_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.5236 1.3963" damping="10.0" />
+
+                <body name="r_upper_arm_roll_link" pos="0 0 0">
+                    <geom name="uar" type="capsule" fromto="-0.1 0 0 0.1 0 0" size="0.02" />
+                    <joint name="r_upper_arm_roll_joint" type="hinge" pos="0 0 0" axis="1 0 0" range="-3.9 0.8" damping="0.1" />
+
+                    <body name="r_upper_arm_link" pos="0 0 0">
+                        <geom name="ua" type="capsule" fromto="0 0 0 0.4 0 0" size="0.06" />
+
+                        <body name="r_elbow_flex_link" pos="0.4 0 0">
+                            <geom name="ef" type="capsule" fromto="0 -0.02 0 0.0 0.02 0" size="0.06" />
+                            <joint name="r_elbow_flex_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-2.3213 0" damping="1.0" />
+
+                            <body name="r_forearm_roll_link" pos="0 0 0">
+                                <geom name="fr" type="capsule" fromto="-0.1 0 0 0.1 0 0" size="0.02" />
+                                <joint name="r_forearm_roll_joint" type="hinge" limited="false" pos="0 0 0" axis="1 0 0" damping=".1" />
+
+                                <body name="r_forearm_link" pos="0 0 0">
+                                    <geom name="fa" type="capsule" fromto="0 0 0 0.321 0 0" size="0.05" />
+
+                                    <body name="r_wrist_flex_link" pos="0.321 0 0">
+                                        <geom name="wf" type="capsule" fromto="0 -0.02 0 0 0.02 0" size="0.01" />
+                                        <joint name="r_wrist_flex_joint" type="hinge" pos="0 0 0" axis="0 1 0" range="-2.094 0" damping=".1" />
+
+                                        <body name="r_wrist_roll_link" pos="0 0 0">
+                                            <geom name="wr" type="capsule" fromto="-0.02 0 0 0.02 0 0" size="0.01" />
+                                            <joint name="r_wrist_roll_joint" type="hinge" pos="0 0 0" limited="false" axis="1 0 0" damping="0.1" />
+
+                                            <body name="r_gripper_palm_link" pos="0 0 0">
+                                                <geom name="pl" type="capsule" fromto="0.05 0 -0.02 0.05 0 0.02" size="0.05" />
+
+                                                <!--
+                                                <body name="r_gripper_tool_frame" pos="0.18 0 0">
+                                                    <site name="leg_bottom" pos="0 0 -0.15" size="0.01" />
+                                                    <site name="leg_top" pos="0 0 0.15" size="0.01" />
+
+                                                    <body name="ball" pos="0 0 0">
+                                                        <geom name="ball_geom" rgba="0.8 0.6 0.6 1" type="cylinder" fromto="0 0 -0.15 0 0 0.15" size="0.028" density="2000" contype="2" conaffinity="1" />
+                                                    </body>
+                                                </body>
+                                                -->
+
+                                                <body name="r_gripper_l_finger_link" pos="0.07691 0.03 0">
+                                                    <geom name="gf3" type="capsule" fromto="0 0 0 0.09137 0.00495 0" size="0.01" />
+
+                                                    <body name="r_gripper_l_finger_tip_link" pos="0.09137 0.00495 0">
+                                                        <geom name="gf4" type="capsule" fromto="0 0 0 0.09137 0.0 0" size="0.01" />
+                                                    </body>
+                                                </body>
+
+                                                <body name="r_gripper_r_finger_link" pos="0.07691 -0.03 0">
+                                                    <geom name="gf1" type="capsule" fromto="0 0 0 0.09137 -0.00495 0" size="0.01" />
+
+                                                    <body name="r_gripper_r_finger_tip_link" pos="0.09137 -0.00495 0">
+                                                        <geom name="gf2" type="capsule" fromto="0 0 0 0.09137 0.0 0" size="0.01" />
+                                                    </body>
+                                                </body>
+                                            </body>
+                                        </body>
+                                    </body>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                </body>
+            </body>
+        </body>
+
+        <!--
+        <body name="g1" pos="0.034 0.3 -0.47" axisangle="0 1 0 0.05">
+            <geom name="g1" rgba="0.2 0.2 0.2 1" type="box" size="0.003 0.01 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="g2" pos="-0.034 0.3 -0.47" axisangle="0 1 0 -0.05">
+            <geom name="g2" rgba="0.2 0.2 0.2 1" type="box" size="0.003 0.01 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="g3" pos="0.0 0.334 -0.47" axisangle="1 0 0 -0.05">
+            <geom name="g3" rgba="0.2 0.2 0.2 1" type="box" size="0.01 0.003 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="g4" pos="0.0 0.266 -0.47" axisangle="1 0 0 0.05">
+            <geom name="g4" rgba="0.2 0.2 0.2 1" type="box" size="0.01 0.003 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="fl" pos="0.0 0.3 -0.55">
+            <geom name="fl" rgba="0.2 0.2 0.2 1" type="box" size="0.2 0.2 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="w1" pos="0.216 0.3 -0.45">
+            <geom name="w1" rgba="0.2 0.2 0.2 1" type="box" size="0.183 0.3 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="w2" pos="-0.216 0.3 -0.45">
+            <geom name="w2" rgba="0.2 0.2 0.2 1" type="box" size="0.183 0.3 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="w3" pos="0.0 0.516 -0.45">
+            <geom name="w3" rgba="0.2 0.2 0.2 1" type="box" size="0.032 0.183 0.05" contype="1" conaffinity="1" />
+        </body>
+
+        <body name="w4" pos="0.0 0.084 -0.45">
+            <geom name="w4" rgba="0.2 0.2 0.2 1" type="box" size="0.032 0.183 0.05" contype="1" conaffinity="1" />
+        </body>
+        -->
+
+        <body name="target" pos="0 0.25 0">
+            <joint armature="0" axis="1 0 0" damping="0" limited="false" name="target_x" pos="0 0 0" ref="0" stiffness="0" type="slide"/>
+            <joint armature="0" axis="0 1 0" damping="0" limited="false" name="target_y" pos="0 0 0" ref="0.25" stiffness="0" type="slide"/>
+            <joint armature="0" axis="0 0 1" damping="0" limited="false" name="target_z" pos="0 0 0" ref="0" stiffness="0" type="slide"/>
+            <geom conaffinity="0" contype="0" name="target" pos="0 0 0" rgba="0.9 0.2 0.2 1" size=".035" type="sphere"/>
+        </body>
+    </worldbody>
+
+    <actuator>
+        <motor joint="r_shoulder_pan_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+        <motor joint="r_shoulder_lift_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+        <motor joint="r_upper_arm_roll_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+        <motor joint="r_elbow_flex_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+        <motor joint="r_forearm_roll_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+        <motor joint="r_wrist_flex_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+        <motor joint="r_wrist_roll_joint" ctrlrange="-20.0 20.0" ctrllimited="true" />
+    </actuator>
+
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/cartpole.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/cartpole.py
new file mode 100644
index 0000000..ddc3d41
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/cartpole.py
@@ -0,0 +1,193 @@
+"""
+Classic cart-pole system implemented by Rich Sutton et al.
+Copied from https://webdocs.cs.ualberta.ca/~sutton/book/code/pole.c
+"""
+
+import logging
+import math
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+from slbo.utils.dataset import Dataset, gen_dtype
+
+
+logger = logging.getLogger(__name__)
+
+
+class CartPoleEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 50
+    }
+
+    def __init__(self):
+        self.gravity = 9.8
+        self.masscart = 1.0
+        self.masspole = 0.1
+        self.total_mass = (self.masspole + self.masscart)
+        self.length = 0.5  # actually half the pole's length
+        self.polemass_length = (self.masspole * self.length)
+        self.force_mag = 10.0
+        self.tau = 0.02  # seconds between state updates
+
+        # Angle at which to fail the episode
+        self.theta_threshold_radians = 12 * 2 * math.pi / 360
+        self.x_threshold = 2.4
+
+        # Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds
+        high = np.array([
+            self.x_threshold * 2,
+            np.finfo(np.float32).max,
+            self.theta_threshold_radians * 2,
+            np.finfo(np.float32).max])
+
+        # self.action_space = spaces.Discrete(2)
+        self.action_space = \
+            spaces.Box(low=np.array([-1.0]), high=np.array([1.0]))
+        self.observation_space = spaces.Box(-high, high)
+
+        self._seed()
+        self.viewer = None
+        self.state = None
+
+        self.steps_beyond_done = None
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, action):
+        action = 1 if action[0] > .0 else 0
+        # assert self.action_space.contains(action), "%r (%s) invalid" % (action, type(action))
+        state = self.state
+        obs = self.state
+        reward = np.cos(obs[2]) - 0.01 * (obs[0] ** 2)
+
+        x, x_dot, theta, theta_dot = state
+        force = self.force_mag if action == 1 else -self.force_mag
+        costheta = math.cos(theta)
+        sintheta = math.sin(theta)
+        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass
+        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta * costheta / self.total_mass))
+        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass
+        x = x + self.tau * x_dot
+        x_dot = x_dot + self.tau * xacc
+        theta = theta + self.tau * theta_dot
+        theta_dot = theta_dot + self.tau * thetaacc
+        self.state = (x, x_dot, theta, theta_dot)
+        '''
+        done = x < -self.x_threshold \
+            or x > self.x_threshold \
+            or theta < -self.theta_threshold_radians \
+            or theta > self.theta_threshold_radians
+        done = bool(done)
+
+        if not done:
+            reward = 1.0
+        elif self.steps_beyond_done is None:
+            # Pole just fell!
+            self.steps_beyond_done = 0
+            reward = 1.0
+        else:
+            if self.steps_beyond_done == 0:
+                logger.warning("You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.")
+            self.steps_beyond_done += 1
+            reward = 0.0
+        '''
+        done = False
+        self.steps_beyond_done = None
+
+        return np.array(self.state), reward, done, {}
+
+    def _reset(self):
+        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))
+        self.steps_beyond_done = None
+        return np.array(self.state)
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        screen_width = 600
+        screen_height = 400
+
+        world_width = self.x_threshold * 2
+        scale = screen_width / world_width
+        carty = 100  # TOP OF CART
+        polewidth = 10.0
+        polelen = scale * 1.0
+        cartwidth = 50.0
+        cartheight = 30.0
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(screen_width, screen_height)
+            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2
+            axleoffset = cartheight / 4.0
+            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            self.carttrans = rendering.Transform()
+            cart.add_attr(self.carttrans)
+            self.viewer.add_geom(cart)
+            l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2
+            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            pole.set_color(.8, .6, .4)
+            self.poletrans = rendering.Transform(translation=(0, axleoffset))
+            pole.add_attr(self.poletrans)
+            pole.add_attr(self.carttrans)
+            self.viewer.add_geom(pole)
+            self.axle = rendering.make_circle(polewidth / 2)
+            self.axle.add_attr(self.poletrans)
+            self.axle.add_attr(self.carttrans)
+            self.axle.set_color(.5, .5, .8)
+            self.viewer.add_geom(self.axle)
+            self.track = rendering.Line((0, carty), (screen_width, carty))
+            self.track.set_color(0, 0, 0)
+            self.viewer.add_geom(self.track)
+
+        if self.state is None:
+            return None
+
+        x = self.state
+        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART
+        self.carttrans.set_translation(cartx, carty)
+        self.poletrans.set_rotation(-x[2])
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        x = obs[:, 0]
+        theta = obs[:, 2]
+        return -(np.cos(theta) - 0.01 * (x ** 2))
+
+    def verify(self, n=2000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
\ No newline at end of file
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cartpoleO001.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cartpoleO001.py
new file mode 100644
index 0000000..2407b55
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cartpoleO001.py
@@ -0,0 +1,178 @@
+"""
+Classic cart-pole system implemented by Rich Sutton et al.
+Copied from https://webdocs.cs.ualberta.ca/~sutton/book/code/pole.c
+"""
+
+import logging
+import math
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+
+
+logger = logging.getLogger(__name__)
+
+
+class CartPoleEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 50
+    }
+
+    def __init__(self):
+        self.gravity = 9.8
+        self.masscart = 1.0
+        self.masspole = 0.1
+        self.total_mass = (self.masspole + self.masscart)
+        self.length = 0.5  # actually half the pole's length
+        self.polemass_length = (self.masspole * self.length)
+        self.force_mag = 10.0
+        self.tau = 0.02  # seconds between state updates
+
+        # Angle at which to fail the episode
+        self.theta_threshold_radians = 12 * 2 * math.pi / 360
+        self.x_threshold = 2.4
+
+        # Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds
+        high = np.array([
+            self.x_threshold * 2,
+            np.finfo(np.float32).max,
+            self.theta_threshold_radians * 2,
+            np.finfo(np.float32).max])
+
+        # self.action_space = spaces.Discrete(2)
+        self.action_space = \
+            spaces.Box(low=np.array([-1.0]), high=np.array([1.0]))
+        self.observation_space = spaces.Box(-high, high)
+
+        self._seed()
+        self.viewer = None
+        self.state = None
+
+        self.steps_beyond_done = None
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, action):
+        action = 1 if action[0] > .0 else 0
+        # assert self.action_space.contains(action), "%r (%s) invalid" % (action, type(action))
+        state = self.state
+        obs = self.state
+        reward = np.cos(obs[2]) - 0.01 * (obs[0] ** 2)
+
+        x, x_dot, theta, theta_dot = state
+        force = self.force_mag if action == 1 else -self.force_mag
+        costheta = math.cos(theta)
+        sintheta = math.sin(theta)
+        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass
+        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta * costheta / self.total_mass))
+        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass
+        x = x + self.tau * x_dot
+        x_dot = x_dot + self.tau * xacc
+        theta = theta + self.tau * theta_dot
+        theta_dot = theta_dot + self.tau * thetaacc
+        self.state = (x, x_dot, theta, theta_dot)
+        '''
+        done = x < -self.x_threshold \
+            or x > self.x_threshold \
+            or theta < -self.theta_threshold_radians \
+            or theta > self.theta_threshold_radians
+        done = bool(done)
+
+        if not done:
+            reward = 1.0
+        elif self.steps_beyond_done is None:
+            # Pole just fell!
+            self.steps_beyond_done = 0
+            reward = 1.0
+        else:
+            if self.steps_beyond_done == 0:
+                logger.warning("You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.")
+            self.steps_beyond_done += 1
+            reward = 0.0
+        '''
+        done = False
+        self.steps_beyond_done = None
+
+        ob = np.array(self.state)
+        ob += np.random.uniform(low=-0.01, high=0.01, size=ob.shape)
+
+        return ob, reward, done, {}
+
+    def _reset(self):
+        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))
+        self.steps_beyond_done = None
+        return np.array(self.state)
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        screen_width = 600
+        screen_height = 400
+
+        world_width = self.x_threshold * 2
+        scale = screen_width / world_width
+        carty = 100  # TOP OF CART
+        polewidth = 10.0
+        polelen = scale * 1.0
+        cartwidth = 50.0
+        cartheight = 30.0
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(screen_width, screen_height)
+            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2
+            axleoffset = cartheight / 4.0
+            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            self.carttrans = rendering.Transform()
+            cart.add_attr(self.carttrans)
+            self.viewer.add_geom(cart)
+            l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2
+            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            pole.set_color(.8, .6, .4)
+            self.poletrans = rendering.Transform(translation=(0, axleoffset))
+            pole.add_attr(self.poletrans)
+            pole.add_attr(self.carttrans)
+            self.viewer.add_geom(pole)
+            self.axle = rendering.make_circle(polewidth / 2)
+            self.axle.add_attr(self.poletrans)
+            self.axle.add_attr(self.carttrans)
+            self.axle.set_color(.5, .5, .8)
+            self.viewer.add_geom(self.axle)
+            self.track = rendering.Line((0, carty), (screen_width, carty))
+            self.track.set_color(0, 0, 0)
+            self.viewer.add_geom(self.track)
+
+        if self.state is None:
+            return None
+
+        x = self.state
+        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART
+        self.carttrans.set_translation(cartx, carty)
+        self.poletrans.set_rotation(-x[2])
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        x = obs[:, 0]
+        theta = obs[:, 2]
+        return -(np.cos(theta) - 0.01 * (x ** 2))
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cartpoleO01.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cartpoleO01.py
new file mode 100644
index 0000000..62516c3
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cartpoleO01.py
@@ -0,0 +1,177 @@
+"""
+Classic cart-pole system implemented by Rich Sutton et al.
+Copied from https://webdocs.cs.ualberta.ca/~sutton/book/code/pole.c
+"""
+
+import logging
+import math
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+
+logger = logging.getLogger(__name__)
+
+
+class CartPoleEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 50
+    }
+
+    def __init__(self):
+        self.gravity = 9.8
+        self.masscart = 1.0
+        self.masspole = 0.1
+        self.total_mass = (self.masspole + self.masscart)
+        self.length = 0.5  # actually half the pole's length
+        self.polemass_length = (self.masspole * self.length)
+        self.force_mag = 10.0
+        self.tau = 0.02  # seconds between state updates
+
+        # Angle at which to fail the episode
+        self.theta_threshold_radians = 12 * 2 * math.pi / 360
+        self.x_threshold = 2.4
+
+        # Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds
+        high = np.array([
+            self.x_threshold * 2,
+            np.finfo(np.float32).max,
+            self.theta_threshold_radians * 2,
+            np.finfo(np.float32).max])
+
+        # self.action_space = spaces.Discrete(2)
+        self.action_space = \
+            spaces.Box(low=np.array([-1.0]), high=np.array([1.0]))
+        self.observation_space = spaces.Box(-high, high)
+
+        self._seed()
+        self.viewer = None
+        self.state = None
+
+        self.steps_beyond_done = None
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, action):
+        action = 1 if action[0] > .0 else 0
+        # assert self.action_space.contains(action), "%r (%s) invalid" % (action, type(action))
+        state = self.state
+        obs = self.state
+        reward = np.cos(obs[2]) - 0.01 * (obs[0] ** 2)
+
+        x, x_dot, theta, theta_dot = state
+        force = self.force_mag if action == 1 else -self.force_mag
+        costheta = math.cos(theta)
+        sintheta = math.sin(theta)
+        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass
+        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta * costheta / self.total_mass))
+        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass
+        x = x + self.tau * x_dot
+        x_dot = x_dot + self.tau * xacc
+        theta = theta + self.tau * theta_dot
+        theta_dot = theta_dot + self.tau * thetaacc
+        self.state = (x, x_dot, theta, theta_dot)
+        '''
+        done = x < -self.x_threshold \
+            or x > self.x_threshold \
+            or theta < -self.theta_threshold_radians \
+            or theta > self.theta_threshold_radians
+        done = bool(done)
+
+        if not done:
+            reward = 1.0
+        elif self.steps_beyond_done is None:
+            # Pole just fell!
+            self.steps_beyond_done = 0
+            reward = 1.0
+        else:
+            if self.steps_beyond_done == 0:
+                logger.warning("You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.")
+            self.steps_beyond_done += 1
+            reward = 0.0
+        '''
+        done = False
+        self.steps_beyond_done = None
+
+        ob = np.array(self.state)
+        ob += np.random.uniform(low=-0.1, high=0.1, size=ob.shape)
+
+        return ob, reward, done, {}
+
+    def _reset(self):
+        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))
+        self.steps_beyond_done = None
+        return np.array(self.state)
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        screen_width = 600
+        screen_height = 400
+
+        world_width = self.x_threshold * 2
+        scale = screen_width / world_width
+        carty = 100  # TOP OF CART
+        polewidth = 10.0
+        polelen = scale * 1.0
+        cartwidth = 50.0
+        cartheight = 30.0
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(screen_width, screen_height)
+            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2
+            axleoffset = cartheight / 4.0
+            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            self.carttrans = rendering.Transform()
+            cart.add_attr(self.carttrans)
+            self.viewer.add_geom(cart)
+            l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2
+            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            pole.set_color(.8, .6, .4)
+            self.poletrans = rendering.Transform(translation=(0, axleoffset))
+            pole.add_attr(self.poletrans)
+            pole.add_attr(self.carttrans)
+            self.viewer.add_geom(pole)
+            self.axle = rendering.make_circle(polewidth / 2)
+            self.axle.add_attr(self.poletrans)
+            self.axle.add_attr(self.carttrans)
+            self.axle.set_color(.5, .5, .8)
+            self.viewer.add_geom(self.axle)
+            self.track = rendering.Line((0, carty), (screen_width, carty))
+            self.track.set_color(0, 0, 0)
+            self.viewer.add_geom(self.track)
+
+        if self.state is None:
+            return None
+
+        x = self.state
+        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART
+        self.carttrans.set_translation(cartx, carty)
+        self.poletrans.set_rotation(-x[2])
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        x = obs[:, 0]
+        theta = obs[:, 2]
+        return -(np.cos(theta) - 0.01 * (x ** 2))
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahA003.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahA003.py
new file mode 100644
index 0000000..e759126
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahA003.py
@@ -0,0 +1,81 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from slbo.envs import BaseModelBasedEnv
+from gym.envs.mujoco import mujoco_env
+
+
+class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/half_cheetah.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        action = np.array(action)
+        action += np.random.uniform(low=-0.03, high=0.03, size=action.shape)
+        start_ob = self._get_obs()
+        reward_run = start_ob[8]
+
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        reward_ctrl = -0.1 * np.square(action).sum()
+
+        reward = reward_run + reward_ctrl
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        qpos = self.init_qpos + \
+            self.np_random.uniform(low=-.1, high=.1, size=self.model.nq)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahA01.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahA01.py
new file mode 100644
index 0000000..e496056
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahA01.py
@@ -0,0 +1,81 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/half_cheetah.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        action = np.array(action)
+        action += np.random.uniform(low=-0.1, high=0.1, size=action.shape)
+        start_ob = self._get_obs()
+        reward_run = start_ob[8]
+
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        reward_ctrl = -0.1 * np.square(action).sum()
+
+        reward = reward_run + reward_ctrl
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        qpos = self.init_qpos + \
+            self.np_random.uniform(low=-.1, high=.1, size=self.model.nq)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahO001.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahO001.py
new file mode 100644
index 0000000..252505f
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahO001.py
@@ -0,0 +1,80 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/half_cheetah.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        start_ob = self._get_obs()
+        reward_run = start_ob[8]
+
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        reward_ctrl = -0.1 * np.square(action).sum()
+
+        reward = reward_run + reward_ctrl
+        done = False
+        ob += np.random.uniform(low=-0.01, high=0.01, size=ob.shape)
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        qpos = self.init_qpos + \
+            self.np_random.uniform(low=-.1, high=.1, size=self.model.nq)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahO01.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahO01.py
new file mode 100644
index 0000000..4e6fe93
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_cheetahO01.py
@@ -0,0 +1,80 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/half_cheetah.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        start_ob = self._get_obs()
+        reward_run = start_ob[8]
+
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        reward_ctrl = -0.1 * np.square(action).sum()
+
+        reward = reward_run + reward_ctrl
+        done = False
+        ob += np.random.uniform(low=-0.1, high=0.1, size=ob.shape)
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        qpos = self.init_qpos + \
+            self.np_random.uniform(low=-.1, high=.1, size=self.model.nq)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fant.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fant.py
new file mode 100644
index 0000000..29633e2
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fant.py
@@ -0,0 +1,84 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+
+from slbo.envs import BaseModelBasedEnv
+
+
+class AntEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/ant.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        ob = self._get_obs()
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = old_ob[13]
+        reward_height = -3.0 * np.square(old_ob[0] - 0.57)
+
+        # the alive bonus
+        height = ob[0]
+        done = (height > 1.0) or (height < 0.2)
+        alive_reward = float(not done)
+
+        reward = reward_run + reward_ctrl + reward_height + alive_reward
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[2:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        qpos = self.init_qpos + \
+            self.np_random.uniform(size=self.model.nq, low=-.1, high=.1)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        # self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 13]
+        reward_height = -3.0 * np.square(obs[:, 0] - 0.57)
+
+        height = next_obs[:, 0]
+        done = np.logical_or((height > 1.0), (height < 0.2))
+        alive_reward = 1.0 - np.array(done, dtype=np.float)
+
+        reward = reward_run + reward_ctrl + reward_height + alive_reward
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+
+    def mb_step(self, states, actions, next_states):
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        height = next_states[:, 0]
+        done = np.logical_or((height > 1.0), (height < 0.2))
+        return rewards, done
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fhopper.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fhopper.py
new file mode 100644
index 0000000..84edf3d
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fhopper.py
@@ -0,0 +1,91 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HopperEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=4):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/hopper.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = old_ob[5]
+        reward_height = -3.0 * np.square(old_ob[0] - 1.3)
+        height, ang = ob[0], ob[1]
+        done = (height <= 0.7) or (abs(ang) >= 0.2)
+        alive_reward = float(not done)
+        reward = reward_run + reward_ctrl + reward_height + alive_reward
+
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-.005, high=.005, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-.005, high=.005, size=self.model.nv)
+        )
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 2
+        self.viewer.cam.distance = self.model.stat.extent * 0.75
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 5]
+        reward_height = -3.0 * np.square(obs[:, 0] - 1.3)
+        height, ang = next_obs[:, 0], next_obs[:, 1]
+        done = np.logical_or(height <= 0.7, abs(ang) >= 0.2)
+        alive_reward = 1.0 - np.array(done, dtype=np.float)
+        reward = reward_run + reward_ctrl + reward_height + alive_reward
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        # reward_height = -3.0 * tf.square(next_obs[:, 1] - 1.3)
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+        raise NotImplementedError
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        height, ang = next_states[:, 0], next_states[:, 1]
+        done = np.logical_or(height <= 0.7, abs(ang) >= 0.2)
+        return rewards, done
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fswimmer.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fswimmer.py
new file mode 100644
index 0000000..86bdc80
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fswimmer.py
@@ -0,0 +1,69 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+import os
+
+
+class fixedSwimmerEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+
+    def __init__(self):
+        dir_path = os.path.dirname(os.path.realpath(__file__))
+        mujoco_env.MujocoEnv.__init__(self, '%s/assets/fixed_swimmer.xml' % dir_path, 4)
+        utils.EzPickle.__init__(self)
+
+    def _step(self, a):
+        ctrl_cost_coeff = 0.0001
+
+        """
+        xposbefore = self.model.data.qpos[0, 0]
+        self.do_simulation(a, self.frame_skip)
+        xposafter = self.model.data.qpos[0, 0]
+        """
+
+        self.xposbefore = self.model.data.site_xpos[0][0] / self.dt
+        self.do_simulation(a, self.frame_skip)
+        self.xposafter = self.model.data.site_xpos[0][0] / self.dt
+        self.pos_diff = self.xposafter - self.xposbefore
+
+        reward_fwd = self.xposafter - self.xposbefore
+        reward_ctrl = - ctrl_cost_coeff * np.square(a).sum()
+        reward = reward_fwd + reward_ctrl
+        ob = self._get_obs()
+        return ob, reward, False, dict(reward_fwd=reward_fwd, reward_ctrl=reward_ctrl)
+
+    def _get_obs(self):
+        qpos = self.model.data.qpos
+        qvel = self.model.data.qvel
+        return np.concatenate([qpos.flat[2:], qvel.flat, self.pos_diff.flat])
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-.1, high=.1, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-.1, high=.1, size=self.model.nv)
+        )
+        return self._get_obs()
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.0001 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, -1]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+
+    def verify(self):
+        pass
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fwalker2d.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fwalker2d.py
new file mode 100644
index 0000000..25006d3
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_fwalker2d.py
@@ -0,0 +1,99 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class Walker2dEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=4):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/walker2d.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = old_ob[8]
+        reward_height = -3.0 * np.square(old_ob[0] - 1.3)
+
+        height, ang = ob[0], ob[1]
+        done = (height >= 2.0) or (height <= 0.8) or (abs(ang) >= 1.0)
+        alive_reward = float(not done)
+
+        reward = reward_run + reward_ctrl + reward_height + alive_reward
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat
+        ])
+
+    def reset_model(self):
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-.005, high=.005, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-.005, high=.005, size=self.model.nv)
+        )
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 2
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward_height = -3.0 * np.square(next_obs[:, 0] - 1.3)
+        height, ang = next_obs[:, 0], next_obs[:, 1]
+        done = np.logical_or(
+            np.logical_or(height >= 2.0, height <= 0.8),
+            np.abs(ang) >= 1.0
+        )
+        alive_reward = 1.0 - np.array(done, dtype=np.float)
+        reward = reward_run + reward_ctrl + reward_height + alive_reward
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        # reward_height = -3.0 * tf.square(next_obs[:, 1] - 1.3)
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+        raise NotImplementedError
+
+    def verify(self):
+        pass
+
+    def mb_step(self, states, actions, next_states):
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        height, ang = next_states[:, 0], next_states[:, 1]
+        done = np.logical_or(
+            np.logical_or(height >= 2.0, height <= 0.8),
+            np.abs(ang) >= 1.0
+        )
+        return rewards, done
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_humanoid.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_humanoid.py
new file mode 100644
index 0000000..7d220ba
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_humanoid.py
@@ -0,0 +1,89 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+import numpy as np
+from gym.envs.mujoco import mujoco_env
+from gym import utils
+from slbo.envs import BaseModelBasedEnv
+
+
+class HumanoidEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self):
+        mujoco_env.MujocoEnv.__init__(self, 'humanoid.xml', 5)
+        utils.EzPickle.__init__(self)
+
+    def _get_obs(self):
+        data = self.model.data
+        return np.concatenate([data.qpos.flat[2:],
+                               data.qvel.flat,
+                               data.cinert.flat,
+                               data.cvel.flat,
+                               data.qfrc_actuator.flat,
+                               data.cfrc_ext.flat])
+
+    def _step(self, a):
+        data = self.model.data
+        action = a
+        if getattr(self, 'action_space', None):
+            action = np.clip(a, self.action_space.low,
+                             self.action_space.high)
+
+        # reward
+        alive_bonus = 5.0
+        lin_vel_cost = 0.25 / 0.015 * data.qvel.flat[0]
+        quad_ctrl_cost = 0.1 * np.square(action).sum()
+        quad_impact_cost = .5e-6 * np.square(data.cfrc_ext).sum()
+        quad_impact_cost = min(quad_impact_cost, 10)
+
+        self.do_simulation(action, self.frame_skip)
+        reward = lin_vel_cost - quad_ctrl_cost - quad_impact_cost + alive_bonus
+        qpos = self.model.data.qpos
+        done = bool((qpos[2] < 1.0) or (qpos[2] > 2.0))
+        return self._get_obs(), reward, done, dict(reward_linvel=lin_vel_cost, reward_quadctrl=-quad_ctrl_cost, reward_alive=alive_bonus, reward_impact=-quad_impact_cost)
+
+    def reset_model(self):
+        c = 0.01
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-c, high=c, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-c, high=c, size=self.model.nv,)
+        )
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 1
+        self.viewer.cam.distance = self.model.stat.extent * 1.0
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = 0.25 / 0.015 * obs[:, 22]
+
+        quad_impact_cost = .5e-6 * np.square(obs[:, -84:]).sum()
+        quad_impact_cost = min(quad_impact_cost, 10)
+
+        height = next_obs[:, 0]
+        done = np.logical_or((height > 2.0), (height < 1.0))
+        alive_reward = 5 * (1.0 - np.array(done, dtype=np.float))
+
+        reward = reward_run + reward_ctrl + (-quad_impact_cost) + alive_reward
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+
+        height = next_states[:, 0]
+        done = np.logical_or((height > 2.0), (height < 1.0))
+        return rewards, done
+
+    def verify(self):
+        pass
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_nostopslimhumanoid.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_nostopslimhumanoid.py
new file mode 100644
index 0000000..1c87b20
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_nostopslimhumanoid.py
@@ -0,0 +1,81 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+import numpy as np
+from gym.envs.mujoco import mujoco_env
+from gym import utils
+from slbo.envs import BaseModelBasedEnv
+
+
+class HumanoidEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self):
+        mujoco_env.MujocoEnv.__init__(self, 'humanoid.xml', 5)
+        utils.EzPickle.__init__(self)
+
+    def _get_obs(self):
+        data = self.model.data
+        return np.concatenate([data.qpos.flat[2:],
+                               data.qvel.flat])
+
+    def _step(self, a):
+        data = self.model.data
+        action = a
+        if getattr(self, 'action_space', None):
+            action = np.clip(a, self.action_space.low,
+                             self.action_space.high)
+        qpos = self.model.data.qpos
+        done = bool((qpos[2] < 1.0) or (qpos[2] > 2.0))
+
+        # reward
+        alive_bonus = 5 * (1 - float(done))
+        lin_vel_cost = 0.25 / 0.015 * data.qvel.flat[0]
+        quad_ctrl_cost = 0.1 * np.square(action).sum()
+        quad_impact_cost = 0.0
+
+        self.do_simulation(action, self.frame_skip)
+        reward = lin_vel_cost - quad_ctrl_cost - quad_impact_cost + alive_bonus
+        done = False
+        return self._get_obs(), reward, done, dict(reward_linvel=lin_vel_cost, reward_quadctrl=-quad_ctrl_cost, reward_alive=alive_bonus, reward_impact=-quad_impact_cost)
+
+    def reset_model(self):
+        c = 0.01
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-c, high=c, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-c, high=c, size=self.model.nv,)
+        )
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 1
+        self.viewer.cam.distance = self.model.stat.extent * 1.0
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = 0.25 / 0.015 * obs[:, 22]
+
+        quad_impact_cost = 0.0
+
+        height = next_obs[:, 0]
+        done = np.logical_or((height > 2.0), (height < 1.0))
+        alive_reward = 5 * (1.0 - np.array(done, dtype=np.float))
+
+        reward = reward_run + reward_ctrl + (-quad_impact_cost) + alive_reward
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_pendulumO001.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_pendulumO001.py
new file mode 100644
index 0000000..de873df
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_pendulumO001.py
@@ -0,0 +1,138 @@
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+from os import path
+
+
+class PendulumEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 30
+    }
+
+    def __init__(self):
+        self.max_speed = 8
+        self.max_torque = 2.
+        self.dt = .05
+        self.viewer = None
+
+        high = np.array([1., 1., self.max_speed])
+        self.action_space = spaces.Box(low=-self.max_torque, high=self.max_torque, shape=(1,))
+        self.observation_space = spaces.Box(low=-high, high=high)
+
+        self._seed()
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, u):
+        th, thdot = self.state  # th := theta
+        '''
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+        '''
+
+        # for the reward
+        y, x, thetadot = np.cos(th), np.sin(th), thdot
+        u = np.clip(u, -self.max_torque, self.max_torque)[0]
+        costs = y + .1 * np.abs(x) + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        reward = -costs
+
+        g = 10.
+        m = 1.
+        l = 1.
+        dt = self.dt
+
+        self.last_u = u  # for rendering
+        # costs = angle_normalize(th) ** 2 + .1 * thdot ** 2 + .001 * (u ** 2)
+
+        newthdot = thdot + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3. / (m * l ** 2) * u) * dt
+        newth = th + newthdot * dt
+        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)  # pylint: disable=E1111
+
+        self.state = np.array([newth, newthdot])
+        ob = self._get_obs()
+        ob += np.random.uniform(low=-0.01, high=0.01, size=ob.shape)
+        return ob, reward, False, {}
+
+    def _reset(self):
+        high = np.array([np.pi, 1])
+        self.state = self.np_random.uniform(low=-high, high=high)
+        self.last_u = None
+        return self._get_obs()
+
+    def _get_obs(self):
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(500, 500)
+            self.viewer.set_bounds(-2.2, 2.2, -2.2, 2.2)
+            rod = rendering.make_capsule(1, .2)
+            rod.set_color(.8, .3, .3)
+            self.pole_transform = rendering.Transform()
+            rod.add_attr(self.pole_transform)
+            self.viewer.add_geom(rod)
+            axle = rendering.make_circle(.05)
+            axle.set_color(0, 0, 0)
+            self.viewer.add_geom(axle)
+            fname = path.join(path.dirname(__file__), "assets/clockwise.png")
+            self.img = rendering.Image(fname, 1., 1.)
+            self.imgtrans = rendering.Transform()
+            self.img.add_attr(self.imgtrans)
+
+        self.viewer.add_onetime(self.img)
+        self.pole_transform.set_rotation(self.state[0] + np.pi / 2)
+        if self.last_u:
+            self.imgtrans.scale = (-self.last_u / 2, np.abs(self.last_u) / 2)
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+        dist_vec = obs[:, -3:]
+        reward_dist = - np.linalg.norm(dist_vec, axis=1)
+        reward_ctrl = - np.sum(np.square(acts), axis=1)
+        reward = reward_dist + reward_ctrl
+
+        # for the reward
+        y, x, thetadot = np.cos(th), np.sin(th), thdot
+        u = np.clip(u, -self.max_torque, self.max_torque)[0]
+        costs = y + .1 * x + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        reward = -costs
+
+        def _get_obs(self):
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+
+        """
+        y, x, thetadot = obs[:, 0], obs[:, 1], obs[:, 2]
+        u = np.clip(acts[:, 0], -self.max_torque, self.max_torque)
+        costs = y + .1 * np.abs(x) + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        return costs
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
+
+
+def angle_normalize(x):
+    return (((x + np.pi) % (2 * np.pi)) - np.pi)
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_pendulumO01.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_pendulumO01.py
new file mode 100644
index 0000000..dbd460d
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_pendulumO01.py
@@ -0,0 +1,138 @@
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+from os import path
+
+
+class PendulumEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 30
+    }
+
+    def __init__(self):
+        self.max_speed = 8
+        self.max_torque = 2.
+        self.dt = .05
+        self.viewer = None
+
+        high = np.array([1., 1., self.max_speed])
+        self.action_space = spaces.Box(low=-self.max_torque, high=self.max_torque, shape=(1,))
+        self.observation_space = spaces.Box(low=-high, high=high)
+
+        self._seed()
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, u):
+        th, thdot = self.state  # th := theta
+        '''
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+        '''
+
+        # for the reward
+        y, x, thetadot = np.cos(th), np.sin(th), thdot
+        u = np.clip(u, -self.max_torque, self.max_torque)[0]
+        costs = y + .1 * np.abs(x) + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        reward = -costs
+
+        g = 10.
+        m = 1.
+        l = 1.
+        dt = self.dt
+
+        self.last_u = u  # for rendering
+        # costs = angle_normalize(th) ** 2 + .1 * thdot ** 2 + .001 * (u ** 2)
+
+        newthdot = thdot + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3. / (m * l ** 2) * u) * dt
+        newth = th + newthdot * dt
+        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)  # pylint: disable=E1111
+
+        self.state = np.array([newth, newthdot])
+        ob = self._get_obs()
+        ob += np.random.uniform(low=-0.1, high=0.1, size=ob.shape)
+        return ob, reward, False, {}
+
+    def _reset(self):
+        high = np.array([np.pi, 1])
+        self.state = self.np_random.uniform(low=-high, high=high)
+        self.last_u = None
+        return self._get_obs()
+
+    def _get_obs(self):
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(500, 500)
+            self.viewer.set_bounds(-2.2, 2.2, -2.2, 2.2)
+            rod = rendering.make_capsule(1, .2)
+            rod.set_color(.8, .3, .3)
+            self.pole_transform = rendering.Transform()
+            rod.add_attr(self.pole_transform)
+            self.viewer.add_geom(rod)
+            axle = rendering.make_circle(.05)
+            axle.set_color(0, 0, 0)
+            self.viewer.add_geom(axle)
+            fname = path.join(path.dirname(__file__), "assets/clockwise.png")
+            self.img = rendering.Image(fname, 1., 1.)
+            self.imgtrans = rendering.Transform()
+            self.img.add_attr(self.imgtrans)
+
+        self.viewer.add_onetime(self.img)
+        self.pole_transform.set_rotation(self.state[0] + np.pi / 2)
+        if self.last_u:
+            self.imgtrans.scale = (-self.last_u / 2, np.abs(self.last_u) / 2)
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+        dist_vec = obs[:, -3:]
+        reward_dist = - np.linalg.norm(dist_vec, axis=1)
+        reward_ctrl = - np.sum(np.square(acts), axis=1)
+        reward = reward_dist + reward_ctrl
+
+        # for the reward
+        y, x, thetadot = np.cos(th), np.sin(th), thdot
+        u = np.clip(u, -self.max_torque, self.max_torque)[0]
+        costs = y + .1 * x + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        reward = -costs
+
+        def _get_obs(self):
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+
+        """
+        y, x, thetadot = obs[:, 0], obs[:, 1], obs[:, 2]
+        u = np.clip(acts[:, 0], -self.max_torque, self.max_torque)
+        costs = y + .1 * np.abs(x) + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        return costs
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
+
+
+def angle_normalize(x):
+    return (((x + np.pi) % (2 * np.pi)) - np.pi)
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_slimhumanoid.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_slimhumanoid.py
new file mode 100644
index 0000000..cfaf6ad
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/gym_slimhumanoid.py
@@ -0,0 +1,82 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+import numpy as np
+from gym.envs.mujoco import mujoco_env
+from gym import utils
+from slbo.envs import BaseModelBasedEnv
+
+
+class HumanoidEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self):
+        mujoco_env.MujocoEnv.__init__(self, 'humanoid.xml', 5)
+        utils.EzPickle.__init__(self)
+
+    def _get_obs(self):
+        data = self.model.data
+        return np.concatenate([data.qpos.flat[2:],
+                               data.qvel.flat])
+
+    def _step(self, a):
+        data = self.model.data
+        action = a
+        if getattr(self, 'action_space', None):
+            action = np.clip(a, self.action_space.low,
+                             self.action_space.high)
+
+        # reward
+        alive_bonus = 5.0
+        lin_vel_cost = 0.25 / 0.015 * data.qvel.flat[0]
+        quad_ctrl_cost = 0.1 * np.square(action).sum()
+        quad_impact_cost = 0.0
+
+        self.do_simulation(action, self.frame_skip)
+        reward = lin_vel_cost - quad_ctrl_cost - quad_impact_cost + alive_bonus
+        qpos = self.model.data.qpos
+        done = bool((qpos[2] < 1.0) or (qpos[2] > 2.0))
+        return self._get_obs(), reward, done, dict(reward_linvel=lin_vel_cost, reward_quadctrl=-quad_ctrl_cost, reward_alive=alive_bonus, reward_impact=-quad_impact_cost)
+
+    def reset_model(self):
+        c = 0.01
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-c, high=c, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-c, high=c, size=self.model.nv,)
+        )
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 1
+        self.viewer.cam.distance = self.model.stat.extent * 1.0
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = 0.25 / 0.015 * obs[:, 22]
+
+        quad_impact_cost = 0.0
+
+        height = next_obs[:, 0]
+        done = np.logical_or((height > 2.0), (height < 1.0))
+        alive_reward = 5 * (1.0 - np.array(done, dtype=np.float))
+
+        reward = reward_run + reward_ctrl + (-quad_impact_cost) + alive_reward
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        height = next_states[:, 0]
+        done = np.logical_or((height > 2.0), (height < 1.0))
+        return rewards, done
+
+    def verify(self):
+        pass
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/half_cheetah.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/half_cheetah.py
new file mode 100644
index 0000000..97be1c5
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/half_cheetah.py
@@ -0,0 +1,76 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=5):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/half_cheetah.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        start_ob = self._get_obs()
+        reward_run = start_ob[8]
+
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        reward_ctrl = -0.1 * np.square(action).sum()
+
+        reward = reward_run + reward_ctrl
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                             self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        qpos = self.init_qpos + \
+            self.np_random.uniform(low=-.1, high=.1, size=self.model.nq)
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        raise NotImplementedError
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/hopper.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/hopper.py
new file mode 100644
index 0000000..bb2f509
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/hopper.py
@@ -0,0 +1,84 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HopperEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=4):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/hopper.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = old_ob[5]
+        reward_height = -3.0 * np.square(old_ob[0] - 1.3)
+        reward = reward_run + reward_ctrl + reward_height + 1.0
+
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-.005, high=.005, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-.005, high=.005, size=self.model.nv)
+        )
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 2
+        self.viewer.cam.distance = self.model.stat.extent * 0.75
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 5]
+        reward_height = -3.0 * np.square(obs[:, 0] - 1.3)
+        reward = reward_run + reward_ctrl + reward_height + 1.0
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        # reward_height = -3.0 * tf.square(next_obs[:, 1] - 1.3)
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+        raise NotImplementedError
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/inverted_pendulum.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/inverted_pendulum.py
new file mode 100644
index 0000000..f05af7b
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/inverted_pendulum.py
@@ -0,0 +1,74 @@
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.utils.dataset import Dataset, gen_dtype
+from lunzi.Logger import logger
+
+
+class InvertedPendulumEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+
+    def __init__(self):
+        utils.EzPickle.__init__(self)
+        mujoco_env.MujocoEnv.__init__(self, 'inverted_pendulum.xml', 2)
+
+    def _step(self, a):
+        # reward = 1.0
+        reward = self._get_reward()
+        self.do_simulation(a, self.frame_skip)
+        ob = self._get_obs()
+        # notdone = np.isfinite(ob).all() and (np.abs(ob[1]) <= .2)
+        # done = not notdone
+        done = False
+        return ob, reward, done, {}
+
+    def reset_model(self):
+        qpos = self.init_qpos + self.np_random.uniform(size=self.model.nq, low=-0.01, high=0.01)
+        qvel = self.init_qvel + self.np_random.uniform(size=self.model.nv, low=-0.01, high=0.01)
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def _get_reward(self):
+        old_ob = self._get_obs()
+        reward = -((old_ob[1]) ** 2)
+        return reward
+
+    def _get_obs(self):
+        return np.concatenate([self.model.data.qpos, self.model.data.qvel]).ravel()
+
+    def viewer_setup(self):
+        v = self.viewer
+        v.cam.trackbodyid = 0
+        v.cam.distance = v.model.stat.extent
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        return ((obs[:, 1]) ** 2)
+
+    def verify(self, n=2000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/mountain_car.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/mountain_car.py
new file mode 100644
index 0000000..01ed157
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/mountain_car.py
@@ -0,0 +1,199 @@
+# -*- coding: utf-8 -*-
+"""
+@author: Olivier Sigaud
+A merge between two sources:
+* Adaptation of the MountainCar Environment from the "FAReinforcement" library
+of Jose Antonio Martin H. (version 1.0), adapted by  'Tom Schaul, tom@idsia.ch'
+and then modified by Arnaud de Broissia
+* the OpenAI/gym MountainCar environment
+itself from
+https://webdocs.cs.ualberta.ca/~sutton/MountainCar/MountainCar1.cp
+"""
+
+import math
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+from slbo.utils.dataset import Dataset, gen_dtype
+from lunzi.Logger import logger
+
+
+class Continuous_MountainCarEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 30
+    }
+
+    def __init__(self):
+        self.min_action = -1.0
+        self.max_action = 1.0
+        self.min_position = -1.2
+        self.max_position = 0.6
+        self.max_speed = 0.07
+        self.goal_position = 0.45  # was 0.5 in gym, 0.45 in Arnaud de Broissia's version
+        self.power = 0.0015
+
+        self.low_state = np.array([self.min_position, -self.max_speed])
+        self.high_state = np.array([self.max_position, self.max_speed])
+
+        self.viewer = None
+
+        self.action_space = spaces.Box(self.min_action, self.max_action, shape=(1,))
+        self.observation_space = spaces.Box(self.low_state, self.high_state)
+
+        self._seed()
+        self.reset()
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, action):
+
+        position = self.state[0]
+        velocity = self.state[1]
+        force = min(max(action[0], -1.0), 1.0)
+        #reward = position
+
+        velocity += force * self.power - 0.0025 * math.cos(3 * position)
+        if (velocity > self.max_speed):
+            velocity = self.max_speed
+        if (velocity < -self.max_speed):
+            velocity = -self.max_speed
+        position += velocity
+        if (position > self.max_position):
+            position = self.max_position
+        if (position < self.min_position):
+            position = self.min_position
+        if (position == self.min_position and velocity < 0):
+            velocity = 0
+
+
+        done = bool(position >= self.goal_position)
+
+        reward = 0
+        if done:
+            reward = 100.0
+        reward -= math.pow(action[0], 2) * 0.1
+
+
+        #done = False
+        self.state = np.array([position, velocity])
+        return self.state, reward, done, {}
+
+    def _reset(self):
+        self.state = np.array([self.np_random.uniform(low=-0.6, high=-0.4), 0])
+        return np.array(self.state)
+
+#    def get_state(self):
+#        return self.state
+
+    def _height(self, xs):
+        return np.sin(3 * xs) * .45 + .55
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        screen_width = 600
+        screen_height = 400
+
+        world_width = self.max_position - self.min_position
+        scale = screen_width / world_width
+        carwidth = 40
+        carheight = 20
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(screen_width, screen_height)
+            xs = np.linspace(self.min_position, self.max_position, 100)
+            ys = self._height(xs)
+            xys = list(zip((xs - self.min_position) * scale, ys * scale))
+
+            self.track = rendering.make_polyline(xys)
+            self.track.set_linewidth(4)
+            self.viewer.add_geom(self.track)
+
+            clearance = 10
+
+            l, r, t, b = -carwidth / 2, carwidth / 2, carheight, 0
+            car = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])
+            car.add_attr(rendering.Transform(translation=(0, clearance)))
+            self.cartrans = rendering.Transform()
+            car.add_attr(self.cartrans)
+            self.viewer.add_geom(car)
+            frontwheel = rendering.make_circle(carheight / 2.5)
+            frontwheel.set_color(.5, .5, .5)
+            frontwheel.add_attr(rendering.Transform(translation=(carwidth / 4, clearance)))
+            frontwheel.add_attr(self.cartrans)
+            self.viewer.add_geom(frontwheel)
+            backwheel = rendering.make_circle(carheight / 2.5)
+            backwheel.add_attr(rendering.Transform(translation=(-carwidth / 4, clearance)))
+            backwheel.add_attr(self.cartrans)
+            backwheel.set_color(.5, .5, .5)
+            self.viewer.add_geom(backwheel)
+            flagx = (self.goal_position - self.min_position) * scale
+            flagy1 = self._height(self.goal_position) * scale
+            flagy2 = flagy1 + 50
+            flagpole = rendering.Line((flagx, flagy1), (flagx, flagy2))
+            self.viewer.add_geom(flagpole)
+            flag = rendering.FilledPolygon([(flagx, flagy2), (flagx, flagy2 - 10), (flagx + 25, flagy2 - 5)])
+            flag.set_color(.8, .8, 0)
+            self.viewer.add_geom(flag)
+
+        pos = self.state[0]
+        self.cartrans.set_translation((pos - self.min_position) * scale, self._height(pos) * scale)
+        self.cartrans.set_rotation(math.cos(3 * pos))
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        dones = rewards > 0
+        return rewards, dones
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+        position = self.state[0]
+        velocity = self.state[1]
+        force = min(max(action[0], -1.0), 1.0)
+        reward = position
+        """
+        positions = next_obs[:,0]
+        rewards = np.zeros(len(positions))
+        for i in range(len(rewards)):
+            if positions[i] >= self.goal_position:
+                rewards[i] = 100.0
+
+        rewards = rewards - np.power(acts[:,0], 2) * 0.1
+
+        return -rewards
+
+    def verify(self, n=2000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pendulum.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pendulum.py
new file mode 100644
index 0000000..a64b39c
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pendulum.py
@@ -0,0 +1,155 @@
+import gym
+from gym import spaces
+from gym.utils import seeding
+import numpy as np
+from os import path
+from slbo.utils.dataset import Dataset, gen_dtype
+from lunzi.Logger import logger
+
+
+class PendulumEnv(gym.Env):
+    metadata = {
+        'render.modes': ['human', 'rgb_array'],
+        'video.frames_per_second': 30
+    }
+
+    def __init__(self):
+        self.max_speed = 8
+        self.max_torque = 2.
+        self.dt = .05
+        self.viewer = None
+
+        high = np.array([1., 1., self.max_speed])
+        self.action_space = spaces.Box(low=-self.max_torque, high=self.max_torque, shape=(1,))
+        self.observation_space = spaces.Box(low=-high, high=high)
+
+        self._seed()
+
+    def _seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def _step(self, u):
+        th, thdot = self.state  # th := theta
+        '''
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+        '''
+
+        # for the reward
+        y, x, thetadot = np.cos(th), np.sin(th), thdot
+        u = np.clip(u, -self.max_torque, self.max_torque)[0]
+        costs = y + .1 * np.abs(x) + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        reward = -costs
+
+        g = 10.
+        m = 1.
+        l = 1.
+        dt = self.dt
+
+        self.last_u = u  # for rendering
+        # costs = angle_normalize(th) ** 2 + .1 * thdot ** 2 + .001 * (u ** 2)
+
+        newthdot = thdot + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3. / (m * l ** 2) * u) * dt
+        newth = th + newthdot * dt
+        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)  # pylint: disable=E1111
+
+        self.state = np.array([newth, newthdot])
+        return self._get_obs(), reward, False, {}
+
+    def _reset(self):
+        high = np.array([np.pi, 1])
+        self.state = self.np_random.uniform(low=-high, high=high)
+        self.last_u = None
+        return self._get_obs()
+
+    def _get_obs(self):
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+
+    def _render(self, mode='human', close=False):
+        if close:
+            if self.viewer is not None:
+                self.viewer.close()
+                self.viewer = None
+            return
+
+        if self.viewer is None:
+            from gym.envs.classic_control import rendering
+            self.viewer = rendering.Viewer(500, 500)
+            self.viewer.set_bounds(-2.2, 2.2, -2.2, 2.2)
+            rod = rendering.make_capsule(1, .2)
+            rod.set_color(.8, .3, .3)
+            self.pole_transform = rendering.Transform()
+            rod.add_attr(self.pole_transform)
+            self.viewer.add_geom(rod)
+            axle = rendering.make_circle(.05)
+            axle.set_color(0, 0, 0)
+            self.viewer.add_geom(axle)
+            fname = path.join(path.dirname(__file__), "assets/clockwise.png")
+            self.img = rendering.Image(fname, 1., 1.)
+            self.imgtrans = rendering.Transform()
+            self.img.add_attr(self.imgtrans)
+
+        self.viewer.add_onetime(self.img)
+        self.pole_transform.set_rotation(self.state[0] + np.pi / 2)
+        if self.last_u:
+            self.imgtrans.scale = (-self.last_u / 2, np.abs(self.last_u) / 2)
+
+        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+        dist_vec = obs[:, -3:]
+        reward_dist = - np.linalg.norm(dist_vec, axis=1)
+        reward_ctrl = - np.sum(np.square(acts), axis=1)
+        reward = reward_dist + reward_ctrl
+
+        # for the reward
+        y, x, thetadot = np.cos(th), np.sin(th), thdot
+        u = np.clip(u, -self.max_torque, self.max_torque)[0]
+        costs = y + .1 * x + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        reward = -costs
+
+        def _get_obs(self):
+        theta, thetadot = self.state
+        return np.array([np.cos(theta), np.sin(theta), thetadot])
+
+        """
+        y, x, thetadot = obs[:, 0], obs[:, 1], obs[:, 2]
+        u = np.clip(acts[:, 0], -self.max_torque, self.max_torque)
+        costs = y + .1 * np.abs(x) + .1 * (thetadot ** 2) + .001 * (u ** 2)
+        return costs
+
+    def verify(self, n=2000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
+
+
+def angle_normalize(x):
+    return (((x + np.pi) % (2 * np.pi)) - np.pi)
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_cartpole.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_cartpole.py
new file mode 100644
index 0000000..a048535
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_cartpole.py
@@ -0,0 +1,53 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+
+
+class CartpoleEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+    PENDULUM_LENGTH = 0.6
+
+    def __init__(self):
+        utils.EzPickle.__init__(self)
+        dir_path = os.path.dirname(os.path.realpath(__file__))
+        mujoco_env.MujocoEnv.__init__(self, '%s/assets/cartpole.xml' % dir_path, 2)
+
+    def _step(self, a):
+        self.do_simulation(a, self.frame_skip)
+        ob = self._get_obs()
+
+        cost_lscale = CartpoleEnv.PENDULUM_LENGTH
+        reward = np.exp(
+            -np.sum(np.square(self._get_ee_pos(ob) - np.array([0.0, CartpoleEnv.PENDULUM_LENGTH]))) / (cost_lscale ** 2)
+        )
+        reward -= 0.01 * np.sum(np.square(a))
+
+        done = False
+        return ob, reward, done, {}
+
+    def reset_model(self):
+        qpos = self.init_qpos + np.random.normal(0, 0.1, np.shape(self.init_qpos))
+        qvel = self.init_qvel + np.random.normal(0, 0.1, np.shape(self.init_qvel))
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def _get_obs(self):
+        return np.concatenate([self.model.data.qpos, self.model.data.qvel]).ravel()
+
+    @staticmethod
+    def _get_ee_pos(x):
+        x0, theta = x[0], x[1]
+        return np.array([
+            x0 - CartpoleEnv.PENDULUM_LENGTH * np.sin(theta),
+            -CartpoleEnv.PENDULUM_LENGTH * np.cos(theta)
+        ])
+
+    def viewer_setup(self):
+        v = self.viewer
+        v.cam.trackbodyid = 0
+        v.cam.distance = v.model.stat.extent
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_cheetah.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_cheetah.py
new file mode 100644
index 0000000..1f73b66
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_cheetah.py
@@ -0,0 +1,54 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+
+
+class HalfCheetahEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+
+    def __init__(self):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.realpath(__file__))
+        mujoco_env.MujocoEnv.__init__(self, '%s/assets/half_cheetah.xml' % dir_path, 5)
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = ob[0] - 0.0 * np.square(ob[2])
+        reward = reward_run + reward_ctrl
+
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            (self.model.data.qpos.flat[:1] - self.prev_qpos[:1]) / self.dt,
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat,
+        ])
+
+    def reset_model(self):
+        qpos = self.init_qpos + np.random.normal(loc=0, scale=0.001, size=self.model.nq)
+        qvel = self.init_qvel + np.random.normal(loc=0, scale=0.001, size=self.model.nv)
+        self.set_state(qpos, qvel)
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def viewer_setup(self):
+        self.viewer.cam.distance = self.model.stat.extent * 0.25
+        self.viewer.cam.elevation = -55
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_pusher.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_pusher.py
new file mode 100644
index 0000000..854f477
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_pusher.py
@@ -0,0 +1,85 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+
+
+class PusherEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+
+    def __init__(self):
+        dir_path = os.path.dirname(os.path.realpath(__file__))
+        mujoco_env.MujocoEnv.__init__(self, '%s/assets/pusher.xml' % dir_path, 4)
+        utils.EzPickle.__init__(self)
+        self.reset_model()
+
+    def _step(self, a):
+        obj_pos = self.get_body_com("object"),
+        vec_1 = obj_pos - self.get_body_com("tips_arm")
+        vec_2 = obj_pos - self.get_body_com("goal")
+
+        reward_near = -np.sum(np.abs(vec_1))
+        reward_dist = -np.sum(np.abs(vec_2))
+        reward_ctrl = -np.square(a).sum()
+        reward = 1.25 * reward_dist + 0.1 * reward_ctrl + 0.5 * reward_near
+
+        self.do_simulation(a, self.frame_skip)
+        ob = self._get_obs()
+        done = False
+        return ob, reward, done, {}
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = -1
+        self.viewer.cam.distance = 4.0
+
+    def reset_model(self):
+        qpos = self.init_qpos
+
+        self.goal_pos = np.asarray([0, 0])
+        self.cylinder_pos = np.array([-0.25, 0.15]) + np.random.normal(0, 0.025, [2])
+
+        qpos[-4:-2] = self.cylinder_pos
+        qpos[-2:] = self.goal_pos
+        qvel = self.init_qvel + \
+            self.np_random.uniform(low=-0.005, high=0.005, size=self.model.nv)
+        qvel[-4:] = 0
+        self.set_state(qpos, qvel)
+        self.ac_goal_pos = self.get_body_com("goal")
+
+        return self._get_obs()
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[:7],
+            self.model.data.qvel.flat[:7],
+            self.get_body_com("tips_arm"),
+            self.get_body_com("object"),
+            self.get_body_com("goal"),
+        ])
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+        to_w, og_w = 0.5, 1.25
+        tip_pos, obj_pos, goal_pos = obs[:, 14:17], obs[:, 17:20], obs[:, -3:]
+
+        tip_obj_dist = np.sum(np.abs(tip_pos - obj_pos), axis=1)
+        obj_goal_dist = np.sum(np.abs(goal_pos - obj_pos), axis=1)
+        return to_w * tip_obj_dist + og_w * obj_goal_dist
+
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward = reward_run + reward_ctrl
+        """
+        to_w, og_w = 0.5, 1.25
+        tip_pos, obj_pos, goal_pos = obs[:, 14:17], obs[:, 17:20], obs[:, -3:]
+
+        tip_obj_dist = -np.sum(np.abs(tip_pos - obj_pos), axis=1)
+        obj_goal_dist = -np.sum(np.abs(goal_pos - obj_pos), axis=1)
+        ctrl_reward = -0.1 * np.sum(np.square(acts), axis=1)
+
+        reward = to_w * tip_obj_dist + og_w * obj_goal_dist + ctrl_reward
+        return -reward
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_reacher.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_reacher.py
new file mode 100644
index 0000000..ba419ac
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/pets_reacher.py
@@ -0,0 +1,95 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+
+
+class Reacher3DEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+
+    def __init__(self):
+        self.viewer = None
+        utils.EzPickle.__init__(self)
+        dir_path = os.path.dirname(os.path.realpath(__file__))
+        self.goal = np.zeros(3)
+        mujoco_env.MujocoEnv.__init__(self, os.path.join(dir_path, 'assets/reacher3d.xml'), 2)
+
+    def _step(self, a):
+        self.do_simulation(a, self.frame_skip)
+        ob = self._get_obs()
+        reward = -np.sum(np.square(self.get_EE_pos(ob[None]) - self.goal))
+        reward -= 0.01 * np.square(a).sum()
+        done = False
+        return ob, reward, done, dict(reward_dist=0, reward_ctrl=0)
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 1
+        self.viewer.cam.distance = 2.5
+        self.viewer.cam.elevation = -30
+        self.viewer.cam.azimuth = 270
+
+    def reset_model(self):
+        qpos, qvel = np.copy(self.init_qpos), np.copy(self.init_qvel)
+        qpos[-3:] += np.random.normal(loc=0, scale=0.1, size=[3])
+        qvel[-3:] = 0
+        self.goal = qpos[-3:]
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def _get_obs(self):
+        raw_obs = np.concatenate([
+            self.model.data.qpos.flat, self.model.data.qvel.flat[:-3],
+        ])
+
+        EE_pos = np.reshape(self.get_EE_pos(raw_obs[None]), [-1])
+
+        return np.concatenate([raw_obs, EE_pos])
+
+    def get_EE_pos(self, states):
+        theta1, theta2, theta3, theta4, theta5, theta6, theta7 = \
+            states[:, :1], states[:, 1:2], states[:, 2:3], states[:, 3:4], states[:, 4:5], states[:, 5:6], states[:, 6:]
+
+        rot_axis = np.concatenate([np.cos(theta2) * np.cos(theta1), np.cos(theta2) * np.sin(theta1), -np.sin(theta2)],
+                                  axis=1)
+        rot_perp_axis = np.concatenate([-np.sin(theta1), np.cos(theta1), np.zeros(theta1.shape)], axis=1)
+        cur_end = np.concatenate([
+            0.1 * np.cos(theta1) + 0.4 * np.cos(theta1) * np.cos(theta2),
+            0.1 * np.sin(theta1) + 0.4 * np.sin(theta1) * np.cos(theta2) - 0.188,
+            -0.4 * np.sin(theta2)
+        ], axis=1)
+
+        for length, hinge, roll in [(0.321, theta4, theta3), (0.16828, theta6, theta5)]:
+            perp_all_axis = np.cross(rot_axis, rot_perp_axis)
+            x = np.cos(hinge) * rot_axis
+            y = np.sin(hinge) * np.sin(roll) * rot_perp_axis
+            z = -np.sin(hinge) * np.cos(roll) * perp_all_axis
+            new_rot_axis = x + y + z
+            new_rot_perp_axis = np.cross(new_rot_axis, rot_axis)
+            new_rot_perp_axis[np.linalg.norm(new_rot_perp_axis, axis=1) < 1e-30] = \
+                rot_perp_axis[np.linalg.norm(new_rot_perp_axis, axis=1) < 1e-30]
+            new_rot_perp_axis /= np.linalg.norm(new_rot_perp_axis, axis=1, keepdims=True)
+            rot_axis, rot_perp_axis, cur_end = new_rot_axis, new_rot_perp_axis, cur_end + length * new_rot_axis
+
+        return cur_end
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        """
+        def obs_cost_fn(self, obs):
+            self.ENV.goal = obs[:, 7: 10]
+            ee_pos = obs[:, -3:]
+            return np.sum(np.square(ee_pos - self.ENV.goal), axis=1)
+
+        @staticmethod
+        def ac_cost_fn(acs):
+            return 0.01 * np.sum(np.square(acs), axis=1)
+        """
+        reward_ctrl = -0.01 * np.sum(np.square(acts), axis=1)
+        goal = obs[:, 7: 10]
+        ee_pos = obs[:, -3:]
+
+        reward = -np.sum(np.square(ee_pos - goal), axis=1) + reward_ctrl
+        return -reward
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/reacher.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/reacher.py
new file mode 100644
index 0000000..636cdde
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/reacher.py
@@ -0,0 +1,66 @@
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class ReacherEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self):
+        utils.EzPickle.__init__(self)
+        mujoco_env.MujocoEnv.__init__(self, 'reacher.xml', 2)
+
+    def _step(self, a):
+        vec = self.get_body_com("fingertip") - self.get_body_com("target")
+
+        if getattr(self, 'action_space', None):
+            a = np.clip(a, self.action_space.low,
+                        self.action_space.high)
+        reward_dist = - np.linalg.norm(vec)
+        reward_ctrl = - np.square(a).sum()
+        reward = reward_dist + reward_ctrl
+        self.do_simulation(a, self.frame_skip)
+        ob = self._get_obs()
+        done = False
+        return ob, reward, done, dict(reward_dist=reward_dist, reward_ctrl=reward_ctrl)
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 0
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                             self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        qpos = self.np_random.uniform(low=-0.1, high=0.1, size=self.model.nq) + self.init_qpos
+        while True:
+            self.goal = self.np_random.uniform(low=-.2, high=.2, size=2)
+            if np.linalg.norm(self.goal) < 2:
+                break
+        qpos[-2:] = self.goal
+        qvel = self.init_qvel + self.np_random.uniform(low=-.005, high=.005, size=self.model.nv)
+        qvel[-2:] = 0
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def _get_obs(self):
+        theta = self.model.data.qpos.flat[:2]
+        return np.concatenate([
+            np.cos(theta),
+            np.sin(theta),
+            self.model.data.qpos.flat[2:],
+            self.model.data.qvel.flat[:2],
+            self.get_body_com("fingertip") - self.get_body_com("target")
+        ])
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        dist_vec = obs[:, -3:]
+        reward_dist = - np.linalg.norm(dist_vec, axis=1)
+        reward_ctrl = - np.sum(np.square(acts), axis=1)
+        reward = reward_dist + reward_ctrl
+        return -reward
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/readme.md b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/readme.md
new file mode 100644
index 0000000..28f84a0
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/readme.md
@@ -0,0 +1,5 @@
+# reference
+
+1. mbbl/env/gym_env/walker.py or mbbl/env/gym_env/reacher.py 
+
+2. https://github.com/openai/gym/blob/v0.7.4/gym/envs/mujoco/half_cheetah.py
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/swimmer.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/swimmer.py
new file mode 100644
index 0000000..33e8d79
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/swimmer.py
@@ -0,0 +1,77 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class SwimmerEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=4):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/swimmer.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+        ob = self._get_obs()
+
+        reward_ctrl = -0.0001 * np.square(action).sum()
+        reward_run = old_ob[3]
+        reward = reward_run + reward_ctrl
+
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            # (self.model.data.qpos.flat[:1] - self.prev_qpos[:1]) / self.dt,
+            # self.get_body_comvel("torso")[:1],
+            self.model.data.qpos.flat[2:],
+            self.model.data.qvel.flat,
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                             self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-.1, high=.1, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-.1, high=.1, size=self.model.nv)
+        )
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.0001 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 3]
+        reward = reward_run + reward_ctrl
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        """
+        reward_ctrl = -0.0001 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+        raise NotImplementedError
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/walker2d.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/walker2d.py
new file mode 100644
index 0000000..1031b32
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym/walker2d.py
@@ -0,0 +1,84 @@
+from __future__ import division
+from __future__ import print_function
+from __future__ import absolute_import
+
+import os
+
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class Walker2dEnv(mujoco_env.MujocoEnv, utils.EzPickle, BaseModelBasedEnv):
+
+    def __init__(self, frame_skip=4):
+        self.prev_qpos = None
+        dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
+        mujoco_env.MujocoEnv.__init__(
+            self, '%s/assets/walker2d.xml' % dir_path, frame_skip=frame_skip
+        )
+        utils.EzPickle.__init__(self)
+
+    def _step(self, action):
+        old_ob = self._get_obs()
+        self.do_simulation(action, self.frame_skip)
+        ob = self._get_obs()
+
+        if getattr(self, 'action_space', None):
+            action = np.clip(action, self.action_space.low,
+                             self.action_space.high)
+
+        reward_ctrl = -0.1 * np.square(action).sum()
+        reward_run = old_ob[8]
+        reward_height = -3.0 * np.square(old_ob[0] - 1.3)
+        reward = reward_run + reward_ctrl + reward_height + 1.0
+
+        done = False
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat[1:],
+            self.model.data.qvel.flat
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        if getattr(self, 'action_space', None):
+            actions = np.clip(actions, self.action_space.low,
+                              self.action_space.high)
+        rewards = - self.cost_np_vec(states, actions, next_states)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def reset_model(self):
+        self.set_state(
+            self.init_qpos + self.np_random.uniform(low=-.005, high=.005, size=self.model.nq),
+            self.init_qvel + self.np_random.uniform(low=-.005, high=.005, size=self.model.nv)
+        )
+        self.prev_qpos = np.copy(self.model.data.qpos.flat)
+        return self._get_obs()
+
+    def viewer_setup(self):
+        self.viewer.cam.trackbodyid = 2
+        self.viewer.cam.distance = self.model.stat.extent * 0.5
+        self.viewer.cam.lookat[2] += .8
+        self.viewer.cam.elevation = -20
+
+    def cost_np_vec(self, obs, acts, next_obs):
+        reward_ctrl = -0.1 * np.sum(np.square(acts), axis=1)
+        reward_run = obs[:, 8]
+        reward_height = -3.0 * np.square(obs[:, 0] - 1.3)
+        reward = reward_run + reward_ctrl + reward_height + 1.0
+        return -reward
+
+    def cost_tf_vec(self, obs, acts, next_obs):
+        """
+        reward_ctrl = -0.1 * tf.reduce_sum(tf.square(acts), axis=1)
+        reward_run = next_obs[:, 0]
+        # reward_height = -3.0 * tf.square(next_obs[:, 1] - 1.3)
+        reward = reward_run + reward_ctrl
+        return -reward
+        """
+        raise NotImplementedError
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym_env.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym_env.py
new file mode 100644
index 0000000..32b9434
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/gym_env.py
@@ -0,0 +1,141 @@
+# import gym
+# import gym.wrappers
+# import gym.envs
+# import gym.spaces
+# import traceback
+# import logging
+#
+# try:
+#     from gym.wrappers.monitoring import logger as monitor_logger
+#
+#     monitor_logger.setLevel(logging.WARNING)
+# except Exception as e:
+#     traceback.print_exc()
+#
+# import os
+# import numpy as np
+# from rllab.misc import logger
+#
+#
+# class CappedCubicVideoSchedule(object):
+#     # Copied from gym, since this method is frequently moved around
+#     def __call__(self, count):
+#         if count < 1000:
+#             return int(round(count ** (1. / 3))) ** 3 == count
+#         else:
+#             return count % 1000 == 0
+#
+#
+# class FixedIntervalVideoSchedule(object):
+#     def __init__(self, interval):
+#         self.interval = interval
+#
+#     def __call__(self, count):
+#         return count % self.interval == 0
+#
+#
+# class NoVideoSchedule(object):
+#     def __call__(self, count):
+#         return False
+#
+#
+# class GymEnv(object):
+#     def __init__(self, env_name, record_video=True, video_schedule=None, log_dir=None, record_log=True,
+#                  force_reset=False):
+#         if log_dir is None:
+#             if logger.get_snapshot_dir() is None:
+#                 logger.log("Warning: skipping Gym environment monitoring since snapshot_dir not configured.")
+#             else:
+#                 log_dir = os.path.join(logger.get_snapshot_dir(), "gym_log")
+#
+#         env = gym.make(env_name)
+#         self.env = env
+#         self.env_id = env.spec.id
+#
+#         assert not (not record_log and record_video)
+#
+#         if log_dir is None or record_log is False:
+#             self.monitoring = False
+#         else:
+#             if not record_video:
+#                 video_schedule = NoVideoSchedule()
+#             else:
+#                 if video_schedule is None:
+#                     video_schedule = CappedCubicVideoSchedule()
+#             self.env = gym.wrappers.Monitor(self.env, log_dir, video_callable=video_schedule, force=True)
+#             self.monitoring = True
+#
+#         self._observation_space = env.observation_space
+#         self._action_space = env.action_space
+#         self._horizon = env.spec.tags['wrapper_config.TimeLimit.max_episode_steps']
+#         self._log_dir = log_dir
+#         self._force_reset = force_reset
+#
+#         self.metadata = {'render.modes': ['human', 'rgb_array']}
+#         self.reward_range = (-np.inf, np.inf)
+#         self.unwrapped = self
+#         self._configured = False
+#         self.spec = None
+#
+#     @property
+#     def inner_env(self):
+#         env = self.env
+#         while hasattr(env, "env"):
+#             env = env.env
+#         return env
+#
+#     @property
+#     def observation_space(self):
+#         return self._observation_space
+#
+#     @property
+#     def action_space(self):
+#         return self._action_space
+#
+#     @property
+#     def horizon(self):
+#         return self._horizon
+#
+#     def reset(self):
+#         if self._force_reset and hasattr(self.env, 'stats_recorder'):
+#             recorder = self.env.stats_recorder
+#             if recorder is not None:
+#                 recorder.done = True
+#
+#         return self.env.reset()
+#
+#     def step(self, action_or_predicted_result):
+#         if isinstance(action_or_predicted_result, dict):
+#             return self._step_with_predicted_dynamics(**action_or_predicted_result)
+#         else:
+#             next_obs, reward, done, info = self.env.step(action_or_predicted_result)
+#             return next_obs, reward, done, info
+#
+#     def _step_with_predicted_dynamics(self, next_obs, reward, done):
+#         qpos = self.inner_env.model.data.qpos.flatten()
+#         qvel = self.inner_env.model.data.qvel.flatten()
+#         self.env.env.set_state(qpos, qvel)
+#         return next_obs, reward, done, {}
+#
+#     def cost_np_vec(self, obs, acts, next_obs):
+#         return self.env.env.cost_np_vec(obs, acts, next_obs)
+#
+#     def cost_tf_vec(self, obs, acts, next_obs):
+#         return self.env.env.cost_tf_vec(obs, acts, next_obs)
+#
+#     def render(self, **kwargs):
+#         return self.env.render(**kwargs)
+#
+#     def terminate(self):
+#         if self.monitoring:
+#             self.env._close()
+#             if self._log_dir is not None:
+#                 print("""
+#     ***************************
+#     Training finished! You can upload results to OpenAI Gym by running the following command:
+#     python scripts/submit_gym.py %s
+#     ***************************
+#                 """ % self._log_dir)
+#
+#     def get_geom_xpos(self):
+#         return self.inner_env.data.geom_xpos
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/neural_env.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/neural_env.py
new file mode 100644
index 0000000..243aa73
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/neural_env.py
@@ -0,0 +1,112 @@
+import numpy as np
+
+
+class NeuralNetEnv:
+
+    def __init__(self, env, inner_env, dynamics):
+        self.vectorized = True
+        self.env = env
+        self.inner_env = inner_env
+        self.is_done = getattr(inner_env, 'is_done', lambda x, y: np.asarray([False] * len(x)))
+        self.dynamics = dynamics
+
+    @property
+    def observation_space(self):
+        return self.env.observation_space
+
+    @property
+    def action_space(self):
+        return self.env.action_space
+
+    @property
+    def spec(self):
+        return self.env.spec
+
+    def reset(self):
+        self.state = self.env.reset()
+        observation = np.copy(self.state)
+        return observation
+
+    def step(self, action, use_states=None):
+        action = np.clip(action, *self.action_space.bounds)
+        if use_states is not None:
+            next_observation = self.dynamics.predict([use_states], [action])[0]
+            obs_dim = self.env.observation_space.shape[0]
+            next_observation[:obs_dim] = np.clip(next_observation[:obs_dim], *self.observation_space.bounds)
+            next_observation[:obs_dim] = np.clip(next_observation[:obs_dim], -1e5, 1e5)
+        else:
+            next_observation = self.dynamics.predict([self.state], [action])[0]
+            next_observation = np.clip(next_observation, *self.observation_space.bounds)
+            next_observation = np.clip(next_observation, -1e5, 1e5)
+
+        if hasattr(self.inner_env, "env"):
+            reward = - self.inner_env.env.cost_np_vec(self.state[None], action[None], np.array([next_observation]))[0]
+        else:
+            reward = - self.inner_env.cost_np_vec(self.state[None], action[None], np.array([next_observation]))[0]
+
+        done = self.is_done(self.state[None], next_observation)[0]
+        self.state = np.reshape(next_observation, -1)
+        return self.inner_env.step({"next_obs": next_observation, "reward": reward, "done": done})
+
+    def render(self):
+        print('current state:', self.state)
+
+    def vec_env_executor(self, n_envs, max_path_length):
+        return VecSimpleEnv(env=self, inner_env=self.inner_env, n_envs=n_envs, max_path_length=max_path_length)
+
+    def terminate(self):
+        self.env.terminate()
+
+
+class VecSimpleEnv(object):
+
+    def __init__(self, env, inner_env, n_envs, max_path_length):
+        self.env = env
+        self.inner_env = inner_env
+        self.n_envs = n_envs
+        self.num_envs = n_envs
+        self.ts = np.zeros((self.n_envs,))
+        self.max_path_length = max_path_length
+        self.obs_dim = env.observation_space.shape[0]
+        self.states = np.zeros((self.n_envs, self.obs_dim))
+
+    def reset(self, dones=None):
+        if dones is None:
+            dones = np.asarray([True] * self.n_envs)
+        else:
+            dones = np.cast['bool'](dones)
+        for i, done in enumerate(dones):
+            if done:
+                self.states[i] = self.env.reset()
+        self.ts[dones] = 0
+        return self.states[dones]
+
+    def step(self, actions, use_states=None):
+        self.ts += 1
+        actions = np.clip(actions, *self.env.action_space.bounds)
+        next_observations = self.get_next_observation(actions, use_states=use_states)
+        if use_states is not None:
+            obs_dim = self.env.observation_space.shape[0]
+            next_observations[:, :obs_dim] = np.clip(next_observations[:, :obs_dim], *self.env.observation_space.bounds)
+            next_observations[:, :obs_dim] = np.clip(next_observations[:, :obs_dim], -1e5, 1e5)
+        else:
+            next_observations = np.clip(next_observations, *self.env.observation_space.bounds)
+            next_observations = np.clip(next_observations, -1e5, 1e5)
+        if hasattr(self.env.inner_env, "cost_np_vec"):
+            rewards = - self.env.inner_env.cost_np_vec(self.states, actions, next_observations)
+        else:
+            rewards = - self.env.inner_env.env.cost_np_vec(self.states, actions, next_observations)
+        self.states = next_observations
+        dones = self.env.is_done(self.states, next_observations)
+        dones[self.ts >= self.max_path_length] = True
+        if np.any(dones):
+            self.reset(dones)
+        return self.states, rewards, dones, dict()
+
+    def get_next_observation(self, actions, use_states=None):
+        if use_states is not None:
+            return self.env.dynamics.predict(use_states, actions)
+        return self.env.dynamics.predict(self.states, actions)
+
+    def terminate(self):
+        self.env.terminate()
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/proxy_env.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/proxy_env.py
new file mode 100644
index 0000000..6381ab4
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/proxy_env.py
@@ -0,0 +1,89 @@
+from gym.core import Env
+from gym.spaces import Box as GymBox
+from gym.wrappers.monitoring import Monitor
+import numpy as np
+import tensorflow as tf
+
+
+class Box:
+
+    def __init__(self, gym_box: GymBox):
+        self.gym_box = gym_box
+
+    @property
+    def flat_dim(self):
+        return np.prod(self.gym_box.shape)
+
+    @property
+    def shape(self):
+        return self.gym_box.shape
+
+    @property
+    def dtype(self):
+        return tf.float32
+
+    @property
+    def bounds(self):
+        return self.gym_box.low, self.gym_box.high
+
+    def flatten_n(self, xs):
+        xs = np.asarray(xs)
+        return xs.reshape((xs.shape[0], -1))
+
+    def sample(self):
+        return self.gym_box.sample()
+
+    def flatten(self, x):
+        return np.asarray(x).flatten()
+
+    def __repr__(self):
+        return "Box Wrapper of shape {}".format(self.shape)
+
+    def __eq__(self, other):
+        return self.gym_box.__eq__(other)
+
+
+class ProxyEnv(Env):
+
+    def __init__(self, wrapped_env: Env):
+        self._wrapped_env = wrapped_env
+        self._wrapped_observation_space = Box(wrapped_env.observation_space)
+        self._wrapped_action_space = Box(wrapped_env.action_space)
+
+    @property
+    def wrapped_env(self):
+        return self._wrapped_env
+
+    def reset(self, **kwargs):
+        return self._wrapped_env.reset(**kwargs)
+
+    @property
+    def action_space(self):
+        return self._wrapped_action_space
+
+    @property
+    def observation_space(self):
+        return self._wrapped_observation_space
+
+    def step(self, action, **kwargs):
+        return self._wrapped_env.step(action, **kwargs)
+
+    def render(self, *args, **kwargs):
+        return self._wrapped_env.render(*args, **kwargs)
+
+    def log_diagnostics(self, paths, *args, **kwargs):
+        self._wrapped_env.log_diagnostics(paths, *args, **kwargs)
+
+    @property
+    def horizon(self):
+        return self._wrapped_env.horizon
+
+    def terminate(self):
+        if isinstance(self._wrapped_env, Monitor):
+            self._wrapped_env._close()
+
+    def get_param_values(self):
+        return self._wrapped_env.get_param_values()
+
+    def set_param_values(self, params):
+        self._wrapped_env.set_param_values(params)
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/vec_env.py b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/vec_env.py
new file mode 100644
index 0000000..d0369e1
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/bm_envs/vec_env.py
@@ -0,0 +1,45 @@
+import numpy as np
+from libs.misc import tensor_utils
+
+
+class VecEnvExecutor(object):
+    def __init__(self, envs, max_path_length, **kwargs):
+        self.envs = envs
+        self._action_space = envs[0].action_space
+        self._observation_space = envs[0].observation_space
+        self.ts = np.zeros(len(self.envs), dtype='int')
+        self.max_path_length = max_path_length
+
+    def step(self, action_n, **kwargs):
+        all_results = [env.step(a) for (a, env) in zip(action_n, self.envs)]
+        obs, rewards, dones, env_infos = list(map(list, list(zip(*all_results))))
+        dones = np.asarray(dones)
+        rewards = np.asarray(rewards)
+        self.ts += 1
+        if self.max_path_length is not None:
+            dones[self.ts >= self.max_path_length] = True
+        for (i, done) in enumerate(dones):
+            if done:
+                obs[i] = self.envs[i].reset()
+                self.ts[i] = 0
+        return obs, rewards, dones, tensor_utils.stack_tensor_dict_list(env_infos)
+
+    def reset(self):
+        results = [env.reset() for env in self.envs]
+        self.ts[:] = 0
+        return results
+
+    @property
+    def num_envs(self):
+        return len(self.envs)
+
+    @property
+    def action_space(self):
+        return self._action_space
+
+    @property
+    def observation_space(self):
+        return self._observation_space
+
+    def terminate(self):
+        pass
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/__init__.py b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/__init__.py
new file mode 100644
index 0000000..5c7f19c
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/__init__.py
@@ -0,0 +1 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/ant_env.py b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/ant_env.py
new file mode 100644
index 0000000..2a300af
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/ant_env.py
@@ -0,0 +1,50 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from rllab.envs.mujoco import ant_env
+from rllab.envs.base import Step
+from slbo.envs import BaseModelBasedEnv
+
+
+class AntEnv(ant_env.AntEnv, BaseModelBasedEnv):
+    def get_current_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat,  # 15
+            self.model.data.qvel.flat,  # 14
+            # np.clip(self.model.data.cfrc_ext, -1, 1).flat,  # 84
+            self.get_body_xmat("torso").flat,  # 9
+            self.get_body_com("torso"),  # 9
+            self.get_body_comvel("torso"),  # 3
+        ]).reshape(-1)
+
+    def step(self, action):
+        self.forward_dynamics(action)
+        comvel = self.get_body_comvel("torso")
+        forward_reward = comvel[0]
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+        ctrl_cost = 0.5 * 1e-2 * np.sum(np.square(action / scaling))
+        contact_cost = 0.
+        # contact_cost = 0.5 * 1e-3 * np.sum(
+        #     np.square(np.clip(self.model.data.cfrc_ext, -1, 1))),
+        survive_reward = 0.05
+        reward = forward_reward - ctrl_cost - contact_cost + survive_reward
+        state = self._state
+        notdone = np.isfinite(state).all() and state[2] >= 0.2 and state[2] <= 1.0
+        done = not notdone
+        ob = self.get_current_obs()
+        return Step(ob, float(reward), done)
+
+    def mb_step(self, states: np.ndarray, actions: np.ndarray, next_states: np.ndarray):
+        comvel = next_states[..., -3:]
+        forward_reward = comvel[..., 0]
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+        ctrl_cost = 0.5 * 1e-2 * np.sum(np.square(actions / scaling), axis=-1)
+        contact_cost = 0.
+        # contact_cost = 0.5 * 1e-3 * np.sum(
+        #     np.square(np.clip(self.model.data.cfrc_ext, -1, 1))),
+        survive_reward = 0.05
+        reward = forward_reward - ctrl_cost - contact_cost + survive_reward
+        notdone = np.all([next_states[..., 2] >= 0.2, next_states[..., 2] <= 1.0], axis=0)
+        return reward, 1. - notdone
+
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/half_cheetah_env.py b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/half_cheetah_env.py
new file mode 100644
index 0000000..29b1502
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/half_cheetah_env.py
@@ -0,0 +1,20 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from rllab.envs.mujoco import half_cheetah_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class HalfCheetahEnv(half_cheetah_env.HalfCheetahEnv, BaseModelBasedEnv):
+    def get_current_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat,  # 9
+            self.model.data.qvel.flat,  # 9
+            self.get_body_com("torso").flat,  # 3
+            self.get_body_comvel("torso").flat,  # 3
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        actions = np.clip(actions, *self.action_bounds)
+        reward_ctrl = -0.05 * np.sum(np.square(actions), axis=-1)
+        reward_fwd = next_states[..., 21]
+        return reward_ctrl + reward_fwd, np.zeros_like(reward_fwd, dtype=np.bool)
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/hopper_env.py b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/hopper_env.py
new file mode 100644
index 0000000..23c9c50
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/hopper_env.py
@@ -0,0 +1,26 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from rllab.envs.mujoco import hopper_env
+from rllab.envs.base import Step
+from slbo.envs import BaseModelBasedEnv
+
+
+class HopperEnv(hopper_env.HopperEnv, BaseModelBasedEnv):
+    def get_current_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat,  # 6
+            self.model.data.qvel.flat,  # 6
+            self.get_body_com("torso").flat,  # 3
+            self.get_body_comvel("torso"),  # 3
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+        vel = next_states[:, -3]
+        reward = vel + self.alive_coeff - 0.5 * self.ctrl_cost_coeff * np.sum(np.square(actions / scaling), axis=-1)
+
+        done = ~((next_states[:, 3:12] < 100).all(axis=-1) &
+                 (next_states[:, 0] > 0.7) &
+                 (np.abs(next_states[:, 2]) < 0.2))
+        return reward, done
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/humanoid_env.py b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/humanoid_env.py
new file mode 100644
index 0000000..a5e55ae
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/humanoid_env.py
@@ -0,0 +1,53 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from rllab.envs.mujoco import simple_humanoid_env
+from rllab.envs.base import Step
+import numpy as np
+from slbo.envs import BaseModelBasedEnv
+
+
+class HumanoidEnv(simple_humanoid_env.SimpleHumanoidEnv, BaseModelBasedEnv):
+    def get_current_obs(self):
+        data = self.model.data
+        return np.concatenate([
+            data.qpos.flat,  # 17
+            data.qvel.flat,  # 16
+            self.get_body_com("torso").flat,  # 3
+            self.get_body_comvel("torso").flat,  # 3
+        ])
+
+    def step(self, action):
+        self.forward_dynamics(action)
+        alive_bonus = self.alive_bonus
+        data = self.model.data
+
+        comvel = self.get_body_comvel("torso")
+        lin_vel_reward = comvel[0]
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+        ctrl_cost = .5 * self.ctrl_cost_coeff * np.sum(
+            np.square(action / scaling))
+        impact_cost = 0.
+        vel_deviation_cost = 0.5 * self.vel_deviation_cost_coeff * np.sum(
+            np.square(comvel[1:]))
+        reward = lin_vel_reward + alive_bonus - ctrl_cost - \
+            impact_cost - vel_deviation_cost
+        pos = data.qpos.flat[2]
+        done = pos < 0.8 or pos > 2.0
+
+        next_obs = self.get_current_obs()
+        return Step(next_obs, reward, done)
+
+    def mb_step(self, states, actions, next_states):
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+
+        alive_bonus = 0.2
+        lin_vel_reward = next_states[:, 36]
+        ctrl_cost = 5.e-4 * np.square(actions / scaling).sum(axis=1)
+        impact_cost = 0.
+        vel_deviation_cost = 5.e-3 * np.square(next_states[:, 37:39]).sum(axis=1)
+        reward = lin_vel_reward + alive_bonus - ctrl_cost - impact_cost - vel_deviation_cost
+
+        dones = (next_states[:, 2] < 0.8) | (next_states[:, 2] > 2.0)
+        return reward, dones
+
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/swimmer_env.py b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/swimmer_env.py
new file mode 100644
index 0000000..f39bee4
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/swimmer_env.py
@@ -0,0 +1,22 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from rllab.envs.mujoco import swimmer_env
+from slbo.envs import BaseModelBasedEnv
+
+
+class SwimmerEnv(swimmer_env.SwimmerEnv, BaseModelBasedEnv):
+    def get_current_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat,  # 5
+            self.model.data.qvel.flat,  # 5
+            self.get_body_com("torso").flat,  # 3
+            self.get_body_comvel("torso"),  # 3
+        ]).reshape(-1)
+
+    def mb_step(self, states: np.ndarray, actions: np.ndarray, next_states: np.ndarray):
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+        ctrl_cost = 0.5 * self.ctrl_cost_coeff * np.sum(np.square(actions / scaling), axis=-1)
+        forward_reward = next_states[:, -3]
+        reward = forward_reward - ctrl_cost
+        return reward, np.zeros_like(reward, dtype=np.bool)
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/walker2d_env.py b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/walker2d_env.py
new file mode 100644
index 0000000..8eb16bc
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco/walker2d_env.py
@@ -0,0 +1,43 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from rllab.envs.mujoco import walker2d_env
+from rllab.envs.base import Step
+from slbo.envs import BaseModelBasedEnv
+
+
+class Walker2DEnv(walker2d_env.Walker2DEnv, BaseModelBasedEnv):
+    def get_current_obs(self):
+        return np.concatenate([
+            self.model.data.qpos.flat,
+            self.model.data.qvel.flat,
+            self.get_body_com("torso").flat,
+            self.get_body_comvel("torso").flat
+        ])
+
+    def step(self, action):
+        self.forward_dynamics(action)
+        forward_reward = self.get_body_comvel("torso")[0]
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+        ctrl_cost = 1e-3 * np.sum(np.square(action / scaling))
+        alive_bonus = 1.
+        reward = forward_reward - ctrl_cost + alive_bonus
+        qpos = self.model.data.qpos
+        done = not (qpos[0] > 0.8 and qpos[0] < 2.0 and qpos[2] > -1.0 and qpos[2] < 1.0)
+        next_obs = self.get_current_obs()
+        return Step(next_obs, reward, done)
+
+    def mb_step(self, states, actions, next_states):
+        lb, ub = self.action_bounds
+        scaling = (ub - lb) * 0.5
+
+        reward_ctrl = -0.001 * np.sum(np.square(actions / scaling), axis=-1)
+        reward_fwd = next_states[:, 21]
+        alive_bonus = 1.
+        rewards = reward_ctrl + reward_fwd + alive_bonus
+
+        dones = ~((next_states[:, 0] > 0.8) &
+                  (next_states[:, 0] < 2.0) &
+                  (next_states[:, 2] > -1.0) &
+                  (next_states[:, 2] < 1.0))
+        return rewards, dones
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/__init__.py b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/__init__.py
new file mode 100644
index 0000000..95924e3
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/__init__.py
@@ -0,0 +1,81 @@
+"""
+Mujoco Maze
+----------
+
+A maze environment using mujoco that supports custom tasks and robots.
+"""
+
+
+import gym
+
+from slbo.envs.mujoco_maze.ant import AntEnv
+from slbo.envs.mujoco_maze.maze_task import TaskRegistry
+from slbo.envs.mujoco_maze.point import PointEnv
+from slbo.envs.mujoco_maze.reacher import ReacherEnv
+from slbo.envs.mujoco_maze.swimmer import SwimmerEnv
+
+for maze_id in TaskRegistry.keys():
+    for i, task_cls in enumerate(TaskRegistry.tasks(maze_id)):
+        point_scale = task_cls.MAZE_SIZE_SCALING.point
+        if point_scale is not None:
+            # Point
+            gym.envs.register(
+                id=f"Point{maze_id}-v{i}",
+                entry_point="slbo.envs.mujoco_maze.maze_env:MazeEnv",
+                kwargs=dict(
+                    model_cls=PointEnv,
+                    maze_task=task_cls,
+                    maze_size_scaling=point_scale,
+                    inner_reward_scaling=task_cls.INNER_REWARD_SCALING,
+                ),
+                max_episode_steps=1000,
+                reward_threshold=task_cls.REWARD_THRESHOLD,
+            )
+
+        ant_scale = task_cls.MAZE_SIZE_SCALING.ant
+        if ant_scale is not None:
+            # Ant
+            gym.envs.register(
+                id=f"Ant{maze_id}-v{i}",
+                entry_point="slbo.envs.mujoco_maze.maze_env:MazeEnv",
+                kwargs=dict(
+                    model_cls=AntEnv,
+                    maze_task=task_cls,
+                    maze_size_scaling=ant_scale,
+                    inner_reward_scaling=task_cls.INNER_REWARD_SCALING,
+                ),
+                max_episode_steps=1000,
+                reward_threshold=task_cls.REWARD_THRESHOLD,
+            )
+
+        swimmer_scale = task_cls.MAZE_SIZE_SCALING.swimmer
+        if swimmer_scale is not None:
+            # Reacher
+            gym.envs.register(
+                id=f"Reacher{maze_id}-v{i}",
+                entry_point="mujoco_maze.maze_env:MazeEnv",
+                kwargs=dict(
+                    model_cls=ReacherEnv,
+                    maze_task=task_cls,
+                    maze_size_scaling=task_cls.MAZE_SIZE_SCALING.swimmer,
+                    inner_reward_scaling=task_cls.INNER_REWARD_SCALING,
+                ),
+                max_episode_steps=1000,
+                reward_threshold=task_cls.REWARD_THRESHOLD,
+            )
+            # Swimmer
+            gym.envs.register(
+                id=f"Swimmer{maze_id}-v{i}",
+                entry_point="mujoco_maze.maze_env:MazeEnv",
+                kwargs=dict(
+                    model_cls=SwimmerEnv,
+                    maze_task=task_cls,
+                    maze_size_scaling=task_cls.MAZE_SIZE_SCALING.swimmer,
+                    inner_reward_scaling=task_cls.INNER_REWARD_SCALING,
+                ),
+                max_episode_steps=1000,
+                reward_threshold=task_cls.REWARD_THRESHOLD,
+            )
+
+
+__version__ = "0.1.0"
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/agent_model.py b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/agent_model.py
new file mode 100644
index 0000000..d209d78
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/agent_model.py
@@ -0,0 +1,42 @@
+"""Common APIs for defining mujoco robot.
+"""
+from abc import ABC, abstractmethod
+from typing import Optional
+
+import numpy as np
+from gym.envs.mujoco.mujoco_env import MujocoEnv
+from gym.utils import EzPickle
+
+
+class AgentModel(ABC, MujocoEnv, EzPickle):
+    FILE: str
+    MANUAL_COLLISION: bool
+    ORI_IND: int
+    RADIUS: Optional[float] = None
+
+    def __init__(self, file_path: str, frame_skip: int) -> None:
+        MujocoEnv.__init__(self, file_path, frame_skip)
+        EzPickle.__init__(self)
+
+    def close(self):
+        if self.viewer is not None and hasattr(self.viewer, "window"):
+            import glfw
+
+            glfw.destroy_window(self.viewer.window)
+        super().close()
+
+    @abstractmethod
+    def _get_obs(self) -> np.ndarray:
+        """Returns the observation from the model.
+        """
+        pass
+
+    def get_xy(self) -> np.ndarray:
+        """Returns the coordinate of the agent.
+        """
+        pass
+
+    def set_xy(self, xy: np.ndarray) -> None:
+        """Set the coordinate of the agent.
+        """
+        pass
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/ant.py b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/ant.py
new file mode 100644
index 0000000..b6a725a
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/ant.py
@@ -0,0 +1,108 @@
+"""
+A four-legged robot as an explorer in the maze.
+Based on `models`_ and `gym`_ (both ant and ant-v3).
+
+.. _models: https://github.com/tensorflow/models/tree/master/research/efficient-hrl
+.. _gym: https://github.com/openai/gym
+"""
+
+from typing import Callable, Tuple
+
+import numpy as np
+
+from slbo.envs.mujoco_maze.agent_model import AgentModel
+
+ForwardRewardFn = Callable[[float, float], float]
+
+
+def forward_reward_vabs(xy_velocity: float) -> float:
+    return np.sum(np.abs(xy_velocity))
+
+
+def forward_reward_vnorm(xy_velocity: float) -> float:
+    return np.linalg.norm(xy_velocity)
+
+
+def q_inv(a):
+    return [a[0], -a[1], -a[2], -a[3]]
+
+
+def q_mult(a, b):  # multiply two quaternion
+    w = a[0] * b[0] - a[1] * b[1] - a[2] * b[2] - a[3] * b[3]
+    i = a[0] * b[1] + a[1] * b[0] + a[2] * b[3] - a[3] * b[2]
+    j = a[0] * b[2] - a[1] * b[3] + a[2] * b[0] + a[3] * b[1]
+    k = a[0] * b[3] + a[1] * b[2] - a[2] * b[1] + a[3] * b[0]
+    return [w, i, j, k]
+
+
+class AntEnv(AgentModel):
+    FILE: str = "ant.xml"
+    ORI_IND: int = 3
+    MANUAL_COLLISION: bool = False
+
+    def __init__(
+        self,
+        file_path: str,
+        forward_reward_weight: float = 1.0,
+        ctrl_cost_weight: float = 1e-4,
+        forward_reward_fn: ForwardRewardFn = forward_reward_vnorm,
+    ) -> None:
+        self._forward_reward_weight = forward_reward_weight
+        self._ctrl_cost_weight = ctrl_cost_weight
+        self._forward_reward_fn = forward_reward_fn
+        super().__init__(file_path, 5)
+
+    def _forward_reward(self, xy_pos_before: np.ndarray) -> Tuple[float, np.ndarray]:
+        xy_pos_after = self.sim.data.qpos[:2].copy()
+        xy_velocity = (xy_pos_after - xy_pos_before) / self.dt
+        return self._forward_reward_fn(xy_velocity)
+
+    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, dict]:
+        xy_pos_before = self.sim.data.qpos[:2].copy()
+        self.do_simulation(action, self.frame_skip)
+
+        forward_reward = self._forward_reward(xy_pos_before)
+        ctrl_cost = self._ctrl_cost_weight * np.square(action).sum()
+
+        return (
+            self._get_obs(),
+            self._forward_reward_weight * forward_reward - ctrl_cost,
+            False,
+            dict(reward_forward=forward_reward, reward_ctrl=-ctrl_cost),
+        )
+
+    def _get_obs(self):
+        # No cfrc observation
+        return np.concatenate(
+            [
+                self.sim.data.qpos.flat[:15],  # Ensures only ant obs.
+                self.sim.data.qvel.flat[:14],
+            ]
+        )
+
+    def reset_model(self):
+        qpos = self.init_qpos + self.np_random.uniform(
+            size=self.model.nq, low=-0.1, high=0.1,
+        )
+        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * 0.1
+
+        # Set everything other than ant to original position and 0 velocity.
+        qpos[15:] = self.init_qpos[15:]
+        qvel[14:] = 0.0
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def get_ori(self) -> np.ndarray:
+        ori = [0, 1, 0, 0]
+        rot = self.sim.data.qpos[self.ORI_IND : self.ORI_IND + 4]  # take the quaternion
+        ori = q_mult(q_mult(rot, ori), q_inv(rot))[1:3]  # project onto x-y plane
+        ori = np.arctan2(ori[1], ori[0])
+        return ori
+
+    def set_xy(self, xy: np.ndarray) -> None:
+        qpos = self.sim.data.qpos.copy()
+        qpos[:2] = xy
+        self.set_state(qpos, self.sim.data.qvel)
+
+    def get_xy(self) -> np.ndarray:
+        return np.copy(self.sim.data.qpos[:2])
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/ant.xml b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/ant.xml
new file mode 100755
index 0000000..ffe156b
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/ant.xml
@@ -0,0 +1,80 @@
+<mujoco model="ant">
+  <compiler inertiafromgeom="true" angle="degree" coordinate="local" />
+  <option timestep="0.02" integrator="RK4" />
+  <custom>
+    <numeric name="init_qpos" data="0.0 0.0 0.55 1.0 0.0 0.0 0.0 0.0 1.0 0.0 -1.0 0.0 -1.0 0.0 1.0" />
+  </custom>
+  <default>
+    <joint limited="true" armature="1" damping="1" />
+    <geom condim="3" conaffinity="0" margin="0.01" friction="1 0.5 0.5" solref=".02 1" solimp=".8 .8 .01" rgba="0.8 0.6 0.4 1" density="5.0" />
+  </default>
+  <asset>
+    <texture type="skybox" builtin="gradient" width="100" height="100" rgb1="1 1 1" rgb2="0 0 0" />
+    <texture name="texgeom" type="cube" builtin="flat" mark="cross" width="127" height="1278" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" markrgb="1 1 1" random="0.01" />
+    <texture name="texplane" type="2d" builtin="checker" rgb1="0.2 0.3 0.4" rgb2="0.1 0.2 0.3" width="100" height="100" />
+    <material name='MatPlane' texture="texplane" shininess="1" texrepeat="60 60" specular="1"  reflectance="0.5" />
+    <material name='geom' texture="texgeom" texuniform="true" />
+  </asset>
+  <worldbody>
+    <light directional="true" cutoff="100" exponent="1" diffuse="1 1 1" specular=".1 .1 .1" pos="0 0 1.3" dir="-0 0 -1.3" />
+    <geom name="floor" material="MatPlane" pos="0 0 0" size="40 40 40" type="plane" conaffinity="1" rgba="0.8 0.9 0.8 1" condim="3" />
+    <body name="torso" pos="0 0 0.75">
+      <geom name="torso_geom" type="sphere" size="0.25" pos="0 0 0" />
+      <joint name="root" type="free" limited="false" pos="0 0 0" axis="0 0 1" margin="0.01" armature="0" damping="0" />
+      <body name="front_left_leg" pos="0 0 0">
+        <geom name="aux_1_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 0.2 0.2 0.0" />
+        <body name="aux_1" pos="0.2 0.2 0">
+          <joint name="hip_1" type="hinge" pos="0.0 0.0 0.0" axis="0 0 1" range="-30 30" />
+          <geom name="left_leg_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 0.2 0.2 0.0" />
+          <body pos="0.2 0.2 0">
+            <joint name="ankle_1" type="hinge" pos="0.0 0.0 0.0" axis="-1 1 0" range="30 70" />
+            <geom name="left_ankle_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 0.4 0.4 0.0" />
+          </body>
+        </body>
+      </body>
+      <body name="front_right_leg" pos="0 0 0">
+        <geom name="aux_2_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 -0.2 0.2 0.0" />
+        <body name="aux_2" pos="-0.2 0.2 0">
+          <joint name="hip_2" type="hinge" pos="0.0 0.0 0.0" axis="0 0 1" range="-30 30" />
+          <geom name="right_leg_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 -0.2 0.2 0.0" />
+          <body pos="-0.2 0.2 0">
+            <joint name="ankle_2" type="hinge" pos="0.0 0.0 0.0" axis="1 1 0" range="-70 -30" />
+            <geom name="right_ankle_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 -0.4 0.4 0.0" />
+          </body>
+        </body>
+      </body>
+      <body name="back_leg" pos="0 0 0">
+        <geom name="aux_3_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 -0.2 -0.2 0.0" />
+        <body name="aux_3" pos="-0.2 -0.2 0">
+          <joint name="hip_3" type="hinge" pos="0.0 0.0 0.0" axis="0 0 1" range="-30 30" />
+          <geom name="back_leg_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 -0.2 -0.2 0.0" />
+          <body pos="-0.2 -0.2 0">
+            <joint name="ankle_3" type="hinge" pos="0.0 0.0 0.0" axis="-1 1 0" range="-70 -30" />
+            <geom name="third_ankle_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 -0.4 -0.4 0.0" />
+          </body>
+        </body>
+      </body>
+      <body name="right_back_leg" pos="0 0 0">
+        <geom name="aux_4_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 0.2 -0.2 0.0" />
+        <body name="aux_4" pos="0.2 -0.2 0">
+          <joint name="hip_4" type="hinge" pos="0.0 0.0 0.0" axis="0 0 1" range="-30 30" />
+          <geom name="rightback_leg_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 0.2 -0.2 0.0" />
+          <body pos="0.2 -0.2 0">
+            <joint name="ankle_4" type="hinge" pos="0.0 0.0 0.0" axis="1 1 0" range="30 70" />
+            <geom name="fourth_ankle_geom" type="capsule" size="0.08" fromto="0.0 0.0 0.0 0.4 -0.4 0.0" />
+          </body>
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor joint="hip_4" ctrlrange="-30.0 30.0" ctrllimited="true" />
+    <motor joint="ankle_4" ctrlrange="-30.0 30.0" ctrllimited="true" />
+    <motor joint="hip_1" ctrlrange="-30.0 30.0" ctrllimited="true" />
+    <motor joint="ankle_1" ctrlrange="-30.0 30.0" ctrllimited="true" />
+    <motor joint="hip_2" ctrlrange="-30.0 30.0" ctrllimited="true" />
+    <motor joint="ankle_2" ctrlrange="-30.0 30.0" ctrllimited="true" />
+    <motor joint="hip_3" ctrlrange="-30.0 30.0" ctrllimited="true" />
+    <motor joint="ankle_3" ctrlrange="-30.0 30.0" ctrllimited="true" />
+  </actuator>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/point.xml b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/point.xml
new file mode 100755
index 0000000..4c06cb1
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/point.xml
@@ -0,0 +1,33 @@
+<mujoco>
+    <compiler inertiafromgeom="true" angle="degree" coordinate="local" />
+    <option timestep="0.02" integrator="RK4" />
+    <default>
+        <joint limited="false" armature="0" damping="0" />
+        <geom condim="3" conaffinity="0" margin="0" friction="1.0 0.5 0.5" rgba="0.8 0.6 0.4 1" density="100" />
+    </default>
+    <asset>
+        <texture type="skybox" builtin="gradient" width="100" height="100" rgb1="1 1 1" rgb2="0 0 0" />
+        <texture name="texgeom" type="cube" builtin="flat" mark="cross" width="127" height="1278" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" markrgb="1 1 1" random="0.01" />
+        <texture name="texplane" type="2d" builtin="checker" rgb1="0.2 0.3 0.4" rgb2="0.1 0.2 0.3" width="100" height="100" />
+        <material name="MatPlane" texture="texplane" shininess="1" texrepeat="30 30" specular="1"  reflectance="0.5" />
+        <material name="geom" texture="texgeom" texuniform="true" />
+    </asset>
+    <worldbody>
+        <light directional="true" cutoff="100" exponent="1" diffuse="1 1 1" specular=".1 .1 .1" pos="0 0 1.3" dir="-0 0 -1.3" />
+        <geom name="floor" material="MatPlane" pos="0 0 0" size="40 40 40" type="plane" conaffinity="1" rgba="0.8 0.9 0.8 1" condim="3" />
+        <!--  ================= Point ================= /-->
+        <!--  Note that the solimp is modified from rllab to prevent the point from going through the wall /-->
+        <body name="torso" pos="0 0 0">
+            <geom name="pointbody" type="sphere" size="0.5" pos="0 0 0.5" rgba="0.8 0.4 0.1 1" solimp="0.9 0.99 0.001" />
+            <geom name="pointarrow" type="box" size="0.5 0.1 0.1" pos="0.6 0 0.5" rgba="0.8 0.4 0.1 1" solimp="0.9 0.99 0.001" />
+            <joint name="ballx" type="slide" axis="1 0 0" pos="0 0 0" />
+            <joint name="bally" type="slide" axis="0 1 0" pos="0 0 0" />
+            <joint name="rot" type="hinge" axis="0 0 1" pos="0 0 0" limited="false" />
+        </body>
+    </worldbody>
+    <actuator>
+        <!-- Those are just dummy actuators for providing ranges -->
+        <motor joint="ballx" ctrlrange="-1 1" ctrllimited="true" />
+        <motor joint="rot" ctrlrange="-0.25 0.25" ctrllimited="true" />
+    </actuator>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/reacher.xml b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/reacher.xml
new file mode 100644
index 0000000..0d238c8
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/reacher.xml
@@ -0,0 +1,34 @@
+<mujoco model="swimmer">
+  <compiler angle="degree" coordinate="local" inertiafromgeom="true" />
+  <option collision="predefined" density="4000" integrator="RK4" timestep="0.01" viscosity="0.1" />
+  <default>
+    <geom conaffinity="1" condim="1" contype="1" material="geom" rgba="0.8 0.6 .4 1" />
+    <joint armature="0.1" />
+  </default>
+  <asset>
+    <texture type="skybox" builtin="gradient" width="100" height="100" rgb1="1 1 1" rgb2="0 0 0" />
+    <texture name="texgeom" type="cube" builtin="flat" mark="cross" width="127" height="1278" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" markrgb="1 1 1" random="0.01" />
+    <texture name="texplane" type="2d" builtin="checker" rgb1="0.2 0.3 0.4" rgb2="0.1 0.2 0.3" width="100" height="100" />
+    <material name='MatPlane' texture="texplane" shininess="1" texrepeat="60 60" specular="1"  reflectance="0.5" />
+    <material name='geom' texture="texgeom" texuniform="true" />
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0s 1.3" specular=".1 .1 .1" />
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 -0.1" rgba="0.8 0.9 0.8 1" size="40 40 0.1" type="plane" />
+    <!-- Reacher -->
+    <body name="torso" pos="0 0 0">
+      <camera name="track" mode="trackcom" pos="0 -3 3" xyaxes="1 0 0 0 1 1" />
+      <geom name="frontbody" density="1000" fromto="1.5 0 0 0.5 0 0" size="0.1" type="capsule" />
+      <joint axis="1 0 0" name="slider1" pos="0 0 0" type="slide" />
+      <joint axis="0 1 0" name="slider2" pos="0 0 0" type="slide" />
+      <joint axis="0 0 1" name="rot" pos="0 0 0" type="hinge" />
+      <body name="mid" pos="0.5 0 0">
+        <geom name="midbody" density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule" />
+        <joint axis="0 0 1" limited="true" name="rot2" pos="0 0 0" range="-100 100" type="hinge" />
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot2" />
+  </actuator>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/swimmer.xml b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/swimmer.xml
new file mode 100644
index 0000000..3c6c21a
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/assets/swimmer.xml
@@ -0,0 +1,39 @@
+<mujoco model="swimmer">
+  <compiler angle="degree" coordinate="local" inertiafromgeom="true" />
+  <option collision="predefined" density="4000" integrator="RK4" timestep="0.01" viscosity="0.1" />
+  <default>
+    <geom conaffinity="1" condim="1" contype="1" material="geom" rgba="0.8 0.6 .4 1" />
+    <joint armature="0.1" />
+  </default>
+  <asset>
+    <texture type="skybox" builtin="gradient" width="100" height="100" rgb1="1 1 1" rgb2="0 0 0" />
+    <texture name="texgeom" type="cube" builtin="flat" mark="cross" width="127" height="1278" rgb1="0.8 0.6 0.4" rgb2="0.8 0.6 0.4" markrgb="1 1 1" random="0.01" />
+    <texture name="texplane" type="2d" builtin="checker" rgb1="0.2 0.3 0.4" rgb2="0.1 0.2 0.3" width="100" height="100" />
+    <material name='MatPlane' texture="texplane" shininess="1" texrepeat="60 60" specular="1"  reflectance="0.5" />
+    <material name='geom' texture="texgeom" texuniform="true" />
+  </asset>
+  <worldbody>
+    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0s 1.3" specular=".1 .1 .1" />
+    <geom conaffinity="1" condim="3" material="MatPlane" name="floor" pos="0 0 -0.1" rgba="0.8 0.9 0.8 1" size="40 40 0.1" type="plane" />
+    <!--  ================= SWIMMER ================= /-->
+    <body name="torso" pos="0 0 0">
+      <camera name="track" mode="trackcom" pos="0 -3 3" xyaxes="1 0 0 0 1 1" />
+      <geom name="frontbody" density="1000" fromto="1.5 0 0 0.5 0 0" size="0.1" type="capsule" />
+      <joint axis="1 0 0" name="slider1" pos="0 0 0" type="slide" />
+      <joint axis="0 1 0" name="slider2" pos="0 0 0" type="slide" />
+      <joint axis="0 0 1" name="rot" pos="0 0 0" type="hinge" />
+      <body name="mid" pos="0.5 0 0">
+        <geom name="midbody" density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule" />
+        <joint axis="0 0 1" limited="true" name="rot2" pos="0 0 0" range="-100 100" type="hinge" />
+        <body name="back" pos="-1 0 0">
+          <geom name="backbody" density="1000" fromto="0 0 0 -1 0 0" size="0.1" type="capsule" />
+          <joint axis="0 0 1" limited="true" name="rot3" pos="0 0 0" range="-100 100" type="hinge" />
+        </body>
+      </body>
+    </body>
+  </worldbody>
+  <actuator>
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot2" />
+    <motor ctrllimited="true" ctrlrange="-1 1" gear="150.0" joint="rot3" />
+  </actuator>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_env.py b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_env.py
new file mode 100644
index 0000000..dd11988
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_env.py
@@ -0,0 +1,595 @@
+"""
+Mujoco Maze environment.
+Based on `models`_ and `rllab`_.
+
+.. _models: https://github.com/tensorflow/models/tree/master/research/efficient-hrl
+.. _rllab: https://github.com/rll/rllab
+"""
+
+import itertools as it
+import os
+import tempfile
+import xml.etree.ElementTree as ET
+from typing import List, Tuple, Type
+
+import gym
+import numpy as np
+
+from slbo.envs.mujoco_maze import maze_env_utils, maze_task
+from slbo.envs.mujoco_maze.agent_model import AgentModel
+from slbo.utils.dataset import Dataset, gen_dtype
+
+
+# Directory that contains mujoco xml files.
+MODEL_DIR = os.path.dirname(os.path.abspath(__file__)) + "/assets"
+
+
+class MazeEnv(gym.Env):
+    def __init__(
+        self,
+        model_cls: Type[AgentModel],
+        maze_task: Type[maze_task.MazeTask] = maze_task.MazeTask,
+        top_down_view: float = False,
+        maze_height: float = 0.5,
+        maze_size_scaling: float = 4.0,
+        inner_reward_scaling: float = 1.0,
+        restitution_coef: float = 0.8,
+        task_kwargs: dict = {},
+        *args,
+        **kwargs,
+    ) -> None:
+        self._task = maze_task(maze_size_scaling, **task_kwargs)
+
+        xml_path = os.path.join(MODEL_DIR, model_cls.FILE)
+        tree = ET.parse(xml_path)
+        worldbody = tree.find(".//worldbody")
+
+        self._maze_height = height = maze_height
+        self._maze_size_scaling = size_scaling = maze_size_scaling
+        self._inner_reward_scaling = inner_reward_scaling
+        self.t = 0  # time steps
+        self._observe_blocks = self._task.OBSERVE_BLOCKS
+        self._put_spin_near_agent = self._task.PUT_SPIN_NEAR_AGENT
+        # Observe other objectives
+        self._observe_balls = self._task.OBSERVE_BALLS
+        self._top_down_view = self._task.TOP_DOWN_VIEW
+        self._restitution_coef = restitution_coef
+
+        self._maze_structure = structure = self._task.create_maze()
+        # Elevate the maze to allow for falling.
+        self.elevated = any(maze_env_utils.MazeCell.CHASM in row for row in structure)
+        # Are there any movable blocks?
+        self.blocks = any(any(r.can_move() for r in row) for row in structure)
+
+        torso_x, torso_y = self._find_robot()
+        self._init_torso_x = torso_x
+        self._init_torso_y = torso_y
+        self._init_positions = [
+            (x - torso_x, y - torso_y) for x, y in self._find_all_robots()
+        ]
+
+        if model_cls.MANUAL_COLLISION:
+            if model_cls.RADIUS is None:
+                raise ValueError("Manual collision needs radius of the model")
+            self._collision = maze_env_utils.CollisionDetector(
+                structure, size_scaling, torso_x, torso_y, model_cls.RADIUS,
+            )
+            # Now all object balls have size=1.0
+            self._objball_collision = maze_env_utils.CollisionDetector(
+                structure, size_scaling, torso_x, torso_y, self._task.OBJECT_BALL_SIZE,
+            )
+        else:
+            self._collision = None
+
+        self._xy_to_rowcol = lambda x, y: (
+            2 + (y + size_scaling / 2) / size_scaling,
+            2 + (x + size_scaling / 2) / size_scaling,
+        )
+        # walls (immovable), chasms (fall), movable blocks
+        self._view = np.zeros([5, 5, 3])
+
+        height_offset = 0.0
+        if self.elevated:
+            # Increase initial z-pos of ant.
+            height_offset = height * size_scaling
+            torso = tree.find(".//body[@name='torso']")
+            torso.set("pos", f"0 0 {0.75 + height_offset:.2f}")
+        if self.blocks:
+            # If there are movable blocks, change simulation settings to perform
+            # better contact detection.
+            default = tree.find(".//default")
+            default.find(".//geom").set("solimp", ".995 .995 .01")
+
+        self.movable_blocks = []
+        self.object_balls = []
+        for i in range(len(structure)):
+            for j in range(len(structure[0])):
+                struct = structure[i][j]
+                if struct.is_robot() and self._put_spin_near_agent:
+                    struct = maze_env_utils.MazeCell.SPIN
+                x, y = j * size_scaling - torso_x, i * size_scaling - torso_y
+                h = height / 2 * size_scaling
+                size = size_scaling * 0.5
+                if self.elevated and not struct.is_chasm():
+                    # Create elevated platform.
+                    ET.SubElement(
+                        worldbody,
+                        "geom",
+                        name=f"elevated_{i}_{j}",
+                        pos=f"{x} {y} {h}",
+                        size=f"{size} {size} {h}",
+                        type="box",
+                        material="",
+                        contype="1",
+                        conaffinity="1",
+                        rgba="0.9 0.9 0.9 1",
+                    )
+                if struct.is_block():
+                    # Unmovable block.
+                    # Offset all coordinates so that robot starts at the origin.
+                    ET.SubElement(
+                        worldbody,
+                        "geom",
+                        name=f"block_{i}_{j}",
+                        pos=f"{x} {y} {h + height_offset}",
+                        size=f"{size} {size} {h}",
+                        type="box",
+                        material="",
+                        contype="1",
+                        conaffinity="1",
+                        rgba="0.4 0.4 0.4 1",
+                    )
+                elif struct.can_move():
+                    # Movable block.
+                    self.movable_blocks.append(f"movable_{i}_{j}")
+                    _add_movable_block(
+                        worldbody, struct, i, j, size_scaling, x, y, h, height_offset,
+                    )
+                elif struct.is_object_ball():
+                    # Movable Ball
+                    self.object_balls.append(f"objball_{i}_{j}")
+                    _add_object_ball(worldbody, i, j, x, y, self._task.OBJECT_BALL_SIZE)
+
+        torso = tree.find(".//body[@name='torso']")
+        geoms = torso.findall(".//geom")
+        for geom in geoms:
+            if "name" not in geom.attrib:
+                raise Exception("Every geom of the torso must have a name")
+
+        # Set goals
+        for i, goal in enumerate(self._task.goals):
+            z = goal.pos[2] if goal.dim >= 3 else 0.0
+            if goal.custom_size is None:
+                size = f"{maze_size_scaling * 0.1}"
+            else:
+                size = f"{goal.custom_size}"
+            ET.SubElement(
+                worldbody,
+                "site",
+                name=f"goal_site{i}",
+                pos=f"{goal.pos[0]} {goal.pos[1]} {z}",
+                size=f"{maze_size_scaling * 0.1}",
+                rgba=goal.rgb.rgba_str(),
+            )
+
+        _, file_path = tempfile.mkstemp(text=True, suffix=".xml")
+        tree.write(file_path)
+        self.world_tree = tree
+        self.wrapped_env = model_cls(*args, file_path=file_path, **kwargs)
+        self.observation_space = self._get_obs_space()
+
+    @property
+    def has_extended_obs(self) -> bool:
+        return self._top_down_view or self._observe_blocks or self._observe_balls
+
+    def get_ori(self) -> float:
+        return self.wrapped_env.get_ori()
+
+    def _get_obs_space(self) -> gym.spaces.Box:
+        shape = self._get_obs().shape
+        high = np.inf * np.ones(shape, dtype=np.float32)
+        low = -high
+        # Set velocity limits
+        wrapped_obs_space = self.wrapped_env.observation_space
+        high[: wrapped_obs_space.shape[0]] = wrapped_obs_space.high
+        low[: wrapped_obs_space.shape[0]] = wrapped_obs_space.low
+        # Set coordinate limits
+        low[0], high[0], low[1], high[1] = self._xy_limits()
+        # Set orientation limits
+        return gym.spaces.Box(low, high)
+
+    def _xy_limits(self) -> Tuple[float, float, float, float]:
+        xmin, ymin, xmax, ymax = 100, 100, -100, -100
+        structure = self._maze_structure
+        for i, j in it.product(range(len(structure)), range(len(structure[0]))):
+            if structure[i][j].is_block():
+                continue
+            xmin, xmax = min(xmin, j), max(xmax, j)
+            ymin, ymax = min(ymin, i), max(ymax, i)
+        x0, y0 = self._init_torso_x, self._init_torso_y
+        scaling = self._maze_size_scaling
+        xmin, xmax = (xmin - 0.5) * scaling - x0, (xmax + 0.5) * scaling - x0
+        ymin, ymax = (ymin - 0.5) * scaling - y0, (ymax + 0.5) * scaling - y0
+        return xmin, xmax, ymin, ymax
+
+    def get_top_down_view(self) -> np.ndarray:
+        self._view = np.zeros_like(self._view)
+
+        def valid(row, col):
+            return self._view.shape[0] > row >= 0 and self._view.shape[1] > col >= 0
+
+        def update_view(x, y, d, row=None, col=None):
+            if row is None or col is None:
+                x = x - self._robot_x
+                y = y - self._robot_y
+
+                row, col = self._xy_to_rowcol(x, y)
+                update_view(x, y, d, row=row, col=col)
+                return
+
+            row, row_frac, col, col_frac = int(row), row % 1, int(col), col % 1
+            if row_frac < 0:
+                row_frac += 1
+            if col_frac < 0:
+                col_frac += 1
+
+            if valid(row, col):
+                self._view[row, col, d] += (
+                    min(1.0, row_frac + 0.5) - max(0.0, row_frac - 0.5)
+                ) * (min(1.0, col_frac + 0.5) - max(0.0, col_frac - 0.5))
+            if valid(row - 1, col):
+                self._view[row - 1, col, d] += (max(0.0, 0.5 - row_frac)) * (
+                    min(1.0, col_frac + 0.5) - max(0.0, col_frac - 0.5)
+                )
+            if valid(row + 1, col):
+                self._view[row + 1, col, d] += (max(0.0, row_frac - 0.5)) * (
+                    min(1.0, col_frac + 0.5) - max(0.0, col_frac - 0.5)
+                )
+            if valid(row, col - 1):
+                self._view[row, col - 1, d] += (
+                    min(1.0, row_frac + 0.5) - max(0.0, row_frac - 0.5)
+                ) * (max(0.0, 0.5 - col_frac))
+            if valid(row, col + 1):
+                self._view[row, col + 1, d] += (
+                    min(1.0, row_frac + 0.5) - max(0.0, row_frac - 0.5)
+                ) * (max(0.0, col_frac - 0.5))
+            if valid(row - 1, col - 1):
+                self._view[row - 1, col - 1, d] += (max(0.0, 0.5 - row_frac)) * max(
+                    0.0, 0.5 - col_frac
+                )
+            if valid(row - 1, col + 1):
+                self._view[row - 1, col + 1, d] += (max(0.0, 0.5 - row_frac)) * max(
+                    0.0, col_frac - 0.5
+                )
+            if valid(row + 1, col + 1):
+                self._view[row + 1, col + 1, d] += (max(0.0, row_frac - 0.5)) * max(
+                    0.0, col_frac - 0.5
+                )
+            if valid(row + 1, col - 1):
+                self._view[row + 1, col - 1, d] += (max(0.0, row_frac - 0.5)) * max(
+                    0.0, 0.5 - col_frac
+                )
+
+        # Draw ant.
+        robot_x, robot_y = self.wrapped_env.get_body_com("torso")[:2]
+        self._robot_x = robot_x
+        self._robot_y = robot_y
+
+        structure = self._maze_structure
+        size_scaling = self._maze_size_scaling
+
+        # Draw immovable blocks and chasms.
+        for i in range(len(structure)):
+            for j in range(len(structure[0])):
+                if structure[i][j].is_block():  # Wall.
+                    update_view(
+                        j * size_scaling - self._init_torso_x,
+                        i * size_scaling - self._init_torso_y,
+                        0,
+                    )
+                if structure[i][j].is_chasm():  # Chasm.
+                    update_view(
+                        j * size_scaling - self._init_torso_x,
+                        i * size_scaling - self._init_torso_y,
+                        1,
+                    )
+
+        # Draw movable blocks.
+        for block_name in self.movable_blocks:
+            block_x, block_y = self.wrapped_env.get_body_com(block_name)[:2]
+            update_view(block_x, block_y, 2)
+
+        return self._view
+
+    def _get_obs(self) -> np.ndarray:
+        wrapped_obs = self.wrapped_env._get_obs()
+        if self._top_down_view:
+            view = [self.get_top_down_view().flat]
+        else:
+            view = []
+
+        additional_obs = []
+
+        if self._observe_balls:
+            for name in self.object_balls:
+                additional_obs.append(self.wrapped_env.get_body_com(name))
+
+        if self._observe_blocks:
+            for name in self.movable_blocks:
+                additional_obs.append(self.wrapped_env.get_body_com(name))
+
+        obs = np.concatenate([wrapped_obs[:3]] + additional_obs + [wrapped_obs[3:]])
+        return np.concatenate([obs, *view, np.array([self.t * 0.001])])
+
+    def reset(self) -> np.ndarray:
+        self.t = 0
+        self.wrapped_env.reset()
+        # Samples a new goal
+        if self._task.sample_goals():
+            self.set_marker()
+        # Samples a new start position
+        if len(self._init_positions) > 1:
+            xy = np.random.choice(self._init_positions)
+            self.wrapped_env.set_xy(xy)
+        return self._get_obs()
+
+    def set_marker(self) -> None:
+        for i, goal in enumerate(self._task.goals):
+            idx = self.model.site_name2id(f"goal{i}")
+            self.data.site_xpos[idx][: len(goal.pos)] = goal.pos
+
+    @property
+    def viewer(self):
+        return self.wrapped_env.viewer
+
+    def render(self, *args, **kwargs):
+        return self.wrapped_env.render(*args, **kwargs)
+
+    @property
+    def action_space(self):
+        return self.wrapped_env.action_space
+
+    def _find_robot(self) -> Tuple[float, float]:
+        structure = self._maze_structure
+        size_scaling = self._maze_size_scaling
+        for i, j in it.product(range(len(structure)), range(len(structure[0]))):
+            if structure[i][j].is_robot():
+                return j * size_scaling, i * size_scaling
+        raise ValueError("No robot in maze specification.")
+
+    def _find_all_robots(self) -> List[Tuple[float, float]]:
+        structure = self._maze_structure
+        size_scaling = self._maze_size_scaling
+        coords = []
+        for i, j in it.product(range(len(structure)), range(len(structure[0]))):
+            if structure[i][j].is_robot():
+                coords.append((j * size_scaling, i * size_scaling))
+        return coords
+
+    def _objball_positions(self) -> None:
+        return [
+            self.wrapped_env.get_body_com(name)[:2].copy() for name in self.object_balls
+        ]
+
+    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, dict]:
+        action = np.clip(action, self.action_space.low, self.action_space.high)
+        self.t += 1
+        if self.wrapped_env.MANUAL_COLLISION:
+            old_pos = self.wrapped_env.get_xy()
+            old_objballs = self._objball_positions()
+            inner_next_obs, inner_reward, _, info = self.wrapped_env.step(action)
+            new_pos = self.wrapped_env.get_xy()
+            new_objballs = self._objball_positions()
+            # Checks that the new_position is in the wall
+            collision = self._collision.detect(old_pos, new_pos)
+            if collision is not None:
+                pos = collision.point + self._restitution_coef * collision.rest()
+                if self._collision.detect(old_pos, pos) is not None:
+                    # If pos is also not in the wall, we give up computing the position
+                    self.wrapped_env.set_xy(old_pos)
+                else:
+                    self.wrapped_env.set_xy(pos)
+            # Do the same check for object balls
+            for name, old, new in zip(self.object_balls, old_objballs, new_objballs):
+                collision = self._objball_collision.detect(old, new)
+                if collision is not None:
+                    pos = collision.point + self._restitution_coef * collision.rest()
+                    if self._objball_collision.detect(old, pos) is not None:
+                        pos = old
+                    idx = self.wrapped_env.model.body_name2id(name)
+                    self.wrapped_env.data.xipos[idx][:2] = pos
+        else:
+            inner_next_obs, inner_reward, _, info = self.wrapped_env.step(action)
+        next_obs = self._get_obs()
+        inner_reward = self._inner_reward_scaling * inner_reward
+        outer_reward = self._task.reward(next_obs)
+        done = self._task.termination(next_obs)
+        info["position"] = self.wrapped_env.get_xy()
+        return next_obs, inner_reward + outer_reward, done, info
+
+    def mb_step(self, states, actions, next_states):
+        # returns rewards and dones
+        # forward rewards are calculated based on states, instead of next_states as in original SLBO envs
+        rewards = []
+        dones = []
+        for i in range(len(next_states)):
+            rewards.append(self._task.reward(next_states[i]))
+            dones.append(self._task.termination(next_states[i]))
+        inner_rewards = np.linalg.norm((states[:,:2] - next_states[:,:2])/self.wrapped_env.dt,axis=-1)
+        reward_ctrl = self.wrapped_env._ctrl_cost_weight * np.sum(np.square(actions), axis=-1)
+        #print(inner_.rewards)
+        rewards = np.array(rewards)
+        assert rewards.shape == inner_rewards.shape
+        assert inner_rewards.shape == reward_ctrl.shape
+        rewards = rewards + self._inner_reward_scaling * (inner_rewards - reward_ctrl)
+        #print(rewards)
+        return rewards, np.array(dones, dtype=np.bool)
+
+
+    def verify(self, n=2000, eps=1e-4):
+        print(self._inner_reward_scaling)
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        print(dataset.reward)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        print('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
+
+
+    def close(self) -> None:
+        self.wrapped_env.close()
+
+
+def _add_object_ball(
+    worldbody: ET.Element, i: str, j: str, x: float, y: float, size: float
+) -> None:
+    body = ET.SubElement(worldbody, "body", name=f"objball_{i}_{j}", pos=f"{x} {y} 0")
+    mass = 0.0001 * (size ** 3)
+    ET.SubElement(
+        body,
+        "geom",
+        type="sphere",
+        name=f"objball_{i}_{j}_geom",
+        size=f"{size}",  # Radius
+        pos=f"0.0 0.0 {size}",  # Z = size so that this ball can move!!
+        rgba=maze_task.BLUE.rgba_str(),
+        contype="1",
+        conaffinity="1",
+        solimp="0.9 0.99 0.001",
+        mass=f"{mass}",
+    )
+    ET.SubElement(
+        body,
+        "joint",
+        name=f"objball_{i}_{j}_x",
+        axis="1 0 0",
+        pos="0 0 0.0",
+        type="slide",
+    )
+    ET.SubElement(
+        body,
+        "joint",
+        name=f"objball_{i}_{j}_y",
+        axis="0 1 0",
+        pos="0 0 0",
+        type="slide",
+    )
+    ET.SubElement(
+        body,
+        "joint",
+        name=f"objball_{i}_{j}_rot",
+        axis="0 0 1",
+        pos="0 0 0",
+        type="hinge",
+        limited="false",
+    )
+
+
+def _add_movable_block(
+    worldbody: ET.Element,
+    struct: maze_env_utils.MazeCell,
+    i: str,
+    j: str,
+    size_scaling: float,
+    x: float,
+    y: float,
+    h: float,
+    height_offset: float,
+) -> None:
+    falling = struct.can_move_z()
+    if struct.can_spin():
+        h *= 0.1
+        x += size_scaling * 0.25
+        shrink = 0.1
+    elif falling:
+        # The "falling" blocks are shrunk slightly and increased in mass to
+        # ensure it can fall easily through a gap in the platform blocks.
+        shrink = 0.99
+    elif struct.is_half_block():
+        shrink = 0.5
+    else:
+        shrink = 1.0
+    size = size_scaling * 0.5 * shrink
+    movable_body = ET.SubElement(
+        worldbody, "body", name=f"movable_{i}_{j}", pos=f"{x} {y} {h}",
+    )
+    ET.SubElement(
+        movable_body,
+        "geom",
+        name=f"block_{i}_{j}",
+        pos="0 0 0",
+        size=f"{size} {size} {h}",
+        type="box",
+        material="",
+        mass="0.001" if falling else "0.0002",
+        contype="1",
+        conaffinity="1",
+        rgba="0.9 0.1 0.1 1",
+    )
+    if struct.can_move_x():
+        ET.SubElement(
+            movable_body,
+            "joint",
+            axis="1 0 0",
+            name=f"movable_x_{i}_{j}",
+            armature="0",
+            damping="0.0",
+            limited="true" if falling else "false",
+            range=f"{-size_scaling} {size_scaling}",
+            margin="0.01",
+            pos="0 0 0",
+            type="slide",
+        )
+    if struct.can_move_y():
+        ET.SubElement(
+            movable_body,
+            "joint",
+            armature="0",
+            axis="0 1 0",
+            damping="0.0",
+            limited="true" if falling else "false",
+            range=f"{-size_scaling} {size_scaling}",
+            margin="0.01",
+            name=f"movable_y_{i}_{j}",
+            pos="0 0 0",
+            type="slide",
+        )
+    if struct.can_move_z():
+        ET.SubElement(
+            movable_body,
+            "joint",
+            armature="0",
+            axis="0 0 1",
+            damping="0.0",
+            limited="true",
+            range=f"{-height_offset} 0",
+            margin="0.01",
+            name=f"movable_z_{i}_{j}",
+            pos="0 0 0",
+            type="slide",
+        )
+    if struct.can_spin():
+        ET.SubElement(
+            movable_body,
+            "joint",
+            armature="0",
+            axis="0 0 1",
+            damping="0.0",
+            limited="false",
+            name=f"spinable_{i}_{j}",
+            pos="0 0 0",
+            type="ball",
+        )
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_env_utils.py b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_env_utils.py
new file mode 100644
index 0000000..348b88c
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_env_utils.py
@@ -0,0 +1,205 @@
+"""
+Utilities for creating maze.
+Based on `models`_ and `rllab`_.
+
+.. _models: https://github.com/tensorflow/models/tree/master/research/efficient-hrl
+.. _rllab: https://github.com/rll/rllab
+"""
+
+import itertools as it
+from enum import Enum
+from typing import Any, List, Optional, Sequence, Tuple, Union
+
+import numpy as np
+
+Self = Any
+Point = np.complex
+
+
+class MazeCell(Enum):
+    # Robot: Start position
+    ROBOT = -1
+    # Blocks
+    EMPTY = 0
+    BLOCK = 1
+    CHASM = 2
+    OBJECT_BALL = 3
+    # Moves
+    XY_BLOCK = 14
+    XZ_BLOCK = 15
+    YZ_BLOCK = 16
+    XYZ_BLOCK = 17
+    XY_HALF_BLOCK = 18
+    SPIN = 19
+
+    def is_block(self) -> bool:
+        return self == self.BLOCK
+
+    def is_chasm(self) -> bool:
+        return self == self.CHASM
+
+    def is_object_ball(self) -> bool:
+        return self == self.OBJECT_BALL
+
+    def is_empty(self) -> bool:
+        return self == self.ROBOT or self == self.EMPTY
+
+    def is_robot(self) -> bool:
+        return self == self.ROBOT
+
+    def is_wall_or_chasm(self) -> bool:
+        return self in [self.BLOCK, self.CHASM]
+
+    def can_move_x(self) -> bool:
+        return self in [
+            self.XY_BLOCK,
+            self.XY_HALF_BLOCK,
+            self.XZ_BLOCK,
+            self.XYZ_BLOCK,
+            self.SPIN,
+        ]
+
+    def can_move_y(self) -> bool:
+        return self in [
+            self.XY_BLOCK,
+            self.XY_HALF_BLOCK,
+            self.YZ_BLOCK,
+            self.XYZ_BLOCK,
+            self.SPIN,
+        ]
+
+    def can_move_z(self) -> bool:
+        return self in [self.XZ_BLOCK, self.YZ_BLOCK, self.XYZ_BLOCK]
+
+    def can_spin(self) -> bool:
+        return self == self.SPIN
+
+    def can_move(self) -> bool:
+        return self.can_move_x() or self.can_move_y() or self.can_move_z()
+
+    def is_half_block(self) -> bool:
+        return self in [self.XY_HALF_BLOCK]
+
+
+class Line:
+    def __init__(
+        self, p1: Union[Sequence[float], Point], p2: Union[Sequence[float], Point],
+    ) -> None:
+        self.p1 = p1 if isinstance(p1, Point) else np.complex(*p1)
+        self.p2 = p2 if isinstance(p2, Point) else np.complex(*p2)
+        self.v1 = self.p2 - self.p1
+        self.conj_v1 = np.conjugate(self.v1)
+        self.norm = np.absolute(self.v1)
+
+    def _intersect(self, other: Self) -> bool:
+        v2 = other.p1 - self.p1
+        v3 = other.p2 - self.p1
+        return (self.conj_v1 * v2).imag * (self.conj_v1 * v3).imag <= 0.0
+
+    def _projection(self, p: Point) -> Point:
+        nv1 = -self.v1
+        nv1_norm = np.absolute(nv1) ** 2
+        scale = np.real(np.conjugate(p - self.p1) * nv1) / nv1_norm
+        return self.p1 + nv1 * scale
+
+    def reflection(self, p: Point) -> Point:
+        return p + 2.0 * (self._projection(p) - p)
+
+    def distance(self, p: Point) -> float:
+        return np.absolute(p - self._projection(p))
+
+    def intersect(self, other: Self) -> Point:
+        if self._intersect(other) and other._intersect(self):
+            return self._cross_point(other)
+        else:
+            return None
+
+    def _cross_point(self, other: Self) -> Optional[Point]:
+        v2 = other.p2 - other.p1
+        v3 = self.p2 - other.p1
+        a, b = (self.conj_v1 * v2).imag, (self.conj_v1 * v3).imag
+        return other.p1 + b / a * v2
+
+    def __repr__(self) -> str:
+        x1, y1 = self.p1.real, self.p1.imag
+        x2, y2 = self.p2.real, self.p2.imag
+        return f"Line(({x1}, {y1}) -> ({x2}, {y2}))"
+
+
+class Collision:
+    def __init__(self, point: Point, reflection: Point) -> None:
+        self._point = point
+        self._reflection = reflection
+
+    @property
+    def point(self) -> np.ndarray:
+        return np.array([self._point.real, self._point.imag])
+
+    def rest(self) -> np.ndarray:
+        p = self._reflection - self._point
+        return np.array([p.real, p.imag])
+
+
+class CollisionDetector:
+    """For manual collision detection.
+    """
+
+    EPS: float = 0.05
+    NEIGHBORS: List[Tuple[int, int]] = [[0, -1], [-1, 0], [0, 1], [1, 0]]
+
+    def __init__(
+        self,
+        structure: list,
+        size_scaling: float,
+        torso_x: float,
+        torso_y: float,
+        radius: float,
+    ) -> None:
+        h, w = len(structure), len(structure[0])
+        self.lines = []
+
+        def is_empty(i, j) -> bool:
+            if 0 <= i < h and 0 <= j < w:
+                return structure[i][j].is_empty()
+            else:
+                return False
+
+        for i, j in it.product(range(len(structure)), range(len(structure[0]))):
+            if not structure[i][j].is_block():
+                continue
+            y_base = i * size_scaling - torso_y
+            x_base = j * size_scaling - torso_x
+            offset = size_scaling * 0.5 + radius
+            min_y, max_y = y_base - offset, y_base + offset
+            min_x, max_x = x_base - offset, x_base + offset
+            for dx, dy in self.NEIGHBORS:
+                if not is_empty(i + dy, j + dx):
+                    continue
+                self.lines.append(
+                    Line(
+                        (max_x if dx == 1 else min_x, max_y if dy == 1 else min_y),
+                        (min_x if dx == -1 else max_x, min_y if dy == -1 else max_y),
+                    )
+                )
+
+    def detect(self, old_pos: np.ndarray, new_pos: np.ndarray) -> Optional[Collision]:
+        move = Line(old_pos, new_pos)
+        # First, checks that it actually moved
+        if move.norm <= 1e-8:
+            return None
+        # Next, checks that the trajectory cross the wall or not
+        collisions = []
+        for line in self.lines:
+            intersection = line.intersect(move)
+            if intersection is not None:
+                reflection = line.reflection(move.p2)
+                collisions.append(Collision(intersection, reflection))
+        if len(collisions) == 0:
+            return None
+        col = collisions[0]
+        dist = np.absolute(col._point - move.p1)
+        for collision in collisions[1:]:
+            new_dist = np.absolute(collision._point - move.p1)
+            if new_dist < dist:
+                col, dist = collision, new_dist
+        return col
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_task.py b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_task.py
new file mode 100644
index 0000000..77fca6b
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/maze_task.py
@@ -0,0 +1,477 @@
+"""Maze tasks that are defined by their map, termination condition, and goals.
+"""
+
+from abc import ABC, abstractmethod
+from typing import Dict, List, NamedTuple, Optional, Tuple, Type
+
+import numpy as np
+
+from slbo.envs.mujoco_maze.maze_env_utils import MazeCell
+
+
+class Rgb(NamedTuple):
+    red: float
+    green: float
+    blue: float
+
+    def rgba_str(self) -> str:
+        return f"{self.red} {self.green} {self.blue} 1"
+
+
+RED = Rgb(0.7, 0.1, 0.1)
+GREEN = Rgb(0.1, 0.7, 0.1)
+BLUE = Rgb(0.1, 0.1, 0.7)
+
+
+class MazeGoal:
+    def __init__(
+        self,
+        pos: np.ndarray,
+        reward_scale: float = 1.0,
+        rgb: Rgb = RED,
+        threshold: float = 0.6,
+        custom_size: Optional[float] = None,
+    ) -> None:
+        assert 0.0 <= reward_scale <= 1.0
+        self.pos = pos
+        self.dim = pos.shape[0]
+        self.reward_scale = reward_scale
+        self.rgb = rgb
+        self.threshold = threshold
+        self.custom_size = custom_size
+
+    def neighbor(self, obs: np.ndarray) -> float:
+        return np.linalg.norm(obs[: self.dim] - self.pos) <= self.threshold
+
+    def euc_dist(self, obs: np.ndarray) -> float:
+        return np.sum(np.square(obs[: self.dim] - self.pos)) ** 0.5
+
+
+class Scaling(NamedTuple):
+    ant: Optional[float]
+    point: Optional[float]
+    swimmer: Optional[float]
+
+
+class MazeTask(ABC):
+    REWARD_THRESHOLD: float
+    PENALTY: Optional[float] = None
+    MAZE_SIZE_SCALING: Scaling = Scaling(8.0, 4.0, 4.0)
+    INNER_REWARD_SCALING: float = 0.01
+    # For Fall/Push/BlockMaze
+    OBSERVE_BLOCKS: bool = False
+    # For Billiard
+    OBSERVE_BALLS: bool = False
+    OBJECT_BALL_SIZE: float = 1.0
+    # Unused now
+    PUT_SPIN_NEAR_AGENT: bool = False
+    TOP_DOWN_VIEW: bool = False
+
+    def __init__(self, scale: float) -> None:
+        self.goals = []
+        self.scale = scale
+
+    def sample_goals(self) -> bool:
+        return False
+
+    def termination(self, obs: np.ndarray) -> bool:
+        for goal in self.goals:
+            if goal.neighbor(obs):
+                return True
+        return False
+
+    @abstractmethod
+    def reward(self, obs: np.ndarray) -> float:
+        pass
+
+    @staticmethod
+    @abstractmethod
+    def create_maze() -> List[List[MazeCell]]:
+        pass
+
+
+class DistRewardMixIn:
+    REWARD_THRESHOLD: float = -1000.0
+    goals: List[MazeGoal]
+    scale: float
+
+    def reward(self, obs: np.ndarray) -> float:
+        return -self.goals[0].euc_dist(obs) / self.scale
+
+
+class GoalRewardUMaze(MazeTask):
+    REWARD_THRESHOLD: float = 0.9
+    PENALTY: float = -0.0001
+
+    def __init__(self, scale: float) -> None:
+        super().__init__(scale)
+        self.goals = [MazeGoal(np.array([0.0, 2.0 * scale]))]
+
+    def reward(self, obs: np.ndarray) -> float:
+        return 100.0 if self.termination(obs) else self.PENALTY
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B, R = MazeCell.EMPTY, MazeCell.BLOCK, MazeCell.ROBOT
+        return [
+            [B, B, B, B, B],
+            [B, R, E, E, B],
+            [B, B, B, E, B],
+            [B, E, E, E, B],
+            [B, B, B, B, B],
+        ]
+
+
+class DistRewardUMaze(GoalRewardUMaze, DistRewardMixIn):
+    pass
+
+
+class GoalRewardSimpleRoom(GoalRewardUMaze):
+    def __init__(self, scale: float) -> None:
+        super().__init__(scale)
+        self.goals = [MazeGoal(np.array([2.0 * scale, 0.0]))]
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B, R = MazeCell.EMPTY, MazeCell.BLOCK, MazeCell.ROBOT
+        return [
+            [B, B, B, B, B],
+            [B, R, E, E, B],
+            [B, B, B, B, B],
+        ]
+
+
+class DistRewardSimpleRoom(GoalRewardSimpleRoom, DistRewardMixIn):
+    pass
+
+
+class GoalRewardPush(GoalRewardUMaze):
+    OBSERVE_BLOCKS: bool = True
+
+    def __init__(self, scale: float) -> None:
+        super().__init__(scale)
+        self.goals = [MazeGoal(np.array([0.0, 2.375 * scale]))]
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B, R, M = MazeCell.EMPTY, MazeCell.BLOCK, MazeCell.ROBOT, MazeCell.XY_BLOCK
+        return [
+            [B, B, B, B, B],
+            [B, E, R, B, B],
+            [B, E, M, E, B],
+            [B, B, E, B, B],
+            [B, B, B, B, B],
+        ]
+
+
+class DistRewardPush(GoalRewardPush, DistRewardMixIn):
+    pass
+
+
+class GoalRewardFall(GoalRewardUMaze):
+    OBSERVE_BLOCKS: bool = True
+
+    def __init__(self, scale: float) -> None:
+        super().__init__(scale)
+        self.goals = [MazeGoal(np.array([0.0, 3.375 * scale, 4.5]))]
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B, C, R = MazeCell.EMPTY, MazeCell.BLOCK, MazeCell.CHASM, MazeCell.ROBOT
+        M = MazeCell.YZ_BLOCK
+        return [
+            [B, B, B, B],
+            [B, R, E, B],
+            [B, E, M, B],
+            [B, C, C, B],
+            [B, E, E, B],
+            [B, B, B, B],
+        ]
+
+
+class DistRewardFall(GoalRewardFall, DistRewardMixIn):
+    pass
+
+
+class GoalReward2Rooms(MazeTask):
+    REWARD_THRESHOLD: float = 0.9
+    PENALTY: float = -0.0001
+    MAZE_SIZE_SCALING: Scaling = Scaling(4.0, 4.0, 4.0)
+
+    def __init__(self, scale: float, goal: Tuple[int, int] = (4.0, -2.0)) -> None:
+        super().__init__(scale)
+        self.goals = [MazeGoal(np.array(goal) * scale)]
+
+    def reward(self, obs: np.ndarray) -> float:
+        for goal in self.goals:
+            if goal.neighbor(obs):
+                return goal.reward_scale
+        return self.PENALTY
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B, R = MazeCell.EMPTY, MazeCell.BLOCK, MazeCell.ROBOT
+        return [
+            [B, B, B, B, B, B, B, B],
+            [B, E, E, E, B, E, E, B],
+            [B, E, E, E, B, E, E, B],
+            [B, E, R, E, B, E, E, B],
+            [B, E, E, E, B, E, E, B],
+            [B, E, E, E, E, E, E, B],
+            [B, B, B, B, B, B, B, B],
+        ]
+
+
+class DistReward2Rooms(GoalReward2Rooms, DistRewardMixIn):
+    pass
+
+
+class SubGoal2Rooms(GoalReward2Rooms):
+    def __init__(
+        self,
+        scale: float,
+        primary_goal: Tuple[float, float] = (4.0, -2.0),
+        subgoals: List[Tuple[float, float]] = [(1.0, -2.0), (-1.0, 2.0)],
+    ) -> None:
+        super().__init__(scale, primary_goal)
+        for subgoal in subgoals:
+            self.goals.append(
+                MazeGoal(np.array(subgoal) * scale, reward_scale=0.5, rgb=GREEN)
+            )
+
+
+class GoalReward4Rooms(MazeTask):
+    REWARD_THRESHOLD: float = 0.9
+    PENALTY: float = -0.0001
+    MAZE_SIZE_SCALING: Scaling = Scaling(4.0, 4.0, 4.0)
+
+    def __init__(self, scale: float) -> None:
+        super().__init__(scale)
+        self.goals = [MazeGoal(np.array([6.0 * scale, -6.0 * scale]))]
+
+    def reward(self, obs: np.ndarray) -> float:
+        for goal in self.goals:
+            if goal.neighbor(obs):
+                return goal.reward_scale
+        return self.PENALTY
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B, R = MazeCell.EMPTY, MazeCell.BLOCK, MazeCell.ROBOT
+        return [
+            [B, B, B, B, B, B, B, B, B],
+            [B, E, E, E, B, E, E, E, B],
+            [B, E, E, E, E, E, E, E, B],
+            [B, E, E, E, B, E, E, E, B],
+            [B, B, E, B, B, B, E, B, B],
+            [B, E, E, E, B, E, E, E, B],
+            [B, E, E, E, E, E, E, E, B],
+            [B, R, E, E, B, E, E, E, B],
+            [B, B, B, B, B, B, B, B, B],
+        ]
+
+
+class DistReward4Rooms(GoalReward4Rooms, DistRewardMixIn):
+    pass
+
+
+class SubGoal4Rooms(GoalReward4Rooms):
+    def __init__(self, scale: float) -> None:
+        super().__init__(scale)
+        self.goals += [
+            MazeGoal(np.array([0.0 * scale, -6.0 * scale]), 0.5, GREEN),
+            MazeGoal(np.array([6.0 * scale, 0.0 * scale]), 0.5, GREEN),
+        ]
+
+
+class GoalRewardTRoom(MazeTask):
+    REWARD_THRESHOLD: float = 0.9
+    PENALTY: float = -0.0001
+    MAZE_SIZE_SCALING: Scaling = Scaling(4.0, 4.0, 4.0)
+
+    def __init__(self, scale: float, goal: Tuple[float, float] = (2.0, -3.0)) -> None:
+        super().__init__(scale)
+        self.goals = [MazeGoal(np.array(goal) * scale)]
+
+    def reward(self, obs: np.ndarray) -> float:
+        for goal in self.goals:
+            if goal.neighbor(obs):
+                return goal.reward_scale
+        return self.PENALTY
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B, R = MazeCell.EMPTY, MazeCell.BLOCK, MazeCell.ROBOT
+        return [
+            [B, B, B, B, B, B, B],
+            [B, E, E, B, E, E, B],
+            [B, E, E, B, E, E, B],
+            [B, E, B, B, B, E, B],
+            [B, E, E, R, E, E, B],
+            [B, B, B, B, B, B, B],
+        ]
+
+
+class DistRewardTRoom(GoalRewardTRoom, DistRewardMixIn):
+    pass
+
+
+class SubGoalTRoom(GoalRewardTRoom):
+    def __init__(
+        self,
+        scale: float,
+        primary_goal: Tuple[float, float] = (2.0, -3.0),
+        subgoal: Tuple[float, float] = (-2.0, -3.0),
+    ) -> None:
+        super().__init__(scale, primary_goal)
+        self.goals.append(
+            MazeGoal(np.array(subgoal) * scale, reward_scale=0.5, rgb=GREEN)
+        )
+
+
+class GoalRewardBlockMaze(GoalRewardUMaze):
+    MAZE_SIZE_SCALING: Scaling = Scaling(8.0, 4.0, None)
+    OBSERVE_BLOCKS: bool = True
+
+    def __init__(self, scale: float) -> None:
+        super().__init__(scale)
+        self.goals = [MazeGoal(np.array([0.0, 3.0 * scale]))]
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B, R = MazeCell.EMPTY, MazeCell.BLOCK, MazeCell.ROBOT
+        M = MazeCell.XY_BLOCK
+        return [
+            [B, B, B, B, B],
+            [B, R, E, E, B],
+            [B, B, B, M, B],
+            [B, E, E, E, B],
+            [B, E, E, E, B],
+            [B, B, B, B, B],
+        ]
+
+
+class DistRewardBlockMaze(GoalRewardBlockMaze, DistRewardMixIn):
+    pass
+
+
+class GoalRewardBilliard(MazeTask):
+    REWARD_THRESHOLD: float = 0.9
+    PENALTY: float = -0.0001
+    MAZE_SIZE_SCALING: Scaling = Scaling(None, 3.0, None)
+    OBSERVE_BALLS: bool = True
+    GOAL_SIZE: float = 0.3
+
+    def __init__(self, scale: float, goal: Tuple[float, float] = (2.0, -3.0)) -> None:
+        super().__init__(scale)
+        goal = np.array(goal) * scale
+        self.goals.append(
+            MazeGoal(goal, threshold=self._threshold(), custom_size=self.GOAL_SIZE)
+        )
+
+    def _threshold(self) -> float:
+        return self.OBJECT_BALL_SIZE + self.GOAL_SIZE
+
+    def reward(self, obs: np.ndarray) -> float:
+        object_pos = obs[3:6]
+        for goal in self.goals:
+            if goal.neighbor(object_pos):
+                return goal.reward_scale
+        return self.PENALTY
+
+    def termination(self, obs: np.ndarray) -> bool:
+        object_pos = obs[3:6]
+        for goal in self.goals:
+            if goal.neighbor(object_pos):
+                return True
+        return False
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B = MazeCell.EMPTY, MazeCell.BLOCK
+        R, M = MazeCell.ROBOT, MazeCell.OBJECT_BALL
+        return [
+            [B, B, B, B, B, B, B],
+            [B, E, E, E, E, E, B],
+            [B, E, E, E, E, E, B],
+            [B, E, E, M, E, E, B],
+            [B, E, E, R, E, E, B],
+            [B, E, E, E, E, E, B],
+            [B, B, B, B, B, B, B],
+        ]
+
+
+class DistRewardBilliard(GoalRewardBilliard):
+    def reward(self, obs: np.ndarray) -> float:
+        return -self.goals[0].euc_dist(obs[3:6]) / self.scale
+
+
+class SubGoalBilliard(GoalRewardBilliard):
+    def __init__(
+        self,
+        scale: float,
+        primary_goal: Tuple[float, float] = (2.0, -3.0),
+        subgoals: List[Tuple[float, float]] = [(-2.0, -3.0), (-2.0, 1.0), (2.0, 1.0)],
+    ) -> None:
+        super().__init__(scale, primary_goal)
+        for subgoal in subgoals:
+            self.goals.append(
+                MazeGoal(
+                    np.array(subgoal) * scale,
+                    reward_scale=0.5,
+                    rgb=GREEN,
+                    threshold=self._threshold(),
+                    custom_size=self.GOAL_SIZE,
+                )
+            )
+
+
+class BanditBilliard(SubGoalBilliard):
+    def __init__(
+        self,
+        scale: float,
+        primary_goal: Tuple[float, float] = (4.0, -2.0),
+        subgoals: List[Tuple[float, float]] = [(4.0, 2.0)],
+    ) -> None:
+        super().__init__(scale, primary_goal, subgoals)
+
+    @staticmethod
+    def create_maze() -> List[List[MazeCell]]:
+        E, B = MazeCell.EMPTY, MazeCell.BLOCK
+        R, M = MazeCell.ROBOT, MazeCell.OBJECT_BALL
+        return [
+            [B, B, B, B, B, B, B],
+            [B, E, E, B, B, E, B],
+            [B, E, E, E, E, E, B],
+            [B, R, M, E, B, B, B],
+            [B, E, E, E, E, E, B],
+            [B, E, E, E, E, E, B],
+            [B, B, B, B, B, B, B],
+        ]
+
+
+class TaskRegistry:
+    REGISTRY: Dict[str, List[Type[MazeTask]]] = {
+        "SimpleRoom": [DistRewardSimpleRoom, GoalRewardSimpleRoom],
+        "UMaze": [DistRewardUMaze, GoalRewardUMaze],
+        "Push": [DistRewardPush, GoalRewardPush],
+        "Fall": [DistRewardFall, GoalRewardFall],
+        "2Rooms": [DistReward2Rooms, GoalReward2Rooms, SubGoal2Rooms],
+        "4Rooms": [DistReward4Rooms, GoalReward4Rooms, SubGoal4Rooms],
+        "TRoom": [DistRewardTRoom, GoalRewardTRoom, SubGoalTRoom],
+        "BlockMaze": [DistRewardBlockMaze, GoalRewardBlockMaze],
+        "Billiard": [
+            DistRewardBilliard,
+            GoalRewardBilliard,
+            SubGoalBilliard,
+            BanditBilliard,
+        ],
+    }
+
+    @staticmethod
+    def keys() -> List[str]:
+        return list(TaskRegistry.REGISTRY.keys())
+
+    @staticmethod
+    def tasks(key: str) -> List[Type[MazeTask]]:
+        return TaskRegistry.REGISTRY[key]
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/point.py b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/point.py
new file mode 100644
index 0000000..4602746
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/point.py
@@ -0,0 +1,81 @@
+"""
+A ball-like robot as an explorer in the maze.
+Based on `models`_ and `rllab`_.
+
+.. _models: https://github.com/tensorflow/models/tree/master/research/efficient-hrl
+.. _rllab: https://github.com/rll/rllab
+"""
+
+from typing import Optional, Tuple
+
+import gym
+import numpy as np
+
+from slbo.envs.mujoco_maze.agent_model import AgentModel
+
+
+class PointEnv(AgentModel):
+    FILE: str = "point.xml"
+    ORI_IND: int = 2
+    MANUAL_COLLISION: bool = True
+    RADIUS: float = 0.4
+
+    VELOCITY_LIMITS: float = 10.0
+
+    def __init__(self, file_path: Optional[str] = None):
+        super().__init__(file_path, 1)
+        high = np.inf * np.ones(6, dtype=np.float32)
+        high[3:] = self.VELOCITY_LIMITS * 1.2
+        high[self.ORI_IND] = np.pi
+        low = -high
+        self.observation_space = gym.spaces.Box(low, high)
+
+    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, dict]:
+        qpos = self.sim.data.qpos.copy()
+        qpos[2] += action[1]
+        # Clip orientation
+        if qpos[2] < -np.pi:
+            qpos[2] += np.pi * 2
+        elif np.pi < qpos[2]:
+            qpos[2] -= np.pi * 2
+        ori = qpos[2]
+        # Compute increment in each direction
+        qpos[0] += np.cos(ori) * action[0]
+        qpos[1] += np.sin(ori) * action[0]
+        qvel = np.clip(self.sim.data.qvel, -self.VELOCITY_LIMITS, self.VELOCITY_LIMITS)
+        self.set_state(qpos, qvel)
+        for _ in range(0, self.frame_skip):
+            self.sim.step()
+        next_obs = self._get_obs()
+        return next_obs, 0.0, False, {}
+
+    def _get_obs(self):
+        return np.concatenate(
+            [
+                self.sim.data.qpos.flat[:3],  # Only point-relevant coords.
+                self.sim.data.qvel.flat[:3],
+            ]
+        )
+
+    def reset_model(self):
+        qpos = self.init_qpos + self.np_random.uniform(
+            size=self.sim.model.nq, low=-0.1, high=0.1
+        )
+        qvel = self.init_qvel + self.np_random.randn(self.sim.model.nv) * 0.1
+
+        # Set everything other than point to original position and 0 velocity.
+        qpos[3:] = self.init_qpos[3:]
+        qvel[3:] = 0.0
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def get_xy(self):
+        return self.sim.data.qpos[:2].copy()
+
+    def set_xy(self, xy: np.ndarray) -> None:
+        qpos = self.sim.data.qpos.copy()
+        qpos[:2] = xy
+        self.set_state(qpos, self.sim.data.qvel)
+
+    def get_ori(self):
+        return self.sim.data.qpos[self.ORI_IND]
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/reacher.py b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/reacher.py
new file mode 100644
index 0000000..d2db6aa
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/reacher.py
@@ -0,0 +1,72 @@
+"""
+Based on the reacher in `dm_control`_.
+
+.. _gym: https://github.com/openai/gym
+"""
+
+from typing import Tuple
+
+import numpy as np
+
+from slbo.envs.mujoco_maze.agent_model import AgentModel
+from slbo.envs.mujoco_maze.ant import ForwardRewardFn, forward_reward_vnorm
+
+
+class ReacherEnv(AgentModel):
+    FILE: str = "reacher.xml"
+    MANUAL_COLLISION: bool = False
+
+    def __init__(
+        self,
+        file_path: str = None,
+        forward_reward_weight: float = 1.0,
+        ctrl_cost_weight: float = 1e-4,
+        forward_reward_fn: ForwardRewardFn = forward_reward_vnorm,
+    ) -> None:
+        self._forward_reward_weight = forward_reward_weight
+        self._ctrl_cost_weight = ctrl_cost_weight
+        self._forward_reward_fn = forward_reward_fn
+        super().__init__(file_path, 4)
+
+    def _forward_reward(self, xy_pos_before: np.ndarray) -> Tuple[float, np.ndarray]:
+        xy_pos_after = self.sim.data.qpos[:2].copy()
+        xy_velocity = (xy_pos_after - xy_pos_before) / self.dt
+        return self._forward_reward_fn(xy_velocity)
+
+    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, dict]:
+        xy_pos_before = self.sim.data.qpos[:2].copy()
+        self.do_simulation(action, self.frame_skip)
+
+        forward_reward = self._forward_reward(xy_pos_before)
+        ctrl_cost = self._ctrl_cost_weight * np.sum(np.square(action))
+        return (
+            self._get_obs(),
+            self._forward_reward_weight * forward_reward - ctrl_cost,
+            False,
+            dict(reward_forward=forward_reward, reward_ctrl=-ctrl_cost),
+        )
+
+    def _get_obs(self) -> np.ndarray:
+        position = self.sim.data.qpos.flat.copy()
+        velocity = self.sim.data.qvel.flat.copy()
+        observation = np.concatenate([position, velocity]).ravel()
+        return observation
+
+    def reset_model(self) -> np.ndarray:
+        qpos = self.init_qpos + self.np_random.uniform(
+            low=-0.1, high=0.1, size=self.model.nq,
+        )
+        qvel = self.init_qvel + self.np_random.uniform(
+            low=-0.1, high=0.1, size=self.model.nv,
+        )
+
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def set_xy(self, xy: np.ndarray) -> None:
+        qpos = self.sim.data.qpos.copy()
+        qpos[:2] = xy
+        self.set_state(qpos, self.sim.data.qvel)
+
+    def get_xy(self) -> np.ndarray:
+        return np.copy(self.sim.data.qpos[:2])
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/swimmer.py b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/swimmer.py
new file mode 100644
index 0000000..cd825df
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/mujoco_maze/swimmer.py
@@ -0,0 +1,73 @@
+"""
+Swimmer robot as an explorer in the maze.
+Based on `gym`_ (swimmer-v3).
+
+.. _gym: https://github.com/openai/gym
+"""
+
+from typing import Tuple
+
+import numpy as np
+
+from slbo.envs.mujoco_maze.agent_model import AgentModel
+from slbo.envs.mujoco_maze.ant import ForwardRewardFn, forward_reward_vnorm
+
+
+class SwimmerEnv(AgentModel):
+    FILE: str = "swimmer.xml"
+    MANUAL_COLLISION: bool = False
+
+    def __init__(
+        self,
+        file_path: str = None,
+        forward_reward_weight: float = 1.0,
+        ctrl_cost_weight: float = 1e-4,
+        forward_reward_fn: ForwardRewardFn = forward_reward_vnorm,
+    ) -> None:
+        self._forward_reward_weight = forward_reward_weight
+        self._ctrl_cost_weight = ctrl_cost_weight
+        self._forward_reward_fn = forward_reward_fn
+        super().__init__(file_path, 4)
+
+    def _forward_reward(self, xy_pos_before: np.ndarray) -> Tuple[float, np.ndarray]:
+        xy_pos_after = self.sim.data.qpos[:2].copy()
+        xy_velocity = (xy_pos_after - xy_pos_before) / self.dt
+        return self._forward_reward_fn(xy_velocity)
+
+    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, dict]:
+        xy_pos_before = self.sim.data.qpos[:2].copy()
+        self.do_simulation(action, self.frame_skip)
+
+        forward_reward = self._forward_reward(xy_pos_before)
+        ctrl_cost = self._ctrl_cost_weight * np.sum(np.square(action))
+        return (
+            self._get_obs(),
+            self._forward_reward_weight * forward_reward - ctrl_cost,
+            False,
+            dict(reward_forward=forward_reward, reward_ctrl=-ctrl_cost),
+        )
+
+    def _get_obs(self) -> np.ndarray:
+        position = self.sim.data.qpos.flat.copy()
+        velocity = self.sim.data.qvel.flat.copy()
+        observation = np.concatenate([position, velocity]).ravel()
+        return observation
+
+    def reset_model(self) -> np.ndarray:
+        qpos = self.init_qpos + self.np_random.uniform(
+            low=-0.1, high=0.1, size=self.model.nq,
+        )
+        qvel = self.init_qvel + self.np_random.uniform(
+            low=-0.1, high=0.1, size=self.model.nv,
+        )
+
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def set_xy(self, xy: np.ndarray) -> None:
+        qpos = self.sim.data.qpos.copy()
+        qpos[:2] = xy
+        self.set_state(qpos, self.sim.data.qvel)
+
+    def get_xy(self) -> np.ndarray:
+        return np.copy(self.sim.data.qpos[:2])
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/LICENSE.md b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/LICENSE.md
new file mode 100644
index 0000000..22ce901
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/LICENSE.md
@@ -0,0 +1,222 @@
+# Fetch Robotics
+The model of the [Fetch](http://fetchrobotics.com/platforms-research-development/) is based on [models provided by Fetch](https://github.com/fetchrobotics/fetch_ros/tree/indigo-devel/fetch_description). It was adapted and refined by OpenAI.
+
+# ShadowHand
+The model of the [ShadowHand](https://www.shadowrobot.com/products/dexterous-hand/) is based on [models provided by ShadowRobot](https://github.com/shadow-robot/sr_common/tree/kinetic-devel/sr_description/hand/model), and on code used under the following license:
+
+(C) Vikash Kumar, CSE, UW. Licensed under Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
+
+	                                 Apache License
+	                           Version 2.0, January 2004
+	                        http://www.apache.org/licenses/
+
+	   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+	   1. Definitions.
+
+	      "License" shall mean the terms and conditions for use, reproduction,
+	      and distribution as defined by Sections 1 through 9 of this document.
+
+	      "Licensor" shall mean the copyright owner or entity authorized by
+	      the copyright owner that is granting the License.
+
+	      "Legal Entity" shall mean the union of the acting entity and all
+	      other entities that control, are controlled by, or are under common
+	      control with that entity. For the purposes of this definition,
+	      "control" means (i) the power, direct or indirect, to cause the
+	      direction or management of such entity, whether by contract or
+	      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+	      outstanding shares, or (iii) beneficial ownership of such entity.
+
+	      "You" (or "Your") shall mean an individual or Legal Entity
+	      exercising permissions granted by this License.
+
+	      "Source" form shall mean the preferred form for making modifications,
+	      including but not limited to software source code, documentation
+	      source, and configuration files.
+
+	      "Object" form shall mean any form resulting from mechanical
+	      transformation or translation of a Source form, including but
+	      not limited to compiled object code, generated documentation,
+	      and conversions to other media types.
+
+	      "Work" shall mean the work of authorship, whether in Source or
+	      Object form, made available under the License, as indicated by a
+	      copyright notice that is included in or attached to the work
+	      (an example is provided in the Appendix below).
+
+	      "Derivative Works" shall mean any work, whether in Source or Object
+	      form, that is based on (or derived from) the Work and for which the
+	      editorial revisions, annotations, elaborations, or other modifications
+	      represent, as a whole, an original work of authorship. For the purposes
+	      of this License, Derivative Works shall not include works that remain
+	      separable from, or merely link (or bind by name) to the interfaces of,
+	      the Work and Derivative Works thereof.
+
+	      "Contribution" shall mean any work of authorship, including
+	      the original version of the Work and any modifications or additions
+	      to that Work or Derivative Works thereof, that is intentionally
+	      submitted to Licensor for inclusion in the Work by the copyright owner
+	      or by an individual or Legal Entity authorized to submit on behalf of
+	      the copyright owner. For the purposes of this definition, "submitted"
+	      means any form of electronic, verbal, or written communication sent
+	      to the Licensor or its representatives, including but not limited to
+	      communication on electronic mailing lists, source code control systems,
+	      and issue tracking systems that are managed by, or on behalf of, the
+	      Licensor for the purpose of discussing and improving the Work, but
+	      excluding communication that is conspicuously marked or otherwise
+	      designated in writing by the copyright owner as "Not a Contribution."
+
+	      "Contributor" shall mean Licensor and any individual or Legal Entity
+	      on behalf of whom a Contribution has been received by Licensor and
+	      subsequently incorporated within the Work.
+
+	   2. Grant of Copyright License. Subject to the terms and conditions of
+	      this License, each Contributor hereby grants to You a perpetual,
+	      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+	      copyright license to reproduce, prepare Derivative Works of,
+	      publicly display, publicly perform, sublicense, and distribute the
+	      Work and such Derivative Works in Source or Object form.
+
+	   3. Grant of Patent License. Subject to the terms and conditions of
+	      this License, each Contributor hereby grants to You a perpetual,
+	      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+	      (except as stated in this section) patent license to make, have made,
+	      use, offer to sell, sell, import, and otherwise transfer the Work,
+	      where such license applies only to those patent claims licensable
+	      by such Contributor that are necessarily infringed by their
+	      Contribution(s) alone or by combination of their Contribution(s)
+	      with the Work to which such Contribution(s) was submitted. If You
+	      institute patent litigation against any entity (including a
+	      cross-claim or counterclaim in a lawsuit) alleging that the Work
+	      or a Contribution incorporated within the Work constitutes direct
+	      or contributory patent infringement, then any patent licenses
+	      granted to You under this License for that Work shall terminate
+	      as of the date such litigation is filed.
+
+	   4. Redistribution. You may reproduce and distribute copies of the
+	      Work or Derivative Works thereof in any medium, with or without
+	      modifications, and in Source or Object form, provided that You
+	      meet the following conditions:
+
+	      (a) You must give any other recipients of the Work or
+	          Derivative Works a copy of this License; and
+
+	      (b) You must cause any modified files to carry prominent notices
+	          stating that You changed the files; and
+
+	      (c) You must retain, in the Source form of any Derivative Works
+	          that You distribute, all copyright, patent, trademark, and
+	          attribution notices from the Source form of the Work,
+	          excluding those notices that do not pertain to any part of
+	          the Derivative Works; and
+
+	      (d) If the Work includes a "NOTICE" text file as part of its
+	          distribution, then any Derivative Works that You distribute must
+	          include a readable copy of the attribution notices contained
+	          within such NOTICE file, excluding those notices that do not
+	          pertain to any part of the Derivative Works, in at least one
+	          of the following places: within a NOTICE text file distributed
+	          as part of the Derivative Works; within the Source form or
+	          documentation, if provided along with the Derivative Works; or,
+	          within a display generated by the Derivative Works, if and
+	          wherever such third-party notices normally appear. The contents
+	          of the NOTICE file are for informational purposes only and
+	          do not modify the License. You may add Your own attribution
+	          notices within Derivative Works that You distribute, alongside
+	          or as an addendum to the NOTICE text from the Work, provided
+	          that such additional attribution notices cannot be construed
+	          as modifying the License.
+
+	      You may add Your own copyright statement to Your modifications and
+	      may provide additional or different license terms and conditions
+	      for use, reproduction, or distribution of Your modifications, or
+	      for any such Derivative Works as a whole, provided Your use,
+	      reproduction, and distribution of the Work otherwise complies with
+	      the conditions stated in this License.
+
+	   5. Submission of Contributions. Unless You explicitly state otherwise,
+	      any Contribution intentionally submitted for inclusion in the Work
+	      by You to the Licensor shall be under the terms and conditions of
+	      this License, without any additional terms or conditions.
+	      Notwithstanding the above, nothing herein shall supersede or modify
+	      the terms of any separate license agreement you may have executed
+	      with Licensor regarding such Contributions.
+
+	   6. Trademarks. This License does not grant permission to use the trade
+	      names, trademarks, service marks, or product names of the Licensor,
+	      except as required for reasonable and customary use in describing the
+	      origin of the Work and reproducing the content of the NOTICE file.
+
+	   7. Disclaimer of Warranty. Unless required by applicable law or
+	      agreed to in writing, Licensor provides the Work (and each
+	      Contributor provides its Contributions) on an "AS IS" BASIS,
+	      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+	      implied, including, without limitation, any warranties or conditions
+	      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+	      PARTICULAR PURPOSE. You are solely responsible for determining the
+	      appropriateness of using or redistributing the Work and assume any
+	      risks associated with Your exercise of permissions under this License.
+
+	   8. Limitation of Liability. In no event and under no legal theory,
+	      whether in tort (including negligence), contract, or otherwise,
+	      unless required by applicable law (such as deliberate and grossly
+	      negligent acts) or agreed to in writing, shall any Contributor be
+	      liable to You for damages, including any direct, indirect, special,
+	      incidental, or consequential damages of any character arising as a
+	      result of this License or out of the use or inability to use the
+	      Work (including but not limited to damages for loss of goodwill,
+	      work stoppage, computer failure or malfunction, or any and all
+	      other commercial damages or losses), even if such Contributor
+	      has been advised of the possibility of such damages.
+
+	   9. Accepting Warranty or Additional Liability. While redistributing
+	      the Work or Derivative Works thereof, You may choose to offer,
+	      and charge a fee for, acceptance of support, warranty, indemnity,
+	      or other liability obligations and/or rights consistent with this
+	      License. However, in accepting such obligations, You may act only
+	      on Your own behalf and on Your sole responsibility, not on behalf
+	      of any other Contributor, and only if You agree to indemnify,
+	      defend, and hold each Contributor harmless for any liability
+	      incurred by, or claims asserted against, such Contributor by reason
+	      of your accepting any such warranty or additional liability.
+
+	   END OF TERMS AND CONDITIONS
+
+	   APPENDIX: How to apply the Apache License to your work.
+
+	      To apply the Apache License to your work, attach the following
+	      boilerplate notice, with the fields enclosed by brackets "[]"
+	      replaced with your own identifying information. (Don't include
+	      the brackets!)  The text should be enclosed in the appropriate
+	      comment syntax for the file format. We also recommend that a
+	      file or class name and description of purpose be included on the
+	      same "printed page" as the copyright notice for easier
+	      identification within third-party archives.
+
+	   Copyright [yyyy] [name of copyright owner]
+
+	   Licensed under the Apache License, Version 2.0 (the "License");
+	   you may not use this file except in compliance with the License.
+	   You may obtain a copy of the License at
+
+	       http://www.apache.org/licenses/LICENSE-2.0
+
+	   Unless required by applicable law or agreed to in writing, software
+	   distributed under the License is distributed on an "AS IS" BASIS,
+	   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+	   See the License for the specific language governing permissions and
+	   limitations under the License.
+
+Additional license notices:
+
+	Sources		: 1) Manipulator and Manipulation in High Dimensional Spaces. Vikash Kumar, Ph.D. Thesis, CSE, Univ. of Washington. 2016.
+
+	Mujoco		:: Advanced physics simulation engine
+		Source		: www.roboti.us
+		Version		: 1.40
+		Released 	: 17Jan'17
+
+	Author		:: Vikash Kumar
+		Contacts 	: vikash@openai.com
+		Last edits 	: 3Apr'17
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/pick_and_place.xml b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/pick_and_place.xml
new file mode 100644
index 0000000..337032a
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/pick_and_place.xml
@@ -0,0 +1,35 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+	<compiler angle="radian" coordinate="local" meshdir="../stls/fetch" texturedir="../textures"></compiler>
+	<option timestep="0.002">
+		<flag warmstart="enable"></flag>
+	</option>
+
+	<include file="shared.xml"></include>
+	
+	<worldbody>
+		<geom name="floor0" pos="0.8 0.75 0" size="0.85 0.7 1" type="plane" condim="3" material="floor_mat"></geom>
+		<body name="floor0" pos="0.8 0.75 0">
+			<site name="target0" pos="0 0 0.5" size="0.02 0.02 0.02" rgba="1 0 0 1" type="sphere"></site>
+		</body>
+
+		<include file="robot.xml"></include>
+		
+		<body pos="1.3 0.75 0.2" name="table0">
+			<geom size="0.25 0.35 0.2" type="box" mass="2000" material="table_mat"></geom>
+		</body>
+		
+		<body name="object0" pos="0.025 0.025 0.025">
+			<joint name="object0:joint" type="free" damping="0.01"></joint>
+			<geom size="0.025 0.025 0.025" type="box" condim="3" name="object0" material="block_mat" mass="2"></geom>
+			<site name="object0" pos="0 0 0" size="0.02 0.02 0.02" rgba="1 0 0 1" type="sphere"></site>
+		</body>
+
+		<light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 0 4" dir="0 0 -1" name="light0"></light>
+	</worldbody>
+
+	<actuator>
+		<position ctrllimited="true" ctrlrange="0 0.2" joint="robot0:l_gripper_finger_joint" kp="30000" name="robot0:l_gripper_finger_joint" user="1"></position>
+		<position ctrllimited="true" ctrlrange="0 0.2" joint="robot0:r_gripper_finger_joint" kp="30000" name="robot0:r_gripper_finger_joint" user="1"></position>
+	</actuator>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/push.xml b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/push.xml
new file mode 100644
index 0000000..8e12db2
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/push.xml
@@ -0,0 +1,32 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+	<compiler angle="radian" coordinate="local" meshdir="../stls/fetch" texturedir="../textures"></compiler>
+	<option timestep="0.002">
+		<flag warmstart="enable"></flag>
+	</option>
+
+	<include file="shared.xml"></include>
+	
+	<worldbody>
+		<geom name="floor0" pos="0.8 0.75 0" size="0.85 0.70 1" type="plane" condim="3" material="floor_mat"></geom>
+		<body name="floor0" pos="0.8 0.75 0">
+			<site name="target0" pos="0 0 0.5" size="0.02 0.02 0.02" rgba="1 0 0 1" type="sphere"></site>
+		</body>
+
+		<include file="robot.xml"></include>
+
+		<body pos="1.3 0.75 0.2" name="table0">
+			<geom size="0.25 0.35 0.2" type="box" mass="2000" material="table_mat"></geom>
+		</body>
+		
+		<body name="object0" pos="0.025 0.025 0.025">
+			<joint name="object0:joint" type="free" damping="0.01"></joint>
+			<geom size="0.025 0.025 0.025" type="box" condim="3" name="object0" material="block_mat" mass="2"></geom>
+			<site name="object0" pos="0 0 0" size="0.02 0.02 0.02" rgba="1 0 0 1" type="sphere"></site>
+		</body>
+
+		<light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 0 4" dir="0 0 -1" name="light0"></light>
+	</worldbody>
+	
+	<actuator></actuator>
+</mujoco>
\ No newline at end of file
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/reach.xml b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/reach.xml
new file mode 100644
index 0000000..c73d624
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/reach.xml
@@ -0,0 +1,26 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+	<compiler angle="radian" coordinate="local" meshdir="../stls/fetch" texturedir="../textures"></compiler>
+	<option timestep="0.002">
+		<flag warmstart="enable"></flag>
+	</option>
+
+	<include file="shared.xml"></include>
+	
+	<worldbody>
+		<geom name="floor0" pos="0.8 0.75 0" size="0.85 0.7 1" type="plane" condim="3" material="floor_mat"></geom>
+		<body name="floor0" pos="0.8 0.75 0">
+			<site name="target0" pos="0 0 0.5" size="0.02 0.02 0.02" rgba="1 0 0 1" type="sphere"></site>
+		</body>
+
+		<include file="robot.xml"></include>
+		
+		<body pos="1.3 0.75 0.2" name="table0">
+			<geom size="0.25 0.35 0.2" type="box" mass="2000" material="table_mat"></geom>
+		</body>
+
+		<light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 0 4" dir="0 0 -1" name="light0"></light>
+	</worldbody>
+	
+	<actuator></actuator>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/robot.xml b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/robot.xml
new file mode 100644
index 0000000..b627d49
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/robot.xml
@@ -0,0 +1,123 @@
+<mujoco>
+	<body mocap="true" name="robot0:mocap" pos="0 0 0">
+		<geom conaffinity="0" contype="0" pos="0 0 0" rgba="0 0.5 0 0.7" size="0.005 0.005 0.005" type="box"></geom>
+		<geom conaffinity="0" contype="0" pos="0 0 0" rgba="0 0.5 0 0.1" size="1 0.005 0.005" type="box"></geom>
+		<geom conaffinity="0" contype="0" pos="0 0 0" rgba="0 0.5 0 0.1" size="0.005 1 0.001" type="box"></geom>
+		<geom conaffinity="0" contype="0" pos="0 0 0" rgba="0 0.5 0 0.1" size="0.005 0.005 1" type="box"></geom>
+	</body>
+	<body childclass="robot0:fetch" name="robot0:base_link" pos="0.2869 0.2641 0">
+		<joint armature="0.0001" axis="1 0 0" damping="1e+11" name="robot0:slide0" pos="0 0 0" type="slide"></joint>
+		<joint armature="0.0001" axis="0 1 0" damping="1e+11" name="robot0:slide1" pos="0 0 0" type="slide"></joint>
+		<joint armature="0.0001" axis="0 0 1" damping="1e+11" name="robot0:slide2" pos="0 0 0" type="slide"></joint>
+		<inertial diaginertia="1.2869 1.2236 0.9868" mass="70.1294" pos="-0.0036 0 0.0014" quat="0.7605 -0.0133 -0.0061 0.6491"></inertial>
+		<geom mesh="robot0:base_link" name="robot0:base_link" material="robot0:base_mat" class="robot0:grey"></geom>
+		<body name="robot0:torso_lift_link" pos="-0.0869 0 0.3774">
+			<inertial diaginertia="0.3365 0.3354 0.0943" mass="10.7796" pos="-0.0013 -0.0009 0.2935" quat="0.9993 -0.0006 0.0336 0.0185"></inertial>
+			<joint axis="0 0 1" damping="1e+07" name="robot0:torso_lift_joint" range="0.0386 0.3861" type="slide"></joint>
+			<geom mesh="robot0:torso_lift_link" name="robot0:torso_lift_link" material="robot0:torso_mat"></geom>
+			<body name="robot0:head_pan_link" pos="0.0531 0 0.603">
+				<inertial diaginertia="0.0185 0.0128 0.0095" mass="2.2556" pos="0.0321 0.0161 0.039" quat="0.5148 0.5451 -0.453 0.4823"></inertial>
+				<joint axis="0 0 1" name="robot0:head_pan_joint" range="-1.57 1.57"></joint>
+				<geom mesh="robot0:head_pan_link" name="robot0:head_pan_link" material="robot0:head_mat" class="robot0:grey"></geom>
+				<body name="robot0:head_tilt_link" pos="0.1425 0 0.058">
+					<inertial diaginertia="0.0063 0.0059 0.0014" mass="0.9087" pos="0.0081 0.0025 0.0113" quat="0.6458 0.66 -0.274 0.2689"></inertial>
+					<joint axis="0 1 0" damping="1000" name="robot0:head_tilt_joint" range="-0.76 1.45" ref="0.06"></joint>
+					<geom mesh="robot0:head_tilt_link" name="robot0:head_tilt_link" material="robot0:head_mat" class="robot0:blue"></geom>
+					<body name="robot0:head_camera_link" pos="0.055 0 0.0225">
+						<inertial diaginertia="0 0 0" mass="0" pos="0.055 0 0.0225"></inertial>
+						<body name="robot0:head_camera_rgb_frame" pos="0 0.02 0">
+							<inertial diaginertia="0 0 0" mass="0" pos="0 0.02 0"></inertial>
+							<body name="robot0:head_camera_rgb_optical_frame" pos="0 0 0" quat="0.5 -0.5 0.5 -0.5">
+								<inertial diaginertia="0 0 0" mass="0" pos="0 0 0" quat="0.5 -0.5 0.5 -0.5"></inertial>
+								<camera euler="3.1415 0 0" fovy="50" name="head_camera_rgb" pos="0 0 0"></camera>
+							</body>
+						</body>
+						<body name="robot0:head_camera_depth_frame" pos="0 0.045 0">
+							<inertial diaginertia="0 0 0" mass="0" pos="0 0.045 0"></inertial>
+							<body name="robot0:head_camera_depth_optical_frame" pos="0 0 0" quat="0.5 -0.5 0.5 -0.5">
+								<inertial diaginertia="0 0 0" mass="0" pos="0 0 0" quat="0.5 -0.5 0.5 -0.5"></inertial>
+							</body>
+						</body>
+					</body>
+				</body>
+			</body>
+			<body name="robot0:shoulder_pan_link" pos="0.1195 0 0.3486">
+				<inertial diaginertia="0.009 0.0086 0.0041" mass="2.5587" pos="0.0927 -0.0056 0.0564" quat="-0.1364 0.7624 -0.1562 0.613"></inertial>
+				<joint axis="0 0 1" name="robot0:shoulder_pan_joint" range="-1.6056 1.6056"></joint>
+				<geom mesh="robot0:shoulder_pan_link" name="robot0:shoulder_pan_link" material="robot0:arm_mat"></geom>
+				<body name="robot0:shoulder_lift_link" pos="0.117 0 0.06">
+					<inertial diaginertia="0.0116 0.0112 0.0023" mass="2.6615" pos="0.1432 0.0072 -0.0001" quat="0.4382 0.4382 0.555 0.555"></inertial>
+					<joint axis="0 1 0" name="robot0:shoulder_lift_joint" range="-1.221 1.518"></joint>
+					<geom mesh="robot0:shoulder_lift_link" name="robot0:shoulder_lift_link" material="robot0:arm_mat" class="robot0:blue"></geom>
+					<body name="robot0:upperarm_roll_link" pos="0.219 0 0">
+						<inertial diaginertia="0.0047 0.0045 0.0019" mass="2.3311" pos="0.1165 0.0014 0" quat="-0.0136 0.707 0.0136 0.707"></inertial>
+						<joint axis="1 0 0" limited="false" name="robot0:upperarm_roll_joint"></joint>
+						<geom mesh="robot0:upperarm_roll_link" name="robot0:upperarm_roll_link" material="robot0:arm_mat"></geom>
+						<body name="robot0:elbow_flex_link" pos="0.133 0 0">
+							<inertial diaginertia="0.0086 0.0084 0.002" mass="2.1299" pos="0.1279 0.0073 0" quat="0.4332 0.4332 0.5589 0.5589"></inertial>
+							<joint axis="0 1 0" name="robot0:elbow_flex_joint" range="-2.251 2.251"></joint>
+							<geom mesh="robot0:elbow_flex_link" name="robot0:elbow_flex_link" material="robot0:arm_mat" class="robot0:blue"></geom>
+							<body name="robot0:forearm_roll_link" pos="0.197 0 0">
+								<inertial diaginertia="0.0035 0.0031 0.0015" mass="1.6563" pos="0.1097 -0.0266 0" quat="-0.0715 0.7035 0.0715 0.7035"></inertial>
+								<joint armature="2.7538" axis="1 0 0" damping="3.5247" frictionloss="0" limited="false" name="robot0:forearm_roll_joint" stiffness="10"></joint>
+								<geom mesh="robot0:forearm_roll_link" name="robot0:forearm_roll_link" material="robot0:arm_mat"></geom>
+								<body name="robot0:wrist_flex_link" pos="0.1245 0 0">
+									<inertial diaginertia="0.0042 0.0042 0.0018" mass="1.725" pos="0.0882 0.0009 -0.0001" quat="0.4895 0.4895 0.5103 0.5103"></inertial>
+									<joint axis="0 1 0" name="robot0:wrist_flex_joint" range="-2.16 2.16"></joint>
+									<geom mesh="robot0:wrist_flex_link" name="robot0:wrist_flex_link" material="robot0:arm_mat" class="robot0:blue"></geom>
+									<body name="robot0:wrist_roll_link" pos="0.1385 0 0">
+										<inertial diaginertia="0.0001 0.0001 0.0001" mass="0.1354" pos="0.0095 0.0004 -0.0002"></inertial>
+										<joint axis="1 0 0" limited="false" name="robot0:wrist_roll_joint"></joint>
+										<geom mesh="robot0:wrist_roll_link" name="robot0:wrist_roll_link" material="robot0:arm_mat"></geom>
+										<body euler="0 0 0" name="robot0:gripper_link" pos="0.1664 0 0">
+											<inertial diaginertia="0.0024 0.0019 0.0013" mass="1.5175" pos="-0.09 -0.0001 -0.0017" quat="0 0.7071 0 0.7071"></inertial>
+											<geom mesh="robot0:gripper_link" name="robot0:gripper_link" material="robot0:gripper_mat"></geom>
+											<body name="robot0:gripper_camera_link" pos="0.055 0 0.0225">
+												<body name="robot0:gripper_camera_rgb_frame" pos="0 0.02 0">
+													<body name="robot0:gripper_camera_rgb_optical_frame" pos="0 0 0" quat="0.5 -0.5 0.5 -0.5">
+														<camera euler="3.1415 0 0" fovy="50" name="gripper_camera_rgb" pos="0 0 0"></camera>
+													</body>
+												</body>
+												<body name="robot0:gripper_camera_depth_frame" pos="0 0.045 0">
+													<body name="robot0:gripper_camera_depth_optical_frame" pos="0 0 0" quat="0.5 -0.5 0.5 -0.5"></body>
+												</body>
+											</body>
+
+											<body childclass="robot0:fetchGripper" name="robot0:r_gripper_finger_link" pos="0 0.0159 0">
+												<inertial diaginertia="0.1 0.1 0.1" mass="4" pos="-0.01 0 0"></inertial>
+												<joint axis="0 1 0" name="robot0:r_gripper_finger_joint" range="0 0.05"></joint>
+												<geom pos="0 -0.008 0" size="0.0385 0.007 0.0135" type="box" name="robot0:r_gripper_finger_link" material="robot0:gripper_finger_mat" condim="4" friction="1 0.05 0.01"></geom>
+											</body>
+											<body childclass="robot0:fetchGripper" name="robot0:l_gripper_finger_link" pos="0 -0.0159 0">
+												<inertial diaginertia="0.1 0.1 0.1" mass="4" pos="-0.01 0 0"></inertial>
+												<joint axis="0 -1 0" name="robot0:l_gripper_finger_joint" range="0 0.05"></joint>
+												<geom pos="0 0.008 0" size="0.0385 0.007 0.0135" type="box" name="robot0:l_gripper_finger_link" material="robot0:gripper_finger_mat" condim="4" friction="1 0.05 0.01"></geom>
+											</body>
+											<site name="robot0:grip" pos="0.02 0 0" rgba="0 0 0 0" size="0.02 0.02 0.02"></site>
+										</body>
+									</body>
+								</body>
+							</body>
+						</body>
+					</body>
+				</body>
+			</body>
+		</body>
+		<body name="robot0:estop_link" pos="-0.1246 0.2389 0.3113" quat="0.7071 0.7071 0 0">
+			<inertial diaginertia="0 0 0" mass="0.002" pos="0.0024 -0.0033 0.0067" quat="0.3774 -0.1814 0.1375 0.8977"></inertial>
+			<geom mesh="robot0:estop_link" rgba="0.8 0 0 1" name="robot0:estop_link"></geom>
+		</body>
+		<body name="robot0:laser_link" pos="0.235 0 0.2878" quat="0 1 0 0">
+			<inertial diaginertia="0 0 0" mass="0.0083" pos="-0.0306 0.0007 0.0552" quat="0.5878 0.5378 -0.4578 0.3945"></inertial>
+			<geom mesh="robot0:laser_link" rgba="0.7922 0.8196 0.9333 1" name="robot0:laser_link"></geom>
+			<camera euler="1.55 -1.55 3.14" fovy="25" name="lidar" pos="0 0 0.02"></camera>
+		</body>
+		<body name="robot0:torso_fixed_link" pos="-0.0869 0 0.3774">
+			<inertial diaginertia="0.3865 0.3394 0.1009" mass="13.2775" pos="-0.0722 0.0057 0.2656" quat="0.9995 0.0249 0.0177 0.011"></inertial>
+			<geom mesh="robot0:torso_fixed_link" name="robot0:torso_fixed_link" class="robot0:blue"></geom>
+		</body>
+		<body name="robot0:external_camera_body_0" pos="0 0 0">
+			<camera euler="0 0.75 1.57" fovy="43.3" name="external_camera_0" pos="1.3 0 1.2"></camera>
+		</body>
+	</body>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/shared.xml b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/shared.xml
new file mode 100644
index 0000000..5d61fef
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/shared.xml
@@ -0,0 +1,66 @@
+<mujoco>
+    <asset>
+        <texture type="skybox" builtin="gradient" rgb1="0.44 0.85 0.56" rgb2="0.46 0.87 0.58" width="32" height="32"></texture>
+        <texture name="texture_block" file="block.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+
+        <material name="floor_mat" specular="0" shininess="0.5" reflectance="0" rgba="0.2 0.2 0.2 1"></material>
+        <material name="table_mat" specular="0" shininess="0.5" reflectance="0" rgba="0.93 0.93 0.93 1"></material>
+        <material name="block_mat" specular="0" shininess="0.5" reflectance="0" rgba="0.2 0.2 0.2 1"></material>
+        <material name="puck_mat" specular="0" shininess="0.5" reflectance="0" rgba="0.2 0.2 0.2 1"></material>
+        <material name="robot0:geomMat" shininess="0.03" specular="0.4"></material>
+        <material name="robot0:gripper_finger_mat" shininess="0.03" specular="0.4" reflectance="0"></material>
+        <material name="robot0:gripper_mat" shininess="0.03" specular="0.4" reflectance="0"></material>
+        <material name="robot0:arm_mat" shininess="0.03" specular="0.4" reflectance="0"></material>
+        <material name="robot0:head_mat" shininess="0.03" specular="0.4" reflectance="0"></material>
+        <material name="robot0:torso_mat" shininess="0.03" specular="0.4" reflectance="0"></material>
+        <material name="robot0:base_mat" shininess="0.03" specular="0.4" reflectance="0"></material>
+        
+        <mesh file="base_link_collision.stl" name="robot0:base_link"></mesh>
+        <mesh file="bellows_link_collision.stl" name="robot0:bellows_link"></mesh>
+        <mesh file="elbow_flex_link_collision.stl" name="robot0:elbow_flex_link"></mesh>
+        <mesh file="estop_link.stl" name="robot0:estop_link"></mesh>
+        <mesh file="forearm_roll_link_collision.stl" name="robot0:forearm_roll_link"></mesh>
+        <mesh file="gripper_link.stl" name="robot0:gripper_link"></mesh>
+        <mesh file="head_pan_link_collision.stl" name="robot0:head_pan_link"></mesh>
+        <mesh file="head_tilt_link_collision.stl" name="robot0:head_tilt_link"></mesh>
+        <mesh file="l_wheel_link_collision.stl" name="robot0:l_wheel_link"></mesh>
+        <mesh file="laser_link.stl" name="robot0:laser_link"></mesh>
+        <mesh file="r_wheel_link_collision.stl" name="robot0:r_wheel_link"></mesh>
+        <mesh file="torso_lift_link_collision.stl" name="robot0:torso_lift_link"></mesh>
+        <mesh file="shoulder_pan_link_collision.stl" name="robot0:shoulder_pan_link"></mesh>
+        <mesh file="shoulder_lift_link_collision.stl" name="robot0:shoulder_lift_link"></mesh>
+        <mesh file="upperarm_roll_link_collision.stl" name="robot0:upperarm_roll_link"></mesh>
+        <mesh file="wrist_flex_link_collision.stl" name="robot0:wrist_flex_link"></mesh>
+        <mesh file="wrist_roll_link_collision.stl" name="robot0:wrist_roll_link"></mesh>
+        <mesh file="torso_fixed_link.stl" name="robot0:torso_fixed_link"></mesh>
+    </asset>
+
+    <equality>
+        <weld body1="robot0:mocap" body2="robot0:gripper_link" solimp="0.9 0.95 0.001" solref="0.02 1"></weld>
+    </equality>
+    
+    <contact>
+        <exclude body1="robot0:r_gripper_finger_link" body2="robot0:l_gripper_finger_link"></exclude>
+        <exclude body1="robot0:torso_lift_link" body2="robot0:torso_fixed_link"></exclude>
+        <exclude body1="robot0:torso_lift_link" body2="robot0:shoulder_pan_link"></exclude>
+    </contact>
+    
+    <default>
+        <default class="robot0:fetch">
+            <geom margin="0.001" material="robot0:geomMat" rgba="1 1 1 1" solimp="0.99 0.99 0.01" solref="0.01 1" type="mesh" user="0"></geom>
+            <joint armature="1" damping="50" frictionloss="0" stiffness="0"></joint>
+            
+            <default class="robot0:fetchGripper">
+                <geom condim="4" margin="0.001" type="box" user="0" rgba="0.356 0.361 0.376 1.0"></geom>
+                <joint armature="100" damping="1000" limited="true" solimplimit="0.99 0.999 0.01" solreflimit="0.01 1" type="slide"></joint>
+            </default>
+
+            <default class="robot0:grey">
+                <geom rgba="0.356 0.361 0.376 1.0"></geom>
+            </default>
+            <default class="robot0:blue">
+                <geom rgba="0.086 0.506 0.767 1.0"></geom>
+            </default>
+        </default>
+    </default>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/slide.xml b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/slide.xml
new file mode 100644
index 0000000..efbfb51
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/fetch/slide.xml
@@ -0,0 +1,32 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+	<compiler angle="radian" coordinate="local" meshdir="../stls/fetch" texturedir="../textures"></compiler>
+	<option timestep="0.002">
+		<flag warmstart="enable"></flag>
+	</option>
+
+	<include file="shared.xml"></include>
+	
+	<worldbody>
+		<geom name="floor0" pos="1 0.75 0" size="1.05 0.7 1" type="plane" condim="3" material="floor_mat"></geom>
+		<body name="floor0" pos="1 0.75 0">
+			<site name="target0" pos="0 0 0.5" size="0.02 0.02 0.02" rgba="1 0 0 1" type="sphere"></site>
+		</body>
+
+		<include file="robot.xml"></include>
+		
+		<body name="table0" pos="1.32441906 0.75018422 0.2">
+			<geom size="0.625 0.45 0.2" type="box" condim="3" name="table0" material="table_mat" mass="2000" friction="0.1 0.005 0.0001"></geom>
+		</body>
+
+		<body name="object0" pos="0.025 0.025 0.02">
+			<joint name="object0:joint" type="free" damping="0.01"></joint>
+			<geom size="0.025 0.02" type="cylinder" condim="3" name="object0" material="puck_mat" friction="0.1 0.005 0.0001" mass="2"></geom>
+			<site name="object0" pos="0 0 0" size="0.02 0.02 0.02" rgba="1 0 0 1" type="sphere"></site>
+		</body>
+
+		<light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 0 4" dir="0 0 -1" name="light0"></light>
+	</worldbody>
+
+	<actuator></actuator>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_block.xml b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_block.xml
new file mode 100644
index 0000000..83a6517
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_block.xml
@@ -0,0 +1,41 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+    <compiler angle="radian" coordinate="local" meshdir="../stls/hand" texturedir="../textures"></compiler>
+    <option timestep="0.002" iterations="20" apirate="200">
+        <flag warmstart="enable"></flag>
+    </option>
+
+    <include file="shared.xml"></include>
+
+    <asset>
+        <include file="shared_asset.xml"></include>
+
+        <texture name="texture:object" file="block.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+        <texture name="texture:hidden" file="block_hidden.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+
+        <material name="material:object" texture="texture:object" specular="1" shininess="0.3" reflectance="0"></material>
+        <material name="material:hidden" texture="texture:hidden" specular="1" shininess="0.3" reflectance="0"></material>
+        <material name="material:target" texture="texture:object" specular="1" shininess="0.3" reflectance="0" rgba="1 1 1 0.5"></material>
+    </asset>
+
+    <worldbody>
+        <geom name="floor0" pos="1 1 0" size="1 1 1" type="plane" condim="3" material="floor_mat"></geom>
+        <body name="floor0" pos="1 1 0"></body>
+
+        <include file="robot.xml"></include>
+
+        <body name="object" pos="1 0.87 0.2">
+            <geom name="object" type="box" size="0.025 0.025 0.025" material="material:object" condim="4" density="567"></geom>
+            <geom name="object_hidden" type="box" size="0.024 0.024 0.024" material="material:hidden" condim="4" contype="0" conaffinity="0" mass="0"></geom>
+            <site name="object:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <joint name="object:joint" type="free" damping="0.01"></joint>
+        </body>
+        <body name="target" pos="1 0.87 0.2">
+            <geom name="target" type="box" size="0.025 0.025 0.025" material="material:target" condim="4" group="2" contype="0" conaffinity="0"></geom>
+            <site name="target:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <joint name="target:joint" type="free" damping="0.01"></joint>
+        </body>
+        
+        <light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 1 4" dir="0 0 -1" name="light0"></light>
+    </worldbody>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_block_touch_sensors.xml b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_block_touch_sensors.xml
new file mode 100644
index 0000000..b649f10
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_block_touch_sensors.xml
@@ -0,0 +1,42 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+    <compiler angle="radian" coordinate="local" meshdir="../stls/hand" texturedir="../textures"></compiler>
+    <option timestep="0.002" iterations="20" apirate="200">
+        <flag warmstart="enable"></flag>
+    </option>
+
+    <include file="shared.xml"></include>
+    <include file="shared_touch_sensors_92.xml"></include>
+
+    <asset>
+        <include file="shared_asset.xml"></include>
+
+        <texture name="texture:object" file="block.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+        <texture name="texture:hidden" file="block_hidden.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+
+        <material name="material:object" texture="texture:object" specular="1" shininess="0.3" reflectance="0"></material>
+        <material name="material:hidden" texture="texture:hidden" specular="1" shininess="0.3" reflectance="0"></material>
+        <material name="material:target" texture="texture:object" specular="1" shininess="0.3" reflectance="0" rgba="1 1 1 0.5"></material>
+    </asset>
+
+    <worldbody>
+        <geom name="floor0" pos="1 1 0" size="1 1 1" type="plane" condim="3" material="floor_mat"></geom>
+        <body name="floor0" pos="1 1 0"></body>
+
+        <include file="robot_touch_sensors_92.xml"></include>
+
+        <body name="object" pos="1 0.87 0.2">
+            <geom name="object" type="box" size="0.025 0.025 0.025" material="material:object" condim="4" density="567"></geom>
+            <geom name="object_hidden" type="box" size="0.024 0.024 0.024" material="material:hidden" condim="4" contype="0" conaffinity="0" mass="0"></geom>
+            <site name="object:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <joint name="object:joint" type="free" damping="0.01"></joint>
+        </body>
+        <body name="target" pos="1 0.87 0.2">
+            <geom name="target" type="box" size="0.025 0.025 0.025" material="material:target" condim="4" group="2" contype="0" conaffinity="0"></geom>
+            <site name="target:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <joint name="target:joint" type="free" damping="0.01"></joint>
+        </body>
+
+        <light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 1 4" dir="0 0 -1" name="light0"></light>
+    </worldbody>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_egg.xml b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_egg.xml
new file mode 100644
index 0000000..d60217f
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_egg.xml
@@ -0,0 +1,41 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+    <compiler angle="radian" coordinate="local" meshdir="../stls/hand" texturedir="../textures"></compiler>
+    <option timestep="0.002" iterations="20" apirate="200">
+        <flag warmstart="enable"></flag>
+    </option>
+
+    <include file="shared.xml"></include>
+
+    <asset>
+        <include file="shared_asset.xml"></include>
+
+        <texture name="texture:object" file="block.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+        <texture name="texture:hidden" file="block_hidden.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+
+        <material name="material:object" texture="texture:object" specular="1" shininess="0.3" reflectance="0"></material>
+        <material name="material:hidden" texture="texture:hidden" specular="1" shininess="0.3" reflectance="0"></material>
+        <material name="material:target" texture="texture:object" specular="1" shininess="0.3" reflectance="0" rgba="1 1 1 0.5"></material>
+    </asset>
+
+    <worldbody>
+        <geom name="floor0" pos="1 1 0" size="1 1 1" type="plane" condim="3" material="floor_mat"></geom>
+        <body name="floor0" pos="1 1 0"></body>
+
+        <include file="robot.xml"></include>
+
+        <body name="object" pos="1 0.87 0.2">
+            <geom name="object" type="ellipsoid" size="0.03 0.03 0.04" material="material:object" condim="4"></geom>
+            <geom name="object_hidden" type="ellipsoid" size="0.029 0.029 0.03" material="material:hidden" condim="4" contype="0" conaffinity="0" mass="0"></geom>
+            <site name="object:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <joint name="object:joint" type="free" damping="0.01"></joint>
+        </body>
+        <body name="target" pos="1 0.87 0.2">
+            <geom name="target" type="ellipsoid" size="0.03 0.03 0.04" material="material:target" condim="4" group="2" contype="0" conaffinity="0"></geom>
+            <site name="target:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <joint name="target:joint" type="free" damping="0.01"></joint>
+        </body>
+        
+        <light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 1 4" dir="0 0 -1" name="light0"></light>
+    </worldbody>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_egg_touch_sensors.xml b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_egg_touch_sensors.xml
new file mode 100644
index 0000000..73af83c
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_egg_touch_sensors.xml
@@ -0,0 +1,42 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+    <compiler angle="radian" coordinate="local" meshdir="../stls/hand" texturedir="../textures"></compiler>
+    <option timestep="0.002" iterations="20" apirate="200">
+        <flag warmstart="enable"></flag>
+    </option>
+
+    <include file="shared.xml"></include>
+    <include file="shared_touch_sensors_92.xml"></include>
+
+    <asset>
+        <include file="shared_asset.xml"></include>
+
+        <texture name="texture:object" file="block.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+        <texture name="texture:hidden" file="block_hidden.png" gridsize="3 4" gridlayout=".U..LFRB.D.."></texture>
+
+        <material name="material:object" texture="texture:object" specular="1" shininess="0.3" reflectance="0"></material>
+        <material name="material:hidden" texture="texture:hidden" specular="1" shininess="0.3" reflectance="0"></material>
+        <material name="material:target" texture="texture:object" specular="1" shininess="0.3" reflectance="0" rgba="1 1 1 0.5"></material>
+    </asset>
+
+    <worldbody>
+        <geom name="floor0" pos="1 1 0" size="1 1 1" type="plane" condim="3" material="floor_mat"></geom>
+        <body name="floor0" pos="1 1 0"></body>
+
+        <include file="robot_touch_sensors_92.xml"></include>
+
+        <body name="object" pos="1 0.87 0.2">
+            <geom name="object" type="ellipsoid" size="0.03 0.03 0.04" material="material:object" condim="4"></geom>
+            <geom name="object_hidden" type="ellipsoid" size="0.029 0.029 0.03" material="material:hidden" condim="4" contype="0" conaffinity="0" mass="0"></geom>
+            <site name="object:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <joint name="object:joint" type="free" damping="0.01"></joint>
+        </body>
+        <body name="target" pos="1 0.87 0.2">
+            <geom name="target" type="ellipsoid" size="0.03 0.03 0.04" material="material:target" condim="4" group="2" contype="0" conaffinity="0"></geom>
+            <site name="target:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <joint name="target:joint" type="free" damping="0.01"></joint>
+        </body>
+
+        <light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 1 4" dir="0 0 -1" name="light0"></light>
+    </worldbody>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_pen.xml b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_pen.xml
new file mode 100644
index 0000000..20a6fb5
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_pen.xml
@@ -0,0 +1,40 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+    <compiler angle="radian" coordinate="local" meshdir="../stls/hand" texturedir="../textures"></compiler>
+    <option timestep="0.002" iterations="20" apirate="200">
+        <flag warmstart="enable"></flag>
+    </option>
+
+    <include file="shared.xml"></include>
+
+    <asset>
+        <include file="shared_asset.xml"></include>
+
+        <material name="material:object" specular="0" shininess="0.5" reflectance="0.0" rgba="0.46 0.81 0.88 1.0"></material>
+        <material name="material:target" specular="0" shininess="0.5" reflectance="0.0" rgba="0.46 0.81 0.88 0.5"></material>
+    </asset>
+
+    <worldbody>
+        <geom name="floor0" pos="1 1 -0.2" size="1 1 1" type="plane" condim="3" material="floor_mat"></geom>
+        <body name="floor0" pos="1 1 0"></body>
+
+        <include file="robot.xml"></include>
+
+        <body name="object" pos="1 0.87 0.2" euler="-1 1 0">
+            <geom name="object" type="capsule" size="0.008 0.1" material="material:object" condim="4"></geom>
+            <site name="object:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <site name="object:top" pos="0 0 0.1" rgba="1 0 0 1" size="0.0081"></site>
+            <site name="object:bottom" pos="0 0 -0.1" rgba="0 1 0 1" size="0.0081"></site>
+            <joint name="object:joint" type="free" damping="0.01"></joint>
+        </body>
+        <body name="target" pos="1 0.87 0.2" euler="-1 1 0">
+            <geom name="target" type="capsule" size="0.008 0.1" material="material:target" condim="4" group="2" contype="0" conaffinity="0"></geom>
+            <site name="target:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <site name="target:top" pos="0 0 0.1" rgba="1 0 0 0.5" size="0.0081"></site>
+            <site name="target:bottom" pos="0 0 -0.1" rgba="0 1 0 0.5" size="0.0081"></site>
+            <joint name="target:joint" type="free" damping="0.01"></joint>
+        </body>
+        
+        <light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 1 4" dir="0 0 -1" name="light0"></light>
+    </worldbody>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_pen_touch_sensors.xml b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_pen_touch_sensors.xml
new file mode 100644
index 0000000..758839b
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/manipulate_pen_touch_sensors.xml
@@ -0,0 +1,41 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+    <compiler angle="radian" coordinate="local" meshdir="../stls/hand" texturedir="../textures"></compiler>
+    <option timestep="0.002" iterations="20" apirate="200">
+        <flag warmstart="enable"></flag>
+    </option>
+
+    <include file="shared.xml"></include>
+    <include file="shared_touch_sensors_92.xml"></include>
+
+    <asset>
+        <include file="shared_asset.xml"></include>
+
+        <material name="material:object" specular="0" shininess="0.5" reflectance="0.0" rgba="0.46 0.81 0.88 1.0"></material>
+        <material name="material:target" specular="0" shininess="0.5" reflectance="0.0" rgba="0.46 0.81 0.88 0.5"></material>
+    </asset>
+
+    <worldbody>
+        <geom name="floor0" pos="1 1 -0.2" size="1 1 1" type="plane" condim="3" material="floor_mat"></geom>
+        <body name="floor0" pos="1 1 0"></body>
+
+        <include file="robot_touch_sensors_92.xml"></include>
+
+        <body name="object" pos="1 0.87 0.2" euler="-1 1 0">
+            <geom name="object" type="capsule" size="0.008 0.1" material="material:object" condim="4"></geom>
+            <site name="object:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <site name="object:top" pos="0 0 0.1" rgba="1 0 0 1" size="0.0081"></site>
+            <site name="object:bottom" pos="0 0 -0.1" rgba="0 1 0 1" size="0.0081"></site>
+            <joint name="object:joint" type="free" damping="0.01"></joint>
+        </body>
+        <body name="target" pos="1 0.87 0.2" euler="-1 1 0">
+            <geom name="target" type="capsule" size="0.008 0.1" material="material:target" condim="4" group="2" contype="0" conaffinity="0"></geom>
+            <site name="target:center" pos="0 0 0" rgba="1 0 0 0" size="0.01 0.01 0.01"></site>
+            <site name="target:top" pos="0 0 0.1" rgba="1 0 0 0.5" size="0.0081"></site>
+            <site name="target:bottom" pos="0 0 -0.1" rgba="0 1 0 0.5" size="0.0081"></site>
+            <joint name="target:joint" type="free" damping="0.01"></joint>
+        </body>
+
+        <light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 1 4" dir="0 0 -1" name="light0"></light>
+    </worldbody>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/reach.xml b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/reach.xml
new file mode 100644
index 0000000..71f6dfe
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/reach.xml
@@ -0,0 +1,34 @@
+<?xml version="1.0" encoding="utf-8"?>
+<mujoco>
+    <compiler angle="radian" coordinate="local" meshdir="../stls/hand" texturedir="../textures"></compiler>
+    <option timestep="0.002" iterations="20" apirate="200">
+        <flag warmstart="enable"></flag>
+    </option>
+
+    <include file="shared.xml"></include>
+
+    <asset>
+        <include file="shared_asset.xml"></include>
+    </asset>
+
+    <worldbody>
+        <geom name="floor0" pos="1 1 0" size="1 1 1" type="plane" condim="3" material="floor_mat"></geom>
+        <body name="floor0" pos="1 1 0">
+            <site name="target0" pos="0 0 0" size="0.005" rgba="1 0 0 1" type="sphere"></site>
+            <site name="target1" pos="0 0 0" size="0.005" rgba="0 1 0 1" type="sphere"></site>
+            <site name="target2" pos="0 0 0" size="0.005" rgba="0 0 1 1" type="sphere"></site>
+            <site name="target3" pos="0 0 0" size="0.005" rgba="1 1 0 1" type="sphere"></site>
+            <site name="target4" pos="0 0 0" size="0.005" rgba="1 0 1 1" type="sphere"></site>
+
+            <site name="finger0" pos="0 0 0" size="0.01" rgba="1 0 0 0.2" type="sphere"></site>
+            <site name="finger1" pos="0 0 0" size="0.01" rgba="0 1 0 0.2" type="sphere"></site>
+            <site name="finger2" pos="0 0 0" size="0.01" rgba="0 0 1 0.2" type="sphere"></site>
+            <site name="finger3" pos="0 0 0" size="0.01" rgba="1 1 0 0.2" type="sphere"></site>
+            <site name="finger4" pos="0 0 0" size="0.01" rgba="1 0 1 0.2" type="sphere"></site>
+        </body>
+
+        <include file="robot.xml"></include>
+        
+        <light directional="true" ambient="0.2 0.2 0.2" diffuse="0.8 0.8 0.8" specular="0.3 0.3 0.3" castshadow="false" pos="0 0 4" dir="0 0 -1" name="light0"></light>
+    </worldbody>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/robot.xml b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/robot.xml
new file mode 100644
index 0000000..dbb9e43
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/robot.xml
@@ -0,0 +1,160 @@
+<!-- See LICENSE.md for legal notices. LICENSE.md must be kept together with this file. -->
+<mujoco>
+    <body name="robot0:hand mount" pos="1 1.25 0.15" euler="1.5708 0 3.14159">
+        <inertial mass="0.1" pos="0 0 0" diaginertia="0.001 0.001 0.001"></inertial>
+        <body childclass="robot0:asset_class" name="robot0:forearm" pos="0 0.01 0" euler="0 0 0">
+            <inertial pos="0.001 -0.002 0.29" quat="0.982 -0.016 0 -0.188" mass="4" diaginertia="0.01 0.01 0.0075"></inertial>
+            <geom class="robot0:D_Vizual" pos="0 0.01 0.04" name="robot0:V_forearm" mesh="robot0:forearm" euler="0 0 1.57"></geom>
+            <geom class="robot0:DC_Hand" name="robot0:C_forearm" type="mesh" mesh="robot0:forearm_cvx" pos="0 0.01 0.04" euler="0 0 1.57" rgba="0.4 0.5 0.6 0.7"></geom>
+            <body name="robot0:wrist" pos="0 0 0.256">
+                <inertial pos="0.003 0 0.016" quat="0.504 0.496 0.495 0.504" mass="0.3" diaginertia="0.001 0.001 0.001"></inertial>
+                <joint name="robot0:WRJ1" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.489 0.14" damping="0.5" armature="0.005" user="1123"></joint>
+                <geom class="robot0:D_Vizual" name="robot0:V_wrist" mesh="robot0:wrist"></geom>
+                <geom class="robot0:DC_Hand" name="robot0:C_wrist" type="capsule" pos="0 0 0" quat="0.707 0.707 0 0" size="0.015 0.01" rgba="0.4 0.5 0.6 0.1"></geom>
+                <body name="robot0:palm" pos="0 0 0.034">
+                    <inertial pos="0.006 0 0.036" quat="0.716 0.044 0.075 0.693" mass="0.3" diaginertia="0.001 0.001 0.001"></inertial>
+                    <joint name="robot0:WRJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="-0.698 0.489" damping="0.5" armature="0.005" user="1122"></joint>
+                    <geom class="robot0:D_Vizual" name="robot0:V_palm" mesh="robot0:palm"></geom>
+                    <geom class="robot0:DC_Hand" name="robot0:C_palm0" type="box" pos="0.011 0 0.038" size="0.032 0.0111 0.049" rgba="0.4 0.5 0.6 0.1"></geom>
+                    <geom class="robot0:DC_Hand" name="robot0:C_palm1" type="box" pos="-0.032 0 0.014" size="0.011 0.0111 0.025" rgba="0.4 0.5 0.6 0.1"></geom>
+                    <body name="robot0:ffknuckle" pos="0.033 0 0.095">
+                        <inertial pos="0 0 0" quat="0.52 0.854 0.006 -0.003" mass="0.008" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:FFJ3" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.349 0.349" user="1103"></joint>
+                        <geom class="robot0:D_Vizual" name="robot0:V_ffknuckle" mesh="robot0:knuckle"></geom>
+                        <body name="robot0:ffproximal" pos="0 0 0">
+                            <inertial pos="0 0 0.023" quat="0.707 -0.004 0.004 0.707" mass="0.014" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:FFJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1102"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_ffproximal" mesh="robot0:F3"></geom>
+                            <geom class="robot0:DC_Hand" name="robot0:C_ffproximal" type="capsule" pos="0 0 0.0225" size="0.01 0.0225"></geom>
+                            <body name="robot0:ffmiddle" pos="0 0 0.045">
+                                <inertial pos="0 0 0.011" quat="0.707 0 0 0.707" mass="0.012" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:FFJ1" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1101"></joint>
+                                <geom class="robot0:D_Vizual" name="robot0:V_ffmiddle" mesh="robot0:F2"></geom>
+                                <geom class="robot0:DC_Hand" name="robot0:C_ffmiddle" type="capsule" pos="0 0 0.0125" size="0.00805 0.0125"></geom>
+                                <body name="robot0:ffdistal" pos="0 0 0.025">
+                                    <inertial pos="0 0 0.015" quat="0.707 -0.003 0.003 0.707" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:FFJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1100"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_ffdistal" pos="0 0 0.001" mesh="robot0:F1"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_ffdistal" type="capsule" pos="0 0 0.012" size="0.00705 0.012" condim="4"></geom>
+                                    <site name="robot0:S_fftip" pos="0 0 0.026" group="3"></site>
+                                    <site class="robot0:D_Touch" name="robot0:Tch_fftip"></site>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                    <body name="robot0:mfknuckle" pos="0.011 0 0.099">
+                        <inertial pos="0 0 0" quat="0.52 0.854 0.006 -0.003" mass="0.008" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:MFJ3" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.349 0.349" user="1107"></joint>
+                        <geom class="robot0:D_Vizual" name="robot0:V_mfknuckle" mesh="robot0:knuckle"></geom>
+                        <body name="robot0:mfproximal" pos="0 0 0">
+                            <inertial pos="0 0 0.023" quat="0.707 -0.004 0.004 0.707" mass="0.014" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:MFJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1106"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_mfproximal" mesh="robot0:F3"></geom>
+                            <geom class="robot0:DC_Hand" name="robot0:C_mfproximal" type="capsule" pos="0 0 0.0225" size="0.01 0.0225"></geom>
+                            <body name="robot0:mfmiddle" pos="0 0 0.045">
+                                <inertial pos="0 0 0.012" quat="0.707 0 0 0.707" mass="0.012" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:MFJ1" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1105"></joint>
+                                <geom class="robot0:D_Vizual" name="robot0:V_mfmiddle" mesh="robot0:F2"></geom>
+                                <geom class="robot0:DC_Hand" name="robot0:C_mfmiddle" type="capsule" pos="0 0 0.0125" size="0.00805 0.0125"></geom>
+                                <body name="robot0:mfdistal" pos="0 0 0.025">
+                                    <inertial pos="0 0 0.015" quat="0.707 -0.003 0.003 0.707" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:MFJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1104"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_mfdistal" mesh="robot0:F1"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_mfdistal" type="capsule" pos="0 0 0.012" size="0.00705 0.012" condim="4"></geom>
+                                    <site name="robot0:S_mftip" pos="0 0 0.026" group="3"></site>
+                                    <site class="robot0:D_Touch" name="robot0:Tch_mftip"></site>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                    <body name="robot0:rfknuckle" pos="-0.011 0 0.095">
+                        <inertial pos="0 0 0" quat="0.52 0.854 0.006 -0.003" mass="0.008" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:RFJ3" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.349 0.349" user="1111"></joint>
+                        <geom class="robot0:D_Vizual" name="robot0:V_rfknuckle" mesh="robot0:knuckle"></geom>
+                        <body name="robot0:rfproximal" pos="0 0 0">
+                            <inertial pos="0 0 0.023" quat="0.707 -0.004 0.004 0.707" mass="0.014" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:RFJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1110"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_rfproximal" mesh="robot0:F3"></geom>
+                            <geom class="robot0:DC_Hand" name="robot0:C_rfproximal" type="capsule" pos="0 0 0.0225" size="0.01 0.0225"></geom>
+                            <body name="robot0:rfmiddle" pos="0 0 0.045">
+                                <inertial pos="0 0 0.012" quat="0.707 0 0 0.707" mass="0.012" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:RFJ1" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1109"></joint>
+                                <geom class="robot0:D_Vizual" name="robot0:V_rfmiddle" mesh="robot0:F2"></geom>
+                                <geom class="robot0:DC_Hand" name="robot0:C_rfmiddle" type="capsule" pos="0 0 0.0125" size="0.00805 0.0125"></geom>
+                                <body name="robot0:rfdistal" pos="0 0 0.025">
+                                    <inertial pos="0 0 0.015" quat="0.707 -0.003 0.003 0.707" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:RFJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1108"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_rfdistal" mesh="robot0:F1" pos="0 0 0.001"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_rfdistal" type="capsule" pos="0 0 0.012" size="0.00705 0.012" condim="4"></geom>
+                                    <site name="robot0:S_rftip" pos="0 0 0.026" group="3"></site>
+                                    <site class="robot0:D_Touch" name="robot0:Tch_rftip"></site>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                    <body name="robot0:lfmetacarpal" pos="-0.017 0 0.044">
+                        <inertial pos="-0.014 0.001 0.014" quat="0.709 -0.092 -0.063 0.696" mass="0.075" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:LFJ4" type="hinge" pos="0 0 0" axis="0.571 0 0.821" range="0 0.785" user="1116"></joint>
+                        <geom class="robot0:D_Vizual" name="robot0:V_lfmetacarpal" pos="-0.016 0 -0.023" mesh="robot0:lfmetacarpal"></geom>
+                        <geom class="robot0:DC_Hand" name="robot0:C_lfmetacarpal" type="box" pos="-0.0165 0 0.01" size="0.0095 0.0111 0.025" rgba="0.4 0.5 0.6 0.2"></geom>
+                        <body name="robot0:lfknuckle" pos="-0.017 0 0.044">
+                            <inertial pos="0 0 0" quat="0.52 0.854 0.006 -0.003" mass="0.008" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:LFJ3" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.349 0.349" user="1115"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_lfknuckle" mesh="robot0:knuckle"></geom>
+                            <body name="robot0:lfproximal" pos="0 0 0">
+                                <inertial pos="0 0 0.023" quat="0.707 -0.004 0.004 0.707" mass="0.014" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:LFJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1114"></joint>
+                                <geom class="robot0:D_Vizual" name="robot0:V_lfproximal" mesh="robot0:F3"></geom>
+                                <geom class="robot0:DC_Hand" name="robot0:C_lfproximal" type="capsule" pos="0 0 0.0225" size="0.01 0.0225"></geom>
+                                <body name="robot0:lfmiddle" pos="0 0 0.045">
+                                    <inertial pos="0 0 0.012" quat="0.707 0 0 0.707" mass="0.012" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:LFJ1" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1113"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_lfmiddle" mesh="robot0:F2"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_lfmiddle" type="capsule" pos="0 0 0.0125" size="0.00805 0.0125"></geom>
+                                    <body name="robot0:lfdistal" pos="0 0 0.025">
+                                        <inertial pos="0 0 0.015" quat="0.707 -0.003 0.003 0.707" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                        <joint name="robot0:LFJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1112"></joint>
+                                        <geom class="robot0:D_Vizual" name="robot0:V_lfdistal" mesh="robot0:F1" pos="0 0 0.001"></geom>
+                                        <geom class="robot0:DC_Hand" name="robot0:C_lfdistal" type="capsule" pos="0 0 0.012" size="0.00705 0.012" condim="4"></geom>
+                                        <site name="robot0:S_lftip" pos="0 0 0.026" group="3"></site>
+                                        <site class="robot0:D_Touch" name="robot0:Tch_lftip"></site>
+                                    </body>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                    <body name="robot0:thbase" pos="0.034 -0.009 0.029" axisangle="0 1 0 0.785">
+                        <inertial pos="0 0 0" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:THJ4" type="hinge" pos="0 0 0" axis="0 0 -1" range="-1.047 1.047" user="1121"></joint>
+                        <geom name="robot0:V_thbase" type="box" group="1" pos="0 0 0" size="0.001 0.001 0.001"></geom>
+                        <body name="robot0:thproximal" pos="0 0 0">
+                            <inertial pos="0 0 0.017" quat="0.982 0 0.001 0.191" mass="0.016" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:THJ3" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.222" user="1120"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_thproximal" mesh="robot0:TH3_z"></geom>
+                            <geom class="robot0:DC_Hand" name="robot0:C_thproximal" type="capsule" pos="0 0 0.019" size="0.013 0.019" rgba="0.4 0.5 0.6 0.1"></geom>
+                            <body name="robot0:thhub" pos="0 0 0.038">
+                                <inertial pos="0 0 0" mass="0.002" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:THJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="-0.209 0.209" user="1119"></joint>
+                                <geom name="robot0:V_thhub" type="box" group="1" pos="0 0 0" size="0.001 0.001 0.001"></geom>
+                                <body name="robot0:thmiddle" pos="0 0 0">
+                                    <inertial pos="0 0 0.016" quat="1 -0.001 -0.007 0.003" mass="0.016" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:THJ1" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.524 0.524" user="1118"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_thmiddle" mesh="robot0:TH2_z"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_thmiddle" type="capsule" pos="0 0 0.016" size="0.011 0.016"></geom>
+                                    <body name="robot0:thdistal" pos="0 0 0.032">
+                                        <inertial pos="0 0 0.016" quat="0.999 -0.005 -0.047 0.005" mass="0.016" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                        <joint name="robot0:THJ0" type="hinge" pos="0 0 0" axis="0 1 0" range="-1.571 0" user="1117"></joint>
+                                        <geom class="robot0:D_Vizual" name="robot0:V_thdistal" mesh="robot0:TH1_z"></geom>
+                                        <geom class="robot0:DC_Hand" name="robot0:C_thdistal" type="capsule" pos="0 0 0.013" size="0.00918 0.013" condim="4"></geom>
+                                        <site name="robot0:S_thtip" pos="0 0 0.0275" group="3"></site>
+                                        <site class="robot0:D_Touch" name="robot0:Tch_thtip" size="0.005 0.011 0.016" pos="-0.005 0 0.02"></site>
+                                    </body>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                </body>
+            </body>
+        </body>
+    </body>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/robot_touch_sensors_92.xml b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/robot_touch_sensors_92.xml
new file mode 100644
index 0000000..fa6d41c
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/robot_touch_sensors_92.xml
@@ -0,0 +1,252 @@
+<!-- See LICENSE.md for legal notices. LICENSE.md must be kept together with this file. -->
+<mujoco>
+    <body name="robot0:hand mount" pos="1 1.25 0.15" euler="1.5708 0 3.14159">
+        <inertial mass="0.1" pos="0 0 0" diaginertia="0.001 0.001 0.001"></inertial>
+        <body childclass="robot0:asset_class" name="robot0:forearm" pos="0 0.01 0" euler="0 0 0">
+            <inertial pos="0.001 -0.002 0.29" quat="0.982 -0.016 0 -0.188" mass="4" diaginertia="0.01 0.01 0.0075"></inertial>
+            <geom class="robot0:D_Vizual" pos="0 0.01 0.04" name="robot0:V_forearm" mesh="robot0:forearm" euler="0 0 1.57"></geom>
+            <geom class="robot0:DC_Hand" name="robot0:C_forearm" type="mesh" mesh="robot0:forearm_cvx" pos="0 0.01 0.04" euler="0 0 1.57" rgba="0.4 0.5 0.6 0.7"></geom>
+            <body name="robot0:wrist" pos="0 0 0.256">
+                <inertial pos="0.003 0 0.016" quat="0.504 0.496 0.495 0.504" mass="0.3" diaginertia="0.001 0.001 0.001"></inertial>
+                <joint name="robot0:WRJ1" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.489 0.14" damping="0.5" armature="0.005" user="1123"></joint>
+                <geom class="robot0:D_Vizual" name="robot0:V_wrist" mesh="robot0:wrist"></geom>
+                <geom class="robot0:DC_Hand" name="robot0:C_wrist" type="capsule" pos="0 0 0" quat="0.707 0.707 0 0" size="0.015 0.01" rgba="0.4 0.5 0.6 0.1"></geom>
+                <body name="robot0:palm" pos="0 0 0.034">
+                    <inertial pos="0.006 0 0.036" quat="0.716 0.044 0.075 0.693" mass="0.3" diaginertia="0.001 0.001 0.001"></inertial>
+                    <joint name="robot0:WRJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="-0.698 0.489" damping="0.5" armature="0.005" user="1122"></joint>
+                    <geom class="robot0:D_Vizual" name="robot0:V_palm" mesh="robot0:palm"></geom>
+                    <geom class="robot0:DC_Hand" name="robot0:C_palm0" type="box" pos="0.011 0 0.038" size="0.032 0.0111 0.049" rgba="0.4 0.5 0.6 0.1"></geom>
+                    <site name="robot0:T_palm_b0" type="box" pos="0.011 -0.005 0.006"  size="0.033 0.007 0.019" rgba="0 0 0 0.33"/>
+                    <site name="robot0:T_palm_bl" type="box" pos="-0.011 -0.005 0.041" size="0.011 0.007 0.016" rgba="1 0 0 0.33"/>
+                    <site name="robot0:T_palm_bm" type="box" pos="0.011 -0.005 0.041"  size="0.011 0.007 0.016" rgba="0 0 0 0.33"/>
+                    <site name="robot0:T_palm_br" type="box" pos="0.033 -0.005 0.041"  size="0.011 0.007 0.016" rgba="1 0 0 0.33"/>
+                    <site name="robot0:T_palm_fl" type="box" pos="-0.011 -0.005 0.073" size="0.011 0.007 0.016" rgba="0 0 0 0.33"/>
+                    <site name="robot0:T_palm_fm" type="box" pos="0.011 -0.005 0.073"  size="0.011 0.007 0.016" rgba="1 0 0 0.33"/>
+                    <site name="robot0:T_palm_fr" type="box" pos="0.033 -0.005 0.073"  size="0.011 0.007 0.016" rgba="0 0 0 0.33"/>
+                    <geom class="robot0:DC_Hand" name="robot0:C_palm1" type="box" pos="-0.032 0 0.014" size="0.011 0.0111 0.025" rgba="0.4 0.5 0.6 0.1"></geom>
+                    <site name="robot0:T_palm_b1" type="box" pos="-0.0325 -0.005 0.014" size="0.012 0.007 0.027" rgba="0 0 0 0.33"/>
+                    <body name="robot0:ffknuckle" pos="0.033 0 0.095">
+                        <inertial pos="0 0 0" quat="0.52 0.854 0.006 -0.003" mass="0.008" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:FFJ3" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.349 0.349" user="1103"></joint>
+                        <geom class="robot0:D_Vizual" name="robot0:V_ffknuckle" mesh="robot0:knuckle"></geom>
+                        <body name="robot0:ffproximal" pos="0 0 0">
+                            <inertial pos="0 0 0.023" quat="0.707 -0.004 0.004 0.707" mass="0.014" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:FFJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1102"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_ffproximal" mesh="robot0:F3"></geom>
+                            <geom class="robot0:DC_Hand" name="robot0:C_ffproximal" type="capsule" pos="0 0 0.0225" size="0.01 0.0225"></geom>
+                            <site name="robot0:T_ffproximal_front_left_bottom"  type="box"    pos="-0.005 -0.005 0.00625" size="0.005 0.005 0.01625" rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_ffproximal_front_right_bottom" type="box"    pos="0.005 -0.005 0.00625"  size="0.005 0.005 0.01625" rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_ffproximal_front_left_top"     type="box"    pos="-0.005 -0.005 0.03875" size="0.005 0.005 0.01625" rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_ffproximal_front_right_top"    type="box"    pos="0.005 -0.005 0.03875"  size="0.005 0.005 0.01625" rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_ffproximal_back_left"          type="box"    pos="-0.005 0.005 0.0225"   size="0.005 0.005 0.0325"  rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_ffproximal_back_right"         type="box"    pos="0.005 0.005 0.0225"    size="0.005 0.005 0.0325"  rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_ffproximal_tip"                type="sphere" pos="0 0 0.045"             size="0.011"               rgba="1 1 0 0.33"/>
+                            <body name="robot0:ffmiddle" pos="0 0 0.045">
+                                <inertial pos="0 0 0.011" quat="0.707 0 0 0.707" mass="0.012" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:FFJ1" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1101"></joint>
+                                <geom class="robot0:D_Vizual" name="robot0:V_ffmiddle" mesh="robot0:F2"></geom>
+                                <geom class="robot0:DC_Hand" name="robot0:C_ffmiddle" type="capsule" pos="0 0 0.0125" size="0.00805 0.0125"></geom>
+                                <site name="robot0:T_ffmiddle_front_left"  type="box"     pos="-0.00451 -0.00451 0.0125" size="0.00451 0.00451 0.02055" rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_ffmiddle_front_right" type="box"     pos="0.00451 -0.00451 0.0125"  size="0.00451 0.00451 0.02055" rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_ffmiddle_back_left"   type="box"     pos="-0.00451 0.00451 0.0125"  size="0.00451 0.00451 0.02055" rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_ffmiddle_back_right"  type="box"     pos="0.00451 0.00451 0.0125"   size="0.00451 0.00451 0.02055" rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_ffmiddle_tip"         type="sphere"  pos="0 0 0.025"                size="0.009"                   rgba="1 1 0 0.33"/>
+                                <body name="robot0:ffdistal" pos="0 0 0.025">
+                                    <inertial pos="0 0 0.015" quat="0.707 -0.003 0.003 0.707" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:FFJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1100"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_ffdistal" pos="0 0 0.001" mesh="robot0:F1"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_ffdistal" type="capsule" pos="0 0 0.012" size="0.00705 0.012" condim="4"></geom>
+                                    <site name="robot0:S_fftip" pos="0 0 0.026" group="3"></site>
+                                    <site class="robot0:D_Touch" name="robot0:Tch_fftip"></site>
+                                    <site name="robot0:T_fftip_front_left"  type="box"      pos="-0.00451 -0.00451 0.012" size="0.00451 0.00451 0.01905" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_fftip_front_right" type="box"      pos="0.00451 -0.00451 0.012"  size="0.00451 0.00451 0.01905" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_fftip_back_left"   type="box"      pos="-0.00451 0.00451 0.012"  size="0.00451 0.00451 0.01905" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_fftip_back_right"  type="box"      pos="0.00451 0.00451 0.012"   size="0.00451 0.00451 0.01905" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_fftip_tip"         type="sphere"   pos="0 0 0.024"               size="0.008"                   rgba="1 1 0 0.33"/>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                    <body name="robot0:mfknuckle" pos="0.011 0 0.099">
+                        <inertial pos="0 0 0" quat="0.52 0.854 0.006 -0.003" mass="0.008" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:MFJ3" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.349 0.349" user="1107"></joint>
+                        <geom class="robot0:D_Vizual" name="robot0:V_mfknuckle" mesh="robot0:knuckle"></geom>
+                        <body name="robot0:mfproximal" pos="0 0 0">
+                            <inertial pos="0 0 0.023" quat="0.707 -0.004 0.004 0.707" mass="0.014" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:MFJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1106"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_mfproximal" mesh="robot0:F3"></geom>
+                            <geom class="robot0:DC_Hand" name="robot0:C_mfproximal" type="capsule" pos="0 0 0.0225" size="0.01 0.0225"></geom>
+                            <site name="robot0:T_mfproximal_front_left_bottom"  type="box"    pos="-0.005 -0.005 0.00625" size="0.005 0.005 0.01625" rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_mfproximal_front_right_bottom" type="box"    pos="0.005 -0.005 0.00625"  size="0.005 0.005 0.01625" rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_mfproximal_front_left_top"     type="box"    pos="-0.005 -0.005 0.03875" size="0.005 0.005 0.01625" rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_mfproximal_front_right_top"    type="box"    pos="0.005 -0.005 0.03875"  size="0.005 0.005 0.01625" rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_mfproximal_back_left"          type="box"    pos="-0.005 0.005 0.0225"   size="0.005 0.005 0.0325"  rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_mfproximal_back_right"         type="box"    pos="0.005 0.005 0.0225"    size="0.005 0.005 0.0325"  rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_mfproximal_tip"                type="sphere" pos="0 0 0.045"             size="0.011"               rgba="1 1 0 0.33"/>
+                            <body name="robot0:mfmiddle" pos="0 0 0.045">
+                                <inertial pos="0 0 0.012" quat="0.707 0 0 0.707" mass="0.012" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:MFJ1" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1105"></joint>
+                                <geom class="robot0:D_Vizual" name="robot0:V_mfmiddle" mesh="robot0:F2"></geom>
+                                <geom class="robot0:DC_Hand" name="robot0:C_mfmiddle" type="capsule" pos="0 0 0.0125" size="0.00805 0.0125"></geom>
+                                <site name="robot0:T_mfmiddle_front_left"  type="box"    pos="-0.00451 -0.00451 0.0125" size="0.00451 0.00451 0.02055" rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_mfmiddle_front_right" type="box"    pos="0.00451 -0.00451 0.0125"  size="0.00451 0.00451 0.02055" rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_mfmiddle_back_left"   type="box"    pos="-0.00451 0.00451 0.0125"  size="0.00451 0.00451 0.02055" rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_mfmiddle_back_right"  type="box"    pos="0.00451 0.00451 0.0125"   size="0.00451 0.00451 0.02055" rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_mfmiddle_tip"         type="sphere" pos="0 0 0.025"                size="0.009"                   rgba="1 1 0 0.33"/>
+                                <body name="robot0:mfdistal" pos="0 0 0.025">
+                                    <inertial pos="0 0 0.015" quat="0.707 -0.003 0.003 0.707" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:MFJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1104"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_mfdistal" mesh="robot0:F1"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_mfdistal" type="capsule" pos="0 0 0.012" size="0.00705 0.012" condim="4"></geom>
+                                    <site name="robot0:S_mftip" pos="0 0 0.026" group="3"></site>
+                                    <site class="robot0:D_Touch" name="robot0:Tch_mftip"></site>
+                                    <site name="robot0:T_mftip_front_left"  type="box"    pos="-0.00451 -0.00451 0.012" size="0.00451 0.00451 0.01905" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_mftip_front_right" type="box"    pos="0.00451 -0.00451 0.012"  size="0.00451 0.00451 0.01905" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_mftip_back_left"   type="box"    pos="-0.00451 0.00451 0.012"  size="0.00451 0.00451 0.01905" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_mftip_back_right"  type="box"    pos="0.00451 0.00451 0.012"   size="0.00451 0.00451 0.01905" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_mftip_tip"         type="sphere" pos="0 0 0.024"               size="0.008"                   rgba="1 1 0 0.33"/>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                    <body name="robot0:rfknuckle" pos="-0.011 0 0.095">
+                        <inertial pos="0 0 0" quat="0.52 0.854 0.006 -0.003" mass="0.008" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:RFJ3" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.349 0.349" user="1111"></joint>
+                        <geom class="robot0:D_Vizual" name="robot0:V_rfknuckle" mesh="robot0:knuckle"></geom>
+                        <body name="robot0:rfproximal" pos="0 0 0">
+                            <inertial pos="0 0 0.023" quat="0.707 -0.004 0.004 0.707" mass="0.014" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:RFJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1110"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_rfproximal" mesh="robot0:F3"></geom>
+                            <geom class="robot0:DC_Hand" name="robot0:C_rfproximal" type="capsule" pos="0 0 0.0225" size="0.01 0.0225"></geom>
+                            <site name="robot0:T_rfproximal_front_left_bottom"  type="box"    pos="-0.005 -0.005 0.00625" size="0.005 0.005 0.01625" rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_rfproximal_front_right_bottom" type="box"    pos="0.005 -0.005 0.00625"  size="0.005 0.005 0.01625" rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_rfproximal_front_left_top"     type="box"    pos="-0.005 -0.005 0.03875" size="0.005 0.005 0.01625" rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_rfproximal_front_right_top"    type="box"    pos="0.005 -0.005 0.03875"  size="0.005 0.005 0.01625" rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_rfproximal_back_left"          type="box"    pos="-0.005 0.005 0.0225"   size="0.005 0.005 0.0325"  rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_rfproximal_back_right"         type="box"    pos="0.005 0.005 0.0225"    size="0.005 0.005 0.0325"  rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_rfproximal_tip"                type="sphere" pos="0 0 0.045"             size="0.011"               rgba="1 1 0 0.33"/>
+                            <body name="robot0:rfmiddle" pos="0 0 0.045">
+                                <inertial pos="0 0 0.012" quat="0.707 0 0 0.707" mass="0.012" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:RFJ1" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1109"></joint>
+                                <geom class="robot0:D_Vizual" name="robot0:V_rfmiddle" mesh="robot0:F2"></geom>
+                                <geom class="robot0:DC_Hand" name="robot0:C_rfmiddle" type="capsule" pos="0 0 0.0125" size="0.00805 0.0125"></geom>
+                                <site name="robot0:T_rfmiddle_front_left"  type="box"    pos="-0.00451 -0.00451 0.0125" size="0.00451 0.00451 0.02055" rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_rfmiddle_front_right" type="box"    pos="0.00451 -0.00451 0.0125"  size="0.00451 0.00451 0.02055" rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_rfmiddle_back_left"   type="box"    pos="-0.00451 0.00451 0.0125"  size="0.00451 0.00451 0.02055" rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_rfmiddle_back_right"  type="box"    pos="0.00451 0.00451 0.0125"   size="0.00451 0.00451 0.02055" rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_rfmiddle_tip"         type="sphere" pos="0 0 0.025"                size="0.009"                   rgba="1 1 0 0.33"/>
+                                <body name="robot0:rfdistal" pos="0 0 0.025">
+                                    <inertial pos="0 0 0.015" quat="0.707 -0.003 0.003 0.707" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:RFJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1108"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_rfdistal" mesh="robot0:F1" pos="0 0 0.001"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_rfdistal" type="capsule" pos="0 0 0.012" size="0.00705 0.012" condim="4"></geom>
+                                    <site name="robot0:S_rftip" pos="0 0 0.026" group="3"></site>
+                                    <site class="robot0:D_Touch" name="robot0:Tch_rftip"></site>
+                                    <site name="robot0:T_rftip_front_left"  type="box"    pos="-0.00451 -0.00451 0.012" size="0.00451 0.00451 0.01905" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_rftip_front_right" type="box"    pos="0.00451 -0.00451 0.012"  size="0.00451 0.00451 0.01905" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_rftip_back_left"   type="box"    pos="-0.00451 0.00451 0.012"  size="0.00451 0.00451 0.01905" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_rftip_back_right"  type="box"    pos="0.00451 0.00451 0.012"   size="0.00451 0.00451 0.01905" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_rftip_tip"         type="sphere" pos="0 0 0.024"               size="0.008"                   rgba="1 1 0 0.33"/>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                    <body name="robot0:lfmetacarpal" pos="-0.017 0 0.044">
+                        <inertial pos="-0.014 0.001 0.014" quat="0.709 -0.092 -0.063 0.696" mass="0.075" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:LFJ4" type="hinge" pos="0 0 0" axis="0.571 0 0.821" range="0 0.785" user="1116"></joint>
+                        <geom class="robot0:D_Vizual" name="robot0:V_lfmetacarpal" pos="-0.016 0 -0.023" mesh="robot0:lfmetacarpal"></geom>
+                        <geom class="robot0:DC_Hand" name="robot0:C_lfmetacarpal" type="box" pos="-0.0165 0 0.01" size="0.0095 0.0111 0.025" rgba="0.4 0.5 0.6 0.2"></geom>
+                        <site name="robot0:T_lfmetacarpal_front" type="box" pos="-0.0165 -0.005 0.00975" size="0.01 0.007 0.026"  rgba="1 0 0 0.33"/>
+                        <body name="robot0:lfknuckle" pos="-0.017 0 0.044">
+                            <inertial pos="0 0 0" quat="0.52 0.854 0.006 -0.003" mass="0.008" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:LFJ3" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.349 0.349" user="1115"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_lfknuckle" mesh="robot0:knuckle"></geom>
+                            <body name="robot0:lfproximal" pos="0 0 0">
+                                <inertial pos="0 0 0.023" quat="0.707 -0.004 0.004 0.707" mass="0.014" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:LFJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1114"></joint>
+                                <geom class="robot0:D_Vizual" name="robot0:V_lfproximal" mesh="robot0:F3"></geom>
+                                <geom class="robot0:DC_Hand" name="robot0:C_lfproximal" type="capsule" pos="0 0 0.0225" size="0.01 0.0225"></geom>
+                                <site name="robot0:T_lfproximal_front_left_bottom"  type="box"    pos="-0.005 -0.005 0.00625" size="0.005 0.005 0.01625" rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_lfproximal_front_right_bottom" type="box"    pos="0.005 -0.005 0.00625"  size="0.005 0.005 0.01625" rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_lfproximal_front_left_top"     type="box"    pos="-0.005 -0.005 0.03875" size="0.005 0.005 0.01625" rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_lfproximal_front_right_top"    type="box"    pos="0.005 -0.005 0.03875"  size="0.005 0.005 0.01625" rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_lfproximal_back_left"          type="box"    pos="-0.005 0.005 0.0225"   size="0.005 0.005 0.0325"  rgba="0 0 0 0.33"/>
+                                <site name="robot0:T_lfproximal_back_right"         type="box"    pos="0.005 0.005 0.0225"    size="0.005 0.005 0.0325"  rgba="1 0 0 0.33"/>
+                                <site name="robot0:T_lfproximal_tip"                type="sphere" pos="0 0 0.045"             size="0.011"               rgba="1 1 0 0.33"/>
+                                <body name="robot0:lfmiddle" pos="0 0 0.045">
+                                    <inertial pos="0 0 0.012" quat="0.707 0 0 0.707" mass="0.012" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:LFJ1" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1113"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_lfmiddle" mesh="robot0:F2"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_lfmiddle" type="capsule" pos="0 0 0.0125" size="0.00805 0.0125"></geom>
+                                    <site name="robot0:T_lfmiddle_front_left"  type="box"    pos="-0.00451 -0.00451 0.0125" size="0.00451 0.00451 0.02055" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_lfmiddle_front_right" type="box"    pos="0.00451 -0.00451 0.0125"  size="0.00451 0.00451 0.02055" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_lfmiddle_back_left"   type="box"    pos="-0.00451 0.00451 0.0125"  size="0.00451 0.00451 0.02055" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_lfmiddle_back_right"  type="box"    pos="0.00451 0.00451 0.0125"   size="0.00451 0.00451 0.02055" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_lfmiddle_tip"         type="sphere" pos="0 0 0.025"                size="0.009"                   rgba="1 1 0 0.33"/>
+                                    <body name="robot0:lfdistal" pos="0 0 0.025">
+                                        <inertial pos="0 0 0.015" quat="0.707 -0.003 0.003 0.707" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                        <joint name="robot0:LFJ0" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.571" user="1112"></joint>
+                                        <geom class="robot0:D_Vizual" name="robot0:V_lfdistal" mesh="robot0:F1" pos="0 0 0.001"></geom>
+                                        <geom class="robot0:DC_Hand" name="robot0:C_lfdistal" type="capsule" pos="0 0 0.012" size="0.00705 0.012" condim="4"></geom>
+                                        <site name="robot0:S_lftip" pos="0 0 0.026" group="3"></site>
+                                        <site class="robot0:D_Touch" name="robot0:Tch_lftip"></site>
+                                        <site name="robot0:T_lftip_front_left"  type="box"    pos="-0.00451 -0.00451 0.012" size="0.00451 0.00451 0.01905" rgba="1 0 0 0.33"/>
+                                        <site name="robot0:T_lftip_front_right" type="box"    pos="0.00451 -0.00451 0.012"  size="0.00451 0.00451 0.01905" rgba="0 0 0 0.33"/>
+                                        <site name="robot0:T_lftip_back_left"   type="box"    pos="-0.00451 0.00451 0.012"  size="0.00451 0.00451 0.01905" rgba="0 0 0 0.33"/>
+                                        <site name="robot0:T_lftip_back_right"  type="box"    pos="0.00451 0.00451 0.012"   size="0.00451 0.00451 0.01905" rgba="1 0 0 0.33"/>
+                                        <site name="robot0:T_lftip_tip"         type="sphere" pos="0 0 0.024"               size="0.008"                   rgba="1 1 0 0.33"/>
+                                    </body>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                    <body name="robot0:thbase" pos="0.034 -0.009 0.029" axisangle="0 1 0 0.785">
+                        <inertial pos="0 0 0" mass="0.01" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                        <joint name="robot0:THJ4" type="hinge" pos="0 0 0" axis="0 0 -1" range="-1.047 1.047" user="1121"></joint>
+                        <geom name="robot0:V_thbase" type="box" group="1" pos="0 0 0" size="0.001 0.001 0.001"></geom>
+                        <body name="robot0:thproximal" pos="0 0 0">
+                            <inertial pos="0 0 0.017" quat="0.982 0 0.001 0.191" mass="0.016" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                            <joint name="robot0:THJ3" type="hinge" pos="0 0 0" axis="1 0 0" range="0 1.222" user="1120"></joint>
+                            <geom class="robot0:D_Vizual" name="robot0:V_thproximal" mesh="robot0:TH3_z"></geom>
+                            <geom class="robot0:DC_Hand" name="robot0:C_thproximal" type="capsule" pos="0 0 0.019" size="0.013 0.019" rgba="0.4 0.5 0.6 0.1"></geom>
+                            <site name="robot0:T_thproximal_front_left"  type="box"    pos="-0.007 -0.007 0.019" size="0.007 0.007 0.032" rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_thproximal_front_right" type="box"    pos="0.007 -0.007 0.019"  size="0.007 0.007 0.032" rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_thproximal_back_left"   type="box"    pos="-0.007 0.007 0.019"  size="0.007 0.007 0.032" rgba="1 0 0 0.33"/>
+                            <site name="robot0:T_thproximal_back_right"  type="box"    pos="0.007 0.007 0.019"   size="0.007 0.007 0.032" rgba="0 0 0 0.33"/>
+                            <site name="robot0:T_thproximal_tip"         type="sphere" pos="0 0 0.038"           size="0.014"             rgba="1 1 0 0.33"/>
+                            <body name="robot0:thhub" pos="0 0 0.038">
+                                <inertial pos="0 0 0" mass="0.002" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                <joint name="robot0:THJ2" type="hinge" pos="0 0 0" axis="1 0 0" range="-0.209 0.209" user="1119"></joint>
+                                <geom name="robot0:V_thhub" type="box" group="1" pos="0 0 0" size="0.001 0.001 0.001"></geom>
+                                <body name="robot0:thmiddle" pos="0 0 0">
+                                    <inertial pos="0 0 0.016" quat="1 -0.001 -0.007 0.003" mass="0.016" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                    <joint name="robot0:THJ1" type="hinge" pos="0 0 0" axis="0 1 0" range="-0.524 0.524" user="1118"></joint>
+                                    <geom class="robot0:D_Vizual" name="robot0:V_thmiddle" mesh="robot0:TH2_z"></geom>
+                                    <geom class="robot0:DC_Hand" name="robot0:C_thmiddle" type="capsule" pos="0 0 0.016" size="0.011 0.016"></geom>
+                                    <site name="robot0:T_thmiddle_front_left"  type="box"    pos="-0.006 -0.006 0.016" size="0.006 0.006 0.027" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_thmiddle_front_right" type="box"    pos="0.006 -0.006 0.016"  size="0.006 0.006 0.027" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_thmiddle_back_left"   type="box"    pos="-0.006 0.006 0.016"  size="0.006 0.006 0.027" rgba="0 0 0 0.33"/>
+                                    <site name="robot0:T_thmiddle_back_right"  type="box"    pos="0.006 0.006 0.016"   size="0.006 0.006 0.027" rgba="1 0 0 0.33"/>
+                                    <site name="robot0:T_thmiddle_tip"         type="sphere" pos="0 0 0.032"           size="0.012"             rgba="1 1 0 0.33"/>
+                                    <body name="robot0:thdistal" pos="0 0 0.032">
+                                        <inertial pos="0 0 0.016" quat="0.999 -0.005 -0.047 0.005" mass="0.016" diaginertia="1e-05 1e-05 1e-05"></inertial>
+                                        <joint name="robot0:THJ0" type="hinge" pos="0 0 0" axis="0 1 0" range="-1.571 0" user="1117"></joint>
+                                        <geom class="robot0:D_Vizual" name="robot0:V_thdistal" mesh="robot0:TH1_z"></geom>
+                                        <geom class="robot0:DC_Hand" name="robot0:C_thdistal" type="capsule" pos="0 0 0.013" size="0.00918 0.013" condim="4"></geom>
+                                        <site name="robot0:S_thtip" pos="0 0 0.0275" group="3"></site>
+                                        <site class="robot0:D_Touch" name="robot0:Tch_thtip" size="0.005 0.011 0.016" pos="-0.005 0 0.02"></site>
+                                        <site name="robot0:T_thtip_front_left"  type="box"    pos="-0.0056 -0.0056 0.013" size="0.0056 0.0056 0.02218" rgba="0 0 0 0.33"/>
+                                        <site name="robot0:T_thtip_front_right" type="box"    pos="0.0056 -0.0056 0.013"  size="0.0056 0.0056 0.02218" rgba="1 0 0 0.33"/>
+                                        <site name="robot0:T_thtip_back_left"   type="box"    pos="-0.0056 0.0056 0.013"  size="0.0056 0.0056 0.02218" rgba="1 0 0 0.33"/>
+                                        <site name="robot0:T_thtip_back_right"  type="box"    pos="0.0056 0.0056 0.013"   size="0.0056 0.0056 0.02218" rgba="0 0 0 0.33"/>
+                                        <site name="robot0:T_thtip_tip"         type="sphere" pos="0 0 0.026"             size="0.01"                  rgba="1 1 0 0.33"/>
+                                    </body>
+                                </body>
+                            </body>
+                        </body>
+                    </body>
+                </body>
+            </body>
+        </body>
+    </body>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared.xml b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared.xml
new file mode 100644
index 0000000..f27f265
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared.xml
@@ -0,0 +1,254 @@
+<!-- See LICENSE.md for legal notices. LICENSE.md must be kept together with this file. -->
+<mujoco>
+    <size njmax="500" nconmax="100" nuser_jnt="1" nuser_site="1" nuser_tendon="1" nuser_sensor="1" nuser_actuator="16" nstack="600000"></size>
+
+    <visual>
+        <map fogstart="3" fogend="5" force="0.1"></map>
+        <quality shadowsize="4096"></quality>
+    </visual>
+
+    <default>
+        <default class="robot0:asset_class">
+            <geom friction="1 0.005 0.001" condim="3" margin="0.0005" contype="1" conaffinity="1"></geom>
+            <joint limited="true" damping="0.1" armature="0.001" margin="0.01" frictionloss="0.001"></joint>
+            <site size="0.005" rgba="0.4 0.9 0.4 1"></site>
+            <general ctrllimited="true" forcelimited="true"></general>
+        </default>
+        <default class="robot0:D_Touch">
+            <site type="box" size="0.009 0.004 0.013" pos="0 -0.004 0.018" rgba="0.8 0.8 0.8 0.15" group="4"></site>
+        </default>
+        <default class="robot0:DC_Hand">
+            <geom material="robot0:MatColl" contype="1" conaffinity="0" group="4"></geom>
+        </default>
+        <default class="robot0:D_Vizual">
+            <geom material="robot0:MatViz" contype="0" conaffinity="0" group="1" type="mesh"></geom>
+        </default>
+        <default class="robot0:free">
+            <joint type="free" damping="0" armature="0" limited="false"></joint>
+        </default>
+    </default>
+
+    <contact>
+        <pair geom1="robot0:C_ffdistal" geom2="robot0:C_thdistal" condim="1"></pair>
+        <pair geom1="robot0:C_ffmiddle" geom2="robot0:C_thdistal" condim="1"></pair>
+        <pair geom1="robot0:C_ffproximal" geom2="robot0:C_thdistal" condim="1"></pair>
+        <pair geom1="robot0:C_mfproximal" geom2="robot0:C_thdistal" condim="1"></pair>
+        <pair geom1="robot0:C_mfdistal" geom2="robot0:C_thdistal" condim="1"></pair>
+        <pair geom1="robot0:C_rfdistal" geom2="robot0:C_thdistal" condim="1"></pair>
+        <pair geom1="robot0:C_lfdistal" geom2="robot0:C_thdistal" condim="1"></pair>
+        <pair geom1="robot0:C_palm0" geom2="robot0:C_thdistal" condim="1"></pair>
+        <pair geom1="robot0:C_mfdistal" geom2="robot0:C_ffdistal" condim="1"></pair>
+        <pair geom1="robot0:C_rfdistal" geom2="robot0:C_mfdistal" condim="1"></pair>
+        <pair geom1="robot0:C_lfdistal" geom2="robot0:C_rfdistal" condim="1"></pair>
+        <pair geom1="robot0:C_mfproximal" geom2="robot0:C_ffproximal" condim="1"></pair>
+        <pair geom1="robot0:C_rfproximal" geom2="robot0:C_mfproximal" condim="1"></pair>
+        <pair geom1="robot0:C_lfproximal" geom2="robot0:C_rfproximal" condim="1"></pair>
+        <pair geom1="robot0:C_lfdistal" geom2="robot0:C_rfdistal" condim="1"></pair>
+        <pair geom1="robot0:C_lfdistal" geom2="robot0:C_mfdistal" condim="1"></pair>
+        <pair geom1="robot0:C_lfdistal" geom2="robot0:C_rfmiddle" condim="1"></pair>
+        <pair geom1="robot0:C_lfmiddle" geom2="robot0:C_rfdistal" condim="1"></pair>
+        <pair geom1="robot0:C_lfmiddle" geom2="robot0:C_rfmiddle" condim="1"></pair>
+    </contact>
+
+    <tendon>
+        <fixed name="robot0:T_WRJ1r" limited="true" range="-0.032 0.032" user="1236">
+            <joint joint="robot0:WRJ1" coef="0.0325"></joint>
+        </fixed>
+        <fixed name="robot0:T_WRJ1l" limited="true" range="-0.032 0.032" user="1237">
+            <joint joint="robot0:WRJ1" coef="-0.0325"></joint>
+        </fixed>
+        <fixed name="robot0:T_WRJ0u" limited="true" range="-0.032 0.032" user="1236">
+            <joint joint="robot0:WRJ0" coef="0.0175"></joint>
+        </fixed>
+        <fixed name="robot0:T_WRJ0d" limited="true" range="-0.032 0.032" user="1237">
+            <joint joint="robot0:WRJ0" coef="-0.0175"></joint>
+        </fixed>
+        <fixed name="robot0:T_FFJ3r" limited="true" range="-0.018 0.018" user="1204">
+            <joint joint="robot0:FFJ3" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_FFJ3l" limited="true" range="-0.018 0.018" user="1205">
+            <joint joint="robot0:FFJ3" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_FFJ2u" limited="true" range="-0.007 0.03" user="1202">
+            <joint joint="robot0:FFJ2" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_FFJ2d" limited="true" range="-0.03 0.007" user="1203">
+            <joint joint="robot0:FFJ2" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_FFJ1c" limited="true" range="-0.001 0.001">
+            <joint joint="robot0:FFJ0" coef="0.00705"></joint>
+            <joint joint="robot0:FFJ1" coef="-0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_FFJ1u" limited="true" range="-0.007 0.03" user="1200">
+            <joint joint="robot0:FFJ0" coef="0.00705"></joint>
+            <joint joint="robot0:FFJ1" coef="0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_FFJ1d" limited="true" range="-0.03 0.007" user="1201">
+            <joint joint="robot0:FFJ0" coef="-0.00705"></joint>
+            <joint joint="robot0:FFJ1" coef="-0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_MFJ3r" limited="true" range="-0.018 0.018" user="1210">
+            <joint joint="robot0:MFJ3" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_MFJ3l" limited="true" range="-0.018 0.018" user="1211">
+            <joint joint="robot0:MFJ3" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_MFJ2u" limited="true" range="-0.007 0.03" user="1208">
+            <joint joint="robot0:MFJ2" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_MFJ2d" limited="true" range="-0.03 0.007" user="1209">
+            <joint joint="robot0:MFJ2" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_MFJ1c" limited="true" range="-0.001 0.001">
+            <joint joint="robot0:MFJ0" coef="0.00705"></joint>
+            <joint joint="robot0:MFJ1" coef="-0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_MFJ1u" limited="true" range="-0.007 0.03" user="1206">
+            <joint joint="robot0:MFJ0" coef="0.00705"></joint>
+            <joint joint="robot0:MFJ1" coef="0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_MFJ1d" limited="true" range="-0.03 0.007" user="1207">
+            <joint joint="robot0:MFJ0" coef="-0.00705"></joint>
+            <joint joint="robot0:MFJ1" coef="-0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_RFJ3r" limited="true" range="-0.018 0.018" user="1216">
+            <joint joint="robot0:RFJ3" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_RFJ3l" limited="true" range="-0.018 0.018" user="1217">
+            <joint joint="robot0:RFJ3" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_RFJ2u" limited="true" range="-0.007 0.03" user="1214">
+            <joint joint="robot0:RFJ2" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_RFJ2d" limited="true" range="-0.03 0.007" user="1215">
+            <joint joint="robot0:RFJ2" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_RFJ1c" limited="true" range="-0.001 0.001">
+            <joint joint="robot0:RFJ0" coef="0.00705"></joint>
+            <joint joint="robot0:RFJ1" coef="-0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_RFJ1u" limited="true" range="-0.007 0.03" user="1212">
+            <joint joint="robot0:RFJ0" coef="0.00705"></joint>
+            <joint joint="robot0:RFJ1" coef="0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_RFJ1d" limited="true" range="-0.03 0.007" user="1213">
+            <joint joint="robot0:RFJ0" coef="-0.00705"></joint>
+            <joint joint="robot0:RFJ1" coef="-0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ4u" limited="true" range="-0.007 0.03" user="1224">
+            <joint joint="robot0:LFJ4" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ4d" limited="true" range="-0.03 0.007" user="1225">
+            <joint joint="robot0:LFJ4" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ3r" limited="true" range="-0.018 0.018" user="1222">
+            <joint joint="robot0:LFJ3" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ3l" limited="true" range="-0.018 0.018" user="1223">
+            <joint joint="robot0:LFJ3" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ2u" limited="true" range="-0.007 0.03" user="1220">
+            <joint joint="robot0:LFJ2" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ2d" limited="true" range="-0.03 0.007" user="1221">
+            <joint joint="robot0:LFJ2" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ1c" limited="true" range="-0.001 0.001">
+            <joint joint="robot0:LFJ0" coef="0.00705"></joint>
+            <joint joint="robot0:LFJ1" coef="-0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ1u" limited="true" range="-0.007 0.03" user="1218">
+            <joint joint="robot0:LFJ0" coef="0.00705"></joint>
+            <joint joint="robot0:LFJ1" coef="0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_LFJ1d" limited="true" range="-0.03 0.007" user="1219">
+            <joint joint="robot0:LFJ0" coef="-0.00705"></joint>
+            <joint joint="robot0:LFJ1" coef="-0.00805"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ4a" limited="true" range="-0.018 0.018" user="1234">
+            <joint joint="robot0:THJ4" coef="0.01636"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ4c" limited="true" range="-0.018 0.018" user="1235">
+            <joint joint="robot0:THJ4" coef="-0.01636"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ3u" limited="true" range="-0.007 0.03" user="1232">
+            <joint joint="robot0:THJ3" coef="0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ3d" limited="true" range="-0.03 0.007" user="1233">
+            <joint joint="robot0:THJ3" coef="-0.01"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ2u" limited="true" range="-0.018 0.018" user="1230">
+            <joint joint="robot0:THJ2" coef="0.011"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ2d" limited="true" range="-0.018 0.018" user="1231">
+            <joint joint="robot0:THJ2" coef="-0.011"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ1r" limited="true" range="-0.018 0.018" user="1228">
+            <joint joint="robot0:THJ1" coef="0.011"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ1l" limited="true" range="-0.018 0.018" user="1229">
+            <joint joint="robot0:THJ1" coef="-0.011"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ0r" limited="true" range="-0.03 0.007" user="1226">
+            <joint joint="robot0:THJ0" coef="0.009"></joint>
+        </fixed>
+        <fixed name="robot0:T_THJ0l" limited="true" range="-0.007 0.03" user="1227">
+            <joint joint="robot0:THJ0" coef="-0.009"></joint>
+        </fixed>
+    </tendon>
+
+    <sensor>
+        <jointpos name="robot0:Sjp_WRJ1" joint="robot0:WRJ1"></jointpos>
+        <jointpos name="robot0:Sjp_WRJ0" joint="robot0:WRJ0"></jointpos>
+        <jointpos name="robot0:Sjp_FFJ3" joint="robot0:FFJ3"></jointpos>
+        <jointpos name="robot0:Sjp_FFJ2" joint="robot0:FFJ2"></jointpos>
+        <jointpos name="robot0:Sjp_FFJ1" joint="robot0:FFJ1"></jointpos>
+        <jointpos name="robot0:Sjp_FFJ0" joint="robot0:FFJ0"></jointpos>
+        <jointpos name="robot0:Sjp_MFJ3" joint="robot0:MFJ3"></jointpos>
+        <jointpos name="robot0:Sjp_MFJ2" joint="robot0:MFJ2"></jointpos>
+        <jointpos name="robot0:Sjp_MFJ1" joint="robot0:MFJ1"></jointpos>
+        <jointpos name="robot0:Sjp_MFJ0" joint="robot0:MFJ0"></jointpos>
+        <jointpos name="robot0:Sjp_RFJ3" joint="robot0:RFJ3"></jointpos>
+        <jointpos name="robot0:Sjp_RFJ2" joint="robot0:RFJ2"></jointpos>
+        <jointpos name="robot0:Sjp_RFJ1" joint="robot0:RFJ1"></jointpos>
+        <jointpos name="robot0:Sjp_RFJ0" joint="robot0:RFJ0"></jointpos>
+        <jointpos name="robot0:Sjp_LFJ4" joint="robot0:LFJ4"></jointpos>
+        <jointpos name="robot0:Sjp_LFJ3" joint="robot0:LFJ3"></jointpos>
+        <jointpos name="robot0:Sjp_LFJ2" joint="robot0:LFJ2"></jointpos>
+        <jointpos name="robot0:Sjp_LFJ1" joint="robot0:LFJ1"></jointpos>
+        <jointpos name="robot0:Sjp_LFJ0" joint="robot0:LFJ0"></jointpos>
+        <jointpos name="robot0:Sjp_THJ4" joint="robot0:THJ4"></jointpos>
+        <jointpos name="robot0:Sjp_THJ3" joint="robot0:THJ3"></jointpos>
+        <jointpos name="robot0:Sjp_THJ2" joint="robot0:THJ2"></jointpos>
+        <jointpos name="robot0:Sjp_THJ1" joint="robot0:THJ1"></jointpos>
+        <jointpos name="robot0:Sjp_THJ0" joint="robot0:THJ0"></jointpos>
+        <touch name="robot0:ST_Tch_fftip" site="robot0:Tch_fftip"></touch>
+        <touch name="robot0:ST_Tch_mftip" site="robot0:Tch_mftip"></touch>
+        <touch name="robot0:ST_Tch_rftip" site="robot0:Tch_rftip"></touch>
+        <touch name="robot0:ST_Tch_lftip" site="robot0:Tch_lftip"></touch>
+        <touch name="robot0:ST_Tch_thtip" site="robot0:Tch_thtip"></touch>
+    </sensor>
+
+    <actuator>
+        <position name="robot0:A_WRJ1" class="robot0:asset_class" user="2038" joint="robot0:WRJ1" ctrlrange="-0.489 0.14" kp="5" forcerange="-4.785 4.785"></position>
+        <position name="robot0:A_WRJ0" class="robot0:asset_class" user="2036" joint="robot0:WRJ0" ctrlrange="-0.698 0.489" kp="5" forcerange="-2.175 2.175"></position>
+        <position name="robot0:A_FFJ3" class="robot0:asset_class" user="2004" joint="robot0:FFJ3" ctrlrange="-0.349 0.349" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_FFJ2" class="robot0:asset_class" user="2002" joint="robot0:FFJ2" ctrlrange="0 1.571" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_FFJ1" class="robot0:asset_class" user="2000" joint="robot0:FFJ1" ctrlrange="0 1.571" kp="1" forcerange="-0.7245 0.7245"></position>
+        <position name="robot0:A_MFJ3" class="robot0:asset_class" user="2010" joint="robot0:MFJ3" ctrlrange="-0.349 0.349" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_MFJ2" class="robot0:asset_class" user="2008" joint="robot0:MFJ2" ctrlrange="0 1.571" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_MFJ1" class="robot0:asset_class" user="2006" joint="robot0:MFJ1" ctrlrange="0 1.571" kp="1" forcerange="-0.7245 0.7245"></position>
+        <position name="robot0:A_RFJ3" class="robot0:asset_class" user="2016" joint="robot0:RFJ3" ctrlrange="-0.349 0.349" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_RFJ2" class="robot0:asset_class" user="2014" joint="robot0:RFJ2" ctrlrange="0 1.571" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_RFJ1" class="robot0:asset_class" user="2012" joint="robot0:RFJ1" ctrlrange="0 1.571" kp="1" forcerange="-0.7245 0.7245"></position>
+        <position name="robot0:A_LFJ4" class="robot0:asset_class" user="2024" joint="robot0:LFJ4" ctrlrange="0 0.785" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_LFJ3" class="robot0:asset_class" user="2022" joint="robot0:LFJ3" ctrlrange="-0.349 0.349" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_LFJ2" class="robot0:asset_class" user="2020" joint="robot0:LFJ2" ctrlrange="0 1.571" kp="1" forcerange="-0.9 0.9"></position>
+        <position name="robot0:A_LFJ1" class="robot0:asset_class" user="2018" joint="robot0:LFJ1" ctrlrange="0 1.571" kp="1" forcerange="-0.7245 0.7245"></position>
+        <position name="robot0:A_THJ4" class="robot0:asset_class" user="2034" joint="robot0:THJ4" ctrlrange="-1.047 1.047" kp="1" forcerange="-2.3722 2.3722"></position>
+        <position name="robot0:A_THJ3" class="robot0:asset_class" user="2032" joint="robot0:THJ3" ctrlrange="0 1.222" kp="1" forcerange="-1.45 1.45"></position>
+        <position name="robot0:A_THJ2" class="robot0:asset_class" user="2030" joint="robot0:THJ2" ctrlrange="-0.209 0.209" kp="1" forcerange="-0.99 0.99"></position>
+        <position name="robot0:A_THJ1" class="robot0:asset_class" user="2028" joint="robot0:THJ1" ctrlrange="-0.524 0.524" kp="1" forcerange="-0.99 0.99"></position>
+        <position name="robot0:A_THJ0" class="robot0:asset_class" user="2026" joint="robot0:THJ0" ctrlrange="-1.571 0" kp="1" forcerange="-0.81 0.81"></position>
+    </actuator>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared_asset.xml b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared_asset.xml
new file mode 100644
index 0000000..ec9a0b0
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared_asset.xml
@@ -0,0 +1,26 @@
+<!-- See LICENSE.md for legal notices. LICENSE.md must be kept together with this file. -->
+<mujoco>
+    <texture type="skybox" builtin="gradient" rgb1="0.44 0.85 0.56" rgb2="0.46 0.87 0.58" width="32" height="32"></texture>
+
+    <texture name="robot0:texplane" type="2d" builtin="checker" rgb1="0.2 0.3 0.4" rgb2="0.1 0.15 0.2" width="512" height="512"></texture>
+    <texture name="robot0:texgeom" type="cube" builtin="flat" mark="cross" width="127" height="127" rgb1="0.3 0.6 0.5" rgb2="0.3 0.6 0.5" markrgb="0 0 0" random="0.01"></texture>
+
+    <material name="robot0:MatGnd" reflectance="0.5" texture="robot0:texplane" texrepeat="1 1" texuniform="true"></material>
+    <material name="robot0:MatColl" specular="1" shininess="0.3" reflectance="0.5" rgba="0.4 0.5 0.6 1"></material>
+    <material name="robot0:MatViz" specular="0.75" shininess="0.1" reflectance="0.5" rgba="0.93 0.93 0.93 1"></material>
+    <material name="robot0:object" texture="robot0:texgeom" texuniform="false"></material>
+    <material name="floor_mat" specular="0" shininess="0.5" reflectance="0" rgba="0.2 0.2 0.2 0"></material>
+
+    <mesh name="robot0:forearm" file="forearm_electric.stl"></mesh>
+    <mesh name="robot0:forearm_cvx" file="forearm_electric_cvx.stl"></mesh>
+    <mesh name="robot0:wrist" scale="0.001 0.001 0.001" file="wrist.stl"></mesh>
+    <mesh name="robot0:palm" scale="0.001 0.001 0.001" file="palm.stl"></mesh>
+    <mesh name="robot0:knuckle" scale="0.001 0.001 0.001" file="knuckle.stl"></mesh>
+    <mesh name="robot0:F3" scale="0.001 0.001 0.001" file="F3.stl"></mesh>
+    <mesh name="robot0:F2" scale="0.001 0.001 0.001" file="F2.stl"></mesh>
+    <mesh name="robot0:F1" scale="0.001 0.001 0.001" file="F1.stl"></mesh>
+    <mesh name="robot0:lfmetacarpal" scale="0.001 0.001 0.001" file="lfmetacarpal.stl"></mesh>
+    <mesh name="robot0:TH3_z" scale="0.001 0.001 0.001" file="TH3_z.stl"></mesh>
+    <mesh name="robot0:TH2_z" scale="0.001 0.001 0.001" file="TH2_z.stl"></mesh>
+    <mesh name="robot0:TH1_z" scale="0.001 0.001 0.001" file="TH1_z.stl"></mesh>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared_touch_sensors_92.xml b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared_touch_sensors_92.xml
new file mode 100644
index 0000000..472c84c
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/hand/shared_touch_sensors_92.xml
@@ -0,0 +1,120 @@
+<mujoco>
+    <sensor>
+
+        <!--PALM-->
+        <touch name="robot0:TS_palm_b0" site="robot0:T_palm_b0"></touch>
+        <touch name="robot0:TS_palm_bl" site="robot0:T_palm_bl"></touch>
+        <touch name="robot0:TS_palm_bm" site="robot0:T_palm_bm"></touch>
+        <touch name="robot0:TS_palm_br" site="robot0:T_palm_br"></touch>
+        <touch name="robot0:TS_palm_fl" site="robot0:T_palm_fl"></touch>
+        <touch name="robot0:TS_palm_fm" site="robot0:T_palm_fm"></touch>
+        <touch name="robot0:TS_palm_fr" site="robot0:T_palm_fr"></touch>
+        <touch name="robot0:TS_palm_b1" site="robot0:T_palm_b1"></touch>
+
+         <!--FOREFINGER-->
+        <touch name="robot0:TS_ffproximal_front_left_bottom"  site="robot0:T_ffproximal_front_left_bottom"></touch>
+        <touch name="robot0:TS_ffproximal_front_right_bottom" site="robot0:T_ffproximal_front_right_bottom"></touch>
+        <touch name="robot0:TS_ffproximal_front_left_top"     site="robot0:T_ffproximal_front_left_top"></touch>
+        <touch name="robot0:TS_ffproximal_front_right_top"    site="robot0:T_ffproximal_front_right_top"></touch>
+        <touch name="robot0:TS_ffproximal_back_left"          site="robot0:T_ffproximal_back_left"></touch>
+        <touch name="robot0:TS_ffproximal_back_right"         site="robot0:T_ffproximal_back_right"></touch>
+        <touch name="robot0:TS_ffproximal_tip"                site="robot0:T_ffproximal_tip"></touch>
+
+        <touch name="robot0:TS_ffmiddle_front_left"  site="robot0:T_ffmiddle_front_left"></touch>
+        <touch name="robot0:TS_ffmiddle_front_right" site="robot0:T_ffmiddle_front_right"></touch>
+        <touch name="robot0:TS_ffmiddle_back_left"   site="robot0:T_ffmiddle_back_left"></touch>
+        <touch name="robot0:TS_ffmiddle_back_right"  site="robot0:T_ffmiddle_back_right"></touch>
+        <touch name="robot0:TS_ffmiddle_tip"         site="robot0:T_ffmiddle_tip"></touch>
+
+        <touch name="robot0:TS_fftip_front_left"  site="robot0:T_fftip_front_left"></touch>
+        <touch name="robot0:TS_fftip_front_right" site="robot0:T_fftip_front_right"></touch>
+        <touch name="robot0:TS_fftip_back_left"   site="robot0:T_fftip_back_left"></touch>
+        <touch name="robot0:TS_fftip_back_right"  site="robot0:T_fftip_back_right"></touch>
+        <touch name="robot0:TS_fftip_tip"         site="robot0:T_fftip_tip"></touch>
+
+        <!-- MIDDLE FINGER -->
+        <touch name="robot0:TS_mfproximal_front_left_bottom"  site="robot0:T_mfproximal_front_left_bottom"></touch>
+        <touch name="robot0:TS_mfproximal_front_right_bottom" site="robot0:T_mfproximal_front_right_bottom"></touch>
+        <touch name="robot0:TS_mfproximal_front_left_top"     site="robot0:T_mfproximal_front_left_top"></touch>
+        <touch name="robot0:TS_mfproximal_front_right_top"    site="robot0:T_mfproximal_front_right_top"></touch>
+        <touch name="robot0:TS_mfproximal_back_left"          site="robot0:T_mfproximal_back_left"></touch>
+        <touch name="robot0:TS_mfproximal_back_right"         site="robot0:T_mfproximal_back_right"></touch>
+        <touch name="robot0:TS_mfproximal_tip"                site="robot0:T_mfproximal_tip"></touch>
+
+        <touch name="robot0:TS_mfmiddle_front_left"  site="robot0:T_mfmiddle_front_left"></touch>
+        <touch name="robot0:TS_mfmiddle_front_right" site="robot0:T_mfmiddle_front_right"></touch>
+        <touch name="robot0:TS_mfmiddle_back_left"   site="robot0:T_mfmiddle_back_left"></touch>
+        <touch name="robot0:TS_mfmiddle_back_right"  site="robot0:T_mfmiddle_back_right"></touch>
+        <touch name="robot0:TS_mfmiddle_tip"         site="robot0:T_mfmiddle_tip"></touch>
+
+        <touch name="robot0:TS_mftip_front_left"  site="robot0:T_mftip_front_left"></touch>
+        <touch name="robot0:TS_mftip_front_right" site="robot0:T_mftip_front_right"></touch>
+        <touch name="robot0:TS_mftip_back_left"   site="robot0:T_mftip_back_left"></touch>
+        <touch name="robot0:TS_mftip_back_right"  site="robot0:T_mftip_back_right"></touch>
+        <touch name="robot0:TS_mftip_tip"         site="robot0:T_mftip_tip"></touch>
+
+        <!-- RING FINGER -->
+        <touch name="robot0:TS_rfproximal_front_left_bottom"  site="robot0:T_rfproximal_front_left_bottom"></touch>
+        <touch name="robot0:TS_rfproximal_front_right_bottom" site="robot0:T_rfproximal_front_right_bottom"></touch>
+        <touch name="robot0:TS_rfproximal_front_left_top"     site="robot0:T_rfproximal_front_left_top"></touch>
+        <touch name="robot0:TS_rfproximal_front_right_top"    site="robot0:T_rfproximal_front_right_top"></touch>
+        <touch name="robot0:TS_rfproximal_back_left"          site="robot0:T_rfproximal_back_left"></touch>
+        <touch name="robot0:TS_rfproximal_back_right"         site="robot0:T_rfproximal_back_right"></touch>
+        <touch name="robot0:TS_rfproximal_tip"                site="robot0:T_rfproximal_tip"></touch>
+
+        <touch name="robot0:TS_rfmiddle_front_left"  site="robot0:T_rfmiddle_front_left"></touch>
+        <touch name="robot0:TS_rfmiddle_front_right" site="robot0:T_rfmiddle_front_right"></touch>
+        <touch name="robot0:TS_rfmiddle_back_left"   site="robot0:T_rfmiddle_back_left"></touch>
+        <touch name="robot0:TS_rfmiddle_back_right"  site="robot0:T_rfmiddle_back_right"></touch>
+        <touch name="robot0:TS_rfmiddle_tip"         site="robot0:T_rfmiddle_tip"></touch>
+
+        <touch name="robot0:TS_rftip_front_left"  site="robot0:T_rftip_front_left"></touch>
+        <touch name="robot0:TS_rftip_front_right" site="robot0:T_rftip_front_right"></touch>
+        <touch name="robot0:TS_rftip_back_left"   site="robot0:T_rftip_back_left"></touch>
+        <touch name="robot0:TS_rftip_back_right"  site="robot0:T_rftip_back_right"></touch>
+        <touch name="robot0:TS_rftip_tip"         site="robot0:T_rftip_tip"></touch>
+
+        <!-- LITTLE FINGER -->
+        <touch name="robot0:TS_lfmetacarpal_front" site="robot0:T_lfmetacarpal_front"></touch>
+
+        <touch name="robot0:TS_lfproximal_front_left_bottom"  site="robot0:T_lfproximal_front_left_bottom"></touch>
+        <touch name="robot0:TS_lfproximal_front_right_bottom" site="robot0:T_lfproximal_front_right_bottom"></touch>
+        <touch name="robot0:TS_lfproximal_front_left_top"     site="robot0:T_lfproximal_front_left_top"></touch>
+        <touch name="robot0:TS_lfproximal_front_right_top"    site="robot0:T_lfproximal_front_right_top"></touch>
+        <touch name="robot0:TS_lfproximal_back_left"          site="robot0:T_lfproximal_back_left"></touch>
+        <touch name="robot0:TS_lfproximal_back_right"         site="robot0:T_lfproximal_back_right"></touch>
+        <touch name="robot0:TS_lfproximal_tip"                site="robot0:T_lfproximal_tip"></touch>
+
+        <touch name="robot0:TS_lfmiddle_front_left"  site="robot0:T_lfmiddle_front_left"></touch>
+        <touch name="robot0:TS_lfmiddle_front_right" site="robot0:T_lfmiddle_front_right"></touch>
+        <touch name="robot0:TS_lfmiddle_back_left"   site="robot0:T_lfmiddle_back_left"></touch>
+        <touch name="robot0:TS_lfmiddle_back_right"  site="robot0:T_lfmiddle_back_right"></touch>
+        <touch name="robot0:TS_lfmiddle_tip"         site="robot0:T_lfmiddle_tip"></touch>
+
+        <touch name="robot0:TS_lftip_front_left"  site="robot0:T_lftip_front_left"></touch>
+        <touch name="robot0:TS_lftip_front_right" site="robot0:T_lftip_front_right"></touch>
+        <touch name="robot0:TS_lftip_back_left"   site="robot0:T_lftip_back_left"></touch>
+        <touch name="robot0:TS_lftip_back_right"  site="robot0:T_lftip_back_right"></touch>
+        <touch name="robot0:TS_lftip_tip"         site="robot0:T_lftip_tip"></touch>
+
+        <!--THUMB-->
+        <touch name="robot0:TS_thproximal_front_left"  site="robot0:T_thproximal_front_left"></touch>
+        <touch name="robot0:TS_thproximal_front_right" site="robot0:T_thproximal_front_right"></touch>
+        <touch name="robot0:TS_thproximal_back_left"   site="robot0:T_thproximal_back_left"></touch>
+        <touch name="robot0:TS_thproximal_back_right"  site="robot0:T_thproximal_back_right"></touch>
+        <touch name="robot0:TS_thproximal_tip"         site="robot0:T_thproximal_tip"></touch>
+
+        <touch name="robot0:TS_thmiddle_front_left"  site="robot0:T_thmiddle_front_left"></touch>
+        <touch name="robot0:TS_thmiddle_front_right" site="robot0:T_thmiddle_front_right"></touch>
+        <touch name="robot0:TS_thmiddle_back_left"   site="robot0:T_thmiddle_back_left"></touch>
+        <touch name="robot0:TS_thmiddle_back_right"  site="robot0:T_thmiddle_back_right"></touch>
+        <touch name="robot0:TS_thmiddle_tip"         site="robot0:T_thmiddle_tip"></touch>
+
+        <touch name="robot0:TS_thtip_front_left"  site="robot0:T_thtip_front_left"></touch>
+        <touch name="robot0:TS_thtip_front_right" site="robot0:T_thtip_front_right"></touch>
+        <touch name="robot0:TS_thtip_back_left"   site="robot0:T_thtip_back_left"></touch>
+        <touch name="robot0:TS_thtip_back_right"  site="robot0:T_thtip_back_right"></touch>
+        <touch name="robot0:TS_thtip_tip"         site="robot0:T_thtip_tip"></touch>
+
+    </sensor>
+</mujoco>
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/base_link_collision.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/base_link_collision.stl
new file mode 100644
index 0000000..1ef459f
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/base_link_collision.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/bellows_link_collision.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/bellows_link_collision.stl
new file mode 100644
index 0000000..a7e5ab7
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/bellows_link_collision.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/elbow_flex_link_collision.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/elbow_flex_link_collision.stl
new file mode 100644
index 0000000..b0eea07
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/elbow_flex_link_collision.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/estop_link.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/estop_link.stl
new file mode 100644
index 0000000..f6d1c72
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/estop_link.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/forearm_roll_link_collision.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/forearm_roll_link_collision.stl
new file mode 100644
index 0000000..fe468c5
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/forearm_roll_link_collision.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/gripper_link.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/gripper_link.stl
new file mode 100644
index 0000000..8a14874
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/gripper_link.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/head_pan_link_collision.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/head_pan_link_collision.stl
new file mode 100644
index 0000000..c77b5b1
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/head_pan_link_collision.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/head_tilt_link_collision.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/head_tilt_link_collision.stl
new file mode 100644
index 0000000..53c2ddc
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/head_tilt_link_collision.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/l_wheel_link_collision.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/l_wheel_link_collision.stl
new file mode 100644
index 0000000..5c17524
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/l_wheel_link_collision.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/laser_link.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/laser_link.stl
new file mode 100644
index 0000000..fa4882f
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/laser_link.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/r_wheel_link_collision.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/r_wheel_link_collision.stl
new file mode 100644
index 0000000..3742b24
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/r_wheel_link_collision.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/shoulder_lift_link_collision.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/shoulder_lift_link_collision.stl
new file mode 100644
index 0000000..c9aff0d
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/shoulder_lift_link_collision.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/shoulder_pan_link_collision.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/shoulder_pan_link_collision.stl
new file mode 100644
index 0000000..ac17a94
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/shoulder_pan_link_collision.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/torso_fixed_link.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/torso_fixed_link.stl
new file mode 100644
index 0000000..7cf7fc1
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/torso_fixed_link.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/torso_lift_link_collision.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/torso_lift_link_collision.stl
new file mode 100644
index 0000000..4ce5fcf
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/torso_lift_link_collision.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/upperarm_roll_link_collision.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/upperarm_roll_link_collision.stl
new file mode 100644
index 0000000..1207932
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/upperarm_roll_link_collision.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/wrist_flex_link_collision.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/wrist_flex_link_collision.stl
new file mode 100644
index 0000000..3215d2e
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/wrist_flex_link_collision.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/wrist_roll_link_collision.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/wrist_roll_link_collision.stl
new file mode 100644
index 0000000..742bdd9
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/fetch/wrist_roll_link_collision.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F1.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F1.stl
new file mode 100644
index 0000000..515d3c9
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F1.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F2.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F2.stl
new file mode 100644
index 0000000..7bc5e20
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F2.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F3.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F3.stl
new file mode 100644
index 0000000..223f06f
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/F3.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH1_z.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH1_z.stl
new file mode 100644
index 0000000..400ee2d
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH1_z.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH2_z.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH2_z.stl
new file mode 100644
index 0000000..5ace838
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH2_z.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH3_z.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH3_z.stl
new file mode 100644
index 0000000..23485ab
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/TH3_z.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/forearm_electric.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/forearm_electric.stl
new file mode 100644
index 0000000..80f6f3d
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/forearm_electric.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/forearm_electric_cvx.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/forearm_electric_cvx.stl
new file mode 100644
index 0000000..3c30f57
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/forearm_electric_cvx.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/knuckle.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/knuckle.stl
new file mode 100644
index 0000000..4faedd7
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/knuckle.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/lfmetacarpal.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/lfmetacarpal.stl
new file mode 100644
index 0000000..535cf4d
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/lfmetacarpal.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/palm.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/palm.stl
new file mode 100644
index 0000000..65e47eb
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/palm.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/wrist.stl b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/wrist.stl
new file mode 100644
index 0000000..420d5f9
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/stls/hand/wrist.stl differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/textures/block.png b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/textures/block.png
new file mode 100644
index 0000000..0243b8f
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/textures/block.png differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/textures/block_hidden.png b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/textures/block_hidden.png
new file mode 100644
index 0000000..e08b861
Binary files /dev/null and b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/assets/textures/block_hidden.png differ
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/__init__.py b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/pick_and_place.py b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/pick_and_place.py
new file mode 100644
index 0000000..c6c5e7e
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/pick_and_place.py
@@ -0,0 +1,23 @@
+import os
+from gym import utils
+from gym.envs.robotics import fetch_env
+
+
+# Ensure we get the path separator correct on windows
+MODEL_XML_PATH = os.path.join('fetch', 'pick_and_place.xml')
+
+
+class FetchPickAndPlaceEnv(fetch_env.FetchEnv, utils.EzPickle):
+    def __init__(self, reward_type='sparse'):
+        initial_qpos = {
+            'robot0:slide0': 0.405,
+            'robot0:slide1': 0.48,
+            'robot0:slide2': 0.0,
+            'object0:joint': [1.25, 0.53, 0.4, 1., 0., 0., 0.],
+        }
+        fetch_env.FetchEnv.__init__(
+            self, MODEL_XML_PATH, has_object=True, block_gripper=False, n_substeps=20,
+            gripper_extra_height=0.2, target_in_the_air=True, target_offset=0.0,
+            obj_range=0.15, target_range=0.15, distance_threshold=0.05,
+            initial_qpos=initial_qpos, reward_type=reward_type)
+        utils.EzPickle.__init__(self)
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/push.py b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/push.py
new file mode 100644
index 0000000..043d101
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/push.py
@@ -0,0 +1,23 @@
+import os
+from gym import utils
+from slbo.envs.robotics import fetch_env
+
+
+# Ensure we get the path separator correct on windows
+MODEL_XML_PATH = os.path.join('fetch', 'push.xml')
+
+
+class FetchPushEnv(fetch_env.FetchEnv, utils.EzPickle):
+    def __init__(self, reward_type='sparse'):
+        initial_qpos = {
+            'robot0:slide0': 0.405,
+            'robot0:slide1': 0.48,
+            'robot0:slide2': 0.0,
+            'object0:joint': [1.25, 0.53, 0.4, 1., 0., 0., 0.],
+        }
+        fetch_env.FetchEnv.__init__(
+            self, MODEL_XML_PATH, has_object=True, block_gripper=True, n_substeps=20,
+            gripper_extra_height=0.0, target_in_the_air=False, target_offset=0.0,
+            obj_range=0.15, target_range=0.15, distance_threshold=0.05,
+            initial_qpos=initial_qpos, reward_type=reward_type)
+        utils.EzPickle.__init__(self)
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/reach.py b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/reach.py
new file mode 100644
index 0000000..cc3fc46
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/reach.py
@@ -0,0 +1,22 @@
+import os
+from gym import utils
+from gym.envs.robotics import fetch_env
+
+
+# Ensure we get the path separator correct on windows
+MODEL_XML_PATH = os.path.join('fetch', 'reach.xml')
+
+
+class FetchReachEnv(fetch_env.FetchEnv, utils.EzPickle):
+    def __init__(self, reward_type='sparse'):
+        initial_qpos = {
+            'robot0:slide0': 0.4049,
+            'robot0:slide1': 0.48,
+            'robot0:slide2': 0.0,
+        }
+        fetch_env.FetchEnv.__init__(
+            self, MODEL_XML_PATH, has_object=False, block_gripper=True, n_substeps=20,
+            gripper_extra_height=0.2, target_in_the_air=True, target_offset=0.0,
+            obj_range=0.15, target_range=0.15, distance_threshold=0.05,
+            initial_qpos=initial_qpos, reward_type=reward_type)
+        utils.EzPickle.__init__(self)
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/slide.py b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/slide.py
new file mode 100644
index 0000000..63234db
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch/slide.py
@@ -0,0 +1,25 @@
+import os
+import numpy as np
+
+from gym import utils
+from gym.envs.robotics import fetch_env
+
+
+# Ensure we get the path separator correct on windows
+MODEL_XML_PATH = os.path.join('fetch', 'slide.xml')
+
+
+class FetchSlideEnv(fetch_env.FetchEnv, utils.EzPickle):
+    def __init__(self, reward_type='sparse'):
+        initial_qpos = {
+            'robot0:slide0': 0.05,
+            'robot0:slide1': 0.48,
+            'robot0:slide2': 0.0,
+            'object0:joint': [1.7, 1.1, 0.41, 1., 0., 0., 0.],
+        }
+        fetch_env.FetchEnv.__init__(
+            self, MODEL_XML_PATH, has_object=True, block_gripper=True, n_substeps=20,
+            gripper_extra_height=-0.02, target_in_the_air=False, target_offset=np.array([0.4, 0.0, 0.0]),
+            obj_range=0.1, target_range=0.3, distance_threshold=0.05,
+            initial_qpos=initial_qpos, reward_type=reward_type)
+        utils.EzPickle.__init__(self)
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch_env.py b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch_env.py
new file mode 100644
index 0000000..8e19c42
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/fetch_env.py
@@ -0,0 +1,197 @@
+import numpy as np
+
+from gym.envs.robotics import rotations, robot_env, utils
+
+
+def goal_distance(goal_a, goal_b):
+    assert goal_a.shape == goal_b.shape
+    return np.linalg.norm(goal_a - goal_b, axis=-1)
+
+
+class FetchEnv(robot_env.RobotEnv):
+    """Superclass for all Fetch environments.
+    """
+
+    def __init__(
+        self, model_path, n_substeps, gripper_extra_height, block_gripper,
+        has_object, target_in_the_air, target_offset, obj_range, target_range,
+        distance_threshold, initial_qpos, reward_type,
+    ):
+        """Initializes a new Fetch environment.
+
+        Args:
+            model_path (string): path to the environments XML file
+            n_substeps (int): number of substeps the simulation runs on every call to step
+            gripper_extra_height (float): additional height above the table when positioning the gripper
+            block_gripper (boolean): whether or not the gripper is blocked (i.e. not movable) or not
+            has_object (boolean): whether or not the environment has an object
+            target_in_the_air (boolean): whether or not the target should be in the air above the table or on the table surface
+            target_offset (float or array with 3 elements): offset of the target
+            obj_range (float): range of a uniform distribution for sampling initial object positions
+            target_range (float): range of a uniform distribution for sampling a target
+            distance_threshold (float): the threshold after which a goal is considered achieved
+            initial_qpos (dict): a dictionary of joint names and values that define the initial configuration
+            reward_type ('sparse' or 'dense'): the reward type, i.e. sparse or dense
+        """
+        self.gripper_extra_height = gripper_extra_height
+        self.block_gripper = block_gripper
+        self.has_object = has_object
+        self.target_in_the_air = target_in_the_air
+        self.target_offset = target_offset
+        self.obj_range = obj_range
+        self.target_range = target_range
+        self.distance_threshold = distance_threshold
+        self.reward_type = reward_type
+
+        super(FetchEnv, self).__init__(
+            model_path=model_path, n_substeps=n_substeps, n_actions=4,
+            initial_qpos=initial_qpos)
+
+    # GoalEnv methods
+    # ----------------------------
+
+    def compute_reward(self, achieved_goal, goal, info):
+        # Compute distance between goal and the achieved goal.
+        d = goal_distance(achieved_goal, goal)
+        return -(d > self.distance_threshold).astype(np.float32)
+
+    # RobotEnv methods
+    # ----------------------------
+
+    def _step_callback(self):
+        if self.block_gripper:
+            self.sim.data.set_joint_qpos('robot0:l_gripper_finger_joint', 0.)
+            self.sim.data.set_joint_qpos('robot0:r_gripper_finger_joint', 0.)
+            self.sim.forward()
+
+    def _set_action(self, action):
+        assert action.shape == (4,)
+        action = action.copy()  # ensure that we don't change the action outside of this scope
+        pos_ctrl, gripper_ctrl = action[:3], action[3]
+
+        pos_ctrl *= 0.05  # limit maximum change in position
+        rot_ctrl = [1., 0., 1., 0.]  # fixed rotation of the end effector, expressed as a quaternion
+        gripper_ctrl = np.array([gripper_ctrl, gripper_ctrl])
+        assert gripper_ctrl.shape == (2,)
+        if self.block_gripper:
+            gripper_ctrl = np.zeros_like(gripper_ctrl)
+        action = np.concatenate([pos_ctrl, rot_ctrl, gripper_ctrl])
+
+        # Apply action to simulation.
+        utils.ctrl_set_action(self.sim, action)
+        utils.mocap_set_action(self.sim, action)
+
+    def _get_obs(self):
+        # positions
+        grip_pos = self.sim.data.get_site_xpos('robot0:grip')
+        dt = self.sim.nsubsteps * self.sim.model.opt.timestep
+        grip_velp = self.sim.data.get_site_xvelp('robot0:grip') * dt
+        robot_qpos, robot_qvel = utils.robot_get_obs(self.sim)
+        if self.has_object:
+            object_pos = self.sim.data.get_site_xpos('object0')
+            # rotations
+            object_rot = rotations.mat2euler(self.sim.data.get_site_xmat('object0'))
+            # velocities
+            object_velp = self.sim.data.get_site_xvelp('object0') * dt
+            object_velr = self.sim.data.get_site_xvelr('object0') * dt
+            # gripper state
+            object_rel_pos = object_pos - grip_pos
+            object_velp -= grip_velp
+        else:
+            object_pos = object_rot = object_velp = object_velr = object_rel_pos = np.zeros(0)
+        gripper_state = robot_qpos[-2:]
+        gripper_vel = robot_qvel[-2:] * dt  # change to a scalar if the gripper is made symmetric
+
+        if not self.has_object:
+            achieved_goal = grip_pos.copy()
+        else:
+            achieved_goal = np.squeeze(object_pos.copy())
+        obs = np.concatenate([
+            grip_pos, object_pos.ravel(), object_rel_pos.ravel(), gripper_state, object_rot.ravel(),
+            object_velp.ravel(), object_velr.ravel(), grip_velp, gripper_vel,
+        ])
+
+        return {
+            'observation': obs.copy(),
+            'achieved_goal': achieved_goal.copy(),
+            'desired_goal': self.goal.copy(),
+        }
+
+    def mb_step(self, states, actions, next_states):
+        a = next_states[:,:3]
+        b = next_states[:,3:6]
+        d = np.linalg.norm(a - b, axis=-1)
+        rewards = -(d <= self.distance_threshold).astype(np.float32) 
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def verify(self):
+        pass
+
+    def _viewer_setup(self):
+        body_id = self.sim.model.body_name2id('robot0:gripper_link')
+        lookat = self.sim.data.body_xpos[body_id]
+        for idx, value in enumerate(lookat):
+            self.viewer.cam.lookat[idx] = value
+        self.viewer.cam.distance = 2.5
+        self.viewer.cam.azimuth = 132.
+        self.viewer.cam.elevation = -14.
+
+    def _render_callback(self):
+        # Visualize target.
+        sites_offset = (self.sim.data.site_xpos - self.sim.model.site_pos).copy()
+        site_id = self.sim.model.site_name2id('target0')
+        self.sim.model.site_pos[site_id] = self.goal - sites_offset[0]
+        self.sim.forward()
+
+    def _reset_sim(self):
+        self.sim.set_state(self.initial_state)
+
+        # Randomize start position of object.
+        if self.has_object:
+            object_xpos = self.initial_gripper_xpos[:2]
+            while np.linalg.norm(object_xpos - self.initial_gripper_xpos[:2]) < 0.1:
+                object_xpos = self.initial_gripper_xpos[:2] + self.np_random.uniform(-self.obj_range, self.obj_range, size=2)
+            object_qpos = self.sim.data.get_joint_qpos('object0:joint')
+            assert object_qpos.shape == (7,)
+            object_qpos[:2] = object_xpos
+            self.sim.data.set_joint_qpos('object0:joint', object_qpos)
+
+        self.sim.forward()
+        return True
+
+    def _sample_goal(self):
+        if self.has_object:
+            goal = self.initial_gripper_xpos[:3] + self.np_random.uniform(-self.target_range, self.target_range, size=3)
+            goal += self.target_offset
+            goal[2] = self.height_offset
+            if self.target_in_the_air and self.np_random.uniform() < 0.5:
+                goal[2] += self.np_random.uniform(0, 0.45)
+        else:
+            goal = self.initial_gripper_xpos[:3] + self.np_random.uniform(-self.target_range, self.target_range, size=3)
+        return goal.copy()
+
+    def _is_success(self, achieved_goal, desired_goal):
+        d = goal_distance(achieved_goal, desired_goal)
+        return (d < self.distance_threshold).astype(np.float32)
+
+    def _env_setup(self, initial_qpos):
+        for name, value in initial_qpos.items():
+            self.sim.data.set_joint_qpos(name, value)
+        utils.reset_mocap_welds(self.sim)
+        self.sim.forward()
+
+        # Move end effector into position.
+        gripper_target = np.array([-0.498, 0.005, -0.431 + self.gripper_extra_height]) + self.sim.data.get_site_xpos('robot0:grip')
+        gripper_rotation = np.array([1., 0., 1., 0.])
+        self.sim.data.set_mocap_pos('robot0:mocap', gripper_target)
+        self.sim.data.set_mocap_quat('robot0:mocap', gripper_rotation)
+        for _ in range(10):
+            self.sim.step()
+
+        # Extract information for sampling goals.
+        self.initial_gripper_xpos = self.sim.data.get_site_xpos('robot0:grip').copy()
+        if self.has_object:
+            self.height_offset = self.sim.data.get_site_xpos('object0')[2]
+
+    def render(self, mode='human', width=500, height=500):
+        return super(FetchEnv, self).render(mode, width, height)
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand/__init__.py b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand/manipulate.py b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand/manipulate.py
new file mode 100644
index 0000000..85a7dd0
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand/manipulate.py
@@ -0,0 +1,344 @@
+import os
+import numpy as np
+
+from gym import utils, error
+from gym.envs.robotics import rotations, hand_env
+from gym.envs.robotics.utils import robot_get_obs
+from gym.wrappers import FlattenObservation
+
+
+from slbo.utils.dataset import Dataset, gen_dtype
+
+try:
+    import mujoco_py
+except ImportError as e:
+    raise error.DependencyNotInstalled("{}. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)".format(e))
+
+
+def quat_from_angle_and_axis(angle, axis):
+    assert axis.shape == (3,)
+    axis /= np.linalg.norm(axis)
+    quat = np.concatenate([[np.cos(angle / 2.)], np.sin(angle / 2.) * axis])
+    quat /= np.linalg.norm(quat)
+    return quat
+
+
+# Ensure we get the path separator correct on windows
+MANIPULATE_BLOCK_XML = os.path.join('hand', 'manipulate_block.xml')
+MANIPULATE_EGG_XML = os.path.join('hand', 'manipulate_egg.xml')
+MANIPULATE_PEN_XML = os.path.join('hand', 'manipulate_pen.xml')
+
+
+class ManipulateEnv(hand_env.HandEnv):
+    def __init__(
+        self, model_path, target_position, target_rotation,
+        target_position_range, reward_type, initial_qpos=None,
+        randomize_initial_position=True, randomize_initial_rotation=True,
+        distance_threshold=0.01, rotation_threshold=0.1, n_substeps=20, relative_control=False,
+        ignore_z_target_rotation=False,
+    ):
+        """Initializes a new Hand manipulation environment.
+
+        Args:
+            model_path (string): path to the environments XML file
+            target_position (string): the type of target position:
+                - ignore: target position is fully ignored, i.e. the object can be positioned arbitrarily
+                - fixed: target position is set to the initial position of the object
+                - random: target position is fully randomized according to target_position_range
+            target_rotation (string): the type of target rotation:
+                - ignore: target rotation is fully ignored, i.e. the object can be rotated arbitrarily
+                - fixed: target rotation is set to the initial rotation of the object
+                - xyz: fully randomized target rotation around the X, Y and Z axis
+                - z: fully randomized target rotation around the Z axis
+                - parallel: fully randomized target rotation around Z and axis-aligned rotation around X, Y
+            ignore_z_target_rotation (boolean): whether or not the Z axis of the target rotation is ignored
+            target_position_range (np.array of shape (3, 2)): range of the target_position randomization
+            reward_type ('sparse' or 'dense'): the reward type, i.e. sparse or dense
+            initial_qpos (dict): a dictionary of joint names and values that define the initial configuration
+            randomize_initial_position (boolean): whether or not to randomize the initial position of the object
+            randomize_initial_rotation (boolean): whether or not to randomize the initial rotation of the object
+            distance_threshold (float, in meters): the threshold after which the position of a goal is considered achieved
+            rotation_threshold (float, in radians): the threshold after which the rotation of a goal is considered achieved
+            n_substeps (int): number of substeps the simulation runs on every call to step
+            relative_control (boolean): whether or not the hand is actuated in absolute joint positions or relative to the current state
+        """
+        self.target_position = target_position
+        self.target_rotation = target_rotation
+        self.target_position_range = target_position_range
+        self.parallel_quats = [rotations.euler2quat(r) for r in rotations.get_parallel_rotations()]
+        self.randomize_initial_rotation = randomize_initial_rotation
+        self.randomize_initial_position = randomize_initial_position
+        self.distance_threshold = distance_threshold
+        self.rotation_threshold = rotation_threshold
+        self.reward_type = reward_type
+        self.ignore_z_target_rotation = ignore_z_target_rotation
+
+        assert self.target_position in ['ignore', 'fixed', 'random']
+        assert self.target_rotation in ['ignore', 'fixed', 'xyz', 'z', 'parallel']
+        initial_qpos = initial_qpos or {}
+
+        hand_env.HandEnv.__init__(
+            self, model_path, n_substeps=n_substeps, initial_qpos=initial_qpos,
+            relative_control=relative_control)
+
+
+    def _get_achieved_goal(self):
+        # Object position and rotation.
+        object_qpos = self.sim.data.get_joint_qpos('object:joint')
+        assert object_qpos.shape == (7,)
+        return object_qpos
+
+    def _goal_distance(self, goal_a, goal_b):
+        assert goal_a.shape == goal_b.shape
+        assert goal_a.shape[-1] == 7
+
+        d_pos = np.zeros_like(goal_a[..., 0])
+        d_rot = np.zeros_like(goal_b[..., 0])
+        if self.target_position != 'ignore':
+            delta_pos = goal_a[..., :3] - goal_b[..., :3]
+            d_pos = np.linalg.norm(delta_pos, axis=-1)
+
+        if self.target_rotation != 'ignore':
+            quat_a, quat_b = goal_a[..., 3:], goal_b[..., 3:]
+
+            if self.ignore_z_target_rotation:
+                # Special case: We want to ignore the Z component of the rotation.
+                # This code here assumes Euler angles with xyz convention. We first transform
+                # to euler, then set the Z component to be equal between the two, and finally
+                # transform back into quaternions.
+                euler_a = rotations.quat2euler(quat_a)
+                euler_b = rotations.quat2euler(quat_b)
+                euler_a[2] = euler_b[2]
+                quat_a = rotations.euler2quat(euler_a)
+
+            # Subtract quaternions and extract angle between them.
+            quat_diff = rotations.quat_mul(quat_a, rotations.quat_conjugate(quat_b))
+            angle_diff = 2 * np.arccos(np.clip(quat_diff[..., 0], -1., 1.))
+            d_rot = angle_diff
+        assert d_pos.shape == d_rot.shape
+        return d_pos, d_rot
+
+    # GoalEnv methods
+    # ----------------------------
+
+    def compute_reward(self, achieved_goal, goal, info):
+        if self.reward_type == 'sparse':
+            success = self._is_success(achieved_goal, goal).astype(np.float32)
+            return success 
+        else:
+            d_pos, d_rot = self._goal_distance(achieved_goal, goal)
+            # We weigh the difference in position to avoid that `d_pos` (in meters) is completely
+            # dominated by `d_rot` (in radians).
+            return -(10. * d_pos + d_rot)
+
+    # RobotEnv methods
+    # ----------------------------
+
+    def _is_success(self, achieved_goal, desired_goal):
+        d_pos, d_rot = self._goal_distance(achieved_goal, desired_goal)
+        achieved_pos = (d_pos < self.distance_threshold).astype(np.float32)
+        achieved_rot = (d_rot < self.rotation_threshold).astype(np.float32)
+        achieved_both = achieved_pos * achieved_rot
+        return achieved_both
+
+    def _env_setup(self, initial_qpos):
+        for name, value in initial_qpos.items():
+            self.sim.data.set_joint_qpos(name, value)
+        self.sim.forward()
+
+    def _reset_sim(self):
+        self.sim.set_state(self.initial_state)
+        self.sim.forward()
+
+        initial_qpos = self.sim.data.get_joint_qpos('object:joint').copy()
+        initial_pos, initial_quat = initial_qpos[:3], initial_qpos[3:]
+        assert initial_qpos.shape == (7,)
+        assert initial_pos.shape == (3,)
+        assert initial_quat.shape == (4,)
+        initial_qpos = None
+
+        # Randomization initial rotation.
+        if self.randomize_initial_rotation:
+            if self.target_rotation == 'z':
+                angle = self.np_random.uniform(-np.pi, np.pi)
+                axis = np.array([0., 0., 1.])
+                offset_quat = quat_from_angle_and_axis(angle, axis)
+                initial_quat = rotations.quat_mul(initial_quat, offset_quat)
+            elif self.target_rotation == 'parallel':
+                angle = self.np_random.uniform(-np.pi, np.pi)
+                axis = np.array([0., 0., 1.])
+                z_quat = quat_from_angle_and_axis(angle, axis)
+                parallel_quat = self.parallel_quats[self.np_random.randint(len(self.parallel_quats))]
+                offset_quat = rotations.quat_mul(z_quat, parallel_quat)
+                initial_quat = rotations.quat_mul(initial_quat, offset_quat)
+            elif self.target_rotation in ['xyz', 'ignore']:
+                angle = self.np_random.uniform(-np.pi, np.pi)
+                axis = self.np_random.uniform(-1., 1., size=3)
+                offset_quat = quat_from_angle_and_axis(angle, axis)
+                initial_quat = rotations.quat_mul(initial_quat, offset_quat)
+            elif self.target_rotation == 'fixed':
+                pass
+            else:
+                raise error.Error('Unknown target_rotation option "{}".'.format(self.target_rotation))
+
+        # Randomize initial position.
+        if self.randomize_initial_position:
+            if self.target_position != 'fixed':
+                initial_pos += self.np_random.normal(size=3, scale=0.005)
+
+        initial_quat /= np.linalg.norm(initial_quat)
+        initial_qpos = np.concatenate([initial_pos, initial_quat])
+        self.sim.data.set_joint_qpos('object:joint', initial_qpos)
+
+        def is_on_palm():
+            self.sim.forward()
+            cube_middle_idx = self.sim.model.site_name2id('object:center')
+            cube_middle_pos = self.sim.data.site_xpos[cube_middle_idx]
+            is_on_palm = (cube_middle_pos[2] > 0.04)
+            return is_on_palm
+
+        # Run the simulation for a bunch of timesteps to let everything settle in.
+        for _ in range(10):
+            self._set_action(np.zeros(20))
+            try:
+                self.sim.step()
+            except mujoco_py.MujocoException:
+                return False
+        return is_on_palm()
+
+    def _sample_goal(self):
+        # Select a goal for the object position.
+        target_pos = None
+        if self.target_position == 'random':
+            assert self.target_position_range.shape == (3, 2)
+            offset = self.np_random.uniform(self.target_position_range[:, 0], self.target_position_range[:, 1])
+            assert offset.shape == (3,)
+            target_pos = self.sim.data.get_joint_qpos('object:joint')[:3] + offset
+        elif self.target_position in ['ignore', 'fixed']:
+            target_pos = self.sim.data.get_joint_qpos('object:joint')[:3]
+        else:
+            raise error.Error('Unknown target_position option "{}".'.format(self.target_position))
+        assert target_pos is not None
+        assert target_pos.shape == (3,)
+
+        # Select a goal for the object rotation.
+        target_quat = None
+        if self.target_rotation == 'z':
+            angle = self.np_random.uniform(-np.pi, np.pi)
+            axis = np.array([0., 0., 1.])
+            target_quat = quat_from_angle_and_axis(angle, axis)
+        elif self.target_rotation == 'parallel':
+            angle = self.np_random.uniform(-np.pi, np.pi)
+            axis = np.array([0., 0., 1.])
+            target_quat = quat_from_angle_and_axis(angle, axis)
+            parallel_quat = self.parallel_quats[self.np_random.randint(len(self.parallel_quats))]
+            target_quat = rotations.quat_mul(target_quat, parallel_quat)
+        elif self.target_rotation == 'xyz':
+            angle = self.np_random.uniform(-np.pi, np.pi)
+            axis = self.np_random.uniform(-1., 1., size=3)
+            target_quat = quat_from_angle_and_axis(angle, axis)
+        elif self.target_rotation in ['ignore', 'fixed']:
+            target_quat = self.sim.data.get_joint_qpos('object:joint')[3:]
+        else:
+            raise error.Error('Unknown target_rotation option "{}".'.format(self.target_rotation))
+        assert target_quat is not None
+        assert target_quat.shape == (4,)
+
+        target_quat /= np.linalg.norm(target_quat)  # normalized quaternion
+        goal = np.concatenate([target_pos, target_quat])
+        return goal
+
+    def _render_callback(self):
+        # Assign current state to target object but offset a bit so that the actual object
+        # is not obscured.
+        goal = self.goal.copy()
+        assert goal.shape == (7,)
+        if self.target_position == 'ignore':
+            # Move the object to the side since we do not care about it's position.
+            goal[0] += 0.15
+        self.sim.data.set_joint_qpos('target:joint', goal)
+        self.sim.data.set_joint_qvel('target:joint', np.zeros(6))
+
+        if 'object_hidden' in self.sim.model.geom_names:
+            hidden_id = self.sim.model.geom_name2id('object_hidden')
+            self.sim.model.geom_rgba[hidden_id, 3] = 1.
+        self.sim.forward()
+
+    def _get_obs(self):
+        robot_qpos, robot_qvel = robot_get_obs(self.sim)
+        object_qvel = self.sim.data.get_joint_qvel('object:joint')
+        achieved_goal = self._get_achieved_goal().ravel()  # this contains the object position + rotation
+        observation = np.concatenate([robot_qpos, robot_qvel, object_qvel, achieved_goal])
+        return {
+            'observation': observation.copy(),
+            'achieved_goal': achieved_goal.copy(),
+            'desired_goal': self.goal.ravel().copy(),
+        }
+
+
+class HandBlockEnv(ManipulateEnv, utils.EzPickle):
+    def __init__(self, target_position='random', target_rotation='xyz', reward_type='sparse'):
+        utils.EzPickle.__init__(self, target_position, target_rotation, reward_type)
+        ManipulateEnv.__init__(self,
+            model_path=MANIPULATE_BLOCK_XML, target_position=target_position,
+            target_rotation=target_rotation,
+            target_position_range=np.array([(-0.04, 0.04), (-0.06, 0.02), (0.0, 0.06)]),
+            reward_type=reward_type)
+
+
+class HandEggEnv(ManipulateEnv, utils.EzPickle):
+    def __init__(self, target_position='random', target_rotation='fixed', reward_type='sparse'):
+        utils.EzPickle.__init__(self, target_position, target_rotation, reward_type)
+        ManipulateEnv.__init__(self,
+            model_path=MANIPULATE_EGG_XML, target_position=target_position,
+            target_rotation=target_rotation,
+            target_position_range=np.array([(-0.04, 0.04), (-0.06, 0.02), (0.0, 0.06)]),
+            reward_type=reward_type)
+
+
+    def verify(self, n=2000, eps=1e-4):
+        dummy = FlattenObservation(self)
+
+        dataset = Dataset(gen_dtype(dummy, 'state action next_state reward done'), n)
+        state = dummy.reset()
+        #print(state)
+        for _ in range(n):
+            action = dummy.action_space.sample()
+            next_state, reward, done, _ = dummy.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = dummy.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        # logger.info('rewarder difference: %.6f', l_inf)
+
+        assert np.allclose(dones_, dataset.done)
+        assert l_inf < eps
+
+    def mb_step(self, states, actions, next_states):
+        a = next_states[:,:7]
+        b = next_states[:,7:14]
+        if self.reward_type == 'sparse':
+            success = self._is_success(a, b).astype(np.float32)
+            rewards = (success * 10 - 1.)
+        else:
+            d_pos, d_rot = self._goal_distance(a, b)
+            # We weigh the difference in position to avoid that `d_pos` (in meters) is completely
+            # dominated by `d_rot` (in radians).
+            rewards = -(10. * d_pos + d_rot)
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+
+class HandPenEnv(ManipulateEnv, utils.EzPickle):
+    def __init__(self, target_position='random', target_rotation='xyz', reward_type='sparse'):
+        utils.EzPickle.__init__(self, target_position, target_rotation, reward_type)
+        ManipulateEnv.__init__(self,
+            model_path=MANIPULATE_PEN_XML, target_position=target_position,
+            target_rotation=target_rotation,
+            target_position_range=np.array([(-0.04, 0.04), (-0.06, 0.02), (0.0, 0.06)]),
+            randomize_initial_rotation=False, reward_type=reward_type,
+            ignore_z_target_rotation=True, distance_threshold=0.05)
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand/manipulate_touch_sensors.py b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand/manipulate_touch_sensors.py
new file mode 100644
index 0000000..c364868
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand/manipulate_touch_sensors.py
@@ -0,0 +1,131 @@
+import os
+import numpy as np
+
+from gym import utils, error, spaces
+from gym.envs.robotics.hand import manipulate
+
+# Ensure we get the path separator correct on windows
+MANIPULATE_BLOCK_XML = os.path.join('hand', 'manipulate_block_touch_sensors.xml')
+MANIPULATE_EGG_XML = os.path.join('hand', 'manipulate_egg_touch_sensors.xml')
+MANIPULATE_PEN_XML = os.path.join('hand', 'manipulate_pen_touch_sensors.xml')
+
+
+class ManipulateTouchSensorsEnv(manipulate.ManipulateEnv):
+    def __init__(
+        self, model_path, target_position, target_rotation,
+        target_position_range, reward_type, initial_qpos={},
+        randomize_initial_position=True, randomize_initial_rotation=True,
+        distance_threshold=0.01, rotation_threshold=0.1, n_substeps=20, relative_control=False,
+        ignore_z_target_rotation=False, touch_visualisation="on_touch", touch_get_obs="sensordata",
+    ):
+        """Initializes a new Hand manipulation environment with touch sensors.
+
+        Args:
+            touch_visualisation (string): how touch sensor sites are visualised
+                - "on_touch": shows touch sensor sites only when touch values > 0
+                - "always": always shows touch sensor sites
+                - "off" or else: does not show touch sensor sites
+            touch_get_obs (string): touch sensor readings
+                - "boolean": returns 1 if touch sensor reading != 0.0 else 0
+                - "sensordata": returns original touch sensor readings from self.sim.data.sensordata[id]
+                - "log": returns log(x+1) touch sensor readings from self.sim.data.sensordata[id]
+                - "off" or else: does not add touch sensor readings to the observation
+
+        """
+        self.touch_visualisation = touch_visualisation
+        self.touch_get_obs = touch_get_obs
+        self._touch_sensor_id_site_id = []
+        self._touch_sensor_id = []
+        self.touch_color = [1, 0, 0, 0.5]
+        self.notouch_color = [0, 0.5, 0, 0.2]
+
+        manipulate.ManipulateEnv.__init__(
+            self, model_path, target_position, target_rotation,
+            target_position_range, reward_type, initial_qpos=initial_qpos,
+            randomize_initial_position=randomize_initial_position, randomize_initial_rotation=randomize_initial_rotation,
+            distance_threshold=distance_threshold, rotation_threshold=rotation_threshold, n_substeps=n_substeps, relative_control=relative_control,
+            ignore_z_target_rotation=ignore_z_target_rotation,
+        )
+
+        for k, v in self.sim.model._sensor_name2id.items():  # get touch sensor site names and their ids
+            if 'robot0:TS_' in k:
+                self._touch_sensor_id_site_id.append((v, self.sim.model._site_name2id[k.replace('robot0:TS_', 'robot0:T_')]))
+                self._touch_sensor_id.append(v)
+
+        if self.touch_visualisation == 'off':  # set touch sensors rgba values
+            for _, site_id in self._touch_sensor_id_site_id:
+                self.sim.model.site_rgba[site_id][3] = 0.0
+        elif self.touch_visualisation == 'always':
+            pass
+
+        obs = self._get_obs()
+        self.observation_space = spaces.Dict(dict(
+            desired_goal=spaces.Box(-np.inf, np.inf, shape=obs['achieved_goal'].shape, dtype='float32'),
+            achieved_goal=spaces.Box(-np.inf, np.inf, shape=obs['achieved_goal'].shape, dtype='float32'),
+            observation=spaces.Box(-np.inf, np.inf, shape=obs['observation'].shape, dtype='float32'),
+        ))
+
+    def _render_callback(self):
+        super(ManipulateTouchSensorsEnv, self)._render_callback()
+        if self.touch_visualisation == 'on_touch':
+            for touch_sensor_id, site_id in self._touch_sensor_id_site_id:
+                if self.sim.data.sensordata[touch_sensor_id] != 0.0:
+                    self.sim.model.site_rgba[site_id] = self.touch_color
+                else:
+                    self.sim.model.site_rgba[site_id] = self.notouch_color
+
+    def _get_obs(self):
+        robot_qpos, robot_qvel = manipulate.robot_get_obs(self.sim)
+        object_qvel = self.sim.data.get_joint_qvel('object:joint')
+        achieved_goal = self._get_achieved_goal().ravel()  # this contains the object position + rotation
+        touch_values = []  # get touch sensor readings. if there is one, set value to 1
+        if self.touch_get_obs == 'sensordata':
+            touch_values = self.sim.data.sensordata[self._touch_sensor_id]
+        elif self.touch_get_obs == 'boolean':
+            touch_values = self.sim.data.sensordata[self._touch_sensor_id] > 0.0
+        elif self.touch_get_obs == 'log':
+            touch_values = np.log(self.sim.data.sensordata[self._touch_sensor_id] + 1.0)
+        observation = np.concatenate([robot_qpos, robot_qvel, object_qvel, touch_values, achieved_goal])
+
+        return {
+            'observation': observation.copy(),
+            'achieved_goal': achieved_goal.copy(),
+            'desired_goal': self.goal.ravel().copy(),
+        }
+
+
+class HandBlockTouchSensorsEnv(ManipulateTouchSensorsEnv, utils.EzPickle):
+    def __init__(self, target_position='random', target_rotation='xyz', touch_get_obs='sensordata', reward_type='sparse'):
+        utils.EzPickle.__init__(self, target_position, target_rotation, touch_get_obs, reward_type)
+        ManipulateTouchSensorsEnv.__init__(self,
+            model_path=MANIPULATE_BLOCK_XML,
+            touch_get_obs=touch_get_obs,
+            target_rotation=target_rotation,
+            target_position=target_position,
+            target_position_range=np.array([(-0.04, 0.04), (-0.06, 0.02), (0.0, 0.06)]),
+            reward_type=reward_type)
+
+
+class HandEggTouchSensorsEnv(ManipulateTouchSensorsEnv, utils.EzPickle):
+    def __init__(self, target_position='random', target_rotation='xyz', touch_get_obs='sensordata', reward_type='sparse'):
+        utils.EzPickle.__init__(self, target_position, target_rotation, touch_get_obs, reward_type)
+        ManipulateTouchSensorsEnv.__init__(self,
+            model_path=MANIPULATE_EGG_XML,
+            touch_get_obs=touch_get_obs,
+            target_rotation=target_rotation,
+            target_position=target_position,
+            target_position_range=np.array([(-0.04, 0.04), (-0.06, 0.02), (0.0, 0.06)]),
+            reward_type=reward_type)
+
+
+class HandPenTouchSensorsEnv(ManipulateTouchSensorsEnv, utils.EzPickle):
+    def __init__(self, target_position='random', target_rotation='xyz', touch_get_obs='sensordata', reward_type='sparse'):
+        utils.EzPickle.__init__(self, target_position, target_rotation, touch_get_obs, reward_type)
+        ManipulateTouchSensorsEnv.__init__(self,
+            model_path=MANIPULATE_PEN_XML,
+            touch_get_obs=touch_get_obs,
+            target_rotation=target_rotation,
+            target_position=target_position,
+            target_position_range=np.array([(-0.04, 0.04), (-0.06, 0.02), (0.0, 0.06)]),
+            randomize_initial_rotation=False, reward_type=reward_type,
+            ignore_z_target_rotation=True, distance_threshold=0.05)
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand/reach.py b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand/reach.py
new file mode 100644
index 0000000..1459e5a
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand/reach.py
@@ -0,0 +1,159 @@
+import os
+import numpy as np
+
+from gym import utils
+from slbo.envs.robotics import hand_env
+from gym.envs.robotics.utils import robot_get_obs
+
+
+FINGERTIP_SITE_NAMES = [
+    'robot0:S_fftip',
+    'robot0:S_mftip',
+    'robot0:S_rftip',
+    'robot0:S_lftip',
+    'robot0:S_thtip',
+]
+
+
+DEFAULT_INITIAL_QPOS = {
+    'robot0:WRJ1': -0.16514339750464327,
+    'robot0:WRJ0': -0.31973286565062153,
+    'robot0:FFJ3': 0.14340512546557435,
+    'robot0:FFJ2': 0.32028208333591573,
+    'robot0:FFJ1': 0.7126053607727917,
+    'robot0:FFJ0': 0.6705281001412586,
+    'robot0:MFJ3': 0.000246444303701037,
+    'robot0:MFJ2': 0.3152655251085491,
+    'robot0:MFJ1': 0.7659800313729842,
+    'robot0:MFJ0': 0.7323156897425923,
+    'robot0:RFJ3': 0.00038520700007378114,
+    'robot0:RFJ2': 0.36743546201985233,
+    'robot0:RFJ1': 0.7119514095008576,
+    'robot0:RFJ0': 0.6699446327514138,
+    'robot0:LFJ4': 0.0525442258033891,
+    'robot0:LFJ3': -0.13615534724474673,
+    'robot0:LFJ2': 0.39872030433433003,
+    'robot0:LFJ1': 0.7415570009679252,
+    'robot0:LFJ0': 0.704096378652974,
+    'robot0:THJ4': 0.003673823825070126,
+    'robot0:THJ3': 0.5506291436028695,
+    'robot0:THJ2': -0.014515151997119306,
+    'robot0:THJ1': -0.0015229223564485414,
+    'robot0:THJ0': -0.7894883021600622,
+}
+
+
+# Ensure we get the path separator correct on windows
+MODEL_XML_PATH = os.path.join('hand', 'reach.xml')
+
+
+def goal_distance(goal_a, goal_b):
+    assert goal_a.shape == goal_b.shape
+    return np.linalg.norm(goal_a - goal_b, axis=-1)
+
+
+class HandReachEnv(hand_env.HandEnv, utils.EzPickle):
+    def __init__(
+        self, distance_threshold=0.01, n_substeps=20, relative_control=False,
+        initial_qpos=DEFAULT_INITIAL_QPOS, reward_type='sparse',
+    ):
+        utils.EzPickle.__init__(**locals())
+        self.distance_threshold = distance_threshold
+        self.reward_type = 'sparse'
+
+        hand_env.HandEnv.__init__(
+            self, MODEL_XML_PATH, n_substeps=n_substeps, initial_qpos=initial_qpos,
+            relative_control=relative_control)
+
+    def _get_achieved_goal(self):
+        goal = [self.sim.data.get_site_xpos(name) for name in FINGERTIP_SITE_NAMES]
+        return np.array(goal).flatten()
+
+    # GoalEnv methods
+    # ----------------------------
+
+    def compute_reward(self, achieved_goal, goal, info):
+        d = goal_distance(achieved_goal, goal)
+        if self.reward_type == 'sparse':
+            return -(d > self.distance_threshold).astype(np.float32)
+        else:
+            return -d
+
+    # RobotEnv methods
+    # ----------------------------
+
+    def mb_step(self, states, actions, next_states):
+        a = next_states[:,:15]
+        b = next_states[:,15:30]
+        d = goal_distance(achieved_goal, goal)
+        if self.reward_type == 'sparse':
+            rewards =  -(d > self.distance_threshold).astype(np.float32)
+        else:
+            rewards =  -d
+        return rewards, np.zeros_like(rewards, dtype=np.bool)
+
+    def _env_setup(self, initial_qpos):
+        for name, value in initial_qpos.items():
+            self.sim.data.set_joint_qpos(name, value)
+        self.sim.forward()
+
+        self.initial_goal = self._get_achieved_goal().copy()
+        self.palm_xpos = self.sim.data.body_xpos[self.sim.model.body_name2id('robot0:palm')].copy()
+
+    def _get_obs(self):
+        robot_qpos, robot_qvel = robot_get_obs(self.sim)
+        achieved_goal = self._get_achieved_goal().ravel()
+        observation = np.concatenate([robot_qpos, robot_qvel, achieved_goal])
+        return {
+            'observation': observation.copy(),
+            'achieved_goal': achieved_goal.copy(),
+            'desired_goal': self.goal.copy(),
+        }
+
+    def _sample_goal(self):
+        thumb_name = 'robot0:S_thtip'
+        finger_names = [name for name in FINGERTIP_SITE_NAMES if name != thumb_name]
+        finger_name = self.np_random.choice(finger_names)
+
+        thumb_idx = FINGERTIP_SITE_NAMES.index(thumb_name)
+        finger_idx = FINGERTIP_SITE_NAMES.index(finger_name)
+        assert thumb_idx != finger_idx
+
+        # Pick a meeting point above the hand.
+        meeting_pos = self.palm_xpos + np.array([0.0, -0.09, 0.05])
+        meeting_pos += self.np_random.normal(scale=0.005, size=meeting_pos.shape)
+
+        # Slightly move meeting goal towards the respective finger to avoid that they
+        # overlap.
+        goal = self.initial_goal.copy().reshape(-1, 3)
+        for idx in [thumb_idx, finger_idx]:
+            offset_direction = (meeting_pos - goal[idx])
+            offset_direction /= np.linalg.norm(offset_direction)
+            goal[idx] = meeting_pos - 0.005 * offset_direction
+
+        if self.np_random.uniform() < 0.1:
+            # With some probability, ask all fingers to move back to the origin.
+            # This avoids that the thumb constantly stays near the goal position already.
+            goal = self.initial_goal.copy()
+        return goal.flatten()
+
+    def _is_success(self, achieved_goal, desired_goal):
+        d = goal_distance(achieved_goal, desired_goal)
+        return (d < self.distance_threshold).astype(np.float32)
+
+    def _render_callback(self):
+        # Visualize targets.
+        sites_offset = (self.sim.data.site_xpos - self.sim.model.site_pos).copy()
+        goal = self.goal.reshape(5, 3)
+        for finger_idx in range(5):
+            site_name = 'target{}'.format(finger_idx)
+            site_id = self.sim.model.site_name2id(site_name)
+            self.sim.model.site_pos[site_id] = goal[finger_idx] - sites_offset[site_id]
+
+        # Visualize finger positions.
+        achieved_goal = self._get_achieved_goal().reshape(5, 3)
+        for finger_idx in range(5):
+            site_name = 'finger{}'.format(finger_idx)
+            site_id = self.sim.model.site_name2id(site_name)
+            self.sim.model.site_pos[site_id] = achieved_goal[finger_idx] - sites_offset[site_id]
+        self.sim.forward()
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand_env.py b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand_env.py
new file mode 100644
index 0000000..a66e2cd
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/hand_env.py
@@ -0,0 +1,55 @@
+import os
+import copy
+import numpy as np
+
+import gym
+from gym import error, spaces
+from gym.utils import seeding
+from gym.envs.robotics import robot_env
+
+
+class HandEnv(robot_env.RobotEnv):
+    def __init__(self, model_path, n_substeps, initial_qpos, relative_control):
+        self.relative_control = relative_control
+
+        super(HandEnv, self).__init__(
+            model_path=model_path, n_substeps=n_substeps, n_actions=20,
+            initial_qpos=initial_qpos)
+
+    # RobotEnv methods
+    # ----------------------------
+
+    def _set_action(self, action):
+        assert action.shape == (20,)
+
+        ctrlrange = self.sim.model.actuator_ctrlrange
+        actuation_range = (ctrlrange[:, 1] - ctrlrange[:, 0]) / 2.
+        if self.relative_control:
+            actuation_center = np.zeros_like(action)
+            for i in range(self.sim.data.ctrl.shape[0]):
+                actuation_center[i] = self.sim.data.get_joint_qpos(
+                    self.sim.model.actuator_names[i].replace(':A_', ':'))
+            for joint_name in ['FF', 'MF', 'RF', 'LF']:
+                act_idx = self.sim.model.actuator_name2id(
+                    'robot0:A_{}J1'.format(joint_name))
+                actuation_center[act_idx] += self.sim.data.get_joint_qpos(
+                    'robot0:{}J0'.format(joint_name))
+        else:
+            actuation_center = (ctrlrange[:, 1] + ctrlrange[:, 0]) / 2.
+        self.sim.data.ctrl[:] = actuation_center + action * actuation_range
+        self.sim.data.ctrl[:] = np.clip(self.sim.data.ctrl, ctrlrange[:, 0], ctrlrange[:, 1])
+
+    def _viewer_setup(self):
+        body_id = self.sim.model.body_name2id('robot0:palm')
+        lookat = self.sim.data.body_xpos[body_id]
+        for idx, value in enumerate(lookat):
+            self.viewer.cam.lookat[idx] = value
+        self.viewer.cam.distance = 0.5
+        self.viewer.cam.azimuth = 55.
+        self.viewer.cam.elevation = -25.
+
+    def render(self, mode='human', width=500, height=500):
+        return super(HandEnv, self).render(mode, width, height)
+
+    def verify(self):
+        pass
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/robot_env.py b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/robot_env.py
new file mode 100644
index 0000000..584ab1d
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/robot_env.py
@@ -0,0 +1,170 @@
+import os
+import copy
+import numpy as np
+
+import gym
+from gym import error, spaces
+from gym.utils import seeding
+
+try:
+    import mujoco_py
+except ImportError as e:
+    raise error.DependencyNotInstalled("{}. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)".format(e))
+
+DEFAULT_SIZE = 500
+
+class RobotEnv(gym.GoalEnv):
+    def __init__(self, model_path, initial_qpos, n_actions, n_substeps):
+        if model_path.startswith('/'):
+            fullpath = model_path
+        else:
+            fullpath = os.path.join(os.path.dirname(__file__), 'assets', model_path)
+        if not os.path.exists(fullpath):
+            raise IOError('File {} does not exist'.format(fullpath))
+
+        model = mujoco_py.load_model_from_path(fullpath)
+        self.sim = mujoco_py.MjSim(model, nsubsteps=n_substeps)
+        self.viewer = None
+        self._viewers = {}
+
+        self.metadata = {
+            'render.modes': ['human', 'rgb_array'],
+            'video.frames_per_second': int(np.round(1.0 / self.dt))
+        }
+
+        self.seed()
+        self._env_setup(initial_qpos=initial_qpos)
+        self.initial_state = copy.deepcopy(self.sim.get_state())
+
+        self.goal = self._sample_goal()
+        obs = self._get_obs()
+        self.action_space = spaces.Box(-1., 1., shape=(n_actions,), dtype='float32')
+        self.observation_space = spaces.Dict(dict(
+            desired_goal=spaces.Box(-np.inf, np.inf, shape=obs['achieved_goal'].shape, dtype='float32'),
+            achieved_goal=spaces.Box(-np.inf, np.inf, shape=obs['achieved_goal'].shape, dtype='float32'),
+            observation=spaces.Box(-np.inf, np.inf, shape=obs['observation'].shape, dtype='float32'),
+        ))
+
+    @property
+    def dt(self):
+        return self.sim.model.opt.timestep * self.sim.nsubsteps
+
+    # Env methods
+    # ----------------------------
+
+    def seed(self, seed=None):
+        self.np_random, seed = seeding.np_random(seed)
+        return [seed]
+
+    def step(self, action):
+        action = np.clip(action, self.action_space.low, self.action_space.high)
+        self._set_action(action)
+        self.sim.step()
+        self._step_callback()
+        obs = self._get_obs()
+
+        done = False
+        info = {
+            'is_success': self._is_success(obs['achieved_goal'], self.goal),
+        }
+        reward = self.compute_reward(obs['achieved_goal'], self.goal, info)
+        return obs, reward, done, info
+
+    def reset(self):
+        # Attempt to reset the simulator. Since we randomize initial conditions, it
+        # is possible to get into a state with numerical issues (e.g. due to penetration or
+        # Gimbel lock) or we may not achieve an initial condition (e.g. an object is within the hand).
+        # In this case, we just keep randomizing until we eventually achieve a valid initial
+        # configuration.
+        super(RobotEnv, self).reset()
+        did_reset_sim = False
+        while not did_reset_sim:
+            did_reset_sim = self._reset_sim()
+        self.goal = self._sample_goal().copy()
+        obs = self._get_obs()
+        return obs
+
+    def close(self):
+        if self.viewer is not None:
+            # self.viewer.finish()
+            self.viewer = None
+            self._viewers = {}
+
+    def render(self, mode='human', width=DEFAULT_SIZE, height=DEFAULT_SIZE):
+        self._render_callback()
+        if mode == 'rgb_array':
+            self._get_viewer(mode).render(width, height)
+            # window size used for old mujoco-py:
+            data = self._get_viewer(mode).read_pixels(width, height, depth=False)
+            # original image is upside-down, so flip it
+            return data[::-1, :, :]
+        elif mode == 'human':
+            self._get_viewer(mode).render()
+
+    def _get_viewer(self, mode):
+        self.viewer = self._viewers.get(mode)
+        if self.viewer is None:
+            if mode == 'human':
+                self.viewer = mujoco_py.MjViewer(self.sim)
+            elif mode == 'rgb_array':
+                self.viewer = mujoco_py.MjRenderContextOffscreen(self.sim, device_id=-1)
+            self._viewer_setup()
+            self._viewers[mode] = self.viewer
+        return self.viewer
+
+    # Extension methods
+    # ----------------------------
+
+    def _reset_sim(self):
+        """Resets a simulation and indicates whether or not it was successful.
+        If a reset was unsuccessful (e.g. if a randomized state caused an error in the
+        simulation), this method should indicate such a failure by returning False.
+        In such a case, this method will be called again to attempt a the reset again.
+        """
+        self.sim.set_state(self.initial_state)
+        self.sim.forward()
+        return True
+
+    def _get_obs(self):
+        """Returns the observation.
+        """
+        raise NotImplementedError()
+
+    def _set_action(self, action):
+        """Applies the given action to the simulation.
+        """
+        raise NotImplementedError()
+
+    def _is_success(self, achieved_goal, desired_goal):
+        """Indicates whether or not the achieved goal successfully achieved the desired goal.
+        """
+        raise NotImplementedError()
+
+    def _sample_goal(self):
+        """Samples a new goal and returns it.
+        """
+        raise NotImplementedError()
+
+    def _env_setup(self, initial_qpos):
+        """Initial configuration of the environment. Can be used to configure initial state
+        and extract information from the simulation.
+        """
+        pass
+
+    def _viewer_setup(self):
+        """Initial configuration of the viewer. Can be used to set the camera position,
+        for example.
+        """
+        pass
+
+    def _render_callback(self):
+        """A custom callback that is called before rendering. Can be used
+        to implement custom visualizations.
+        """
+        pass
+
+    def _step_callback(self):
+        """A custom callback that is called after stepping the simulation. Can be used
+        to enforce additional constraints on the simulation state.
+        """
+        pass
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/rotations.py b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/rotations.py
new file mode 100644
index 0000000..4aafb64
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/rotations.py
@@ -0,0 +1,369 @@
+# Copyright (c) 2009-2017, Matthew Brett and Christoph Gohlke
+#    All rights reserved.
+#
+#    Redistribution and use in source and binary forms, with or without
+#    modification, are permitted provided that the following conditions are
+#    met:
+#
+#    1. Redistributions of source code must retain the above copyright notice,
+#    this list of conditions and the following disclaimer.
+#
+#    2. Redistributions in binary form must reproduce the above copyright
+#    notice, this list of conditions and the following disclaimer in the
+#    documentation and/or other materials provided with the distribution.
+#
+#    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
+#    IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
+#    THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+#    PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR
+#    CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+#    EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+#    PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#    PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#    LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#    NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#    SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+# Many methods borrow heavily or entirely from transforms3d:
+# https://github.com/matthew-brett/transforms3d
+# They have mostly been modified to support batched operations.
+
+import numpy as np
+import itertools
+
+'''
+Rotations
+=========
+
+Note: these have caused many subtle bugs in the past.
+Be careful while updating these methods and while using them in clever ways.
+
+See MuJoCo documentation here: http://mujoco.org/book/modeling.html#COrientation
+
+Conventions
+-----------
+    - All functions accept batches as well as individual rotations
+    - All rotation conventions match respective MuJoCo defaults
+    - All angles are in radians
+    - Matricies follow LR convention
+    - Euler Angles are all relative with 'xyz' axes ordering
+    - See specific representation for more information
+
+Representations
+---------------
+
+Euler
+    There are many euler angle frames -- here we will strive to use the default
+        in MuJoCo, which is eulerseq='xyz'.
+    This frame is a relative rotating frame, about x, y, and z axes in order.
+        Relative rotating means that after we rotate about x, then we use the
+        new (rotated) y, and the same for z.
+
+Quaternions
+    These are defined in terms of rotation (angle) about a unit vector (x, y, z)
+    We use the following <q0, q1, q2, q3> convention:
+            q0 = cos(angle / 2)
+            q1 = sin(angle / 2) * x
+            q2 = sin(angle / 2) * y
+            q3 = sin(angle / 2) * z
+        This is also sometimes called qw, qx, qy, qz.
+    Note that quaternions are ambiguous, because we can represent a rotation by
+        angle about vector <x, y, z> and -angle about vector <-x, -y, -z>.
+        To choose between these, we pick "first nonzero positive", where we
+        make the first nonzero element of the quaternion positive.
+    This can result in mismatches if you're converting an quaternion that is not
+        "first nonzero positive" to a different representation and back.
+
+Axis Angle
+    (Not currently implemented)
+    These are very straightforward.  Rotation is angle about a unit vector.
+
+XY Axes
+    (Not currently implemented)
+    We are given x axis and y axis, and z axis is cross product of x and y.
+
+Z Axis
+    This is NOT RECOMMENDED.  Defines a unit vector for the Z axis,
+        but rotation about this axis is not well defined.
+    Instead pick a fixed reference direction for another axis (e.g. X)
+        and calculate the other (e.g. Y = Z cross-product X),
+        then use XY Axes rotation instead.
+
+SO3
+    (Not currently implemented)
+    While not supported by MuJoCo, this representation has a lot of nice features.
+    We expect to add support for these in the future.
+
+TODO / Missing
+--------------
+    - Rotation integration or derivatives (e.g. velocity conversions)
+    - More representations (SO3, etc)
+    - Random sampling (e.g. sample uniform random rotation)
+    - Performance benchmarks/measurements
+    - (Maybe) define everything as to/from matricies, for simplicity
+'''
+
+# For testing whether a number is close to zero
+_FLOAT_EPS = np.finfo(np.float64).eps
+_EPS4 = _FLOAT_EPS * 4.0
+
+
+def euler2mat(euler):
+    """ Convert Euler Angles to Rotation Matrix.  See rotation.py for notes """
+    euler = np.asarray(euler, dtype=np.float64)
+    assert euler.shape[-1] == 3, "Invalid shaped euler {}".format(euler)
+
+    ai, aj, ak = -euler[..., 2], -euler[..., 1], -euler[..., 0]
+    si, sj, sk = np.sin(ai), np.sin(aj), np.sin(ak)
+    ci, cj, ck = np.cos(ai), np.cos(aj), np.cos(ak)
+    cc, cs = ci * ck, ci * sk
+    sc, ss = si * ck, si * sk
+
+    mat = np.empty(euler.shape[:-1] + (3, 3), dtype=np.float64)
+    mat[..., 2, 2] = cj * ck
+    mat[..., 2, 1] = sj * sc - cs
+    mat[..., 2, 0] = sj * cc + ss
+    mat[..., 1, 2] = cj * sk
+    mat[..., 1, 1] = sj * ss + cc
+    mat[..., 1, 0] = sj * cs - sc
+    mat[..., 0, 2] = -sj
+    mat[..., 0, 1] = cj * si
+    mat[..., 0, 0] = cj * ci
+    return mat
+
+
+def euler2quat(euler):
+    """ Convert Euler Angles to Quaternions.  See rotation.py for notes """
+    euler = np.asarray(euler, dtype=np.float64)
+    assert euler.shape[-1] == 3, "Invalid shape euler {}".format(euler)
+
+    ai, aj, ak = euler[..., 2] / 2, -euler[..., 1] / 2, euler[..., 0] / 2
+    si, sj, sk = np.sin(ai), np.sin(aj), np.sin(ak)
+    ci, cj, ck = np.cos(ai), np.cos(aj), np.cos(ak)
+    cc, cs = ci * ck, ci * sk
+    sc, ss = si * ck, si * sk
+
+    quat = np.empty(euler.shape[:-1] + (4,), dtype=np.float64)
+    quat[..., 0] = cj * cc + sj * ss
+    quat[..., 3] = cj * sc - sj * cs
+    quat[..., 2] = -(cj * ss + sj * cc)
+    quat[..., 1] = cj * cs - sj * sc
+    return quat
+
+
+def mat2euler(mat):
+    """ Convert Rotation Matrix to Euler Angles.  See rotation.py for notes """
+    mat = np.asarray(mat, dtype=np.float64)
+    assert mat.shape[-2:] == (3, 3), "Invalid shape matrix {}".format(mat)
+
+    cy = np.sqrt(mat[..., 2, 2] * mat[..., 2, 2] + mat[..., 1, 2] * mat[..., 1, 2])
+    condition = cy > _EPS4
+    euler = np.empty(mat.shape[:-1], dtype=np.float64)
+    euler[..., 2] = np.where(condition,
+                             -np.arctan2(mat[..., 0, 1], mat[..., 0, 0]),
+                             -np.arctan2(-mat[..., 1, 0], mat[..., 1, 1]))
+    euler[..., 1] = np.where(condition,
+                             -np.arctan2(-mat[..., 0, 2], cy),
+                             -np.arctan2(-mat[..., 0, 2], cy))
+    euler[..., 0] = np.where(condition,
+                             -np.arctan2(mat[..., 1, 2], mat[..., 2, 2]),
+                             0.0)
+    return euler
+
+
+def mat2quat(mat):
+    """ Convert Rotation Matrix to Quaternion.  See rotation.py for notes """
+    mat = np.asarray(mat, dtype=np.float64)
+    assert mat.shape[-2:] == (3, 3), "Invalid shape matrix {}".format(mat)
+
+    Qxx, Qyx, Qzx = mat[..., 0, 0], mat[..., 0, 1], mat[..., 0, 2]
+    Qxy, Qyy, Qzy = mat[..., 1, 0], mat[..., 1, 1], mat[..., 1, 2]
+    Qxz, Qyz, Qzz = mat[..., 2, 0], mat[..., 2, 1], mat[..., 2, 2]
+    # Fill only lower half of symmetric matrix
+    K = np.zeros(mat.shape[:-2] + (4, 4), dtype=np.float64)
+    K[..., 0, 0] = Qxx - Qyy - Qzz
+    K[..., 1, 0] = Qyx + Qxy
+    K[..., 1, 1] = Qyy - Qxx - Qzz
+    K[..., 2, 0] = Qzx + Qxz
+    K[..., 2, 1] = Qzy + Qyz
+    K[..., 2, 2] = Qzz - Qxx - Qyy
+    K[..., 3, 0] = Qyz - Qzy
+    K[..., 3, 1] = Qzx - Qxz
+    K[..., 3, 2] = Qxy - Qyx
+    K[..., 3, 3] = Qxx + Qyy + Qzz
+    K /= 3.0
+    # TODO: vectorize this -- probably could be made faster
+    q = np.empty(K.shape[:-2] + (4,))
+    it = np.nditer(q[..., 0], flags=['multi_index'])
+    while not it.finished:
+        # Use Hermitian eigenvectors, values for speed
+        vals, vecs = np.linalg.eigh(K[it.multi_index])
+        # Select largest eigenvector, reorder to w,x,y,z quaternion
+        q[it.multi_index] = vecs[[3, 0, 1, 2], np.argmax(vals)]
+        # Prefer quaternion with positive w
+        # (q * -1 corresponds to same rotation as q)
+        if q[it.multi_index][0] < 0:
+            q[it.multi_index] *= -1
+        it.iternext()
+    return q
+
+
+def quat2euler(quat):
+    """ Convert Quaternion to Euler Angles.  See rotation.py for notes """
+    return mat2euler(quat2mat(quat))
+
+
+def subtract_euler(e1, e2):
+    assert e1.shape == e2.shape
+    assert e1.shape[-1] == 3
+    q1 = euler2quat(e1)
+    q2 = euler2quat(e2)
+    q_diff = quat_mul(q1, quat_conjugate(q2))
+    return quat2euler(q_diff)
+
+
+def quat2mat(quat):
+    """ Convert Quaternion to Euler Angles.  See rotation.py for notes """
+    quat = np.asarray(quat, dtype=np.float64)
+    assert quat.shape[-1] == 4, "Invalid shape quat {}".format(quat)
+
+    w, x, y, z = quat[..., 0], quat[..., 1], quat[..., 2], quat[..., 3]
+    Nq = np.sum(quat * quat, axis=-1)
+    s = 2.0 / Nq
+    X, Y, Z = x * s, y * s, z * s
+    wX, wY, wZ = w * X, w * Y, w * Z
+    xX, xY, xZ = x * X, x * Y, x * Z
+    yY, yZ, zZ = y * Y, y * Z, z * Z
+
+    mat = np.empty(quat.shape[:-1] + (3, 3), dtype=np.float64)
+    mat[..., 0, 0] = 1.0 - (yY + zZ)
+    mat[..., 0, 1] = xY - wZ
+    mat[..., 0, 2] = xZ + wY
+    mat[..., 1, 0] = xY + wZ
+    mat[..., 1, 1] = 1.0 - (xX + zZ)
+    mat[..., 1, 2] = yZ - wX
+    mat[..., 2, 0] = xZ - wY
+    mat[..., 2, 1] = yZ + wX
+    mat[..., 2, 2] = 1.0 - (xX + yY)
+    return np.where((Nq > _FLOAT_EPS)[..., np.newaxis, np.newaxis], mat, np.eye(3))
+
+def quat_conjugate(q):
+    inv_q = -q
+    inv_q[..., 0] *= -1
+    return inv_q
+
+def quat_mul(q0, q1):
+    assert q0.shape == q1.shape
+    assert q0.shape[-1] == 4
+    assert q1.shape[-1] == 4
+
+    w0 = q0[..., 0]
+    x0 = q0[..., 1]
+    y0 = q0[..., 2]
+    z0 = q0[..., 3]
+
+    w1 = q1[..., 0]
+    x1 = q1[..., 1]
+    y1 = q1[..., 2]
+    z1 = q1[..., 3]
+
+    w = w0 * w1 - x0 * x1 - y0 * y1 - z0 * z1
+    x = w0 * x1 + x0 * w1 + y0 * z1 - z0 * y1
+    y = w0 * y1 + y0 * w1 + z0 * x1 - x0 * z1
+    z = w0 * z1 + z0 * w1 + x0 * y1 - y0 * x1
+    q = np.array([w, x, y, z])
+    if q.ndim == 2:
+        q = q.swapaxes(0, 1)
+    assert q.shape == q0.shape
+    return q
+
+def quat_rot_vec(q, v0):
+    q_v0 = np.array([0, v0[0], v0[1], v0[2]])
+    q_v = quat_mul(q, quat_mul(q_v0, quat_conjugate(q)))
+    v = q_v[1:]
+    return v
+
+def quat_identity():
+    return np.array([1, 0, 0, 0])
+
+def quat2axisangle(quat):
+    theta = 0;
+    axis = np.array([0, 0, 1]);
+    sin_theta = np.linalg.norm(quat[1:])
+
+    if (sin_theta > 0.0001):
+        theta = 2 * np.arcsin(sin_theta)
+        theta *= 1 if quat[0] >= 0 else -1
+        axis = quat[1:] / sin_theta
+
+    return axis, theta
+
+def euler2point_euler(euler):
+    _euler = euler.copy()
+    if len(_euler.shape) < 2:
+        _euler = np.expand_dims(_euler,0)
+    assert(_euler.shape[1] == 3)
+    _euler_sin = np.sin(_euler)
+    _euler_cos = np.cos(_euler)
+    return np.concatenate([_euler_sin, _euler_cos], axis=-1)
+
+def point_euler2euler(euler):
+    _euler = euler.copy()
+    if len(_euler.shape) < 2:
+        _euler = np.expand_dims(_euler,0)
+    assert(_euler.shape[1] == 6)
+    angle = np.arctan(_euler[..., :3] / _euler[..., 3:])
+    angle[_euler[..., 3:] < 0] += np.pi
+    return angle
+
+def quat2point_quat(quat):
+    # Should be in qw, qx, qy, qz
+    _quat = quat.copy()
+    if len(_quat.shape) < 2:
+        _quat = np.expand_dims(_quat, 0)
+    assert(_quat.shape[1] == 4)
+    angle = np.arccos(_quat[:,[0]]) * 2
+    xyz = _quat[:, 1:]
+    xyz[np.squeeze(np.abs(np.sin(angle/2))) >= 1e-5] = (xyz / np.sin(angle / 2))[np.squeeze(np.abs(np.sin(angle/2))) >= 1e-5]
+    return np.concatenate([np.sin(angle),np.cos(angle), xyz], axis=-1)
+
+def point_quat2quat(quat):
+    _quat = quat.copy()
+    if len(_quat.shape) < 2:
+        _quat = np.expand_dims(_quat, 0)
+    assert(_quat.shape[1] == 5)
+    angle = np.arctan(_quat[:,[0]] / _quat[:,[1]])
+    qw = np.cos(angle / 2)
+
+    qxyz = _quat[:, 2:]
+    qxyz[np.squeeze(np.abs(np.sin(angle/2))) >= 1e-5] = (qxyz * np.sin(angle/2))[np.squeeze(np.abs(np.sin(angle/2))) >= 1e-5]
+    return np.concatenate([qw, qxyz], axis=-1)
+
+def normalize_angles(angles):
+    '''Puts angles in [-pi, pi] range.'''
+    angles = angles.copy()
+    if angles.size > 0:
+        angles = (angles + np.pi) % (2 * np.pi) - np.pi
+        assert -np.pi-1e-6 <= angles.min() and angles.max() <= np.pi+1e-6
+    return angles
+
+def round_to_straight_angles(angles):
+    '''Returns closest angle modulo 90 degrees '''
+    angles = np.round(angles / (np.pi / 2)) * (np.pi / 2)
+    return normalize_angles(angles)
+
+def get_parallel_rotations():
+    mult90 = [0, np.pi/2, -np.pi/2, np.pi]
+    parallel_rotations = []
+    for euler in itertools.product(mult90, repeat=3):
+        canonical = mat2euler(euler2mat(euler))
+        canonical = np.round(canonical / (np.pi / 2))
+        if canonical[0] == -2:
+            canonical[0] = 2
+        if canonical[2] == -2:
+            canonical[2] = 2
+        canonical *= np.pi / 2
+        if all([(canonical != rot).any() for rot in parallel_rotations]):
+            parallel_rotations += [canonical]
+    assert len(parallel_rotations) == 24
+    return parallel_rotations
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/robotics/utils.py b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/utils.py
new file mode 100644
index 0000000..a73e5f6
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/robotics/utils.py
@@ -0,0 +1,96 @@
+import numpy as np
+
+from gym import error
+try:
+    import mujoco_py
+except ImportError as e:
+    raise error.DependencyNotInstalled("{}. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)".format(e))
+
+
+def robot_get_obs(sim):
+    """Returns all joint positions and velocities associated with
+    a robot.
+    """
+    if sim.data.qpos is not None and sim.model.joint_names:
+        names = [n for n in sim.model.joint_names if n.startswith('robot')]
+        return (
+            np.array([sim.data.get_joint_qpos(name) for name in names]),
+            np.array([sim.data.get_joint_qvel(name) for name in names]),
+        )
+    return np.zeros(0), np.zeros(0)
+
+
+def ctrl_set_action(sim, action):
+    """For torque actuators it copies the action into mujoco ctrl field.
+    For position actuators it sets the target relative to the current qpos.
+    """
+    if sim.model.nmocap > 0:
+        _, action = np.split(action, (sim.model.nmocap * 7, ))
+    if sim.data.ctrl is not None:
+        for i in range(action.shape[0]):
+            if sim.model.actuator_biastype[i] == 0:
+                sim.data.ctrl[i] = action[i]
+            else:
+                idx = sim.model.jnt_qposadr[sim.model.actuator_trnid[i, 0]]
+                sim.data.ctrl[i] = sim.data.qpos[idx] + action[i]
+
+
+def mocap_set_action(sim, action):
+    """The action controls the robot using mocaps. Specifically, bodies
+    on the robot (for example the gripper wrist) is controlled with
+    mocap bodies. In this case the action is the desired difference
+    in position and orientation (quaternion), in world coordinates,
+    of the of the target body. The mocap is positioned relative to
+    the target body according to the delta, and the MuJoCo equality
+    constraint optimizer tries to center the welded body on the mocap.
+    """
+    if sim.model.nmocap > 0:
+        action, _ = np.split(action, (sim.model.nmocap * 7, ))
+        action = action.reshape(sim.model.nmocap, 7)
+
+        pos_delta = action[:, :3]
+        quat_delta = action[:, 3:]
+
+        reset_mocap2body_xpos(sim)
+        sim.data.mocap_pos[:] = sim.data.mocap_pos + pos_delta
+        sim.data.mocap_quat[:] = sim.data.mocap_quat + quat_delta
+
+
+def reset_mocap_welds(sim):
+    """Resets the mocap welds that we use for actuation.
+    """
+    if sim.model.nmocap > 0 and sim.model.eq_data is not None:
+        for i in range(sim.model.eq_data.shape[0]):
+            if sim.model.eq_type[i] == mujoco_py.const.EQ_WELD:
+                sim.model.eq_data[i, :] = np.array(
+                    [0., 0., 0., 1., 0., 0., 0.])
+    sim.forward()
+
+
+def reset_mocap2body_xpos(sim):
+    """Resets the position and orientation of the mocap bodies to the same
+    values as the bodies they're welded to.
+    """
+
+    if (sim.model.eq_type is None or
+        sim.model.eq_obj1id is None or
+        sim.model.eq_obj2id is None):
+        return
+    for eq_type, obj1_id, obj2_id in zip(sim.model.eq_type,
+                                         sim.model.eq_obj1id,
+                                         sim.model.eq_obj2id):
+        if eq_type != mujoco_py.const.EQ_WELD:
+            continue
+
+        mocap_id = sim.model.body_mocapid[obj1_id]
+        if mocap_id != -1:
+            # obj1 is the mocap, obj2 is the welded body
+            body_idx = obj2_id
+        else:
+            # obj2 is the mocap, obj1 is the welded body
+            mocap_id = sim.model.body_mocapid[obj2_id]
+            body_idx = obj1_id
+
+        assert (mocap_id != -1)
+        sim.data.mocap_pos[mocap_id][:] = sim.data.body_xpos[body_idx]
+        sim.data.mocap_quat[mocap_id][:] = sim.data.body_xquat[body_idx]
diff --git a/experiments05/ant_umaze_1234/src/slbo/envs/virtual_env.py b/experiments05/ant_umaze_1234/src/slbo/envs/virtual_env.py
new file mode 100644
index 0000000..2d0cc76
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/envs/virtual_env.py
@@ -0,0 +1,94 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from gym.spaces import Box
+from slbo.dynamics_model import DynamicsModel
+from slbo.random_net import RandomNet
+from slbo.envs import BaseBatchedEnv, BaseModelBasedEnv
+from slbo.utils.pc_utils import compute_cov_pi
+
+
+class VirtualEnv(BaseBatchedEnv):
+    _states: np.ndarray
+
+    def __init__(self, model: DynamicsModel, env: BaseModelBasedEnv, random_net:RandomNet,  n_envs: int, 
+                    feature_size: int, bonus_scale: float, lamb: float, opt_model=False):
+        super().__init__()
+        self.n_envs = n_envs
+        self.observation_space = env.observation_space  # ???
+
+        dim_state = env.observation_space.shape[0]
+        dim_action = env.action_space.shape[0]
+        if opt_model:
+            self.action_space = Box(low=np.r_[env.action_space.low, np.zeros(dim_state) - 1.],
+                                    high=np.r_[env.action_space.high, np.zeros(dim_state) + 1.],
+                                    dtype=np.float32)
+        else:
+            self.action_space = env.action_space
+
+        self._opt_model = opt_model
+        self._model = model
+        self._env = env
+        self._random_net = random_net
+
+        self._states = np.zeros((self.n_envs, dim_state), dtype=np.float32)
+
+        self.feature_size = feature_size
+        self.cov_pis = None
+        self.inv_cov = None
+        self.bonus_scale = bonus_scale
+        self.lamb = lamb
+        self.pre = True
+
+    def _scale_action(self, actions):
+        lo, hi = self.action_space.low, self.action_space.high
+        return lo + (actions + 1.) * 0.5 * (hi - lo)
+
+    def step(self, actions):
+        if self._opt_model:
+            actions = actions[..., :self._env.action_space.shape[0]]
+
+        next_states = self._model.eval('next_states', states=self._states, actions=actions)
+        features = self._random_net.eval('features', states=self._states, actions=actions)
+        #print(features.shape)
+        rewards, dones = self._env.mb_step(self._states, self._scale_action(actions), next_states)
+
+        if not self.pre:
+            bonus = self.compute_bonus(features)
+            rewards = rewards + self.bonus_scale * bonus
+
+        self._states = next_states
+        return self._states.copy(), rewards, dones, [{} for _ in range(self.n_envs)]
+
+    def reset(self):
+        return self.partial_reset(range(self.n_envs))
+
+    def partial_reset(self, indices):
+        initial_states = np.array([self._env.reset() for _ in indices])
+
+        self._states = self._states.copy()
+        self._states[indices] = initial_states
+
+        return initial_states.copy()
+
+    def set_state(self, states):
+        self._states = states.copy()
+
+    def render(self, mode='human'):
+        pass
+
+    def update_cov(self, states, actions):
+        features = self._random_net.eval('features', states=states, actions=actions)
+
+        if self.pre:
+            self.cov_pis = compute_cov_pi(features)
+            self.pre = False
+        else:
+            self.cov_pis = self.cov_pis + compute_cov_pi(features)
+        
+        cur_cov = self.lamb * np.identity(self.feature_size) + self.cov_pis
+        self.inv_cov = np.linalg.inv(cur_cov)
+
+
+    def compute_bonus(self,features):
+        bonus = np.sqrt(np.sum(np.dot(features, self.inv_cov)*features,1))
+        return bonus
diff --git a/experiments05/ant_umaze_1234/src/slbo/loss/__init__.py b/experiments05/ant_umaze_1234/src/slbo/loss/__init__.py
new file mode 100644
index 0000000..5c7f19c
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/loss/__init__.py
@@ -0,0 +1 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
diff --git a/experiments05/ant_umaze_1234/src/slbo/loss/multi_step_loss.py b/experiments05/ant_umaze_1234/src/slbo/loss/multi_step_loss.py
new file mode 100644
index 0000000..a2aadb7
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/loss/multi_step_loss.py
@@ -0,0 +1,65 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi import Tensor
+from slbo.utils.normalizer import Normalizers
+
+
+class MultiStepLoss(nn.Module):
+    op_train: Tensor
+    op_grad_norm: Tensor
+    _step: int
+    _criterion: nn.Module
+    _normalizers: Normalizers
+    _model: nn.Module
+
+    def __init__(self, model: nn.Module, normalizers: Normalizers, dim_state: int, dim_action: int,
+                 criterion: nn.Module, step=4):
+        super().__init__()
+        self._step = step
+        self._criterion = criterion
+        self._model = model
+        self._normalizers = normalizers
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, shape=[step, None, dim_state])
+            self.op_actions = tf.placeholder(tf.float32, shape=[step, None, dim_action])
+            self.op_masks = tf.placeholder(tf.float32, shape=[step, None])
+            self.op_next_states_ = tf.placeholder(tf.float32, shape=[step, None, dim_state])
+
+        self.op_loss = self(self.op_states, self.op_actions, self.op_next_states_, self.op_masks)
+
+    def forward(self, states: Tensor, actions: Tensor, next_states_: Tensor, masks: Tensor):
+        """
+            All inputs have shape [num_steps, batch_size, xxx]
+        """
+
+        cur_states = states[0]
+        loss = []
+        for i in range(self._step):
+            next_states = self._model(cur_states, actions[i])
+            diffs = next_states - cur_states - next_states_[i] + states[i]
+            weighted_diffs = diffs / self._normalizers.diff.op_std.maximum(1e-6)
+            loss.append(self._criterion(weighted_diffs, 0, cur_states))
+
+            if i < self._step - 1:
+                cur_states = states[i + 1] + masks[i].expand_dims(-1) * (next_states - states[i + 1])
+
+        return tf.add_n(loss) / self._step
+
+    @nn.make_method(fetch='loss')
+    def get_loss(self, states, next_states_, actions, masks): pass
+
+    def build_backward(self, lr: float, weight_decay: float, max_grad_norm=2.):
+        loss = self.op_loss.reduce_mean(name='Loss')
+
+        optimizer = tf.train.AdamOptimizer(lr)
+        params = self._model.parameters()
+        regularization = weight_decay * tf.add_n([tf.nn.l2_loss(t) for t in params], name='regularization')
+
+        grads_and_vars = optimizer.compute_gradients(loss + regularization, var_list=params)
+        print([var.name for grad, var in grads_and_vars])
+        clip_grads, op_grad_norm = tf.clip_by_global_norm([grad for grad, _ in grads_and_vars], max_grad_norm)
+        clip_grads_and_vars = [(grad, var) for grad, (_, var) in zip(clip_grads, grads_and_vars)]
+        self.op_train = optimizer.apply_gradients(clip_grads_and_vars)
+        self.op_grad_norm = op_grad_norm
diff --git a/experiments05/ant_umaze_1234/src/slbo/partial_envs.py b/experiments05/ant_umaze_1234/src/slbo/partial_envs.py
new file mode 100644
index 0000000..b9aee20
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/partial_envs.py
@@ -0,0 +1,95 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+
+import gym
+import slbo.envs.mujoco_maze
+
+from gym.wrappers import FlattenObservation
+
+
+from slbo.envs.bm_envs.gym.half_cheetah import HalfCheetahEnv
+from slbo.envs.bm_envs.gym.walker2d import Walker2dEnv
+# from slbo.envs.mujoco.humanoid_env import HumanoidEnv
+from slbo.envs.bm_envs.gym.ant import AntEnv
+from slbo.envs.bm_envs.gym.hopper import HopperEnv
+from slbo.envs.bm_envs.gym.swimmer import SwimmerEnv
+from slbo.envs.bm_envs.gym.reacher import ReacherEnv
+from slbo.envs.bm_envs.gym.pendulum import PendulumEnv
+from slbo.envs.bm_envs.gym.inverted_pendulum import InvertedPendulumEnv
+from slbo.envs.bm_envs.gym.acrobot import AcrobotEnv
+from slbo.envs.bm_envs.gym.cartpole import CartPoleEnv
+from slbo.envs.bm_envs.gym.mountain_car import Continuous_MountainCarEnv
+
+from slbo.envs.bm_envs.gym import gym_fswimmer
+from slbo.envs.bm_envs.gym import gym_fwalker2d
+from slbo.envs.bm_envs.gym import gym_fhopper
+from slbo.envs.bm_envs.gym import gym_fant
+
+from slbo.envs.bm_envs.gym import gym_cheetahA01
+from slbo.envs.bm_envs.gym import gym_cheetahA003
+from slbo.envs.bm_envs.gym import gym_cheetahO01
+from slbo.envs.bm_envs.gym import gym_cheetahO001
+from slbo.envs.bm_envs.gym import gym_pendulumO01
+from slbo.envs.bm_envs.gym import gym_pendulumO001
+from slbo.envs.bm_envs.gym import gym_cartpoleO01
+from slbo.envs.bm_envs.gym import gym_cartpoleO001
+
+from slbo.envs.bm_envs.gym import gym_humanoid
+from slbo.envs.bm_envs.gym import gym_nostopslimhumanoid
+from slbo.envs.bm_envs.gym import gym_slimhumanoid
+from slbo.envs.robotics.fetch.push import FetchPushEnv
+from slbo.envs.robotics.hand.reach import HandReachEnv
+from slbo.envs.robotics.hand.manipulate import HandEggEnv
+
+
+def make_env(id: str):
+    if "Point" in id or "Ant" in id:
+        env = gym.make(id)
+        env.seed(np.random.randint(2**60))
+    else:
+        envs = {
+            'HandEgg': HandEggEnv,
+            'FetchPush': FetchPushEnv,
+            'HandReach': HandReachEnv,
+            'HalfCheetah': HalfCheetahEnv,
+            'Walker2D': Walker2dEnv,
+            'Ant': AntEnv,
+            'Hopper': HopperEnv,
+            'Swimmer': SwimmerEnv,
+            'FixedSwimmer': gym_fswimmer.fixedSwimmerEnv,
+            'FixedWalker': gym_fwalker2d.Walker2dEnv,
+            'FixedHopper': gym_fhopper.HopperEnv,
+            'FixedAnt': gym_fant.AntEnv,
+            'Reacher': ReacherEnv,
+            'Pendulum': PendulumEnv,
+            'InvertedPendulum': InvertedPendulumEnv,
+            'Acrobot': AcrobotEnv,
+            'CartPole': CartPoleEnv,
+            'MountainCar': Continuous_MountainCarEnv,
+
+            'HalfCheetahO01': gym_cheetahO01.HalfCheetahEnv,
+            'HalfCheetahO001': gym_cheetahO001.HalfCheetahEnv,
+            'HalfCheetahA01': gym_cheetahA01.HalfCheetahEnv,
+            'HalfCheetahA003': gym_cheetahA003.HalfCheetahEnv,
+
+            'PendulumO01': gym_pendulumO01.PendulumEnv,
+            'PendulumO001': gym_pendulumO001.PendulumEnv,
+
+            'CartPoleO01': gym_cartpoleO01.CartPoleEnv,
+            'CartPoleO001': gym_cartpoleO001.CartPoleEnv,
+
+            'gym_humanoid': gym_humanoid.HumanoidEnv,
+            'gym_slimhumanoid': gym_slimhumanoid.HumanoidEnv,
+            'gym_nostopslimhumanoid': gym_nostopslimhumanoid.HumanoidEnv,
+        }
+        
+        env = envs[id]()
+        if not hasattr(env, 'reward_range'):
+            env.reward_range = (-np.inf, np.inf)
+        if not hasattr(env, 'metadata'):
+            env.metadata = {}
+        env.seed(np.random.randint(2**60))
+        if 'Fetch' in id or 'Hand' in id: 
+            env = FlattenObservation(env)
+
+    return env
\ No newline at end of file
diff --git a/experiments05/ant_umaze_1234/src/slbo/policies/__init__.py b/experiments05/ant_umaze_1234/src/slbo/policies/__init__.py
new file mode 100644
index 0000000..3ffa40d
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/policies/__init__.py
@@ -0,0 +1,13 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import abc
+from typing import Union
+import lunzi.nn as nn
+
+
+class BasePolicy(abc.ABC):
+    @abc.abstractmethod
+    def get_actions(self, states):
+        pass
+
+
+BaseNNPolicy = Union[BasePolicy, nn.Module]  # should be Intersection, see PEP544
diff --git a/experiments05/ant_umaze_1234/src/slbo/policies/gaussian_mlp_policy.py b/experiments05/ant_umaze_1234/src/slbo/policies/gaussian_mlp_policy.py
new file mode 100644
index 0000000..c6bcb27
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/policies/gaussian_mlp_policy.py
@@ -0,0 +1,69 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List
+import tensorflow as tf
+import numpy as np
+from lunzi import Tensor
+from lunzi import nn
+from baselines.common.tf_util import normc_initializer
+from slbo.utils.truncated_normal import LimitedEntNormal
+from . import BasePolicy
+from slbo.utils.normalizer import GaussianNormalizer
+
+
+class GaussianMLPPolicy(nn.Module, BasePolicy):
+    op_states: Tensor
+
+    def __init__(self, dim_state: int, dim_action: int, hidden_sizes: List[int], normalizer: GaussianNormalizer,
+                 init_std=1.):
+        super().__init__()
+        self.dim_state = dim_state
+        self.dim_action = dim_action
+        self.hidden_sizes = hidden_sizes
+        self.init_std = init_std
+        self.normalizer = normalizer
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, shape=[None, dim_state], name='states')
+            self.op_actions_ = tf.placeholder(tf.float32, shape=[None, dim_action], name='actions')
+
+            layers = []
+            # note that the placeholder has size 105.
+            all_sizes = [dim_state, *self.hidden_sizes]
+            for i, (in_features, out_features) in enumerate(zip(all_sizes[:-1], all_sizes[1:])):
+                layers.append(nn.Linear(in_features, out_features, weight_initializer=normc_initializer(1)))
+                layers.append(nn.Tanh())
+            layers.append(nn.Linear(all_sizes[-1], dim_action, weight_initializer=normc_initializer(0.01)))
+            self.net = nn.Sequential(*layers)
+
+            self.op_log_std = nn.Parameter(
+                tf.constant(np.log(self.init_std), shape=[self.dim_action], dtype=tf.float32), name='log_std')
+
+        self.distribution = self(self.op_states)
+        self.op_actions = self.distribution.sample()
+        self.op_actions_mean = self.distribution.mean()
+        self.op_actions_std = self.distribution.stddev()
+        self.op_nlls_ = -self.distribution.log_prob(self.op_actions_).reduce_sum(axis=1)
+
+        self.register_callable('[states] => [actions]', self.fast)
+
+    def forward(self, states):
+        states = self.normalizer(states)
+        actions_mean = self.net(states)
+        distribution = LimitedEntNormal(actions_mean, self.op_log_std.exp())
+
+        return distribution
+
+    @nn.make_method(fetch='actions')
+    def get_actions(self, states): pass
+
+    def fast(self, states, use_log_prob=False):
+        states = self.normalizer.fast(states)
+        actions_mean = self.net.fast(states)
+        noise = np.random.randn(*actions_mean.shape)
+        actions = actions_mean + noise * np.exp(self.op_log_std.numpy())
+        if use_log_prob:
+            log_prob = -noise**2 / 2 - np.log(2 * np.pi) / 2 - self.op_log_std.numpy()
+            return actions, log_prob.sum(axis=1)
+        return actions
+
+    def clone(self):
+        return GaussianMLPPolicy(self.dim_state, self.dim_action, self.hidden_sizes, self.normalizer, self.init_std)
diff --git a/experiments05/ant_umaze_1234/src/slbo/policies/uniform_policy.py b/experiments05/ant_umaze_1234/src/slbo/policies/uniform_policy.py
new file mode 100644
index 0000000..ca9f821
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/policies/uniform_policy.py
@@ -0,0 +1,11 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from . import BasePolicy
+
+
+class UniformPolicy(BasePolicy):
+    def __init__(self, dim_action):
+        self.dim_action = dim_action
+
+    def get_actions(self, states):
+        return np.random.uniform(-1., 1., states.shape[:-1] + (self.dim_action,))
diff --git a/experiments05/ant_umaze_1234/src/slbo/q_function/__init__.py b/experiments05/ant_umaze_1234/src/slbo/q_function/__init__.py
new file mode 100644
index 0000000..e8dfcba
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/q_function/__init__.py
@@ -0,0 +1,13 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import abc
+from typing import Union
+import lunzi.nn as nn
+
+
+class BaseQFunction(abc.ABC):
+    @abc.abstractmethod
+    def get_q(self, states, values):
+        pass
+
+
+BaseNNQFunction = Union[BaseQFunction, nn.Module]
diff --git a/experiments05/ant_umaze_1234/src/slbo/q_function/mlp_q_function.py b/experiments05/ant_umaze_1234/src/slbo/q_function/mlp_q_function.py
new file mode 100644
index 0000000..c8eed83
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/q_function/mlp_q_function.py
@@ -0,0 +1,22 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List
+import tensorflow as tf
+from . import BaseQFunction
+import lunzi.nn as nn
+from slbo.utils.multi_layer_perceptron import MultiLayerPerceptron
+
+
+class MLPQFunction(MultiLayerPerceptron, BaseQFunction):
+    def __init__(self, dim_state: int, dim_action: int, hidden_states: List[int]):
+        super().__init__((dim_state + dim_action, *hidden_states, 1), squeeze=True)
+        self._dim_state = dim_state
+        self._dim_action = dim_action
+
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, [None, dim_state])
+            self.op_actions = tf.placeholder(tf.float32, [None, dim_action])
+
+        self.op_Q = self.forward(self.op_states, self.op_actions)
+
+    @nn.make_method(fetch='Q')
+    def get_q(self, states, actions): pass
diff --git a/experiments05/ant_umaze_1234/src/slbo/random_net.py b/experiments05/ant_umaze_1234/src/slbo/random_net.py
new file mode 100644
index 0000000..c9a94e9
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/random_net.py
@@ -0,0 +1,41 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List
+import tensorflow as tf
+from lunzi import Tensor
+import lunzi.nn as nn
+from slbo.utils.normalizer import Normalizers
+from slbo.utils.multi_layer_perceptron import MultiLayerPerceptron
+
+
+class RandomNet(MultiLayerPerceptron):
+    op_loss: Tensor
+    op_train: Tensor
+    op_grad_norm: Tensor
+
+    def __init__(self, dim_state: int, dim_action: int, normalizers: Normalizers, hidden_sizes: List[int]):
+        initializer = tf.truncated_normal_initializer(mean=0.0, stddev=1e-5)
+
+        self.dim_state = dim_state
+        self.dim_action = dim_action
+        self.hidden_sizes = hidden_sizes
+        self.op_states = tf.placeholder(tf.float32, shape=[None, self.dim_state], name='states')
+        self.op_actions = tf.placeholder(tf.float32, shape=[None, self.dim_action], name='actions')
+        super().__init__([dim_state + dim_action, *hidden_sizes[:-1]],
+                         activation=nn.ReLU,
+                         weight_initializer=initializer, build=False)
+
+        self.normalizers = normalizers
+        self.build()
+
+    def build(self):
+        self.op_features = self.forward(self.op_states, self.op_actions)
+
+    def forward(self, states, actions):
+        assert actions.shape[-1] == self.dim_action
+        inputs = tf.concat([self.normalizers.state(states), actions.clip_by_value(-1., 1.)], axis=1)
+
+        features = super().forward(inputs)
+        return features
+
+    def clone(self):
+        return RandomNet(self.dim_state, self.dim_action, self.normalizers, self.hidden_sizes)
diff --git a/experiments05/ant_umaze_1234/src/slbo/utils/OU_noise.py b/experiments05/ant_umaze_1234/src/slbo/utils/OU_noise.py
new file mode 100644
index 0000000..eae7158
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/utils/OU_noise.py
@@ -0,0 +1,36 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from slbo.policies import BasePolicy
+
+
+class OUNoise(object):
+    _policy: BasePolicy
+
+    def __init__(self, action_space, mu=0.0, theta=0.15, sigma=0.3, shape=None):
+        self.mu = mu
+        self.theta = theta
+        self.sigma = sigma
+        self.action_space = action_space
+        self._state = None
+        if shape:
+            self.shape = shape
+        else:
+            self.shape = action_space.shape
+
+        self.reset()
+
+    def reset(self):
+        self._state = np.ones(self.shape) * self.mu
+
+    def next(self):
+        delta = self.theta * (self.mu - self._state) + self.sigma * np.random.randn(*self._state.shape)
+        self._state = self._state + delta
+        return self._state
+
+    def get_actions(self, states):
+        return self._policy.get_actions(states) + self.next()
+
+    def make(self, policy: BasePolicy):
+        self._policy = policy
+        return self
+
diff --git a/experiments05/ant_umaze_1234/src/slbo/utils/__init__.py b/experiments05/ant_umaze_1234/src/slbo/utils/__init__.py
new file mode 100644
index 0000000..5c7f19c
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/utils/__init__.py
@@ -0,0 +1 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
diff --git a/experiments05/ant_umaze_1234/src/slbo/utils/average_meter.py b/experiments05/ant_umaze_1234/src/slbo/utils/average_meter.py
new file mode 100644
index 0000000..b3e285c
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/utils/average_meter.py
@@ -0,0 +1,20 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+class AverageMeter(object):
+    sum: float
+    count: float
+
+    def __init__(self, discount=1.):
+        self.discount = discount
+        self.reset()
+
+    def update(self, value, count=1):
+        self.sum = self.sum * self.discount + value * count
+        self.count = self.count * self.discount + count
+        return self.get()
+
+    def get(self):
+        return self.sum / (self.count + 1.e-8)
+
+    def reset(self):
+        self.sum = 0.
+        self.count = 0.
diff --git a/experiments05/ant_umaze_1234/src/slbo/utils/dataset.py b/experiments05/ant_umaze_1234/src/slbo/utils/dataset.py
new file mode 100644
index 0000000..c0046d2
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/utils/dataset.py
@@ -0,0 +1,28 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+import lunzi.dataset as dataset
+import gym
+
+
+def gen_dtype(env: gym.Env, fields: str):
+    dtypes = {
+        'state': ('state', 'f8', env.observation_space.shape),
+        'action': ('action', 'f8', env.action_space.shape),
+        'next_state': ('next_state', 'f8', env.observation_space.shape),
+        'reward': ('reward', 'f8'),
+        'done': ('done', 'bool'),
+        'timeout': ('timeout', 'bool'),
+        'return_': ('return_', 'f8'),
+        'advantage': ('advantage', 'f8'),
+    }
+    return [dtypes[field] for field in fields.split(' ')]
+
+
+class Dataset(dataset.Dataset):
+    def sample_multi_step(self, size: int, n_env: int, n_step=1):
+        starts = np.random.randint(0, self._len, size=size)
+        batch = []
+        for step in range(n_step):
+            batch.append(self[(starts + step * n_env) % self._len])
+        return np.concatenate(batch).reshape(n_step, size).view(np.recarray)
+
diff --git a/experiments05/ant_umaze_1234/src/slbo/utils/flags.py b/experiments05/ant_umaze_1234/src/slbo/utils/flags.py
new file mode 100644
index 0000000..8c7bb40
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/utils/flags.py
@@ -0,0 +1,163 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import time
+import os
+import yaml
+from subprocess import check_output, CalledProcessError
+from lunzi.config import BaseFLAGS, expand, parse
+from lunzi.Logger import logger, FileSink
+
+
+class FLAGS(BaseFLAGS):
+    _initialized = False
+
+    use_prev = True
+    seed = 100
+    log_dir: str = None
+    run_id: str = None
+    algorithm = 'OLBO'  # possible options: OLBO, baseline, MF
+
+    class pc(BaseFLAGS):
+        bonus_scale = 1
+        lamb = 0.01
+        bonus_stop_time= 30
+
+    class slbo(BaseFLAGS):
+        n_iters = 50
+        n_policy_iters = 10
+        n_model_iters = 100
+        n_stages = 100
+        n_evaluate_iters = 5
+        opt_model = False
+        start = 'reset'  # possibly 'buffer'
+
+    class plan(BaseFLAGS):
+        max_steps = 1000
+        n_envs = None
+        n_trpo_samples = 4000
+
+        @classmethod
+        def finalize(cls):
+            if cls.n_envs is None:
+                cls.n_envs = cls.n_trpo_samples // cls.max_steps
+            assert cls.n_envs * cls.max_steps == cls.n_trpo_samples
+
+    class env(BaseFLAGS):
+        id = 'HalfCheetah-v2'
+
+    class rollout(BaseFLAGS):
+        normalizer = 'policy'
+        max_buf_size = 100000
+        n_train_samples = 4000
+        n_dev_samples = 0
+        n_test_samples = 10000
+
+        @classmethod
+        def finalize(cls):
+            cls.n_dev_samples = cls.n_dev_samples or cls.n_train_samples
+
+    class ckpt(BaseFLAGS):
+        n_save_stages = 10
+        model_load = None
+        policy_load = None
+        buf_load = None
+        buf_load_index = 0
+        base = '/tmp/mbrl/logs'
+        warm_up = None
+
+        @classmethod
+        def finalize(cls):
+            for key, value in cls.as_dict().items():
+                if isinstance(value, str):
+                    setattr(cls, key, expand(value))
+
+    class OUNoise(BaseFLAGS):
+        theta = 0.15
+        sigma = 0.3
+
+    class model(BaseFLAGS):
+        hidden_sizes = [500, 500]
+        loss = 'L2'  # possibly L1, L2, MSE, G
+        G_coef = 0.5
+        multi_step = 1
+        lr = 1e-3
+        weight_decay = 1e-5
+        validation_freq = 1
+        optimizer = 'Adam'
+        train_batch_size = 256
+        dev_batch_size = 1024
+
+    class policy(BaseFLAGS):
+        hidden_sizes = [32, 32]
+        init_std = 1.
+
+    class PPO(BaseFLAGS):
+        n_minibatches = 32
+        n_opt_epochs = 10
+        ent_coef = 0.005
+        lr = 3e-4
+        clip_range = 0.2
+
+    class TRPO(BaseFLAGS):
+        cg_damping = 0.1
+        n_cg_iters = 10
+        max_kl = 0.01
+        vf_lr = 1e-3
+        n_vf_iters = 5
+        ent_coef = 0.0
+
+    class runner(BaseFLAGS):
+        lambda_ = 0.95
+        gamma = 0.99
+        max_steps = 500
+
+    @classmethod
+    def set_seed(cls):
+        if cls.seed == 0:  # auto seed
+            cls.seed = int.from_bytes(os.urandom(3), 'little') + 1  # never use seed 0 for RNG, 0 is for `urandom`
+        logger.warning("Setting random seed to %s", cls.seed)
+
+        import numpy as np
+        import tensorflow as tf
+        import random
+        np.random.seed(cls.seed)
+        tf.set_random_seed(np.random.randint(2**30))
+        random.seed(np.random.randint(2**30))
+
+    @classmethod
+    def finalize(cls):
+        log_dir = cls.log_dir
+        if log_dir is None:
+            run_id = cls.run_id
+            if run_id is None:
+                run_id = time.strftime('%Y-%m-%d_%H-%M-%S')
+
+            log_dir = os.path.join(cls.ckpt.base, run_id)
+            cls.log_dir = log_dir
+
+        if not os.path.exists(log_dir):
+            os.makedirs(log_dir)
+
+        for t in range(60):
+            try:
+                cls.commit = check_output(['git', 'rev-parse', 'HEAD']).decode('utf-8').strip()
+                check_output(['git', 'add', '.'])
+                check_output(['git', 'checkout-index', '-a', '-f', f'--prefix={log_dir}/src/'])
+                break
+            except CalledProcessError:
+                pass
+            time.sleep(1)
+        else:
+            raise RuntimeError('Failed after 60 trials.')
+
+        yaml.dump(cls.as_dict(), open(os.path.join(log_dir, 'config.yml'), 'w'), default_flow_style=False)
+        open(os.path.join(log_dir, 'diff.patch'), 'w').write(
+            check_output(['git', '--no-pager', 'diff', 'HEAD']).decode('utf-8'))
+
+        logger.add_sink(FileSink(os.path.join(log_dir, 'log.json')))
+        logger.info("log_dir = %s", log_dir)
+
+        cls.set_frozen()
+
+
+parse(FLAGS)
+
diff --git a/experiments05/ant_umaze_1234/src/slbo/utils/multi_layer_perceptron.py b/experiments05/ant_umaze_1234/src/slbo/utils/multi_layer_perceptron.py
new file mode 100644
index 0000000..0fe5dfe
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/utils/multi_layer_perceptron.py
@@ -0,0 +1,51 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+
+
+class MultiLayerPerceptron(nn.Module):
+    def __init__(self, blocks, activation=nn.ReLU, squeeze=False, weight_initializer=None, build=True):
+        super().__init__()
+
+        self._blocks = blocks
+        if build:
+            self.op_inputs = tf.placeholder(tf.float32, [None, self._blocks[0]])
+
+        with self.scope:
+            kwargs = {}
+            if weight_initializer is not None:
+                kwargs['weight_initializer'] = weight_initializer
+            layers = []
+            for in_features, out_features in zip(blocks[:-1], blocks[1:]):
+                if layers:
+                    layers.append(activation())
+                layers.append(nn.Linear(in_features, out_features, **kwargs))
+            if squeeze:
+                layers.append(nn.Squeeze(axis=1))
+            self.net = nn.Sequential(*layers)
+
+        self._squeeze = squeeze
+        self._activation = activation
+
+        if build:
+            self.build()
+
+    def build(self):
+        self.op_outputs = self.forward(self.op_inputs)
+
+    def forward(self, *inputs):
+        if len(inputs) > 1:
+            inputs = tf.concat(inputs, axis=-1)
+        else:
+            inputs = inputs[0]
+        return self.net(inputs)
+
+    def fast(self, *inputs):
+        return self.net.fast(np.concatenate(inputs, axis=-1))
+
+    def clone(self):
+        return MultiLayerPerceptron(self._blocks, self._activation, self._squeeze)
+
+    def extra_repr(self):
+        return f'activation = {self._activation}, blocks = {self._blocks}, squeeze = {self._squeeze}'
diff --git a/experiments05/ant_umaze_1234/src/slbo/utils/normalizer.py b/experiments05/ant_umaze_1234/src/slbo/utils/normalizer.py
new file mode 100644
index 0000000..3bb3685
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/utils/normalizer.py
@@ -0,0 +1,66 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List
+import numpy as np
+import tensorflow as tf
+import lunzi.nn as nn
+from lunzi.Logger import logger
+from lunzi import Tensor
+from slbo.utils.np_utils import gaussian_kl
+
+
+class GaussianNormalizer(nn.Module):
+    def __init__(self, name: str, shape: List[int], eps=1e-8, verbose=False):  # batch_size x ...
+        super().__init__()
+
+        self.name = name
+        self.shape = shape
+        self.eps = eps
+        self._verbose = verbose
+
+        with self.scope:
+            self.op_mean = nn.Parameter(tf.zeros(shape, dtype=tf.float32), name='mean', trainable=False)
+            self.op_std = nn.Parameter(tf.ones(shape, dtype=tf.float32), name='std', trainable=False)
+            self.op_n = nn.Parameter(tf.zeros([], dtype=tf.int64), name='n', trainable=False)
+
+    def extra_repr(self):
+        return f'shape={self.shape}'
+
+    def forward(self, x: Tensor, inverse=False):
+        if inverse:
+            return x * self.op_std + self.op_mean
+        return (x - self.op_mean).div(self.op_std.maximum(self.eps))
+
+    def update(self, samples: np.ndarray):
+        old_mean, old_std, old_n = self.op_mean.numpy(), self.op_std.numpy(), self.op_n.numpy()
+        samples = samples - old_mean
+
+        m = samples.shape[0]
+        delta = samples.mean(axis=0)
+        new_n = old_n + m
+        new_mean = old_mean + delta * m / new_n
+        new_std = np.sqrt((old_std**2 * old_n + samples.var(axis=0) * m + delta**2 * old_n * m / new_n) / new_n)
+
+        kl_old_new = gaussian_kl(new_mean, new_std, old_mean, old_std).sum()
+        self.load_state_dict({'op_mean': new_mean, 'op_std': new_std, 'op_n': new_n})
+
+        if self._verbose:
+            logger.info("updating Normalizer<%s>, KL divergence = %.6f", self.name, kl_old_new)
+
+    def fast(self, samples: np.ndarray, inverse=False) -> np.ndarray:
+        mean, std = self.op_mean.numpy(), self.op_std.numpy()
+        if inverse:
+            return samples * std + mean
+        return (samples - mean) / np.maximum(std, self.eps)
+
+
+class Normalizers(nn.Module):
+    def __init__(self, dim_action: int, dim_state: int):
+        super().__init__()
+        self.action = GaussianNormalizer('action', [dim_action])
+        self.state = GaussianNormalizer('state', [dim_state])
+        self.diff = GaussianNormalizer('diff', [dim_state])
+
+    def forward(self):
+        pass
+
+
diff --git a/experiments05/ant_umaze_1234/src/slbo/utils/np_utils.py b/experiments05/ant_umaze_1234/src/slbo/utils/np_utils.py
new file mode 100644
index 0000000..d170ed4
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/utils/np_utils.py
@@ -0,0 +1,9 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+
+
+def gaussian_kl(mean_1: np.ndarray, std_1: np.ndarray, mean_2: np.ndarray, std_2: np.ndarray) -> np.ndarray:
+    eps = 1e-20
+    std_1 = np.maximum(std_1, eps)
+    std_2 = np.maximum(std_2, eps)
+    return np.log(std_2 / std_1) + (std_1**2 + (mean_1 - mean_2)**2) / std_2**2 / 2. - 0.5
diff --git a/experiments05/ant_umaze_1234/src/slbo/utils/pc_utils.py b/experiments05/ant_umaze_1234/src/slbo/utils/pc_utils.py
new file mode 100644
index 0000000..d488ef2
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/utils/pc_utils.py
@@ -0,0 +1,58 @@
+import numpy as np
+import copy
+import os
+import scipy.spatial
+import scipy.signal
+
+
+def median_trick(X, args):
+    #median trick for computing the bandwith for kernel regression.
+    N = X.shape[0]
+    #print(X.shape)
+    perm = np.random.choice(N, np.min([N,args.update_size * args.buffer_width]), replace=False)
+    dsample = X[perm]
+    pd = scipy.spatial.distance.pdist(dsample)
+    sigma = np.median(pd)
+    return sigma
+
+def compute_cov_pi(phi):
+    #cov = np.zeros((phi.shape[1],phi.shape[1]))
+
+    #for i in range(len(phi)):
+    #    cov += np.outer(phi[i],phi[i])
+    cov = np.dot(phi.T,phi)
+    cov /= phi.shape[0]
+
+    #print(cov)
+
+    return cov
+
+def discount_cumsum(x, discount):
+    """
+    magic from rllab for computing discounted cumulative sums of vectors.
+    input:
+        vector x,
+        [x0,
+         x1,
+         x2]
+    output:
+        [x0 + discount * x1 + discount^2 * x2,
+         x1 + discount * x2,
+         x2]
+    """
+    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]
+
+
+
+
+def soft_update_from_to(source, target, tau):
+    for target_param, param in zip(target.parameters(), source.parameters()):
+        target_param.data.copy_(
+            target_param.data * (1.0 - tau) + param.data * tau
+        )
+
+def copy_model_params_from_to(source, target):
+    for target_param, param in zip(target.parameters(), source.parameters()):
+        target_param.data.copy_(param.data)
+
+
diff --git a/experiments05/ant_umaze_1234/src/slbo/utils/runner.py b/experiments05/ant_umaze_1234/src/slbo/utils/runner.py
new file mode 100644
index 0000000..774665d
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/utils/runner.py
@@ -0,0 +1,98 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from lunzi.dataset import Dataset
+from slbo.envs import BaseBatchedEnv
+from slbo.policies import BasePolicy
+from slbo.utils.dataset import Dataset, gen_dtype
+from slbo.v_function import BaseVFunction
+
+
+class Runner(object):
+    _states: np.ndarray  # [np.float]
+    _n_steps: np.ndarray
+    _returns: np.ndarray
+
+    def __init__(self, env: BaseBatchedEnv, max_steps: int, gamma=0.99, lambda_=0.95, rescale_action=False):
+        self.env = env
+        self.n_envs = env.n_envs
+        self.gamma = gamma
+        self.lambda_ = lambda_
+        self.max_steps = max_steps
+        self.rescale_action = rescale_action
+        self._dtype = gen_dtype(env, 'state action next_state reward done timeout')
+
+        self.reset()
+
+    def reset(self):
+        self.set_state(self.env.reset(), set_env_state=False)
+
+    def set_state(self, states: np.ndarray, set_env_state=True):
+        self._states = states.copy()
+        if set_env_state:
+            self.env.set_state(states)
+        self._n_steps = np.zeros(self.n_envs, 'i4')
+        self._returns = np.zeros(self.n_envs, 'f8')
+
+    def get_state(self):
+        return self._states.copy()
+
+    def run(self, policy: BasePolicy, n_samples: int, render=False):
+        ep_infos = []
+        n_steps = n_samples // self.n_envs
+        assert n_steps * self.n_envs == n_samples
+        dataset = Dataset(self._dtype, n_samples)
+
+        for T in range(n_steps):
+            unscaled_actions = policy.get_actions(self._states)
+            if self.rescale_action:
+                lo, hi = self.env.action_space.low, self.env.action_space.high
+                actions = (lo + (unscaled_actions + 1.) * 0.5 * (hi - lo))
+            else:
+                actions = unscaled_actions
+
+            next_states, rewards, dones, infos = self.env.step(actions)
+            if render:
+                #print(actions)
+                self.env.render()
+            dones = dones.astype(bool)
+            self._returns += rewards
+            self._n_steps += 1
+            timeouts = self._n_steps == self.max_steps
+
+            steps = [self._states.copy(), unscaled_actions, next_states.copy(), rewards, dones, timeouts]
+            dataset.extend(np.rec.fromarrays(steps, dtype=self._dtype))
+
+            indices = np.where(dones | timeouts)[0]
+            if len(indices) > 0:
+                next_states = next_states.copy()
+                next_states[indices] = self.env.partial_reset(indices)
+                for index in indices:
+                    infos[index]['episode'] = {'return': self._returns[index], 'success': (self._returns[index] > 50)}
+                self._n_steps[indices] = 0
+                self._returns[indices] = 0.
+
+            self._states = next_states.copy()
+            ep_infos.extend([info['episode'] for info in infos if 'episode' in info])
+
+        if render:
+            self.env.close()
+
+        return dataset, ep_infos
+
+    def compute_advantage(self, vfn: BaseVFunction, samples: Dataset):
+        n_steps = len(samples) // self.n_envs
+        samples = samples.reshape((n_steps, self.n_envs))
+        use_next_vf = ~samples.done
+        use_next_adv = ~(samples.done | samples.timeout)
+
+        next_values = vfn.get_values(samples[-1].next_state)
+        values = vfn.get_values(samples.reshape(-1).state).reshape(n_steps, self.n_envs)
+        advantages = np.zeros((n_steps, self.n_envs), dtype=np.float32)
+        last_gae_lambda = 0
+
+        for t in reversed(range(n_steps)):
+            delta = samples[t].reward + self.gamma * next_values * use_next_vf[t] - values[t]
+            advantages[t] = last_gae_lambda = delta + self.gamma * self.lambda_ * last_gae_lambda * use_next_adv[t]
+            next_values = values[t]
+        return advantages.reshape(-1), values.reshape(-1)
+
diff --git a/experiments05/ant_umaze_1234/src/slbo/utils/tf_utils.py b/experiments05/ant_umaze_1234/src/slbo/utils/tf_utils.py
new file mode 100644
index 0000000..154c60a
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/utils/tf_utils.py
@@ -0,0 +1,20 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+
+
+def get_tf_config():
+    gpu_frac = 1
+
+    gpu_options = tf.GPUOptions(
+        per_process_gpu_memory_fraction=gpu_frac,
+        allow_growth=True,
+    )
+    config = tf.ConfigProto(
+        gpu_options=gpu_options,
+        log_device_placement=False,
+        allow_soft_placement=True,
+        inter_op_parallelism_threads=1,
+        intra_op_parallelism_threads=1,
+    )
+
+    return config
diff --git a/experiments05/ant_umaze_1234/src/slbo/utils/truncated_normal.py b/experiments05/ant_umaze_1234/src/slbo/utils/truncated_normal.py
new file mode 100644
index 0000000..04a0edc
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/utils/truncated_normal.py
@@ -0,0 +1,12 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+import numpy as np
+
+
+class LimitedEntNormal(tf.distributions.Normal):
+    def _entropy(self):
+        limit = 2.
+        lo, hi = (-limit - self._loc) / self._scale / np.sqrt(2), (limit - self._loc) / self._scale / np.sqrt(2)
+        return 0.5 * (self._scale.log() + np.log(2 * np.pi) / 2) * (hi.erf() - lo.erf()) + 0.5 * \
+            (tf.exp(-hi * hi) * hi - tf.exp(-lo * lo) * lo)
+
diff --git a/experiments05/ant_umaze_1234/src/slbo/v_function/__init__.py b/experiments05/ant_umaze_1234/src/slbo/v_function/__init__.py
new file mode 100644
index 0000000..7794a84
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/v_function/__init__.py
@@ -0,0 +1,13 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import abc
+from typing import Union
+import lunzi.nn as nn
+
+
+class BaseVFunction(abc.ABC):
+    @abc.abstractmethod
+    def get_values(self, states):
+        pass
+
+
+BaseNNVFunction = Union[BaseVFunction, nn.Module]  # in fact it should be Intersection
\ No newline at end of file
diff --git a/experiments05/ant_umaze_1234/src/slbo/v_function/mlp_v_function.py b/experiments05/ant_umaze_1234/src/slbo/v_function/mlp_v_function.py
new file mode 100644
index 0000000..d9749e9
--- /dev/null
+++ b/experiments05/ant_umaze_1234/src/slbo/v_function/mlp_v_function.py
@@ -0,0 +1,24 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+from baselines.common.tf_util import normc_initializer
+from slbo.utils.multi_layer_perceptron import MultiLayerPerceptron
+import lunzi.nn as nn
+from . import BaseVFunction
+
+
+class MLPVFunction(BaseVFunction, nn.Module):
+    def __init__(self, dim_state, hidden_sizes, normalizer=None):
+        super().__init__()
+        self.mlp = MultiLayerPerceptron((dim_state, *hidden_sizes, 1), activation=nn.Tanh, squeeze=True,
+                                        weight_initializer=normc_initializer(1.), build=False)
+        self.normalizer = normalizer
+        self.op_states = tf.placeholder(tf.float32, shape=[None, dim_state])
+        self.op_values = self.forward(self.op_states)
+
+    def forward(self, states):
+        states = self.normalizer(states)
+        return self.mlp(states)
+
+    @nn.make_method(fetch='values')
+    def get_values(self, states): pass
+
diff --git a/experiments05/ant_umaze_1234/stage-0.npy b/experiments05/ant_umaze_1234/stage-0.npy
new file mode 100644
index 0000000..3081364
Binary files /dev/null and b/experiments05/ant_umaze_1234/stage-0.npy differ
diff --git a/experiments05/ant_umaze_1234/stage-10.npy b/experiments05/ant_umaze_1234/stage-10.npy
new file mode 100644
index 0000000..28513ea
Binary files /dev/null and b/experiments05/ant_umaze_1234/stage-10.npy differ
diff --git a/experiments05/ant_umaze_1234/stage-20.npy b/experiments05/ant_umaze_1234/stage-20.npy
new file mode 100644
index 0000000..ac35ca6
Binary files /dev/null and b/experiments05/ant_umaze_1234/stage-20.npy differ
diff --git a/experiments05/ant_umaze_1234/stage-30.npy b/experiments05/ant_umaze_1234/stage-30.npy
new file mode 100644
index 0000000..f5e63a5
Binary files /dev/null and b/experiments05/ant_umaze_1234/stage-30.npy differ
diff --git a/experiments05/ant_umaze_1234/stage-40.npy b/experiments05/ant_umaze_1234/stage-40.npy
new file mode 100644
index 0000000..e06d831
Binary files /dev/null and b/experiments05/ant_umaze_1234/stage-40.npy differ
diff --git a/experiments05/ant_umaze_1234/timesteps.npy b/experiments05/ant_umaze_1234/timesteps.npy
new file mode 100644
index 0000000..f387f40
Binary files /dev/null and b/experiments05/ant_umaze_1234/timesteps.npy differ
diff --git a/lunzi/__pycache__/Logger.cpython-36.pyc b/lunzi/__pycache__/Logger.cpython-36.pyc
index c62e4dc..cd65096 100644
Binary files a/lunzi/__pycache__/Logger.cpython-36.pyc and b/lunzi/__pycache__/Logger.cpython-36.pyc differ
diff --git a/lunzi/__pycache__/__init__.cpython-36.pyc b/lunzi/__pycache__/__init__.cpython-36.pyc
index d0052a1..bb94071 100644
Binary files a/lunzi/__pycache__/__init__.cpython-36.pyc and b/lunzi/__pycache__/__init__.cpython-36.pyc differ
diff --git a/lunzi/__pycache__/config.cpython-36.pyc b/lunzi/__pycache__/config.cpython-36.pyc
index 38e50cc..151d0cd 100644
Binary files a/lunzi/__pycache__/config.cpython-36.pyc and b/lunzi/__pycache__/config.cpython-36.pyc differ
diff --git a/lunzi/__pycache__/dataset.cpython-36.pyc b/lunzi/__pycache__/dataset.cpython-36.pyc
index f5f173f..673cfda 100644
Binary files a/lunzi/__pycache__/dataset.cpython-36.pyc and b/lunzi/__pycache__/dataset.cpython-36.pyc differ
diff --git a/lunzi/__pycache__/stubs.cpython-36.pyc b/lunzi/__pycache__/stubs.cpython-36.pyc
index 40d1d88..ac6d28a 100644
Binary files a/lunzi/__pycache__/stubs.cpython-36.pyc and b/lunzi/__pycache__/stubs.cpython-36.pyc differ
diff --git a/lunzi/nn/__pycache__/__init__.cpython-36.pyc b/lunzi/nn/__pycache__/__init__.cpython-36.pyc
index 2082353..d28b7c9 100644
Binary files a/lunzi/nn/__pycache__/__init__.cpython-36.pyc and b/lunzi/nn/__pycache__/__init__.cpython-36.pyc differ
diff --git a/lunzi/nn/__pycache__/container.cpython-36.pyc b/lunzi/nn/__pycache__/container.cpython-36.pyc
index fd7039b..f16264f 100644
Binary files a/lunzi/nn/__pycache__/container.cpython-36.pyc and b/lunzi/nn/__pycache__/container.cpython-36.pyc differ
diff --git a/lunzi/nn/__pycache__/flat_param.cpython-36.pyc b/lunzi/nn/__pycache__/flat_param.cpython-36.pyc
index dc8e6f9..792725a 100644
Binary files a/lunzi/nn/__pycache__/flat_param.cpython-36.pyc and b/lunzi/nn/__pycache__/flat_param.cpython-36.pyc differ
diff --git a/lunzi/nn/__pycache__/layers.cpython-36.pyc b/lunzi/nn/__pycache__/layers.cpython-36.pyc
index 3f22fba..9fd6751 100644
Binary files a/lunzi/nn/__pycache__/layers.cpython-36.pyc and b/lunzi/nn/__pycache__/layers.cpython-36.pyc differ
diff --git a/lunzi/nn/__pycache__/loss.cpython-36.pyc b/lunzi/nn/__pycache__/loss.cpython-36.pyc
index 9fa218d..10a1d20 100644
Binary files a/lunzi/nn/__pycache__/loss.cpython-36.pyc and b/lunzi/nn/__pycache__/loss.cpython-36.pyc differ
diff --git a/lunzi/nn/__pycache__/module.cpython-36.pyc b/lunzi/nn/__pycache__/module.cpython-36.pyc
index c0f0f4a..9088e26 100644
Binary files a/lunzi/nn/__pycache__/module.cpython-36.pyc and b/lunzi/nn/__pycache__/module.cpython-36.pyc differ
diff --git a/lunzi/nn/__pycache__/parameter.cpython-36.pyc b/lunzi/nn/__pycache__/parameter.cpython-36.pyc
index 5191416..2131727 100644
Binary files a/lunzi/nn/__pycache__/parameter.cpython-36.pyc and b/lunzi/nn/__pycache__/parameter.cpython-36.pyc differ
diff --git a/lunzi/nn/__pycache__/patch.cpython-36.pyc b/lunzi/nn/__pycache__/patch.cpython-36.pyc
index d535500..78a52be 100644
Binary files a/lunzi/nn/__pycache__/patch.cpython-36.pyc and b/lunzi/nn/__pycache__/patch.cpython-36.pyc differ
diff --git a/lunzi/nn/__pycache__/utils.cpython-36.pyc b/lunzi/nn/__pycache__/utils.cpython-36.pyc
index 1c808e8..f1860e7 100644
Binary files a/lunzi/nn/__pycache__/utils.cpython-36.pyc and b/lunzi/nn/__pycache__/utils.cpython-36.pyc differ
diff --git a/run2.sh b/run2.sh
new file mode 100644
index 0000000..9bfb0b2
--- /dev/null
+++ b/run2.sh
@@ -0,0 +1,10 @@
+#!/usr/bin/env bash
+
+for env_name in $1; do
+    echo "=> Running environment ${env_name}"
+    #for random_seed in 1234 2314 2345 1235; do
+    for random_seed in 1234; do
+        python main.py -c configs/algos/slbo_bm_200k.yml configs/env_tingwu/${env_name}.yml \
+	    -s pc.bonus_scale=0.3 log_dir=./experiments02/${env_name}_${random_seed} seed=${random_seed}
+    done
+done
diff --git a/run75.sh b/run75.sh
new file mode 100644
index 0000000..9f40b3d
--- /dev/null
+++ b/run75.sh
@@ -0,0 +1,10 @@
+#!/usr/bin/env bash
+
+for env_name in $1; do
+    echo "=> Running environment ${env_name}"
+    #for random_seed in 1234 2314 2345 1235; do
+    for random_seed in 1234; do
+        python main.py -c configs/algos/slbo_bm_200k.yml configs/env_tingwu/${env_name}.yml \
+	    -s pc.bonus_scale=0.75 log_dir=./experiments75/${env_name}_${random_seed} seed=${random_seed}
+    done
+done
diff --git a/run_experiments.sh b/run_experiments.sh
index 37ce016..e3f2ad7 100644
--- a/run_experiments.sh
+++ b/run_experiments.sh
@@ -3,8 +3,8 @@
 for env_name in $1; do
     echo "=> Running environment ${env_name}"
     #for random_seed in 1234 2314 2345 1235; do
-    for random_seed in 19; do
+    for random_seed in 1234; do
         python main.py -c configs/algos/slbo_bm_200k.yml configs/env_tingwu/${env_name}.yml \
-	    -s log_dir=./experiments/${env_name}_${random_seed} seed=${random_seed}
+	    -s pc.bonus_scale=0.5 log_dir=./experiments05/${env_name}_${random_seed} seed=${random_seed}
     done
 done
diff --git a/slbo/__pycache__/__init__.cpython-36.pyc b/slbo/__pycache__/__init__.cpython-36.pyc
index 069fa36..6a33858 100644
Binary files a/slbo/__pycache__/__init__.cpython-36.pyc and b/slbo/__pycache__/__init__.cpython-36.pyc differ
diff --git a/slbo/__pycache__/dynamics_model.cpython-36.pyc b/slbo/__pycache__/dynamics_model.cpython-36.pyc
index d9193cb..91423ce 100644
Binary files a/slbo/__pycache__/dynamics_model.cpython-36.pyc and b/slbo/__pycache__/dynamics_model.cpython-36.pyc differ
diff --git a/slbo/__pycache__/partial_envs.cpython-36.pyc b/slbo/__pycache__/partial_envs.cpython-36.pyc
index f20520d..7ad21fc 100644
Binary files a/slbo/__pycache__/partial_envs.cpython-36.pyc and b/slbo/__pycache__/partial_envs.cpython-36.pyc differ
diff --git a/slbo/__pycache__/random_net.cpython-36.pyc b/slbo/__pycache__/random_net.cpython-36.pyc
index 7688576..6e866b0 100644
Binary files a/slbo/__pycache__/random_net.cpython-36.pyc and b/slbo/__pycache__/random_net.cpython-36.pyc differ
diff --git a/slbo/algos/__pycache__/TRPO.cpython-36.pyc b/slbo/algos/__pycache__/TRPO.cpython-36.pyc
index 99cdd6a..9ba8329 100644
Binary files a/slbo/algos/__pycache__/TRPO.cpython-36.pyc and b/slbo/algos/__pycache__/TRPO.cpython-36.pyc differ
diff --git a/slbo/algos/__pycache__/__init__.cpython-36.pyc b/slbo/algos/__pycache__/__init__.cpython-36.pyc
index 09f2f9c..9c74708 100644
Binary files a/slbo/algos/__pycache__/__init__.cpython-36.pyc and b/slbo/algos/__pycache__/__init__.cpython-36.pyc differ
diff --git a/slbo/loss/__pycache__/__init__.cpython-36.pyc b/slbo/loss/__pycache__/__init__.cpython-36.pyc
index f1c5979..cc5457b 100644
Binary files a/slbo/loss/__pycache__/__init__.cpython-36.pyc and b/slbo/loss/__pycache__/__init__.cpython-36.pyc differ
diff --git a/slbo/loss/__pycache__/multi_step_loss.cpython-36.pyc b/slbo/loss/__pycache__/multi_step_loss.cpython-36.pyc
index 53c9a3d..695e469 100644
Binary files a/slbo/loss/__pycache__/multi_step_loss.cpython-36.pyc and b/slbo/loss/__pycache__/multi_step_loss.cpython-36.pyc differ
diff --git a/slbo/policies/__pycache__/__init__.cpython-36.pyc b/slbo/policies/__pycache__/__init__.cpython-36.pyc
index a98e999..0f2ff6d 100644
Binary files a/slbo/policies/__pycache__/__init__.cpython-36.pyc and b/slbo/policies/__pycache__/__init__.cpython-36.pyc differ
diff --git a/slbo/policies/__pycache__/gaussian_mlp_policy.cpython-36.pyc b/slbo/policies/__pycache__/gaussian_mlp_policy.cpython-36.pyc
index b355a90..3f0a156 100644
Binary files a/slbo/policies/__pycache__/gaussian_mlp_policy.cpython-36.pyc and b/slbo/policies/__pycache__/gaussian_mlp_policy.cpython-36.pyc differ
diff --git a/slbo/utils/__pycache__/OU_noise.cpython-36.pyc b/slbo/utils/__pycache__/OU_noise.cpython-36.pyc
index 5819e5d..bdfb380 100644
Binary files a/slbo/utils/__pycache__/OU_noise.cpython-36.pyc and b/slbo/utils/__pycache__/OU_noise.cpython-36.pyc differ
diff --git a/slbo/utils/__pycache__/__init__.cpython-36.pyc b/slbo/utils/__pycache__/__init__.cpython-36.pyc
index 0e85b1e..9df4fd9 100644
Binary files a/slbo/utils/__pycache__/__init__.cpython-36.pyc and b/slbo/utils/__pycache__/__init__.cpython-36.pyc differ
diff --git a/slbo/utils/__pycache__/average_meter.cpython-36.pyc b/slbo/utils/__pycache__/average_meter.cpython-36.pyc
index 1bc0297..7fa8339 100644
Binary files a/slbo/utils/__pycache__/average_meter.cpython-36.pyc and b/slbo/utils/__pycache__/average_meter.cpython-36.pyc differ
diff --git a/slbo/utils/__pycache__/dataset.cpython-36.pyc b/slbo/utils/__pycache__/dataset.cpython-36.pyc
index 0cc5d73..46a212c 100644
Binary files a/slbo/utils/__pycache__/dataset.cpython-36.pyc and b/slbo/utils/__pycache__/dataset.cpython-36.pyc differ
diff --git a/slbo/utils/__pycache__/flags.cpython-36.pyc b/slbo/utils/__pycache__/flags.cpython-36.pyc
index dfd7feb..193bff4 100644
Binary files a/slbo/utils/__pycache__/flags.cpython-36.pyc and b/slbo/utils/__pycache__/flags.cpython-36.pyc differ
diff --git a/slbo/utils/__pycache__/multi_layer_perceptron.cpython-36.pyc b/slbo/utils/__pycache__/multi_layer_perceptron.cpython-36.pyc
index ae136c1..eceaef9 100644
Binary files a/slbo/utils/__pycache__/multi_layer_perceptron.cpython-36.pyc and b/slbo/utils/__pycache__/multi_layer_perceptron.cpython-36.pyc differ
diff --git a/slbo/utils/__pycache__/normalizer.cpython-36.pyc b/slbo/utils/__pycache__/normalizer.cpython-36.pyc
index 8c67648..574fc2b 100644
Binary files a/slbo/utils/__pycache__/normalizer.cpython-36.pyc and b/slbo/utils/__pycache__/normalizer.cpython-36.pyc differ
diff --git a/slbo/utils/__pycache__/np_utils.cpython-36.pyc b/slbo/utils/__pycache__/np_utils.cpython-36.pyc
index e1f3150..d9d1d69 100644
Binary files a/slbo/utils/__pycache__/np_utils.cpython-36.pyc and b/slbo/utils/__pycache__/np_utils.cpython-36.pyc differ
diff --git a/slbo/utils/__pycache__/pc_utils.cpython-36.pyc b/slbo/utils/__pycache__/pc_utils.cpython-36.pyc
index 0e894f3..b76bb46 100644
Binary files a/slbo/utils/__pycache__/pc_utils.cpython-36.pyc and b/slbo/utils/__pycache__/pc_utils.cpython-36.pyc differ
diff --git a/slbo/utils/__pycache__/runner.cpython-36.pyc b/slbo/utils/__pycache__/runner.cpython-36.pyc
index 2678071..cce1028 100644
Binary files a/slbo/utils/__pycache__/runner.cpython-36.pyc and b/slbo/utils/__pycache__/runner.cpython-36.pyc differ
diff --git a/slbo/utils/__pycache__/tf_utils.cpython-36.pyc b/slbo/utils/__pycache__/tf_utils.cpython-36.pyc
index 47606c4..59378e0 100644
Binary files a/slbo/utils/__pycache__/tf_utils.cpython-36.pyc and b/slbo/utils/__pycache__/tf_utils.cpython-36.pyc differ
diff --git a/slbo/utils/__pycache__/truncated_normal.cpython-36.pyc b/slbo/utils/__pycache__/truncated_normal.cpython-36.pyc
index 6d8d1b4..a615a74 100644
Binary files a/slbo/utils/__pycache__/truncated_normal.cpython-36.pyc and b/slbo/utils/__pycache__/truncated_normal.cpython-36.pyc differ
diff --git a/slbo/v_function/__pycache__/__init__.cpython-36.pyc b/slbo/v_function/__pycache__/__init__.cpython-36.pyc
index a38ef07..5932be7 100644
Binary files a/slbo/v_function/__pycache__/__init__.cpython-36.pyc and b/slbo/v_function/__pycache__/__init__.cpython-36.pyc differ
diff --git a/slbo/v_function/__pycache__/mlp_v_function.cpython-36.pyc b/slbo/v_function/__pycache__/mlp_v_function.cpython-36.pyc
index dba380d..06cd147 100644
Binary files a/slbo/v_function/__pycache__/mlp_v_function.cpython-36.pyc and b/slbo/v_function/__pycache__/mlp_v_function.cpython-36.pyc differ
